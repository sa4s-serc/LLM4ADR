{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Context We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.\n",
      "### Decision Considerations\n",
      "- Ease of use and maintenance\n",
      "- Community support and resources\n",
      "- Performance and scalability\n",
      "- Security and reliability\n",
      "- Cost and licensing\n",
      "- Compatibility with our technology stack\n",
      "### Considered Options\n",
      "- MySQL\n",
      "- PostgreSQL\n",
      "- Oracle\n",
      "- Microsoft SQL Server\n",
      "- MongoDB\n",
      "\n",
      "## Decision\n",
      "\n",
      "After evaluating the above options based on our decision considerations, we have decided to choose MySQL as our DBMS for Project X.\n",
      "\n",
      "MySQL is a popular open-source system with a strong development community and a large pool of resources for problem-solving and knowledge sharing. It is well-known for its excellent performance and scalability capabilities, making it ideal for handling vast amounts of data with high levels of efficiency. The platform is secure, reliable, and has a wide range of features that are essential for our project, including ACID compliance for transactions, flexible data model, and support for various programming languages and frameworks.\n",
      "\n",
      "MySQL is also compatible with the majority of our technology stack, including our web development framework, hosting solutions, and other essential tools. Plus, its cost and licensing terms are competitive compared to other proprietary systems like Oracle and Microsoft SQL Server.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\"\n",
    "Context = \"## Context We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.\\n### Decision Considerations\\n- Ease of use and maintenance\\n- Community support and resources\\n- Performance and scalability\\n- Security and reliability\\n- Cost and licensing\\n- Compatibility with our technology stack\\n### Considered Options\\n- MySQL\\n- PostgreSQL\\n- Oracle\\n- Microsoft SQL Server\\n- MongoDB\\n\"\n",
    "print(Context)\n",
    "Decision = \"## Decision\\n\\nAfter evaluating the above options based on our decision considerations, we have decided to choose MySQL as our DBMS for Project X.\\n\\nMySQL is a popular open-source system with a strong development community and a large pool of resources for problem-solving and knowledge sharing. It is well-known for its excellent performance and scalability capabilities, making it ideal for handling vast amounts of data with high levels of efficiency. The platform is secure, reliable, and has a wide range of features that are essential for our project, including ACID compliance for transactions, flexible data model, and support for various programming languages and frameworks.\\n\\nMySQL is also compatible with the majority of our technology stack, including our web development framework, hosting solutions, and other essential tools. Plus, its cost and licensing terms are competitive compared to other proprietary systems like Oracle and Microsoft SQL Server.\"\n",
    "print(Decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "path = '/home2/rudra.dhar/codes/SE/ADR/experiments/'\n",
    "load_dotenv(path + '.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Decision\n",
      "We have decided to use MySQL as the DBMS for Project X.\n",
      "\n",
      "### Rationale\n",
      "After careful evaluation of the considered options, we believe that MySQL is the best fit for our needs. Here are the reasons for our decision:\n",
      "\n",
      "- **Ease of use and maintenance:** MySQL is known for its user-friendly interface and ease of use. It also has a large community of users and developers, which provides ample support and resources.\n",
      "- **Performance and scalability:** MySQL is a high-performance DBMS that can handle large amounts of data and concurrent transactions. It also offers scalability options to meet the growing needs of our application.\n",
      "- **Security and reliability:** MySQL provides robust security features to protect data from unauthorized access and breaches. It also offers high reliability with features such as replication and failover.\n",
      "- **Cost and licensing:** MySQL is a cost-effective option compared to other commercial DBMSs. It is open source and has a flexible licensing model that suits our budget and usage requirements.\n",
      "- **Compatibility with our technology stack:** MySQL is compatible with our existing technology stack, which includes PHP, Python, and Node.js. This ensures seamless integration and reduces development efforts.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "At the command line, only need to run once to install the package via pip:\n",
    "\n",
    "$ pip install google-generativeai\n",
    "\"\"\"\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "google_key = os.getenv('google_key')\n",
    "\n",
    "genai.configure(api_key=google_key)\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
    "                              generation_config=generation_config)\n",
    "\n",
    "convo = model.start_chat(history=[])\n",
    "\n",
    "convo.send_message(system_message+ \"\\n\\n\" +Context)\n",
    "print(convo.last.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Decision\n",
      "\n",
      "After evaluating the options against our decision considerations, we have decided to use MySQL as the database management system for Project X. The rationale for this decision is as follows:\n",
      "\n",
      "1. Ease of use and maintenance:\n",
      "   - MySQL offers a user-friendly interface and comprehensive documentation, making it easier for developers to work with and maintain the database.\n",
      "   - It has a large and active community, providing extensive resources, tutorials, and forums for support.\n",
      "\n",
      "2. Community support and resources:\n",
      "   - MySQL has a vast and vibrant community, ensuring access to a wide range of resources, including documentation, tutorials, and forums.\n",
      "   - The community actively contributes to the development and improvement of MySQL, providing regular updates and bug fixes.\n",
      "\n",
      "3. Performance and scalability:\n",
      "   - MySQL delivers high performance for read-heavy workloads, which aligns well with our project requirements.\n",
      "   - It offers various scalability options, such as replication and partitioning, allowing us to handle increasing data volumes and user loads.\n",
      "\n",
      "4. Security and reliability:\n",
      "   - MySQL provides robust security features, including user authentication, access control, and data encryption.\n",
      "   - It has a proven track record of reliability and is widely used in enterprise-level applications.\n",
      "\n",
      "5. Cost and licensing:\n",
      "   - MySQL is available under an open-source license, which means no upfront costs for licensing.\n",
      "   - It offers a cost-effective solution compared to proprietary alternatives like Oracle or Microsoft SQL Server.\n",
      "\n",
      "6. Compatibility with our technology stack:\n",
      "   - MySQL integrates well with our existing technology stack, which includes programming languages like PHP, Python, and Java.\n",
      "   - It has extensive driver support and ORM (Object-Relational Mapping) libraries available for seamless integration.\n",
      "\n",
      "While other options like PostgreSQL and MongoDB were considered, MySQL emerged as the most suitable choice based on our specific requirements and considerations. Its combination of ease of use, community support, performance, security, cost-effectiveness, and compatibility makes it the ideal DBMS for Project X.\n",
      "\n",
      "We acknowledge that there may be trade-offs, such as limited support for complex data types compared to PostgreSQL or the lack of native document storage capabilities offered by MongoDB. However, given our primary focus on handling large amounts of structured data and the need for strong transaction support, MySQL aligns best with our current needs.\n",
      "\n",
      "Moving forward, we will proceed with the implementation of MySQL as the DBMS for Project X. We will ensure proper configuration, optimization, and security measures are in place to maximize its potential and meet our project requirements effectively.\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_key = os.getenv('anthropic_key')\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=anthropic_key,\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": Context\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIIIT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLAB_SE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mADR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLLM4ADR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRD_experiments\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m load_dotenv(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m openai_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_key_karthik\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key \u001b[38;5;241m=\u001b[39m openai_key)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "path = 'D:\\IIIT\\LAB_SE\\ADR\\LLM4ADR\\RD_experiments'\n",
    "load_dotenv(os.path.join(path, '.env'))\n",
    "\n",
    "openai_key = os.getenv('openai_key_karthik')\n",
    "\n",
    "client = OpenAI(api_key = openai_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": Context\n",
    "    }\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Decision\n",
      "After evaluating the options based on the decision considerations, we have decided to go with PostgreSQL as our DBMS for Project X. PostgreSQL is a widely used, open-source relational DBMS with robust community support and resources. It offers high performance, scalability, security, and ease of use, making it suitable for handling large amounts of data from multiple sources. While it may not be as fast as some other options, it excels in ACID compliance and provides strong data security, which are crucial for our project requirements.\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "path = 'D:\\IIIT\\LAB_SE\\ADR\\LLM4ADR\\RD_experiments'\n",
    "load_dotenv(os.path.join(path, '.env'))\n",
    "cohere_key = os.getenv('cohere_key')\n",
    "\n",
    "co = cohere.Client(\n",
    "    api_key=cohere_key, # This is your trial API key\n",
    ") \n",
    "\n",
    "stream = co.chat_stream( \n",
    "    model='command-r-plus',\n",
    "    message=system_message+Context,\n",
    "    temperature=0.3,\n",
    "    chat_history=[],\n",
    "    prompt_truncation='AUTO',\n",
    "    connectors=[{\"id\":\"web-search\"}]\n",
    ") \n",
    "\n",
    "response_text = ''\n",
    "for event in stream:\n",
    "    if event.event_type == \"text-generation\":\n",
    "        response_text += event.text\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "co = cohere.Client(cohere_key) # This is your trial API key\n",
    "\n",
    "response = co.embed(\n",
    "  model='embed-english-v3.0',\n",
    "  texts=[Context],\n",
    "  input_type='classification',\n",
    "  truncate='NONE'\n",
    ")\n",
    "embeddings = response.embeddings\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'The model meta-llama/Meta-Llama-3-8B is too large to be loaded automatically (16GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = 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\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://b62wnh1iatjy0anu.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": \"Bearer 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\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "\t\"parameters\": {}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-api-core in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (2.19.1)\n",
      "Requirement already satisfied: google-cloud-core in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (2.4.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (1.59.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.60.0-py2.py3-none-any.whl (5.1 MB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (2.31.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (2.32.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (1.24.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (2.17.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (2.0.5)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (1.10.12)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-aiplatform) (1.12.4)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (1.62.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core) (1.65.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (4.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-crc32c<2.0dev,>=1.0->google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages (from pydantic<3->google-cloud-aiplatform) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy<3,>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: The script tb-gcp-uploader.exe is installed in 'C:\\Users\\Rudra\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\rudra\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.59.0\n",
      "    Uninstalling google-cloud-aiplatform-1.59.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.59.0\n",
      "Successfully installed google-cloud-aiplatform-1.60.0\n"
     ]
    }
   ],
   "source": [
    "# %pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to authenticate by gcloud CLI. Install:\n",
    "https://cloud.google.com/sdk/docs/install#windows\n",
    "\n",
    "VertexAi configuration: Endpoint, Models:\n",
    "https://console.cloud.google.com/vertex-ai\n",
    "\n",
    "Go to vertexAI -> Model Garden -> Llama3 -> Deploy \\\n",
    "It will take about half an hour to deploy\n",
    "\n",
    "set env variable gcp_endpoint to endpoint id \\\n",
    "Get Project number from project page and set project=\"project_number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DatasetServiceClient' from partially initialized module 'google.cloud.aiplatform_v1beta1.services.dataset_service' (most likely due to a circular import) (C:\\Users\\Rudra\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\dataset_service\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import google.protobuf\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform\\__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m aiplatform_version\n\u001b[0;32m     21\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m aiplatform_version\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     ImageDataset,\n\u001b[0;32m     28\u001b[0m     TabularDataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     VideoDataset,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform\\initializer.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleAuthError\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m constants\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform\\compat\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2023 Google LLC\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m services\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m     21\u001b[0m V1BETA1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1beta1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform\\compat\\services\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2023 Google LLC\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m dataset_service_client_v1beta1,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployment_resource_pool_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m deployment_resource_pool_service_client_v1beta1,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mendpoint_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m endpoint_service_client_v1beta1,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\dataset_service\\__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2024 Google LLC\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceClient\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceAsyncClient\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetServiceClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetServiceAsyncClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\dataset_service\\client.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Dict,\n\u001b[0;32m     21\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     cast,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gapic_version \u001b[38;5;28;01mas\u001b[39;00m package_version\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m client_options \u001b[38;5;28;01mas\u001b[39;00m client_options_lib\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m core_exceptions\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1beta1\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gapic_version \u001b[38;5;28;01mas\u001b[39;00m package_version\n\u001b[0;32m     18\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m package_version\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceClient\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceAsyncClient\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployment_resource_pool_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     DeploymentResourcePoolServiceClient,\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DatasetServiceClient' from partially initialized module 'google.cloud.aiplatform_v1beta1.services.dataset_service' (most likely due to a circular import) (C:\\Users\\Rudra\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\dataset_service\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# import google.protobuf\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "path = 'D:\\IIIT\\LAB_SE\\ADR\\LLM4ADR\\RD_experiments'\n",
    "load_dotenv(os.path.join(path, '.env'))\n",
    "gcp_endpoint = os.getenv('gcp_endpoint')\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "project_number = \"803768827144\"\n",
    "vertexai.init(project=project_number, location=\"us-east4\")\n",
    "\n",
    "# Get the endpoint\n",
    "endpoint = aiplatform.Endpoint(gcp_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "messages = [{\"prompt\": \"What is the capital of France?\"},\n",
    "            {\"prompt\": \"tell me a story?\"}]\n",
    "        \n",
    "# messages = [{\"prompt\": Context}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': ['Prompt:\\n'\n",
      "                  'What is the capital of France?\\n'\n",
      "                  'Output:\\n'\n",
      "                  ' Paris.\\n'\n",
      "                  'Which sport is played on a frozen pond in Canada? Hockey.\\n'\n",
      "                  'What']},\n",
      " {'predictions': ['Prompt:\\n'\n",
      "                  'tell me a story?\\n'\n",
      "                  'Output:\\n'\n",
      "                  ' :D!\\n'\n",
      "                  '\\n'\n",
      "                  'Once upon a time, in a small village nestled in the '\n",
      "                  'rolling']}]\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"maxOutputTokens\": 1,\n",
    "    # \"topK\": 40,\n",
    "    # \"topP\": 0.95\n",
    "  }\n",
    "\n",
    "response = endpoint.predict(instances=messages, parameters=parameters)\n",
    "pprint(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2022.9.14)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.5.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the Architectural Decision Record:\n",
      "\n",
      "**## Decision**\n",
      "\n",
      "We will use **PostgreSQL** as the database management system (DBMS) for Project X.\n",
      "\n",
      "**## Context**\n",
      "We need to decide on which DBMS to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.\n",
      "\n",
      "**## Decision Rationale**\n",
      "\n",
      "After weighing the considerations and evaluating the options, we have decided on PostgreSQL for several reasons:\n",
      "\n",
      "* **Ease of use and maintenance**: PostgreSQL has a reputation for being easy to learn and maintain, with a large community of developers who contribute to its documentation and provide support.\n",
      "* **Community support and resources**: As an open-source database, PostgreSQL has a vast community of users and contributors, ensuring that there will always be someone available to help with any issues or questions we may have.\n",
      "* **Performance and scalability**: PostgreSQL is designed to handle large amounts of data and offers excellent performance and scalability features, making it well-suited for Project X's needs.\n",
      "* **Security and reliability**: PostgreSQL has a strong focus on security and reliability, with features like encryption, backups, and replication ensuring that our data is safe and available even in the event of an outage.\n",
      "* **Cost and licensing**: As an open-source database, PostgreSQL eliminates the need for costly licenses or subscription fees.\n",
      "* **Compatibility with our technology stack**: PostgreSQL has excellent support for various programming languages and frameworks, ensuring seamless integration with our chosen technologies.\n",
      "\n",
      "**## Conclusion**\n",
      "\n",
      "After careful consideration of all options, we have decided to use PostgreSQL as the DBMS for Project X. This decision was based on a thorough evaluation of the considerations and options, and we believe that PostgreSQL will provide us with a reliable, scalable, and secure database solution that meets our needs.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': system_message+Context,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "huggingface_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9528fd827d3b4e98b1b7b35b36557f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", token=huggingface_token, cache_dir='/scratch/llm4adr/cache')\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16, token=huggingface_token, cache_dir='/scratch/llm4adr/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, max_new_tokens=100):\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**encoding, max_new_tokens=max_new_tokens)\n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    generated = [generated_[len(input_):] for input_, generated_ in zip(input_text, generated_texts)]\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['this is a test', 'that is also a test']\n",
    "generated = generate_response(text)\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Context</th>\n",
       "      <th>Decision</th>\n",
       "      <th>tokens</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>libelektra/ingroup_removal.md</td>\n",
       "      <td>## Problem\\nCurrently, an error or warning mes...</td>\n",
       "      <td>The `ingroup` message will be removed as it do...</td>\n",
       "      <td>83</td>\n",
       "      <td>1308</td>\n",
       "      <td>## Problem\\nCurrently, an error or warning mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knot-documentation/ARC-003-amqp-exc-name.md</td>\n",
       "      <td>## Context\\nAs defined by CloudAMQP Blog, \"exc...</td>\n",
       "      <td>* The communication between two components wil...</td>\n",
       "      <td>131</td>\n",
       "      <td>3339</td>\n",
       "      <td>## Context\\nAs defined by CloudAMQP Blog, \"exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decodeweb/0003-store-configuration-into-local-...</td>\n",
       "      <td>## Context\\nTo provide the required functional...</td>\n",
       "      <td>We will store the user's configuration into lo...</td>\n",
       "      <td>52</td>\n",
       "      <td>2221</td>\n",
       "      <td>## Context\\nTo provide the required functional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holochain-rust/0006-splitting-agent-into-front...</td>\n",
       "      <td>## Context\\nFor Holo, we need to have user age...</td>\n",
       "      <td>We decide to **not** emulate a single agency (...</td>\n",
       "      <td>423</td>\n",
       "      <td>1500</td>\n",
       "      <td>## Context\\nFor Holo, we need to have user age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raster-foundry/adr-0006-workflow-manager.md</td>\n",
       "      <td>## Context\\nThis ADR has been superseded by `A...</td>\n",
       "      <td>Raster Foundry will use Airflow as a task mana...</td>\n",
       "      <td>353</td>\n",
       "      <td>1688</td>\n",
       "      <td>## Context\\nThis ADR has been superseded by `A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0                      libelektra/ingroup_removal.md   \n",
       "1        knot-documentation/ARC-003-amqp-exc-name.md   \n",
       "2  decodeweb/0003-store-configuration-into-local-...   \n",
       "3  holochain-rust/0006-splitting-agent-into-front...   \n",
       "4        raster-foundry/adr-0006-workflow-manager.md   \n",
       "\n",
       "                                             Context  \\\n",
       "0  ## Problem\\nCurrently, an error or warning mes...   \n",
       "1  ## Context\\nAs defined by CloudAMQP Blog, \"exc...   \n",
       "2  ## Context\\nTo provide the required functional...   \n",
       "3  ## Context\\nFor Holo, we need to have user age...   \n",
       "4  ## Context\\nThis ADR has been superseded by `A...   \n",
       "\n",
       "                                            Decision  tokens    id  \\\n",
       "0  The `ingroup` message will be removed as it do...      83  1308   \n",
       "1  * The communication between two components wil...     131  3339   \n",
       "2  We will store the user's configuration into lo...      52  2221   \n",
       "3  We decide to **not** emulate a single agency (...     423  1500   \n",
       "4  Raster Foundry will use Airflow as a task mana...     353  1688   \n",
       "\n",
       "                                                text  \n",
       "0  ## Problem\\nCurrently, an error or warning mes...  \n",
       "1  ## Context\\nAs defined by CloudAMQP Blog, \"exc...  \n",
       "2  ## Context\\nTo provide the required functional...  \n",
       "3  ## Context\\nFor Holo, we need to have user age...  \n",
       "4  ## Context\\nThis ADR has been superseded by `A...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_PATH = \"../Data/ADR-data/data_test.jsonl\"\n",
    "test = pd.read_json(TEST_PATH, lines=True)\n",
    "test = test.head()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\n## Solution\\\\n\\n\\nInstead of showing the `Ingroup:` line, we can:\\n\\n1. **Remove it entirely:** This will reduce the verbosity of the error message without losing any information.\\n2. **Replace it with a more informative message:** We can use the `Ingroup:` information to provide a more specific context for the error. For example, instead of just showing `<group>`, we could show something like:\\n\\n```\\nError occurred in kdb module:',\n",
       " '\\n\\n## Proposed Exchange Names\\n\\nHere are some proposed exchange names based on the context you provided:\\n\\n**Scenario 1: BabelTower as Connector Client**\\n\\n* **babeltower_connector_requests:** This name clearly indicates that this exchange handles requests from BabelTower to the Connector service.\\n* **connector_inbound_messages:** This name focuses on the direction of the messages (inbound to the Connector).\\n\\n**Scenario 2: Client communicates directly with Connector**\\n\\n* **connector_client',\n",
       " '\\n\\n## Solution\\n\\nWe can leverage the browser\\'s `localStorage` API to store the configuration data persistently within the user\\'s browser. \\n\\nHere\\'s a breakdown of how to implement this:\\n\\n**1. Storing Configuration Data:**\\n\\n```javascript\\n// Example configuration object\\nconst config = {\\n  theme: \"light\",\\n  language: \"en\",\\n  notifications: true\\n};\\n\\n// Store the configuration in localStorage\\nfor (const key in config) {\\n',\n",
       " \"\\n\\n## Problem\\n\\nThe original proposal to split the Holochain agent into a front-house (user-facing) and back-house (network-facing) components presents a significant challenge:\\n\\n* **Key Management:** The back-house requires the agent's private keys to sign network communications, but these keys are held by the front-house. This creates a dependency where the back-house must constantly communicate with the front-house for every network interaction, even when the user is offline.\",\n",
       " \"\\\\n\\n\\n## Decision\\n\\nWe will use **Airflow** as the workflow management tool for Raster Foundry.\\n\\n## Rationale\\n\\nWhile Celery and SWF are familiar to us, Airflow offers several advantages:\\n\\n* **Mature and Stable:** Airflow has a proven track record and a large community, indicating its reliability and robustness.\\n* **Fits Execution Model:** Airflow's DAG (Directed Acyclic Graph) model aligns well with our needs for defining complex workflows with dependencies.\\n*\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size =2\n",
    "num_batches = len(test) // batch_size + (len(test) % batch_size > 0)\n",
    "\n",
    "generated = []\n",
    "\n",
    "# Iterate over batches\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(test))\n",
    "    batch = test.iloc[start_idx:end_idx]\n",
    "    batch_text = batch['Context'].tolist()\n",
    "    generated.extend(generate_response(batch_text))\n",
    "    print(i)\n",
    "\n",
    "generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetuned model Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af975fbbdecf4f66a104f761325ac757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199f0e384690497a8028f634fea7adb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1082e08b61194fef815a5a1e3c0fb651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b26a51498f445b0903a70ab965d6180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf43e6b057d43ef97bf098ccbe959d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede196bbd97a493498869178f50c269a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de5d3eef49a44d29c869c78e347f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142024f2e5d44b6796ef08ad2c54711a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6883a4d28704d0b93ef10f7d6ada5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b883a7f1a5f84e38a8bed9fc424d33f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9e067f813546df9980fe200fb6d7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c863bca032406f923bcc3aed9001c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d85cd5204554ee58bf33ee32ea8cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4644e9757f374771a7d9514e1de94b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'c_proj', 'c_attn', 'c_fc'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, token\u001b[38;5;241m=\u001b[39mhuggingface_token, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/llm4adr/cache\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, token\u001b[38;5;241m=\u001b[39mhuggingface_token, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/llm4adr/cache\u001b[39m\u001b[38;5;124m'\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/scratch/llm4adr/cache\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:541\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, config, adapter_name, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype)\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(\n\u001b[1;32m    546\u001b[0m     model_id, adapter_name, is_trainable\u001b[38;5;241m=\u001b[39mis_trainable, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:1542\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:155\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:175\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:435\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Handle X-LoRA case.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'c_proj', 'c_attn', 'c_fc'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "base_model = \"google/gemma-2-9b-it\"\n",
    "# adapter_model = \"rudradhar95/autotrain-k4gpm-eo37x\"\n",
    "adapter_model = \"rudradhar/test-100-gemma\"\n",
    "\n",
    "huggingface_token_RD_personal = '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'\n",
    "huggingface_token_RD_research = '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'\n",
    "huggingface_token = huggingface_token_RD_research\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=huggingface_token, cache_dir='/scratch/llm4adr/cache')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, token=huggingface_token, cache_dir='/scratch/llm4adr/cache', device_map=\"auto\", torch_dtype='auto')\n",
    "model = PeftModel.from_pretrained(model, adapter_model, token=huggingface_token, cache_dir='/scratch/llm4adr/cache')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me the recipe for chocolate chip cookie dough balls\n",
      "I think I know the recipe you're talking about! Chocolate chip cookie dough balls are a popular treat that's easy to make and delicious to eat. Here's the recipe:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 1/2 cups all-purpose flour\n",
      "* 1/2 cup unsalted butter, softened\n",
      "* 1/2 cup granulated sugar\n",
      "* 1/4 cup brown sugar\n",
      "* 1/2 teaspoon vanilla extract\n",
      "* 1/2 teaspoon baking soda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(\"Tell me the recipe for chocolate chip cookie\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=100)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
