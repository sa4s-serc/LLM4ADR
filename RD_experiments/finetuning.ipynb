{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 20:27:28.404204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 20:27:28.428093: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 20:27:28.435830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-12 20:27:28.454538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 20:27:30.476822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "# from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb04bf8ba84481498bf101bab2530f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "CACHE_DIR = \"/scratch/LLM4ADR/cache\"\n",
    "huggingface_token = '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'\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=huggingface_token, cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=huggingface_token, cache_dir=CACHE_DIR, device_map=\"auto\", torch_dtype='auto')\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, token=huggingface_token, cache_dir=CACHE_DIR, device_map=\"auto\", attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PATH = \"../Data/ADR-data/data_train.jsonl\"\n",
    "# VAL_PATH = \"../Data/ADR-data/data_val.jsonl\"\n",
    "# TEST_PATH = \"../Data/ADR-data/data_test.jsonl\"\n",
    "\n",
    "# train = pd.read_json(TRAIN_PATH, lines=True)\n",
    "# val = pd.read_json(VAL_PATH, lines=True)\n",
    "# test = pd.read_json(TEST_PATH, lines=True)\n",
    "\n",
    "small_path = \"../Data/ADR-data/data_val_small.jsonl\"\n",
    "small = pd.read_json(small_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5836b6dc74d84c269efe4fffcb69dd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal/modules/cjs/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal/modules/cjs/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = Dataset.from_pandas(train)\n",
    "# val_dataset = Dataset.from_pandas(val)\n",
    "# test_dataset = Dataset.from_pandas(test)\n",
    "small_dataset = Dataset.from_pandas(small)\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Context\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Decision\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# train_dataset = train_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "# val_dataset = val_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "# test_dataset = test_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "small_dataset = small_dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(small_dataset['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_path = \"/scratch/llm4adr/cache/llama-3-8b-ADR\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model_path,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    # warmup_steps=10,\n",
    "    logging_strategy=\"epoch\",\n",
    "    # learning_rate=2e-4,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ADR\",\n",
    "    # eval_on_start=True,\n",
    "    # disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88ab296b1df40d7916f7afe8d47edb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2200088d0dc4428b87d0e389efb47a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=small_dataset,\n",
    "    eval_dataset=small_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrudradhar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments/wandb/run-20240812_195218-6f0vsvlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rudradhar/huggingface/runs/6f0vsvlt' target=\"_blank\">ADR</a></strong> to <a href='https://wandb.ai/rudradhar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rudradhar/huggingface' target=\"_blank\">https://wandb.ai/rudradhar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rudradhar/huggingface/runs/6f0vsvlt' target=\"_blank\">https://wandb.ai/rudradhar/huggingface/runs/6f0vsvlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 18:53, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.039700</td>\n",
       "      <td>2.552960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.403200</td>\n",
       "      <td>2.247710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.177800</td>\n",
       "      <td>2.162761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1b8e-2f1498ce748f288504f084e1;6f2ab56e-6640-438b-92f8-78e060307715)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1c78-4fe89c4538a914064675dca9;e5ac0090-1c28-438d-9dd1-61f9fa832bce)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1d63-3e1f292811bf2cfb5394a258;4c210483-5c7f-4fdd-af0f-bf2ab147bae6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1e4e-2916b2190376dd8f14b4ffe2;d7728b40-0a99-4b1e-92ab-aaced589c35b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1edb-03ced9c66035aa2d79d674a8;d9aeff4d-6694-48b4-85c3-eb97349cbff4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ba1f28-674426ba10f4ae2f781ab979;d18522b3-dc3c-4338-93ef-f3220773886e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=2.422914918263753, metrics={'train_runtime': 1171.918, 'train_samples_per_second': 0.427, 'train_steps_per_second': 0.051, 'total_flos': 6236037165367296.0, 'train_loss': 2.422914918263753, 'epoch': 4.8})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b9f89f-5c411ebf6f5ca8256b98780c;c6049c93-df69-41fc-a9c9-d7fc03242159)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d80258d59134f3393186c1431f3dff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model_path = \"/scratch/llm4adr/cache/llama-3-8b-ADR/checkpoint-60\"\n",
    "adapter_model = new_model_path\n",
    "\n",
    "huggingface_token = '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'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache', device_map=\"auto\", torch_dtype='auto')\n",
    "model = PeftModel.from_pretrained(model, adapter_model, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal/modules/cjs/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal/modules/cjs/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\nI tried to use `npm` but I hit problems with `@hapi/hapi` and `@hapi/plugin-incoming-form` because they were not [compatible with `npm`][npm-issue]. I tried to use `@hapi/hapi` version 18 which worked but I needed to use `@hapi/plugin-incoming-form` version 2 which was not compatible with `@hapi/hapi` version 18.\\nI tried to use `@hapi/hapi` version 17 which was compatible with `@hapi/plugin-incoming-form` version 2 but I hit a problem with `@hapi/plugin-incoming-form` version 2 not being compatible with `@hapi/joi` version 17.\\nI decided to stick with `@hapi/hapi` version 18 and `@hapi/plugin-incoming-form` version 2 and [use `npm`][npm-issue] because it was the most convenient option.\\nI decided to use `npm` and `@hapi/hapi` version 18 and `@hapi/plugin-incoming-form` version 2 because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": small['Context'][0]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompt = small_dataset['Context'][3]\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=500, num_return_sequences=1)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(text.split(\"assistant\")[1])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tried to use `npm` but I hit problems with `@hapi/hapi` and `@hapi/plugin-incoming-form` because they were not [compatible with `npm`][npm-issue]. I tried to use `@hapi/hapi` version 18 which worked but I needed to use `@hapi/plugin-incoming-form` version 2 which was not compatible with `@hapi/hapi` version 18.\\nI tried to use `@hapi/hapi` version 17 which was compatible with `@hapi/plugin-incoming-form` version 2 but I hit a problem with `@hapi/plugin-incoming-form` version 2 not being compatible with `@hapi/joi` version 17.\\nI decided to stick with `@hapi/hapi` version 18 and `@hapi/plugin-incoming-form` version 2 and [use `npm`][npm-issue] because it was the most convenient option.\\nI decided to use `npm` and `@hapi/hapi` version 18 and `@hapi/plugin-incoming-form` version 2 because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming-form` version 2 to work together.\\nI decided to use `npm` because it was the most convenient option and I could not get `@hapi/hapi` version 17 and `@hapi/plugin-incoming\n"
     ]
    }
   ],
   "source": [
    "print(text[len(small_dataset['Context'][3]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset['Decision'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
