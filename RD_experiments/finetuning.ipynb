{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "# from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd3c5c4620c42deae1b34d10809013a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "CACHE_DIR = \"/scratch/LLM4ADR/cache\"\n",
    "huggingface_token = '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'\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=huggingface_token, cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=huggingface_token, cache_dir=CACHE_DIR, device_map=\"auto\", torch_dtype='auto')\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, token=huggingface_token, cache_dir=CACHE_DIR, device_map=\"auto\", attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"llama-3-8b-ADR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PATH = \"../Data/ADR-data/data_train.jsonl\"\n",
    "# VAL_PATH = \"../Data/ADR-data/data_val.jsonl\"\n",
    "# TEST_PATH = \"../Data/ADR-data/data_test.jsonl\"\n",
    "\n",
    "# train = pd.read_json(TRAIN_PATH, lines=True)\n",
    "# val = pd.read_json(VAL_PATH, lines=True)\n",
    "# test = pd.read_json(TEST_PATH, lines=True)\n",
    "\n",
    "small_path = \"../Data/ADR-data/data_val_small.jsonl\"\n",
    "small = pd.read_json(small_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f93ea14874c41c59cbae518ad514ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal/modules/cjs/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal/modules/cjs/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = Dataset.from_pandas(train)\n",
    "# val_dataset = Dataset.from_pandas(val)\n",
    "# test_dataset = Dataset.from_pandas(test)\n",
    "small_dataset = Dataset.from_pandas(small)\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Context\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Decision\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# train_dataset = train_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "# val_dataset = val_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "# test_dataset = test_dataset.map(\n",
    "#     format_chat_template,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "\n",
    "small_dataset = small_dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(small_dataset['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    # fp16=False,\n",
    "    # bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ADR\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656384daec0843ea952f56a3474be32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bde5b13aba488b994dd39767f7a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=small_dataset,\n",
    "    eval_dataset=small_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrudradhar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/rudra.dhar/codes/SE/LLM4ADR/RD_experiments/wandb/run-20240812_163823-hnd9g99d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rudradhar/huggingface/runs/hnd9g99d' target=\"_blank\">ADR</a></strong> to <a href='https://wandb.ai/rudradhar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rudradhar/huggingface' target=\"_blank\">https://wandb.ai/rudradhar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rudradhar/huggingface/runs/hnd9g99d' target=\"_blank\">https://wandb.ai/rudradhar/huggingface/runs/hnd9g99d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 20:38, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.449600</td>\n",
       "      <td>2.327624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.484500</td>\n",
       "      <td>2.008656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.101800</td>\n",
       "      <td>1.693067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.699000</td>\n",
       "      <td>1.403945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.490800</td>\n",
       "      <td>1.266392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b9f21b-0ad7ffd278c02c0a20c9577d;d632e24e-83c8-4230-93dc-d8c99bd9a505)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.9894443601369858, metrics={'train_runtime': 1278.1232, 'train_samples_per_second': 0.391, 'train_steps_per_second': 0.047, 'total_flos': 6236037165367296.0, 'train_loss': 1.9894443601369858, 'epoch': 4.8})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b9f89f-5c411ebf6f5ca8256b98780c;c6049c93-df69-41fc-a9c9-d7fc03242159)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home2/rudra.dhar/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('/scratch/LLM4ADR/llama-3-8b-ADR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "adapter_model = \"/scratch/LLM4ADR/llama-3-8b-ADR\"\n",
    "\n",
    "huggingface_token = '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'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache', device_map=\"auto\", torch_dtype='auto')\n",
    "model = PeftModel.from_pretrained(model, adapter_model, token=huggingface_token, cache_dir='/scratch/LLM4ADR/cache')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "We will use Docker hub to store our built container images.\\nWe will use a single account to store all images from our pipeline, and will\\norganise our images into namespaces and repos to keep things tidy.\\nWe will use environment specific variables to tag our images so that the\\nhosting platform can pick the correct version of an image for a given environment.\\\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": small['Context'][0]\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, \n",
    "                   truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, \n",
    "                         num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'File Name': 'beis-report-official-development-assistance/0023-use-docker-hub-in-deployments.md',\n",
       " 'Context': '## Context\\\\nOur CI/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\\\nPaaS), we need a way to store built images from our pipeline so our hosting\\\\nplatform can access and deploy them.\\\\nDocker hub is one solution offered by the maker of Docker itself.\\\\n',\n",
       " 'Decision': 'Use [Docker hub](https://hub.docker.com/) to store built deployment container\\\\nimages to facilitate continuous delivery.\\\\nHost the built container images on the dxw Docker hub account.\\\\n',\n",
       " 'tokens': 65,\n",
       " 'id': 2392,\n",
       " 'text': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n## Context\\\\nOur CI/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\\\nPaaS), we need a way to store built images from our pipeline so our hosting\\\\nplatform can access and deploy them.\\\\nDocker hub is one solution offered by the maker of Docker itself.\\\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nUse [Docker hub](https://hub.docker.com/) to store built deployment container\\\\nimages to facilitate continuous delivery.\\\\nHost the built container images on the dxw Docker hub account.\\\\n<|eot_id|>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d6dc1d9ca14416aed9a86f88cd2397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▁</td></tr><tr><td>eval/runtime</td><td>▁▁█▃</td></tr><tr><td>eval/samples_per_second</td><td>██▁▅</td></tr><tr><td>eval/steps_per_second</td><td>██▁▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▄▃▆▂▁▃▅▄▁▃▄▂▂▃▅▆▂▂▃▁▁▃▄▆▁▂▄▄▁▃▄█▁▂▃▆▁▂▂</td></tr><tr><td>train/learning_rate</td><td>▄███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▆▇▇▆▇▇▆▇▆▇▆▇▆▇▇▇▆▆▆▅▇▄▆▆▆▇▇▅▇▅▆▆▅▅▇▆▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.83434</td></tr><tr><td>eval/runtime</td><td>595.7203</td></tr><tr><td>eval/samples_per_second</td><td>1.648</td></tr><tr><td>eval/steps_per_second</td><td>1.648</td></tr><tr><td>total_flos</td><td>1.0179532138389504e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>491</td></tr><tr><td>train/grad_norm</td><td>2.09772</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3126</td></tr><tr><td>train_loss</td><td>2.14539</td></tr><tr><td>train_runtime</td><td>3684.6528</td></tr><tr><td>train_samples_per_second</td><td>0.267</td></tr><tr><td>train_steps_per_second</td><td>0.133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ADR</strong> at: <a href='https://wandb.ai/rudradhar/huggingface/runs/1ad004xt' target=\"_blank\">https://wandb.ai/rudradhar/huggingface/runs/1ad004xt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240809_205002-1ad004xt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 2.7156,\n",
       "  'grad_norm': 1.5294764041900635,\n",
       "  'learning_rate': 2e-05,\n",
       "  'epoch': 0.08,\n",
       "  'step': 1},\n",
       " {'loss': 3.2614,\n",
       "  'grad_norm': 2.567246675491333,\n",
       "  'learning_rate': 4e-05,\n",
       "  'epoch': 0.16,\n",
       "  'step': 2},\n",
       " {'loss': 4.4522,\n",
       "  'grad_norm': 6.0462260246276855,\n",
       "  'learning_rate': 6e-05,\n",
       "  'epoch': 0.24,\n",
       "  'step': 3},\n",
       " {'loss': 2.4579,\n",
       "  'grad_norm': 1.317590355873108,\n",
       "  'learning_rate': 8e-05,\n",
       "  'epoch': 0.32,\n",
       "  'step': 4},\n",
       " {'loss': 3.098,\n",
       "  'grad_norm': 3.749480962753296,\n",
       "  'learning_rate': 0.0001,\n",
       "  'epoch': 0.4,\n",
       "  'step': 5},\n",
       " {'loss': 3.8131,\n",
       "  'grad_norm': 4.172453880310059,\n",
       "  'learning_rate': 0.00012,\n",
       "  'epoch': 0.48,\n",
       "  'step': 6},\n",
       " {'loss': 2.5446,\n",
       "  'grad_norm': 0.9494112133979797,\n",
       "  'learning_rate': 0.00014,\n",
       "  'epoch': 0.56,\n",
       "  'step': 7},\n",
       " {'loss': 2.9133,\n",
       "  'grad_norm': 2.0365853309631348,\n",
       "  'learning_rate': 0.00016,\n",
       "  'epoch': 0.64,\n",
       "  'step': 8},\n",
       " {'loss': 2.9078,\n",
       "  'grad_norm': 3.5009210109710693,\n",
       "  'learning_rate': 0.00018,\n",
       "  'epoch': 0.72,\n",
       "  'step': 9},\n",
       " {'loss': 2.2047,\n",
       "  'grad_norm': 1.1660274267196655,\n",
       "  'learning_rate': 0.0002,\n",
       "  'epoch': 0.8,\n",
       "  'step': 10},\n",
       " {'loss': 2.8834,\n",
       "  'grad_norm': 2.1210296154022217,\n",
       "  'learning_rate': 0.000196,\n",
       "  'epoch': 0.88,\n",
       "  'step': 11},\n",
       " {'loss': 2.4496,\n",
       "  'grad_norm': 3.088475227355957,\n",
       "  'learning_rate': 0.000192,\n",
       "  'epoch': 0.96,\n",
       "  'step': 12},\n",
       " {'eval_loss': 2.3276238441467285,\n",
       "  'eval_runtime': 96.5428,\n",
       "  'eval_samples_per_second': 1.036,\n",
       "  'eval_steps_per_second': 0.518,\n",
       "  'epoch': 0.96,\n",
       "  'step': 12},\n",
       " {'loss': 2.0814,\n",
       "  'grad_norm': 0.8437618017196655,\n",
       "  'learning_rate': 0.000188,\n",
       "  'epoch': 1.04,\n",
       "  'step': 13},\n",
       " {'loss': 2.4558,\n",
       "  'grad_norm': 0.8476059436798096,\n",
       "  'learning_rate': 0.00018400000000000003,\n",
       "  'epoch': 1.12,\n",
       "  'step': 14},\n",
       " {'loss': 2.3236,\n",
       "  'grad_norm': 1.3150808811187744,\n",
       "  'learning_rate': 0.00018,\n",
       "  'epoch': 1.2,\n",
       "  'step': 15},\n",
       " {'loss': 1.6815,\n",
       "  'grad_norm': 1.0386756658554077,\n",
       "  'learning_rate': 0.00017600000000000002,\n",
       "  'epoch': 1.28,\n",
       "  'step': 16},\n",
       " {'loss': 2.3372,\n",
       "  'grad_norm': 0.8312689065933228,\n",
       "  'learning_rate': 0.000172,\n",
       "  'epoch': 1.3599999999999999,\n",
       "  'step': 17},\n",
       " {'loss': 2.4276,\n",
       "  'grad_norm': 1.9372203350067139,\n",
       "  'learning_rate': 0.000168,\n",
       "  'epoch': 1.44,\n",
       "  'step': 18},\n",
       " {'loss': 2.0453,\n",
       "  'grad_norm': 1.755394697189331,\n",
       "  'learning_rate': 0.000164,\n",
       "  'epoch': 1.52,\n",
       "  'step': 19},\n",
       " {'loss': 2.5148,\n",
       "  'grad_norm': 1.095856785774231,\n",
       "  'learning_rate': 0.00016,\n",
       "  'epoch': 1.6,\n",
       "  'step': 20},\n",
       " {'loss': 2.2506,\n",
       "  'grad_norm': 1.66090989112854,\n",
       "  'learning_rate': 0.00015600000000000002,\n",
       "  'epoch': 1.6800000000000002,\n",
       "  'step': 21},\n",
       " {'loss': 2.0086,\n",
       "  'grad_norm': 1.4586164951324463,\n",
       "  'learning_rate': 0.000152,\n",
       "  'epoch': 1.76,\n",
       "  'step': 22},\n",
       " {'loss': 2.151,\n",
       "  'grad_norm': 0.9654963612556458,\n",
       "  'learning_rate': 0.000148,\n",
       "  'epoch': 1.8399999999999999,\n",
       "  'step': 23},\n",
       " {'loss': 2.4845,\n",
       "  'grad_norm': 1.1470566987991333,\n",
       "  'learning_rate': 0.000144,\n",
       "  'epoch': 1.92,\n",
       "  'step': 24},\n",
       " {'eval_loss': 2.008655548095703,\n",
       "  'eval_runtime': 96.8773,\n",
       "  'eval_samples_per_second': 1.032,\n",
       "  'eval_steps_per_second': 0.516,\n",
       "  'epoch': 1.92,\n",
       "  'step': 24},\n",
       " {'loss': 2.2233,\n",
       "  'grad_norm': 1.1907100677490234,\n",
       "  'learning_rate': 0.00014,\n",
       "  'epoch': 2.0,\n",
       "  'step': 25},\n",
       " {'loss': 1.9784,\n",
       "  'grad_norm': 0.7144496440887451,\n",
       "  'learning_rate': 0.00013600000000000003,\n",
       "  'epoch': 2.08,\n",
       "  'step': 26},\n",
       " {'loss': 2.0152,\n",
       "  'grad_norm': 1.079576015472412,\n",
       "  'learning_rate': 0.000132,\n",
       "  'epoch': 2.16,\n",
       "  'step': 27},\n",
       " {'loss': 1.5908,\n",
       "  'grad_norm': 1.8306056261062622,\n",
       "  'learning_rate': 0.00012800000000000002,\n",
       "  'epoch': 2.24,\n",
       "  'step': 28},\n",
       " {'loss': 1.8791,\n",
       "  'grad_norm': 0.7325226068496704,\n",
       "  'learning_rate': 0.000124,\n",
       "  'epoch': 2.32,\n",
       "  'step': 29},\n",
       " {'loss': 1.9155,\n",
       "  'grad_norm': 1.029257893562317,\n",
       "  'learning_rate': 0.00012,\n",
       "  'epoch': 2.4,\n",
       "  'step': 30},\n",
       " {'loss': 1.7338,\n",
       "  'grad_norm': 1.6929700374603271,\n",
       "  'learning_rate': 0.000116,\n",
       "  'epoch': 2.48,\n",
       "  'step': 31},\n",
       " {'loss': 1.8441,\n",
       "  'grad_norm': 1.1444588899612427,\n",
       "  'learning_rate': 0.00011200000000000001,\n",
       "  'epoch': 2.56,\n",
       "  'step': 32},\n",
       " {'loss': 2.0269,\n",
       "  'grad_norm': 1.240520715713501,\n",
       "  'learning_rate': 0.00010800000000000001,\n",
       "  'epoch': 2.64,\n",
       "  'step': 33},\n",
       " {'loss': 1.7175,\n",
       "  'grad_norm': 1.6993979215621948,\n",
       "  'learning_rate': 0.00010400000000000001,\n",
       "  'epoch': 2.7199999999999998,\n",
       "  'step': 34},\n",
       " {'loss': 2.0525,\n",
       "  'grad_norm': 0.8734533786773682,\n",
       "  'learning_rate': 0.0001,\n",
       "  'epoch': 2.8,\n",
       "  'step': 35},\n",
       " {'loss': 2.1018,\n",
       "  'grad_norm': 1.2936204671859741,\n",
       "  'learning_rate': 9.6e-05,\n",
       "  'epoch': 2.88,\n",
       "  'step': 36},\n",
       " {'eval_loss': 1.6930670738220215,\n",
       "  'eval_runtime': 96.7811,\n",
       "  'eval_samples_per_second': 1.033,\n",
       "  'eval_steps_per_second': 0.517,\n",
       "  'epoch': 2.88,\n",
       "  'step': 36},\n",
       " {'loss': 1.4609,\n",
       "  'grad_norm': 1.7637814283370972,\n",
       "  'learning_rate': 9.200000000000001e-05,\n",
       "  'epoch': 2.96,\n",
       "  'step': 37},\n",
       " {'loss': 1.6921,\n",
       "  'grad_norm': 0.9178639054298401,\n",
       "  'learning_rate': 8.800000000000001e-05,\n",
       "  'epoch': 3.04,\n",
       "  'step': 38},\n",
       " {'loss': 1.8839,\n",
       "  'grad_norm': 1.190106749534607,\n",
       "  'learning_rate': 8.4e-05,\n",
       "  'epoch': 3.12,\n",
       "  'step': 39},\n",
       " {'loss': 1.4508,\n",
       "  'grad_norm': 1.9689468145370483,\n",
       "  'learning_rate': 8e-05,\n",
       "  'epoch': 3.2,\n",
       "  'step': 40},\n",
       " {'loss': 1.3357,\n",
       "  'grad_norm': 1.5198605060577393,\n",
       "  'learning_rate': 7.6e-05,\n",
       "  'epoch': 3.2800000000000002,\n",
       "  'step': 41},\n",
       " {'loss': 1.5015,\n",
       "  'grad_norm': 1.2288851737976074,\n",
       "  'learning_rate': 7.2e-05,\n",
       "  'epoch': 3.36,\n",
       "  'step': 42},\n",
       " {'loss': 1.4137,\n",
       "  'grad_norm': 2.290989637374878,\n",
       "  'learning_rate': 6.800000000000001e-05,\n",
       "  'epoch': 3.44,\n",
       "  'step': 43},\n",
       " {'loss': 1.1988,\n",
       "  'grad_norm': 2.162031650543213,\n",
       "  'learning_rate': 6.400000000000001e-05,\n",
       "  'epoch': 3.52,\n",
       "  'step': 44},\n",
       " {'loss': 1.6641,\n",
       "  'grad_norm': 1.4653689861297607,\n",
       "  'learning_rate': 6e-05,\n",
       "  'epoch': 3.6,\n",
       "  'step': 45},\n",
       " {'loss': 1.567,\n",
       "  'grad_norm': 2.7404448986053467,\n",
       "  'learning_rate': 5.6000000000000006e-05,\n",
       "  'epoch': 3.68,\n",
       "  'step': 46},\n",
       " {'loss': 1.4143,\n",
       "  'grad_norm': 1.8826656341552734,\n",
       "  'learning_rate': 5.2000000000000004e-05,\n",
       "  'epoch': 3.76,\n",
       "  'step': 47},\n",
       " {'loss': 1.699,\n",
       "  'grad_norm': 1.3864067792892456,\n",
       "  'learning_rate': 4.8e-05,\n",
       "  'epoch': 3.84,\n",
       "  'step': 48},\n",
       " {'eval_loss': 1.403944730758667,\n",
       "  'eval_runtime': 96.8575,\n",
       "  'eval_samples_per_second': 1.032,\n",
       "  'eval_steps_per_second': 0.516,\n",
       "  'epoch': 3.84,\n",
       "  'step': 48},\n",
       " {'loss': 1.567,\n",
       "  'grad_norm': 1.94491446018219,\n",
       "  'learning_rate': 4.4000000000000006e-05,\n",
       "  'epoch': 3.92,\n",
       "  'step': 49},\n",
       " {'loss': 1.0449,\n",
       "  'grad_norm': 2.166889190673828,\n",
       "  'learning_rate': 4e-05,\n",
       "  'epoch': 4.0,\n",
       "  'step': 50},\n",
       " {'loss': 1.4787,\n",
       "  'grad_norm': 0.989453911781311,\n",
       "  'learning_rate': 3.6e-05,\n",
       "  'epoch': 4.08,\n",
       "  'step': 51},\n",
       " {'loss': 1.586,\n",
       "  'grad_norm': 1.5631409883499146,\n",
       "  'learning_rate': 3.2000000000000005e-05,\n",
       "  'epoch': 4.16,\n",
       "  'step': 52},\n",
       " {'loss': 0.8918,\n",
       "  'grad_norm': 2.3347136974334717,\n",
       "  'learning_rate': 2.8000000000000003e-05,\n",
       "  'epoch': 4.24,\n",
       "  'step': 53},\n",
       " {'loss': 1.6502,\n",
       "  'grad_norm': 1.0251556634902954,\n",
       "  'learning_rate': 2.4e-05,\n",
       "  'epoch': 4.32,\n",
       "  'step': 54},\n",
       " {'loss': 1.4853,\n",
       "  'grad_norm': 1.331047534942627,\n",
       "  'learning_rate': 2e-05,\n",
       "  'epoch': 4.4,\n",
       "  'step': 55},\n",
       " {'loss': 1.0834,\n",
       "  'grad_norm': 2.2518746852874756,\n",
       "  'learning_rate': 1.6000000000000003e-05,\n",
       "  'epoch': 4.48,\n",
       "  'step': 56},\n",
       " {'loss': 1.3996,\n",
       "  'grad_norm': 1.32213294506073,\n",
       "  'learning_rate': 1.2e-05,\n",
       "  'epoch': 4.5600000000000005,\n",
       "  'step': 57},\n",
       " {'loss': 1.2628,\n",
       "  'grad_norm': 2.281677722930908,\n",
       "  'learning_rate': 8.000000000000001e-06,\n",
       "  'epoch': 4.64,\n",
       "  'step': 58},\n",
       " {'loss': 0.6022,\n",
       "  'grad_norm': 3.0571722984313965,\n",
       "  'learning_rate': 4.000000000000001e-06,\n",
       "  'epoch': 4.72,\n",
       "  'step': 59},\n",
       " {'loss': 1.4908,\n",
       "  'grad_norm': 1.2212234735488892,\n",
       "  'learning_rate': 0.0,\n",
       "  'epoch': 4.8,\n",
       "  'step': 60},\n",
       " {'eval_loss': 1.266391634941101,\n",
       "  'eval_runtime': 96.6387,\n",
       "  'eval_samples_per_second': 1.035,\n",
       "  'eval_steps_per_second': 0.517,\n",
       "  'epoch': 4.8,\n",
       "  'step': 60},\n",
       " {'train_runtime': 1278.1232,\n",
       "  'train_samples_per_second': 0.391,\n",
       "  'train_steps_per_second': 0.047,\n",
       "  'total_flos': 6236037165367296.0,\n",
       "  'train_loss': 1.9894443601369858,\n",
       "  'epoch': 4.8,\n",
       "  'step': 60}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
