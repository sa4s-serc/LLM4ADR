{"prompt":"Manual serialization of SnakeYAML\n\nThe TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).\n\n## Considered Alternatives\n\n* Manual serialization\n* [SnakeYAML](https:\/\/bitbucket.org\/asomov\/snakeyaml)\n* [jackson-dataformat-yaml](https:\/\/github.com\/FasterXML\/jackson-dataformat-yaml)","completion":"* Chosen Alternative: *Manual serialization*\n* SnakeYAML does not support annotations for serialization\n* jackson-dataformat-yaml seems not to support annotations, such as [jackson-annotations](https:\/\/github.com\/FasterXML\/jackson-annotations)\n\n"}
{"prompt":"We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.","completion":"We have decided to use Python as our primary programming language for our project."}
{"prompt":"XML editor does not enforce validation\n\n**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?\n\n## Considered Alternatives\n\n* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template\n* Winery generate random data to gain schema-conforming XML\n* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.\n* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","completion":"* *Chosen Alternative: D*\n* This is in line with other editors: They allow to save, but warn if the file has compile errors, validation errors, ...\n\n"}
{"prompt":"After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.\n\nThus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.","completion":"We will add the data constuctor MustReferenceOutput TxOutRef to the TxConstraints data type.\n\n    The PlutusV1 on-chain implementation of this new constraint will simply return False. However, cardano-ledger throws a phase-1 validation error if transactions that use the some of the new features (reference inputs, inline datums and reference scripts) try to execute PlutusV1 scripts. See the Babbage era ledger specification. Therefore, the only way to get a phase-2 validation error would be to use this constraint on-chain in a PlutusV1 script, without using any of the new Babbage era features off-chain.\n\n    The PlutusV2 on-chain implementation of this new constraint will check that the provided TxOutRef is part of the ScriptContext\u2019s reference inputs."}
{"prompt":"In Babbage era (available after the Vasil HF), the Cardano blockchain will support \u201creference scripts\u201d by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","completion":"We will add Maybe ScriptHash as a new data constructor parameter for the constraints MustPayToPubKeyAddress, MustPayToOtherScript, ScriptOutputConstraint in TxConstraints. In the off-chain implementation of those constraints, if a reference script hash is provided, we will need to find the actual script in the lookups table so that we can include it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if a reference script is provided because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.\n\n    We will modify the off-chain implementation of MustSpendScriptOutput and ScriptInputConstraint in order to add support for witnessing a script by actually providing it, or by pointing to the reference input which contains the script."}
{"prompt":"IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.\n\nExamples of this are:\n\n    input-output-hk\/cardano-world\n\n    input-output-hk\/ci-world\n\n    input-output-hk\/atala-world\n\nThis initiative appears to be championed by the SRE team who are the creators of divnix\/std. Indeed std is at the heart of the standardization dream.","completion":"Standardization of the repositories has been deemed a worthwhile endeavour, though of very low priority.\n\n    Phase 1 of the standardization process will be carried out in parallel with Move Marconi to a separate repository. A separate repository will be created for Marconi, and from the very beginning it will use std. This way the benefits, limitations and integration costs of std can be experienced and measured, and an informed, definitive decision on standardizing plutus-core and plutus-apps themselves can be made."}
{"prompt":"Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.\n\nThis was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn\u2019t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.\n\nLet\u2019s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.\n\ndata TxOut = TxOut {\n  ...\n  txOutReferenceScript :: Maybe ScriptHash\n  }\n\nAs we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:\n\ndata TxOut = TxOut {\n  ...\n  txOutReferenceScript :: Maybe Script\n  }\n\nwhere the reference script field can store that actual script, not just the hash.\n\nThis proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.","completion":"We will create a cardano-api-extended cabal project, which will contain features and utilities on top of the cardano-api package. A similar idea has emerged with hydra-cardano-api. This package will contain:\n\n            type synonyms for working with the latest era\n\n            a simplified and working transaction balancing function (mainly the Ledger.Fee.makeAutoBalancedTransaction in plutus-apps)\n\n            validation rules (most of what\u2019s in the current Ledger.Validation)\n\n    The cardano-api-extended package will re-export the modules from the hydra-cardano-api package which contain type synonyms for working with the latest era.\n\n    We will remove our data type representation of a Cardano transaction (Ledger.Tx.Internal.Tx in plutus-ledger) and fully commit to Cardano.Api.Tx.Tx era (or Cardano.Api.Tx.TxLatestEra) in the codebase.\n\n    We will replace any use of plutus-ledger-api types by cardano-api and cardano-api-extended types whenever we work with the off-chain part of Plutus applications. For instance, the plutus-contract emulator and types in the Plutus.Contract.Request module of plutus-contract will be updated to use cardano-api types. However, the data types in Ledger.Tx.Constraints.TxConstraints will continue to use plutus-ledger-api types because the constraints are used to generate both Plutus scripts and transactions. Therefore, there should be no breaking change on the API for writing Plutus applications.\n\n    We will improve cardano-api through cardano-api-extended and regularly push changes upstream when possible.\n\n    We will restructure the Ledger.Tx.CardanoApi module in plutus-ledger and move functions in cardano-api-extended.\n\n    We will enhance the plutus-contract emulator by being able to balance and submit cardano-api transactions.\n\n    We will modify the plutus-contract emulator to fully use the cardano-ledger transaction validation rules, and we will remove our custom validation rules (module Ledger.Index in plutus-ledger)."}
{"prompt":"The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.\n\nKey Points:\n\n- Agile emphasizes iterative development, frequent communication, and flexibility in requirements \n\n- Agile can lead to better collaboration among team members\n\n- Agile can help anticipate and respond to changes in project scope and requirements\n\n- Agile can lead to improved time-to-market and better overall project outcomes","completion":"After much deliberation and discussion, the Team has decided to adopt Agile software development methodology. "}
{"prompt":"Marconi is a Haskell executable and library that lives in plutus-chain-index.\n\nIt is desirable to move it into a separate repository for the following reasons:\n\n    Better visibility and easier to discover\n\n    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps\n\n    It is a farily independent component, therefore it warrants its own repository\n\nHowever, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","completion":"We will put Marconi in a separate Github repository\n\n    Until we resolve the issues with creating a separate Github repository (see Context), we will keep Marconi as a separate project in plutus-apps"}
{"prompt":"The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:\n\n +-------------------+     +-------------------+     +-----------------+\n | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |\n +-------------------+     +-----+--------+----+     +--------+--------+\n                                 |        |                   |\n                                 |        |                   v\n+----------------------+         |        |           +---------------+\n| Chain index emulator |<--------+        +---------->| Node emulator |\n+----------------------+                              +---------------+\n\nThe main reason we can\u2019t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.\n\nNow, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.","completion":"We will create a new Haskell package named cardano-node-emulator.\n\n    We will move node related functionality into this new package. Here are some modules (or parts of a module) that will need to be moved over.\n\n        The Ledger.Validation module which validates transactions using cardano-ledger should only be used by cardano-node-emulator and should not be exposed.\n\n        The Ledger.Fee module, which calculates the fees for a given transaction, should be internal to cardano-node-emulator.\n\n        The Ledger.Generators module, which contains generators for constructing blockchains and transactions for use in property-based testing, should be internal to cardano-node-emulator.\n\n        The Ledger.TimeSlot.SlotConfig datatype should only be used by the cardano-node-emulator. An end user should not use this representation in a real world scenario. See ADR 12: Time conversion semantic change for more details.\n\n        The Wallet.Emulator.Chain module in plutus-contract should be moved in cardano-node-emulator.\n\n        The Ledger.Params which allows to configuration the network parameters should be moved over to cardano-node-emulator."}
{"prompt":"Routes in the Repository Angular App\n\n## Considered Alternatives\n* Using Wildcards for the `ToscaTypes`\n* Explicitly Define the Routes for Each `ToscaType`","completion":" Chosen Alternative: *Explicitly Define the Routes for Each `ToscaType`*\n* By choosing this alternative, the whole project gets more type save by the (lightweight) trade-off\n of maintaining a list of all available `MainRoutes` in the `ToscaTypes` enum. It is now harder to add\n new main routes, because you need to add extra `Modules` and `RoutingModules` for each type. However,\n because of this decision, it is easier to define invalid routes which lead to a `404 - Not Found` error page.\n\n"}
{"prompt":"It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\nThere are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\nDecision Drivers\nThe primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.","completion":"The MIT license was chosen, for the following reasons:\nIt is the least restrictive of the licenses.It is very consise, simple and easy to understand and therefore should be clear to users and developers.Digital Iceland does not require protection of patents or existing intelletual property.Well known government lead initiatives like uk.gov and X-Road use the MIT license.The MIT license is the best known and most widely used free and open-source license in the world."}
{"prompt":"How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\nDecision Drivers\nWe need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","completion":"Chosen option: \"OneFlow\" because it provides a single eternal branch with well structured releases.\nWe'll implement OneFlow with these details:\nRelease branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.Release and quality managers from each team are responsible for reviewing and approving releases.Releases apply to all apps in the monorepo.Releases are versioned like this: {cycle}.{sprint}.{hotfix}. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.Feature branches are merged using \"Squash and merge\", so they can be easily reverted.There are two ways to build larger features.If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\nIf the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\nFuture strategy\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\nHosting environments\nWe'll set up continuous delivery to different hosting environments:\nEnvironmentGit sourceDatabases\/servicesFeaturessandboxfeature branchTestAlldevmainTestAllstagingmainProdAllpre-prodrelease\/hotfix branchProdFinishedprodlatest release tagProdFinished\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage."}
{"prompt":"In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: \u201creturn collateral\u201d and \u201ctotal collateral\u201d collateral. Return collateral (also called \u201ccollateral output\u201d) and total collateral are detailed in CIP-40.\n\nIn summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.","completion":"We will add the txReturnCollateral and the txTotalCollateral fields in the Ledger.Tx.Internal.Tx data type.\n\n    We will modify the Wallet.Emulator.Wallet.handleBalance function in plutus-contract to set the correct return and total collateral for an UnbalancedTx (of type Either CardanoBuildTx EmulatorTx). In either type of transaction, we would compute the txTotalCollateral while estimating the fee with the formula quot (txfee txb * (collateralPercent pp)) * 100 and then set txReturnCollateral with the formula sum collateralInputs - txTotalCollateral."}
{"prompt":"The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.","completion":"To implement continuous integration as a solution to the challenges experienced in the development process."}
{"prompt":"In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.\n\nBut there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.\n\nAnd yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. \n\nThe challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.\n\nCompounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)\n\n### Prior Art\n\nSeveral systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http:\/\/guides.rubyonrails.org\/active_record_migrations.html), which is oriented around making schema changes to a relational database.\n\nAnother solution of interest is [Liquibase](http:\/\/www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  \n\n### Scenarios\n\nThere are a variety of \"user stories\" to accomodate. Some examples include:\n\n1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.\n2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.\n3. You want to set up a staging environment that is an exact mirror of your current production system.\n4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.\n5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts.","completion":"Chimera will explicitly define the concept of a migration, and reify migrations as entities in the configuration.\n\nA migration represents an atomic set of changes to the schema of a database. For any given database instance, either a migration has logically been applied, or it hasn't. Migrations have unique IDs, expressed as namespace-qualified keywords.\n\nEvery migration has one or more \"parent\" migrations (except for a single, special \"initial\" migration, which has no parent). A migration may not be applied to a database unless all of its parents have already been applied.\n\nMigrations are also have a _signature_. The signature is an MD5 checksum of the _actual content_ of the migration as it is applied to the database (whether that be txdata for Datomic, a string for SQL DDL, a JSON string, etc.) This is used to ensure that a migration is not \"changed\" after it has already been applied to some persistent database.\n\nAdapters are responsible for exposing an implementation of migrations (and accompanying config DSL) that is appropriate for the database type.\n\nChimera Adapters must additionally satisfy two runtime operations:\n\n- `has-migration?` - takes ID and signature of a particular migration, and returns true if the migration has been successfully applied to the database. This implies that databases must be \"migration aware\" and store the IDs\/signatures of migrations that have already been applied.\n- `migrate` - given a specific migration, run the migration and record that the migration has been applied.\n\n### Migration Types\n\nThere are four basic types of migrations. \n\n1. **Native migrations**. These are instances of the migration type directly implemented by a database adapter, and are specific to the type of DB being used. For example, a native migration against a SQL database would be implemented (primarily) via a SQL string. A native migration can only be used by adapters of the appropriate type.\n2. **Chimera migrations**. These define migrations using Chimera's entity\/attribute data model. They are abstract, and should work against multiple different types of adapters. Chimera migrations should be supported by all Chimera adapters.\n4. **Sentinel migrations**. These are used to coordinate manual changes to an existing database with the code that requires them. They will always fail to automatically apply to an existing database: the database admin must add the migration record explicitly after they perform the manual migration task.  _(Note, actually implementing these can be deferred until if or when they are needed)_.\n\n### Structure & Usage\n\nBecause migrations may have one or more parents, migrations form a directed acyclic graph.\n\nThis is appropriate, and combines well with Arachne's composability model. A module may define a sequence of migrations that build up a data model, and extending modules can branch from any point to build their own data model that shares structure with it. Modules may also depend upon a chain of migrations specified in two dependent modules, to indicate that it requires both of them.\n\nIn the configuration, a Chimera **database component** may depend on any number of migration components. These migrations, and all their ancestors, form a \"database definition\", and represent the complete schema of a concrete database instance (as far as Chimera is concerned.)\n\nWhen a database component is started and connects to the underlying data store, it verifies that all the specifies migrations have been applied. If they have not, it fails to start. This guarantees the safety of an Arachne system; a given application simply will not start if it is not compatible with the specified database.\n\n##### Parallel Migrations\n\nThis does create an opportunity for problems: if two migrations which have no dependency relatinship (\"parallel migrations\") have operations that are incompatible, or would yield different results depending on the order in which they are applied, then these operations \"conflict\" and applying them to a database could result in errors or non-deterministic behavior. \n\nIf the parallel migrations are both Chimera migrations, then Arachne is aware of their internal structure and can detect the conflict and refuse to start or run the migrations, before it actually touches the database. \n\nUnfortunately, Arachne cannot detect conflicting parallel migrations for other migration types. It is the responsibility of application developers to ensure that parallel migrations are logically isolate and can coexist in the same database without conflict. \n\nTherefore, it is advisable in general for public modules to only use Chimera migrations. In addition to making them as broadly compatible as possible, and will also make it more tractable for application authors to avoid conflicting parallel migrations, since they only have to worry about those that they themselves create.\n\n### Chimera Migrations & Entity Types\n\nOne drawback of using Chimera migrations is that you cannot see a full entity type defined in one place, just from reading a config DSL script. This cannot be avoided: in a real, living application, entities are defined over time, in many different migrations as the application grows, not all at once. Each Chimera migration contains only a fragment of the full data model.\n\nHowever, this poses a usability problem; both for developers, and for machine consumption. There are many reasons for developers or modules to view or query the entity type model as a \"point in time\" snapshot, rather than just a series of incremental changes.\n\nTo support this use case, the Chimera module creates a flat entity type model for each database by \"rolling up\" the individual Chimera entity definition forms into a single, full data structure graph. This \"canonical entity model\" can then be used to render schema diagrams for users, or be queried by other modules.\n\n### Applying Migrations\n\nWhen and how to invoke an Adapter's `migrate` function is not defined, since different teams will wish to do it in different ways.\n\nSome possibilities include:\n\n1. The application calls \"migrate\" every time it is started (this is only advisable if the database has excellent support for transactional and atomic migrations.) In this scenario, developers only need to worry about deploying the code.\n2. The devops team can manually invoke the \"migrate\" function for each new configuration, prior to deployment.\n3. In a continuous-deployment setup, a CI server could run a battery of tests against a clone of the production database and invoke \"migrate\" automatically if they pass.\n4. The development team can inspect the set of migrations and generate a set of native SQL or txdata statements for handoff to a dedicated DBA team for review and commit prior to deployment.\n\n### Databases without migrations\n\nNot every application wants to use Chimera's migration system. Some situations where migrations may not be a good fit include:\n\n- You prefer to manage your own database schema.\n- You are working with an existing database that predates Arachne.\n- You need to work with a database administered by a separate team.\n\nHowever, you still may wish to utilize Chimera's entity model, and leverage modules that define Chimera migrations.\n\nTo support this, Chimera allows you to (in the configuration) designate a database component as \"**assert-only**\". Assert-only databases never have migrations applied, and they do not require the database to track any concept of migrations. Instead, they inspect the Chimera entity model (after rolling up all declared migrations) and assert that the database _already_ has compatible schema installed. If it does, everything starts up as normal; if it does not, the component fails to start.\n\nOf course, the schema that Chimera expects most likely will not be an exact match for what is present in the database. To accomodate this, Chimera adapters defines a set of _override_ configuration entities (and accompanying DSL). Users can apply these overrides to change the behavior of the mappings that Chimera uses to query and store data. \n\nNote that Chimera Overrides are incompatible with actually running migrations: they can be used only on an \"assert-only\" database.\n\n### Migration Rollback\n\nGeneralized rollback of migrations is intractable, given the variety of databases Chimera intends to support. Use one of the following strategies instead:\n\n- For development and testing, be constantly creating and throwing away new databases.\n- Back up your database before running a migration\n- If you can't afford the downtime or data loss associated with restoring a backup, manually revert the changes from the unwanted migration.\n   \n"}
{"prompt":"We want our applications to be configurable beyond artifacts\/binaries\/source, such that one build can behave differently depending on its deployment environment.\n\n  * To accomplish this, we want to use environment variable configuration.\n\n  * We want to manage the configuration by using files that we can version control.\n\n  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.\n","completion":"Decided on .env files with related default file and schema file."}
{"prompt":"Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.\n\nAdditionally, prior to the advent of `clojure.spec`, Clojure errors were often \"deep\": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.\n\nThere are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes \"lossy\", dropping information that could be desirable to a developer.\n\nSpec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.\n\nIdeally, Arachne's error handling should exhibit the following qualities:\n\n- Never hide possibly relevant information.\n- Allow module developers to be as helpful as possible to people using their tools.\n- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)\n- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.\n- Not violate expectations of experienced Clojure programmers.\n- Be robust enough not to cause additional problems.\n- Not break existing logging tools for production use.","completion":"We will separate the problems of creating rich exceptions, and catching them and displaying them to the user.\n\n### Creating Errors\n\nWhenever a well-behaved Arachne module needs to report an error, it should throw an info-bearing exception. This exception should be formed such that it is handled gracefully by any JVM tooling; the message should be terse but communicative, containing key information with no newlines.\n\nHowever, in the `ex-data`, the exception will also contain much more detailed information, that can be used (in the correct context) to provide much more detailed or verbose errors. Specifically, it may contain the following keys:\n\n- `:arachne.error\/message` - The short-form error message (the same as the Exception message.)\n- `:arachne.error\/explanation` - a long-form error message, complete with newlines and formatting.\n- `:arachne.error\/suggestions` - Zero or more suggestions on how the error might be fixed.\n- `:arachne.error\/type` - a namespaced keyword that uniquely identifies the type of error.\n- `:arachne.error\/spec` - The spec that failed (if applicable)\n- `:arachne.error\/failed-data` - The data that failed to match the spec (if applicable)\n- `:arachne.error\/explain-data` - An explain-data for the spec that failed (if applicable).\n- `:arachne.error\/env` - A map of the locals in the env at the time the error was thrown.\n\nExceptions may, of course, contain additional data; these are the common keys that tools can use to more effectively render errors.\n\nThere will be a suite of tools, provided with Arachne's core, for conveniently generating errors that match this pattern.\n\n### Displaying Errors\n\nWe will use a pluggable \"error handling system\", where users can explicitly install an exception handler other than the default.\n\nIf the user does not install any exception handlers, errors will be handled the same way as they are by default (usually, dumped with the message and stack trace to  `System\/err`.) This will not change.\n\nHowever, Arachne will also provide a function that a user can invoke in their main process, prior to doing anything else. Invoking this function will install a set of default exception handlers that will handle errors in a richer, more Arachne-specific way. This includes printing out the long-form error, or even (eventually) popping open a graphical data browser\/debugger (if applicable.)\n\n"}
{"prompt":"Use Eclipse Orion as Editor\n\nEditor should provide syntax highlighting and work in browser.\n\n## Considered Alternatives\n\n* [Eclipse Orion](https:\/\/wiki.eclipse.org\/Orion)\n* [Eclipse Che](https:\/\/www.eclipse.org\/che\/)\n* [Codeanywhere](https:\/\/codeanywhere.com)\n* [Cloud9](https:\/\/c9.io\/)\n* [Codiad](http:\/\/codiad.com\/)\n* [Theia](https:\/\/github.com\/theia-ide\/theia)","completion":"* Chosen Alternative: Eclipse Orion\n* Reasons: \n  * Extension of other IDEs either too time consuming or not possible.\n  * Communication between other IDEs and Winery unclear.\n  * Eclipse Orion was already used in the project.\n  \nFor more details see the [ultimate comparison](https:\/\/github.com\/ultimate-comparisons\/ultimate-webIDE-comparison).\n\n"}
