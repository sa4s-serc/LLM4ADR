{"prompt":"We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (\u201cADR\u201d), that we can integrate into our workflow.\n\nThis does not replace actual architecture documentation, but provides people who are contributing:\n\n    the means to understand architectural and design decisions that were made\n\n    a framework for proposing changes to the current architecture\n\nFor each decision, it is important to consider the following factors:\n\n    what we have decided to do\n\n    why we have made this decision\n\n    what we expect the impact of this decision to be\n\n    what we have learned in the process\n\nAs we\u2019re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.","completion":"We will use ADRs to document, propose and discuss any important or significant architectural and design decisions.\n\n    The ADR format will follow the format described in Implications section.\n\n    We will follow the convention of storing those ADRs as rST or Markdown formatted documents stored under the docs\/adr directory, as exemplified in Nat Pryce\u2019s adr-tools. This does not imply that we will be using adr-tools itself, as we might diverge from the proposed structure.\n\n    We will keep rejected ADRs\n\n    We will strive, if possible, to create an ADR as early as possible in relation to the actual implementation."}
{"prompt":"In general, Arachne's philosophy embraces the concepts of immutability and reproducibility; rather than _changing_ something, replace it with something new. Usually, this simplifies the mental model and reduces the number of variables, reducing the ways in which things can go wrong.\n\nBut there is one area where this approach just can't work: administering changes to a production database. Databases must have a stable existence across time. You can't throw away all your data every time you want to make a change.\n\nAnd yet, some changes in the database do need to happen. Data models change. New fields are added. Entity relationships are refactored. \n\nThe challenge is to provide a way to provide measured, safe, reproducible change across time which is _also_ compatible with Arachne's target of defining and describing all relevant parts of an application (including it's data model (and therefore schema)) in a configuration.\n\nCompounding the challenge is the need to build a system that can define concrete schema for different types of databases, based on a common data model (such as Chimera's, as described in [ADR-15](adr-015-data-abstraction-model.md).)\n\n### Prior Art\n\nSeveral systems to do this already exist. The best known is probably Rails' [Active Record Migrations](http:\/\/guides.rubyonrails.org\/active_record_migrations.html), which is oriented around making schema changes to a relational database.\n\nAnother solution of interest is [Liquibase](http:\/\/www.liquibase.org), a system which reifies database changes as data and explicitly applies them to a relation database.  \n\n### Scenarios\n\nThere are a variety of \"user stories\" to accomodate. Some examples include:\n\n1. You are a new developer on a project, and want to create a local database that will work with the current HEAD of the codebase, for local development.\n2. You are responsible for the production deployment of your project, and your team has a new software version ready to go, but it requires some new fields to be added to the database before the new code will run.\n3. You want to set up a staging environment that is an exact mirror of your current production system.\n4. You and a fellow developer are merging your branches for different features. You both made different changes to the data model, and you need to be sure they are compatible after the merge.\n5. You recognize that you made a mistake earlier in development, and stored a currency value as a floating point number. You need to create a new column in the database which uses a fixed-point type, and copy over all the existing values, using rounding logic that you've agreed on with domain experts.","completion":"Chimera will explicitly define the concept of a migration, and reify migrations as entities in the configuration.\n\nA migration represents an atomic set of changes to the schema of a database. For any given database instance, either a migration has logically been applied, or it hasn't. Migrations have unique IDs, expressed as namespace-qualified keywords.\n\nEvery migration has one or more \"parent\" migrations (except for a single, special \"initial\" migration, which has no parent). A migration may not be applied to a database unless all of its parents have already been applied.\n\nMigrations are also have a _signature_. The signature is an MD5 checksum of the _actual content_ of the migration as it is applied to the database (whether that be txdata for Datomic, a string for SQL DDL, a JSON string, etc.) This is used to ensure that a migration is not \"changed\" after it has already been applied to some persistent database.\n\nAdapters are responsible for exposing an implementation of migrations (and accompanying config DSL) that is appropriate for the database type.\n\nChimera Adapters must additionally satisfy two runtime operations:\n\n- `has-migration?` - takes ID and signature of a particular migration, and returns true if the migration has been successfully applied to the database. This implies that databases must be \"migration aware\" and store the IDs\/signatures of migrations that have already been applied.\n- `migrate` - given a specific migration, run the migration and record that the migration has been applied.\n\n### Migration Types\n\nThere are four basic types of migrations. \n\n1. **Native migrations**. These are instances of the migration type directly implemented by a database adapter, and are specific to the type of DB being used. For example, a native migration against a SQL database would be implemented (primarily) via a SQL string. A native migration can only be used by adapters of the appropriate type.\n2. **Chimera migrations**. These define migrations using Chimera's entity\/attribute data model. They are abstract, and should work against multiple different types of adapters. Chimera migrations should be supported by all Chimera adapters.\n4. **Sentinel migrations**. These are used to coordinate manual changes to an existing database with the code that requires them. They will always fail to automatically apply to an existing database: the database admin must add the migration record explicitly after they perform the manual migration task.  _(Note, actually implementing these can be deferred until if or when they are needed)_.\n\n### Structure & Usage\n\nBecause migrations may have one or more parents, migrations form a directed acyclic graph.\n\nThis is appropriate, and combines well with Arachne's composability model. A module may define a sequence of migrations that build up a data model, and extending modules can branch from any point to build their own data model that shares structure with it. Modules may also depend upon a chain of migrations specified in two dependent modules, to indicate that it requires both of them.\n\nIn the configuration, a Chimera **database component** may depend on any number of migration components. These migrations, and all their ancestors, form a \"database definition\", and represent the complete schema of a concrete database instance (as far as Chimera is concerned.)\n\nWhen a database component is started and connects to the underlying data store, it verifies that all the specifies migrations have been applied. If they have not, it fails to start. This guarantees the safety of an Arachne system; a given application simply will not start if it is not compatible with the specified database.\n\n##### Parallel Migrations\n\nThis does create an opportunity for problems: if two migrations which have no dependency relatinship (\"parallel migrations\") have operations that are incompatible, or would yield different results depending on the order in which they are applied, then these operations \"conflict\" and applying them to a database could result in errors or non-deterministic behavior. \n\nIf the parallel migrations are both Chimera migrations, then Arachne is aware of their internal structure and can detect the conflict and refuse to start or run the migrations, before it actually touches the database. \n\nUnfortunately, Arachne cannot detect conflicting parallel migrations for other migration types. It is the responsibility of application developers to ensure that parallel migrations are logically isolate and can coexist in the same database without conflict. \n\nTherefore, it is advisable in general for public modules to only use Chimera migrations. In addition to making them as broadly compatible as possible, and will also make it more tractable for application authors to avoid conflicting parallel migrations, since they only have to worry about those that they themselves create.\n\n### Chimera Migrations & Entity Types\n\nOne drawback of using Chimera migrations is that you cannot see a full entity type defined in one place, just from reading a config DSL script. This cannot be avoided: in a real, living application, entities are defined over time, in many different migrations as the application grows, not all at once. Each Chimera migration contains only a fragment of the full data model.\n\nHowever, this poses a usability problem; both for developers, and for machine consumption. There are many reasons for developers or modules to view or query the entity type model as a \"point in time\" snapshot, rather than just a series of incremental changes.\n\nTo support this use case, the Chimera module creates a flat entity type model for each database by \"rolling up\" the individual Chimera entity definition forms into a single, full data structure graph. This \"canonical entity model\" can then be used to render schema diagrams for users, or be queried by other modules.\n\n### Applying Migrations\n\nWhen and how to invoke an Adapter's `migrate` function is not defined, since different teams will wish to do it in different ways.\n\nSome possibilities include:\n\n1. The application calls \"migrate\" every time it is started (this is only advisable if the database has excellent support for transactional and atomic migrations.) In this scenario, developers only need to worry about deploying the code.\n2. The devops team can manually invoke the \"migrate\" function for each new configuration, prior to deployment.\n3. In a continuous-deployment setup, a CI server could run a battery of tests against a clone of the production database and invoke \"migrate\" automatically if they pass.\n4. The development team can inspect the set of migrations and generate a set of native SQL or txdata statements for handoff to a dedicated DBA team for review and commit prior to deployment.\n\n### Databases without migrations\n\nNot every application wants to use Chimera's migration system. Some situations where migrations may not be a good fit include:\n\n- You prefer to manage your own database schema.\n- You are working with an existing database that predates Arachne.\n- You need to work with a database administered by a separate team.\n\nHowever, you still may wish to utilize Chimera's entity model, and leverage modules that define Chimera migrations.\n\nTo support this, Chimera allows you to (in the configuration) designate a database component as \"**assert-only**\". Assert-only databases never have migrations applied, and they do not require the database to track any concept of migrations. Instead, they inspect the Chimera entity model (after rolling up all declared migrations) and assert that the database _already_ has compatible schema installed. If it does, everything starts up as normal; if it does not, the component fails to start.\n\nOf course, the schema that Chimera expects most likely will not be an exact match for what is present in the database. To accomodate this, Chimera adapters defines a set of _override_ configuration entities (and accompanying DSL). Users can apply these overrides to change the behavior of the mappings that Chimera uses to query and store data. \n\nNote that Chimera Overrides are incompatible with actually running migrations: they can be used only on an \"assert-only\" database.\n\n### Migration Rollback\n\nGeneralized rollback of migrations is intractable, given the variety of databases Chimera intends to support. Use one of the following strategies instead:\n\n- For development and testing, be constantly creating and throwing away new databases.\n- Back up your database before running a migration\n- If you can't afford the downtime or data loss associated with restoring a backup, manually revert the changes from the unwanted migration.\n   \n"}
{"prompt":"In Babbage era (available after the Vasil HF), the Cardano blockchain will support \u201creference scripts\u201d by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","completion":"We will add Maybe ScriptHash as a new data constructor parameter for the constraints MustPayToPubKeyAddress, MustPayToOtherScript, ScriptOutputConstraint in TxConstraints. In the off-chain implementation of those constraints, if a reference script hash is provided, we will need to find the actual script in the lookups table so that we can include it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if a reference script is provided because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.\n\n    We will modify the off-chain implementation of MustSpendScriptOutput and ScriptInputConstraint in order to add support for witnessing a script by actually providing it, or by pointing to the reference input which contains the script."}
{"prompt":"In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: \u201creturn collateral\u201d and \u201ctotal collateral\u201d collateral. Return collateral (also called \u201ccollateral output\u201d) and total collateral are detailed in CIP-40.\n\nIn summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.","completion":"We will add the txReturnCollateral and the txTotalCollateral fields in the Ledger.Tx.Internal.Tx data type.\n\n    We will modify the Wallet.Emulator.Wallet.handleBalance function in plutus-contract to set the correct return and total collateral for an UnbalancedTx (of type Either CardanoBuildTx EmulatorTx). In either type of transaction, we would compute the txTotalCollateral while estimating the fee with the formula quot (txfee txb * (collateralPercent pp)) * 100 and then set txReturnCollateral with the formula sum collateralInputs - txTotalCollateral."}
{"prompt":"Routes in the Repository Angular App\n\n## Considered Alternatives\n* Using Wildcards for the `ToscaTypes`\n* Explicitly Define the Routes for Each `ToscaType`","completion":" Chosen Alternative: *Explicitly Define the Routes for Each `ToscaType`*\n* By choosing this alternative, the whole project gets more type save by the (lightweight) trade-off\n of maintaining a list of all available `MainRoutes` in the `ToscaTypes` enum. It is now harder to add\n new main routes, because you need to add extra `Modules` and `RoutingModules` for each type. However,\n because of this decision, it is easier to define invalid routes which lead to a `404 - Not Found` error page.\n\n"}
{"prompt":"Reflection test for TOSCA YAML builder\n\nThe TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. \n\n```\n...\nmetadata:\n  assert: |\n    repositories.rp1.url = http:\/\/github.com\/kleinech\n    node_types.ntp1.requirements.0.rqr1.capability = cbt1\n...\n```\nEach assert line contains a keyname and a value.\n*[context and problem statement]*\n*[decision drivers | forces]* <!-- optional -->\n\n## Considered Alternatives\n\n* *reflection tests*\n* *manual test*","completion":"* Chosen Alternative: *reflection tests*\n* Only alternative, which meets simplifies the effort to make complete tests \n"}
{"prompt":"As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.\n\nTherefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:\n\n- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.\n- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.\n\nThere are two \"kinds\" of config validation.\n\nThe first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.\n\nThe second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.","completion":"Arachne will perform both kinds of validation. To disambiguate them (since they are logically distinct), we will term the structural\/schema validation \"configuration validation\", while the validation of the runtime objects will be \"runtime validation.\"\n\nBoth styles of validation should be extensible by modules, so modules can specify additional validations, where necessary.\n\n#### Configuration Validation\n\nConfiguration validation is ensuring that an Arachne configuration object is consistent with itself and with its schema.\n\nBecause this is ultimately validating a set of Datomic style `eavt` tuples, the natural form for checking tuple data is Datalog queries and query rules, to search for and locate data that is \"incorrect.\" \n\nEach logical validation will have its own \"validator\", a function which takes a config, queries it, and either returns or throws an exception. To validate a config, it is passed through every validator as the final step of building a module.\n\nThe set of validators is open, and defined in the configuration itself. To add new validators, a module can transact entities for them during its configuration building phase.\n\n#### Runtime Validation\n\nRuntime validation occurs after a runtime is instantiated, but before it is started. Validation happens on the component level; each component may be subject to validation.\n\nUnlike Configuration validation, Runtime validation uses Spec. What specs should be applied to each component are defined in the configuration using a keyword-valued attribute. Specs may be defined on individual component entities, or to the *type* of a component entity. When a component is validated, it is validated using all the specs defined for it or any of its supertypes.\n\n"}
{"prompt":"Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.","completion":"Our organization has decided to use Google Cloud Platform as the cloud infrastructure for our application. The primary considerations for this decision are:\n\n   - Cost-effectiveness\n\n   - Scalability\n\n   - Reliability\n\n   - Flexibility"}
{"prompt":"Our project involves developing three major categories of software:\n\n  * Front-end GUIs\n  * Middleware services\n  * Back-end servers\n\nWhen we develop, our source code management (SCM) version control system (VCS) is git.\n\nWe need to choose how we use git to organize our code.\n\nThe top-level choice is to organize as a \"monorepo\" or \"polyrepo\" or \"hybrid\":\n\n  * Monorepo means we put all pieces into one big repo\n  * Polyrepo means we put each piece in its own repo\n  * Hybrid means some mix of monorepo and polyrepo\n\nFor more please see https:\/\/github.com\/joelparkerhenderson\/monorepo-vs-polyrepo\n","completion":"Monorepo when an organization\/team\/project is relatively small, and rapid iteration is higher priority than sustaining stability.\n\nPolyrepo when an organization\/team\/project is relatively large, and sustaining stability is higher priority than rapid iteration."}
{"prompt":"Use Eclipse Orion as Editor\n\nEditor should provide syntax highlighting and work in browser.\n\n## Considered Alternatives\n\n* [Eclipse Orion](https:\/\/wiki.eclipse.org\/Orion)\n* [Eclipse Che](https:\/\/www.eclipse.org\/che\/)\n* [Codeanywhere](https:\/\/codeanywhere.com)\n* [Cloud9](https:\/\/c9.io\/)\n* [Codiad](http:\/\/codiad.com\/)\n* [Theia](https:\/\/github.com\/theia-ide\/theia)","completion":"* Chosen Alternative: Eclipse Orion\n* Reasons: \n  * Extension of other IDEs either too time consuming or not possible.\n  * Communication between other IDEs and Winery unclear.\n  * Eclipse Orion was already used in the project.\n  \nFor more details see the [ultimate comparison](https:\/\/github.com\/ultimate-comparisons\/ultimate-webIDE-comparison).\n\n"}
{"prompt":"Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.\n\nThis was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldn\u2019t have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.\n\nLet\u2019s take the example of the TxOut representation of plutus-ledger-api for PlutusV2.\n\ndata TxOut = TxOut {\n  ...\n  txOutReferenceScript :: Maybe ScriptHash\n  }\n\nAs we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:\n\ndata TxOut = TxOut {\n  ...\n  txOutReferenceScript :: Maybe Script\n  }\n\nwhere the reference script field can store that actual script, not just the hash.\n\nThis proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.","completion":"We will create a cardano-api-extended cabal project, which will contain features and utilities on top of the cardano-api package. A similar idea has emerged with hydra-cardano-api. This package will contain:\n\n            type synonyms for working with the latest era\n\n            a simplified and working transaction balancing function (mainly the Ledger.Fee.makeAutoBalancedTransaction in plutus-apps)\n\n            validation rules (most of what\u2019s in the current Ledger.Validation)\n\n    The cardano-api-extended package will re-export the modules from the hydra-cardano-api package which contain type synonyms for working with the latest era.\n\n    We will remove our data type representation of a Cardano transaction (Ledger.Tx.Internal.Tx in plutus-ledger) and fully commit to Cardano.Api.Tx.Tx era (or Cardano.Api.Tx.TxLatestEra) in the codebase.\n\n    We will replace any use of plutus-ledger-api types by cardano-api and cardano-api-extended types whenever we work with the off-chain part of Plutus applications. For instance, the plutus-contract emulator and types in the Plutus.Contract.Request module of plutus-contract will be updated to use cardano-api types. However, the data types in Ledger.Tx.Constraints.TxConstraints will continue to use plutus-ledger-api types because the constraints are used to generate both Plutus scripts and transactions. Therefore, there should be no breaking change on the API for writing Plutus applications.\n\n    We will improve cardano-api through cardano-api-extended and regularly push changes upstream when possible.\n\n    We will restructure the Ledger.Tx.CardanoApi module in plutus-ledger and move functions in cardano-api-extended.\n\n    We will enhance the plutus-contract emulator by being able to balance and submit cardano-api transactions.\n\n    We will modify the plutus-contract emulator to fully use the cardano-ledger transaction validation rules, and we will remove our custom validation rules (module Ledger.Index in plutus-ledger)."}
{"prompt":"While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.\n\nThere are a number of possible benefits to this approach:\n\n- Deployments from the same configuration are highly reproducible\n- Organizations can maintain an immutable persistent log of configuration changes over time.\n- External tooling can be used to persistently build and define configurations, up to and including full \"drag and drop\" architecture or application design.\n\nDoing this introduces a number of additional challenges:\n\n- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)\n  \n  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.\n- **Scope & Naming**: It is extremely convenient to use `:db.unique\/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is \"the same\" as an older entity in the system.\n\n  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and \"collide\", causing inadvertent linkages between what ought to be logically distinct configurations.\n \n  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.\n\n- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support \"forking\" or maintaining multiple concurrent versions of the same logical data set.\n\n  This does introduce complexities when thinking about \"modifying\" a configuration, while still keeping the old one. This kind of \"fork\" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique\/identity` attrs.\n  \n  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.\n\n- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the \"same\" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.\n\n  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.\n  \n- **Relationship with module load process**: Would the stored configuration represent only the \"initial\" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?\n\n  Both alternatives present issues.\n  \n  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.\n  \n  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would \"old\" or stale module-generated values be removed?\n\n#### Goals\n\nWe need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:\n\n- Define a new configuration from an init script.\n- Run an init script on an existing configuration, updating it.\n- Edit an existing configuration using the REPL.\n- Edit an existing configuration using a UI.\n- Clone a configuration\n- Deploy based on a specific configuration\n\nAt the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.","completion":"We will not attempt to implement a concrete strategy for config persistence at this time; it runs the risk of becoming a quagmire that will halt forward momentum.\n\nInstead, we will make a minimal set of choices and observations that will enable forward progress while preserving the ability to revisit the issue of persistent configuration at some point in the future.\n\n1. The configuration schema itself should be compatible with having several configurations present in the same persistent database. Specifically:\n  - Each logical configuration should have its own namespace, which will be used as the namespace of all `:db.unique\/identity` values, ensuring their global uniqueness.\n  - There is a 'configuration' entity that reifies a config, its possible root components, how it was constructed, etc.\n  - The entities in a configuration must form a connected graph. That is, every entity in a configuration must be reachable from the base 'config' entity. This is required to have any ability to identify the config as a whole within for any purpose.\n\n2. The current initial _tooling_ for building configurations (including the init scripts) will focus on building configurations from scratch. Tooling capable of \"editing\" an existing configuration is sufficiently different, with a different set of requirements and constraints, that it needs its own design process.\n\n3. Any future tooling for storing, viewing and editing configurations will need to explicitly determine whether it wants to work with the configuration before or after processing by the modules, since there is a distinct set of tradeoffs.\n\n"}
{"prompt":"It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\nThere are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\nDecision Drivers\nThe primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.","completion":"The MIT license was chosen, for the following reasons:\nIt is the least restrictive of the licenses.It is very consise, simple and easy to understand and therefore should be clear to users and developers.Digital Iceland does not require protection of patents or existing intelletual property.Well known government lead initiatives like uk.gov and X-Road use the MIT license.The MIT license is the best known and most widely used free and open-source license in the world."}
{"prompt":"At some point, every Arachne application needs to start; to bootstrap\nitself from a static project or deployment artifact, initialize what\nneeds initializing, and begin servicing requests, connecting to\ndatabases, processing data, etc.\n\nThere are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.\n\n- Starting the JVM\n    - Assembling the project's dependencies\n    - Building a JVM classpath\n    - Starting a JVM\n- Arachne Specific\n    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))\n    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))\n- Application Specific\n    - Instantiate user and module-defined objects that needs to exist at runtime.\n    - Start and stop user and module-defined services\n\nAs discussed in [ADR-004](adr-004-module-loading.md), tasks in the \"starting the JVM\" category are not in-scope for Arachne; rather, they are offloaded to whatever build\/dependency tool the project is using (usually either [boot](http:\/\/boot-clj.com) or [leiningen](http:\/\/leiningen.org).)\n\nThis leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.\n\nAdditionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.","completion":"#### Components\n\nArachne uses the [Component](https:\/\/github.com\/stuartsierra\/component) library to manage system components. Instead of requiring users to define a component system map manually, however, Arachne itself builds one based upon the Arachne config via *Configuration Entities* that appear in the configuration.\n\nComponent entities may be added to the config directly by end users (via a initialization script as per [ADR-005](adr-005-user-facing-config.md)), or by modules in their `configure` function ([ADR-004](adr-004-module-loading.md).)\n\nComponent entities have attributes which indicates which other components they depend upon. Circular dependencies are not allowed; the component dependency structure must form a Directed Acyclic Graph (DAG.) The dependency attributes also specify the key that Component will use to `assoc` dependencies.\n\nComponent entities also have an attribute that specifies a *component constructor function* (via a fully qualified name.) Component constructor functions must take two arguments: the configuration, and the entity ID of the component that is to be constructed. When invoked, a component constructor must return a runtime component object, to be used by the Component library. This may be any object that implements `clojure.lang.Associative`, and may also optionally satisfy Component's `Lifecycle` protocol.\n\n#### Arachne Runtime\n\nThe top-level entity in an Arachne system is a reified *Arachne Runtime* object. This object contains both the Component system object, and the configuration value upon which the runtime is based. It satisfies the `Lifecycle` protocol itself; when it is started or stopped, all of the component objects it contains are started or stopped in the appropriate order.\n\nThe constructor function for a Runtime takes a configuration value and some number of \"roots\"; entity IDs or lookup refs of Component entities in the config. Only these root components and their transitive dependencies will be instantiated or added to the Component system. In other words, only component entities that are actually used will be instantiated; unused component entities defined in the config will be ignored.\n\nA `lookup` function will be provided to find the runtime object instance of a component, given its entity ID or lookup ref in the configuraiton.\n\n#### Startup Procedure\n\nArachne will rely upon an external build tool (such as boot or leiningen.) to handle downloading dependencies, assembling a classpath, and starting a JVM.\n\nOnce JVM with the correct classpath is running, the following steps are required to yield a running Arachne runtime:\n\n1. Determine a set of modules to use (the \"active modules\")\n2. Build a configuration schema by querying each active module using its `schema` function ([ADR-004](module-loading.md))\n3. Update the config with initial configuration data from user init scripts ([ADR-005](adr-005-user-facing-config.md))\n4. In module dependency order, give each module a chance to query and update the configuration using its `configure` function ([ADR-004](module-loading.md))\n5. Create a new Arachne runtime, given the configuration and a set of root components.\n6. Call the runtime's `start` method. \n\nThe Arachne codebase will provide entry points to automatically perform these steps for common development and production scenarios. Alternatively, they can always be be executed individually in a REPL, or composed in custom startup functions.\n \n"}
{"prompt":"island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\nWhich CMS system would best suit the needs of island.is?\nDecision Drivers\nContent needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute","completion":"Devs narrowed the choice down to two options Contentful and Contentstack.\nBoth systems meet the required featureset.\nA decision from management was made to use Contentful. Contentful is deemed to have a larger presence in the Icelandic dev community. Contentful is also believed to have a stronger funding base. Contentful is already implemented in some of our projects."}
{"prompt":"The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:\n\n +-------------------+     +-------------------+     +-----------------+\n | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |\n +-------------------+     +-----+--------+----+     +--------+--------+\n                                 |        |                   |\n                                 |        |                   v\n+----------------------+         |        |           +---------------+\n| Chain index emulator |<--------+        +---------->| Node emulator |\n+----------------------+                              +---------------+\n\nThe main reason we can\u2019t use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.\n\nNow, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.","completion":"We will create a new Haskell package named cardano-node-emulator.\n\n    We will move node related functionality into this new package. Here are some modules (or parts of a module) that will need to be moved over.\n\n        The Ledger.Validation module which validates transactions using cardano-ledger should only be used by cardano-node-emulator and should not be exposed.\n\n        The Ledger.Fee module, which calculates the fees for a given transaction, should be internal to cardano-node-emulator.\n\n        The Ledger.Generators module, which contains generators for constructing blockchains and transactions for use in property-based testing, should be internal to cardano-node-emulator.\n\n        The Ledger.TimeSlot.SlotConfig datatype should only be used by the cardano-node-emulator. An end user should not use this representation in a real world scenario. See ADR 12: Time conversion semantic change for more details.\n\n        The Wallet.Emulator.Chain module in plutus-contract should be moved in cardano-node-emulator.\n\n        The Ledger.Params which allows to configuration the network parameters should be moved over to cardano-node-emulator."}
{"prompt":"The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.\n\nKey Points:\n\n- Agile emphasizes iterative development, frequent communication, and flexibility in requirements \n\n- Agile can lead to better collaboration among team members\n\n- Agile can help anticipate and respond to changes in project scope and requirements\n\n- Agile can lead to improved time-to-market and better overall project outcomes","completion":"After much deliberation and discussion, the Team has decided to adopt Agile software development methodology. "}
{"prompt":"The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype \n\nThis is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.\n\nHowever, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:\n\n- There is a desire for some types to be \"abstract\", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.\n- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.\n- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.\n- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to \"denormalization\", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.\n\nAll of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)","completion":"The concept of type extension and attribute inheritance does not provide benefits proportional to the cost.\n\nWe will remove all concept of supertypes, subtypes and attribute inheritance from Chimera's data model.\n\nChimera's data model will remain \"flat\". In order to achieve attribute reuse for data stores for which that is idiomatic (such as Datomic), multiple Chimera attributes can be mapped to a single DB-level attribute in the adapter mapping metadata.\n   \n"}
{"prompt":"After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.\n\nThus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.","completion":"We will add the data constuctor MustReferenceOutput TxOutRef to the TxConstraints data type.\n\n    The PlutusV1 on-chain implementation of this new constraint will simply return False. However, cardano-ledger throws a phase-1 validation error if transactions that use the some of the new features (reference inputs, inline datums and reference scripts) try to execute PlutusV1 scripts. See the Babbage era ledger specification. Therefore, the only way to get a phase-2 validation error would be to use this constraint on-chain in a PlutusV1 script, without using any of the new Babbage era features off-chain.\n\n    The PlutusV2 on-chain implementation of this new constraint will check that the provided TxOutRef is part of the ScriptContext\u2019s reference inputs."}
{"prompt":"Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance.","completion":"We have decided to implement a 4-day workweek starting from the next quarter for all employees across the organization."}
{"prompt":"One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.\n\nSome examples of the alternative implementations that people might wish to use for various parts of their application:\n\n- HTTP Server: Pedestal or Ring\n- Database: Datomic, an RDBMS or one of many NoSQL options.\n- HTML Templating: Hiccup, Enlive, StringTemplate, etc.\n- Client-side code: ClojureScript, CoffeeScript, Elm, etc.\n- Authentication: Password-based, OpenID, Facebook, Google, etc.\n- Emailing: SMTP, one of many third-party services.\n\nThis is only a representative sample; the actual list is unbounded.\n\nThe need for this kind of flexibility raises some design concerns: \n\n**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the \"least common denominator\" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic \"ORM\" that is also compatible with RDBMSs.\n\n**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.\n\n**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a \"least common denominator\" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.\n\n### What does it mean to use a module?\n\nThe following list enumerates the ways in which it is possible to \"use\" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).\n\n1. You can call code that the module provides (the same as any Clojure library.)\n2. You can extend a protocol that the module provides (the same as any Clojure library.)\n3. You can read the attributes defined in the module from the configuration.\n4. You can write configuration data using the attributes defined in the module.\n\nThese tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface\/implementation, inheritance, etc.","completion":"In order to simultaneously meet the needs for capability, uniformity and composition, Arachne's core modules will (as appropriate) use the pattern of *abstract modules*.\n\nAbstract modules define certain attributes (and possibly also corresponding init script DSLs) that describe entities in a particular domain, *without* providing any runtime implementation which uses them. Then, other modules can \"implement\" the abstract module, reading the abstract entities and doing something concrete with them at runtime, as well as defining their own more specific attributes.\n\nIn this way, user applications and dependent modules can rely either on the common, abstract module or the specific, concrete module as appropriate. Coding against the abstract module will yield a more generic \"least common denominator\" experience, while coding against a specific implementor will give more access to the unique distinguishing features of that particular technology, at the cost of generality.\n\nSimilar relationships should hold in the library code which modules expose (if any.) An abstract module, for example, would be free to define a protocol, intended to be implemented concretely by code in an implementing module.\n\nThis pattern is fully extensible; it isn't limited to a single level of abstraction. An abstract module could itself be a narrowing or refinement of another, even more general abstract module.\n\n### Concrete Example\n\nAs mentioned above, Arachne would like to support both Ring and Pedestal as HTTP servers. Both systems have a number of things in common:\n\n- The concept of a \"server\" running on a port.\n- The concept of a URL path\/route\n- The concept of a terminal \"handler\" function which receives a request and returns a response.\n\nThey also have some key differences:\n\n- Ring composes \"middleware\" functions, whereas Pedestal uses \"interceptor\" objects\n- Asynchronous responses are handled differently\n\nTherefore, it makes sense to define an abstract HTTP module which defines the basic domain concepts; servers, routes, handlers, etc. Many dependent modules and applications will be able to make real use of this subset.\n\nThen, there will be the two modules which provide concrete implementations; one for Pedestal, one for Ring. These will contain the code that actually reads the configuration, and at runtime builds appropriate routing tables, starts server instances, etc. Applications which wish to make direct use of a specific feature like Pedestal interceptors may freely do so, using attributes defined by the Pedestal module.\n\n"}
{"prompt":"We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.\n","completion":"We are choosing TypeScript for the front-end.\n\nWe are choosing Rust for the back-end."}
{"prompt":"TOSCA YAML deserialization using SnakeYAML\n\nThe TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).\n\n## Considered Alternatives\n\n* [SnakeYAML](https:\/\/bitbucket.org\/asomov\/snakeyaml) writing into intermediate model consisting of default Java types\n* SnakeYAML writing into final Java model\n* Manual reading","completion":"* Chosen Alternative: *SnakeYAML writing into intermediate model consisting of default Java types*\n\n# Pros and Cons of the Alternatives\n  \n### SnakeYAML writing into intermediate model consisting of default Java types\n  \n* `+` Basic YAML parsing can be done using SnakeYAML\n* `+` TOSCA Java classes can be filled directly\n* `-` Programming effort for conversion\n\n### SnakeYAML writing into final Java model\n\n* `+` Established library\n* `+` Less error prone \n* `-` SnakeYAML has to be adapted to be able to convert YAML into TOSCA models\n* `-` SnakeYAML is not well-prepared for adaptions\n* `-` SnakeYAML has issues to write into complex Java classes (which are not Java base types). E.g., List of Maps. - see <https:\/\/bitbucket.org\/asomov\/snakeyaml\/issues\/361\/list-does-not-create-property-objects>\n* `-` huge effort, first attempt did not result in a working converter\n\n### Manual reading\n\n* `+` Can write directly into Java model\n* `-` Special cases of YAML have to be handled manually\n* `-` Error prone\n\n"}
{"prompt":"File system folder structure using type-namespace-id structure\n\nWinery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).\nThe contents of the repository should be\n\n- human readable\n- machine processable\n\n## Considered Alternatives\n\n* Folder structure using type-namespace-id\n* Everything in one folder. Hash-based storing similar to git.","completion":"*Chosen Alternative: Folders subdivided in type-namespace-id*\n\nThe final file system layout itself is documented at [RepositoryLayout](..\/dev\/RepositoryLayout).\n\n### human readable\n\nEverything in one directory causes many files listed and thus humans will have difficulties to find the right file.\n\nThe folders in the *top level* are the TOSCA \"components\", e.g., Node Type, Relationship Type, Service Template, ...\n\nThe *second structuring element* are namespaces.\n[Namespaces are an established method to avoid naming conflicts](https:\/\/www.w3schools.com\/xml\/xml_namespaces.asp) and are a structuring element.\nTOSCA is an open system and everyone can create Node Types.\nOne has no global control which names are given to Node Types.\nThus, there might be two different Node Types with the same name.\nThe namespaces provide a natural structuring and Winery reuses this idea.\n\nThe *third structuring element* are the ids of the respective definitions child (type, template, ...):\nEach element has an id contained in the respective namespace.\nThis id can be directly used as folder name.\n\nWithin each folder, the component-specific information is stored.\n\n### machine processable\n\nWindows cannot create directories named `http:\/\/www.example.com`.\nTherefore, the names have to be [encoded](https:\/\/en.wikipedia.org\/wiki\/Character_encoding) so that an appropriate folder can be generated.\n\n\n"}
{"prompt":"In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.\n\nIn Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.\n\nWith inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","completion":"We will replace the Datum parameter in TxConstraints\u2019s data constructor MustPayToPubKeyAddress with Plutus.V2.Ledger.Api.OutputDatum. In the offchain implementation of the constraint, we will use this new data constructor parameter to support either adding the datum in the datum witness set (by using the datum lookups to resolve the hash) or inline it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if the datum value matches OutputDatum Datum because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.\n\n    We will modify the data constructor interface, on-chain implementation and off-chain implementation of MustPayToOtherScript similarly to MustPayToPubKeyAddress.\n\n    We will modify the off-chain implementation of the data constructor MustSpendScriptOutput in order to support inline datums. Currently, the script output\u2019s datum is added in the transaction\u2019s datum witness set. However, if the datum is inlined in the script output, then it is already witnessed. Therefore, we don\u2019t need to add it in the datum witness set."}
{"prompt":"No support for local git source clones\n\nA user wants to edit source files locally in his favourite IDE.\nTherefore, he wants to use the usual ways to retrieve source files.\nTypically, this is a `git clone` from a git repository having the respective source files.\n\nA user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.\nIt would be beneficial to have the source of an artifact template available as git checkout.\n\nThe source files of an artifact implementation are currently directly editable in the winery once they are uploaded. \nThe only way to edit sources locally is to download and upload them again.\nThe solution for the user should be:\n- easy to use\n- scalable in terms of storage required in Winery's repository\n\n\n## Considered Alternatives\n\n* No support for local clones\n* Git repositories as submodules\n* Using filter-branch (https:\/\/help.github.com\/articles\/splitting-a-subfolder-out-into-a-new-repository\/)\n* Using git sparse-checkout to create a local clone (https:\/\/gist.github.com\/sumardi\/5559896)","completion":"* Chosen Alternative: no support for local edit\n\nSince all alternatives require either too many additional git repositories or are very inconvenient to apply for the user,\nwe decided to not support any clone\/push functionality.\n\n\n"}
{"prompt":"Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency.","completion":"We will deprecate the Ledger.TimeSlot.SlotConfig type and all functions in the Ledger.TimeSlot module using the SlotConfig. The only viable functions are the ones that convert between Data.Time types and plutus types (types related to TxInfo).\n\n    We will copy the Ledger.TimeSlot module in the emulator (ideally rename it) and keep it as an internal module. Any functions not used by the emulator will be removed.\n\n    We will move the Ledger.Params module inside the emulator as an internal module and modify the Params datatype name to EmulatorParams.\n\n    We will modify the Plutus.Contract.Request.getParams function to Plutus.Contract.Request.getProtocolParameters. This implies modifying the name of Contract effect GetParamsReq\/GetParamsResp.\n\n    We will create two pairs of effects in Plutus.Contract.Effect:\n\n    data PABReq =\n      ...\n      | SlotToUTCTimeIntervalReq SlotNo\n      | UTCTimeToSlotReq UTCTime\n      ...\n\n     data PABResp =\n      ...\n      | SlotToUTCTimeIntervalResp (UTCTime, SlotLength) -- An alternative can be (UTCTime, UTCTime)\n      | UTCTimeToSlotResp SlotNo\n      ...\n\n    We will implement the emulator effect interpreter by simply using the SlotConfig for the conversions.\n\n    We will implement the PAB effect interpreter by using the local node. There are multiple steps to implement this:\n\n        At startup, the PAB will query the EraHistory from the local node and store it in its local environment.\n\n        We will implement the PAB interpreter by using the EraHistory alongside the consensus functions wallclockToSlot and slotToWallclock. Here\u2019s an example function of how to use them:\n\n        -- Calculate slot number which contains a given timestamp\n        utcTimeToSlotNo\n          :: SystemStart\n          -> EraHistory CardanoMode\n          -> Time.UTCTime\n          -> Either PastHorizonException SlotNo\n        utcTimeToSlotNo systemStart (EraHistory _ interpreter) time = do\n          let relativeTime = toRelativeTime systemStart time\n          (slotNo, _, _) <- interpretQuery interpreter $ wallclockToSlot relativeTime\n          pure slotNo\n\n        slotStart\n          :: SystemStart\n          -> EraHistory CardanoMode\n          -> SlotNo\n          -> Either PastHorizonException Time.UTCTime\n        slotStart systemStart (EraHistory _ interpreter) slotNo = do\n          (relativeTime, _) <- interpretQuery interpreter $ slotToWallclock slotNo\n          pure $ fromRelativeTime systemStart relativeTime\n\n        However, we will also add an additional step. If the conversion returns PastHorizonException, then there is a good probability that the EraHistory is out of date. The reason is that EraHistory only encodes era information from the moment the user ran the query and it cannot predict the future. In that case, if the PastHorizonException is returned, we will re-query the EraHistory of the local node, replace the old value in the PAB environment, and retry the conversion. If it fails again, we return the error message."}
{"prompt":"When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of \"starting point\" projects, for different purposes.\n\n### Lein templates\nIn the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are \"rendered\" into a working project using special tooling.\n\nHowever, they have two major drawbacks:\n\n- They only work when using Leiningen as a build tool.\n- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.\n\n### Rails templates\n\nRails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. \n\nGenerators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. ","completion":"To start with, Arachne templates will be standard git repositories containing an Arachne project. They will use no special syntax, and will be valid, runnable projects out of the box.\n\nIn order to allow users to create their own projects, these template projects will include a `rename` script. The `rename` script will recursively rename an entire project directory to something that the user chooses, and will delete `.git` and re-run `git init`, \n\nTherefore, the process to start a new Arachne project will be:\n\n1. Choose an appropriate project template.\n2. Clone its git repository from Github\n3. Run the `rename` script to rename the project to whatever you wish\n4. Start a repl, and begin editing.\n\n### Maven Distribution\n\nThere are certain development environments where there is not full access to the open internet (particularly in certain governmental applications.) Therefore, accessing GitHub can prove difficult. However, in order to support developers, these organizations often run their own Maven mirrors.\n\nAs a convenience to users in these situations, when it is necessary, we can build a wrapper that can compress and install a project directory as a Maven artifact. Then, using standard Maven command line tooling, it will be possible to download and decompress the artifact into a local filesystem directory, and proceed as normal.\n\n"}
{"prompt":"Custom URI for lifecycle interface\n\nWinery can generate a lifecycle interface.\nThat interface has to take a URI for a name\n\n## Considered Alternatives\n\n* `http:\/\/opentosca.org\/interfaces\/lifecycle`\n* `http:\/\/www.example.com\/interfaces\/lifecycle` (from http:\/\/docs.oasis-open.org\/tosca\/tosca-primer\/v1.0\/tosca-primer-v1.0.html)\n* `tosca.interfaces.node.lifecycle.Standard` (from http:\/\/docs.oasis-open.org\/tosca\/TOSCA-Simple-Profile-YAML\/v1.1\/TOSCA-Simple-Profile-YAML-v1.1.html)  ","completion":"* Chosen Alternative: `http:\/\/www.example.com\/interfaces\/lifecycle`\n* Although the alternative is not standardized, it is the one consistent with the primer for TOSCA 1.0.\n\n\n"}
{"prompt":"Arachne needs to be as modular as possible. Not only do we want the\ncommunity to be able to contribute new abilities and features that\nintegrate well with the core and with eachother, we want some of the\nbasic functionality of Arachne to be swappable for alternatives as\nwell.\n\n[ADR-002](adr-002-configuration.md) specifies that one role of modules\nis to contribute schema to the application config. Other roles of\nmodules would include providing code (as any library does), and\nquerying and updating the config during the startup\nprocess. Additionally, since modules can depend upon each other, they\nmust specify which modules they depend upon.\n\nIdeally there will be as little overhead as possible for creating and\nconsuming modules.\n\nSome of the general problems associated with plugin\/module systems include:\n\n- Finding and downloading the implementation of the module.\n- Discovering and activating the correct set of installed modules.\n- Managing module versions and dependencies.\n\nThere are some existing systems for modularity in the Java\necosystem. The most notable is OSGi, which provides not only a module\nsystem addressing the concerns above, but also service runtime with\nclasspath isolation, dynamic loading and unloading and lazy\nactivation.\n\nOSGi (and other systems of comparable scope) are overkill for\nArachne. Although they come with benefits, they are very heavyweight\nand carry a high complexity burden, not just for Arachne development\nbut also for end users. Specifically, Arachne applications will be\ndrastically simpler if (at runtime) they exist as a straightforward\ncodebase in a single classloader space. Features like lazy loading and\ndynamic start-stop are likewise out of scope; the goal is for an\nArachne runtime itself to be lightweight enough that starting and\nstopping when modules change is not an issue.","completion":"Arachne will not be responsible for packaging, distribution or\ndownloading of modules. These jobs will be delegated to an external\ndependency management & packaging tool. Initially, that tool will be\nMaven\/Leiningen\/Boot, or some other tool that works with Maven\nartifact repositories, since that is currently the standard for JVM\nprojects.\n\nModules that have a dependency on another module must specify a\ndependency using Maven (or other dependency management tool.)\n\nArachne will provide no versioning system beyond what the packaging\ntool provides.\n\nEach module JAR will contain a special `arachne-modules.edn` file at\nthe root of its classpath. This data file (when read) contains a\nsequence of *module definition maps*.\n\nEach module definition map contains the following information:\n\n- The formal name of the module (as a namespaced symbol.)\n- A list of dependencies of the module (as a set of namespaced\n  symbols.) Module dependencies must form a directed acyclic graph;\n  circular dependencies are not allowed.\n- A namespace qualified symbol that resolves to the module's *schema\n  function.* A schema function is a function with no arguments that\n  returns transactable data containing the schema of the module.\n- A namespace qualified symbol that resolves to the module's\n  *configure function*. A configure function is a function that takes a\n  configuration value and returns an updated configuration.\n\nWhen an application is defined, the user must specify a set of module\nnames to use (exact mechanism TBD.) Only the specified modules (and\ntheir dependencies) will be considered by Arachne. In other words,\nmerely including a module as a dependency in the package manager is\nnot sufficient to activate it and cause it to be used in an\napplication.\n\n"}
{"prompt":"Use Markdown Architectural Decision Records\n\nShould we record the architectural decisions made in this project?\nAnd if we do, how to structure these recordings?\n\n## Considered Alternatives\n\n* [MADR](https:\/\/adr.github.io\/madr\/) - Markdown Architectural Decision Records\n* [Michael Nygard's template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) - The first incarnation of the term \"ADR\". Maintainable by [adr-tools](https:\/\/github.com\/npryce\/adr-tools).\n* [Sustainable Architectural Decisions](https:\/\/www.infoq.com\/articles\/sustainable-architectural-design-decisions) - The Y-Statements\n* [DecisionRecord](https:\/\/github.com\/schubmat\/DecisionCapture) - Agile records by [@schubmat](https:\/\/github.com\/schubmat\/)\n* Other templates listed at <https:\/\/github.com\/joelparkerhenderson\/architecture_decision_record>\n* No records","completion":"* Chosen Alternative: MADR\n* Implicit assumptions should be made explicit.\n  Design documentation is important to enable people understanding the decisions later on.\n  See also [A rational design process: How and why to fake it](https:\/\/doi.org\/10.1109\/TSE.1986.6312940).\n* The MADR template is lean and fits our development style.\n\n<!-- Pros and cons of alternatives straight-forward to elicit and therefore not captured. -->\n\n"}
{"prompt":"We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.\n\nRelational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.\n\nDocument databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.\n\nEvent databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.\n","completion":"After carefully evaluating the requirements and constraints of our application, we have decided to use a document database."}
{"prompt":"In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. \n\nAlthough we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types\/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.\n\nIdeally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:\n\n- Types of entities which can exist\n- Relationships between those types\n- Logical constraints on the values of attributes:\n    - more fine grained cardinality; optional\/required attributes\n    - valid value ranges\n    - target entity type (for ref attributes)\n\nThis additional data could serve three purposes:\n\n- Documentation about the intended purpose and structure of the configuration defined by a module.\n- Deeper, more specific validation of user-supplied configuration values\n- Machine-readable integration point for tools which consume and produce Arachne configurations.","completion":"- We will add meta-attributes to the schema of every configuration, expressing basic ontological relationships.\n- These attributes will be semantically compatible with OWL (such that we could conceivably in the future generate an OWL ontology from a config schema)\n- The initial set of these attributes will be minimal, and targeted towards the information necessary to generate rich schema diagrams\n  - classes and superclass\n  - attribute domain\n  - attribute range (for ref attributes)\n  - min and max cardinality\n- Arachne core will provide some (optional) utility functions for schema generation, to make writing module schemas less verbose.\n\n"}
{"prompt":"Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time.","completion":"Chosen option: Sentry, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it."}
{"prompt":"A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.\n\nIn a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test\/modify cycle short is an important goal.\n\nHowever, accommodating change is a source of complexity. Extra code would be required to handle  \"update\" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.\n\nThe \"simplest\" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.\n\nHowever, this simple baseline approach has two major drawbacks:\n\n1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.\n2. The developer will lose any application state stored in the components whenever the config is modified.\n\nThe startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.\n\nThe second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.\n\n\n### Prior Art\n\nThere is a library designed to solve the startup\/shutdown problem, in conjunction with Component: [Suspendable](https:\/\/github.com\/weavejester\/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive.","completion":"Whenever the configuration changes, we will use the simple approach of stopping and discarding the entire old Arachne runtime (and all its components), and starting a new one.\n\nTo mitigate the issue of lost state, Arachne will provide a new protocol called `Preservable` (name subject to change, pending a better one.) Components may optionally implement `Preservable`; it is not required. `Preservable` defines a single method, `preserve`. \n\nWhenever the configuration changes, the following procedure will be used:\n\n1. Call `stop` on the old runtime.\n2. Instantiate the new runtime.\n3. For all components in the new runtime which implement `Preservable`, invoke the `preserve` function, passing it the corresponding component from the old runtime (if there is one).\n4. The `preserve` function will selectively copy state out of the old, stopped component into the new, not-yet-started component. It should be careful not to copy any state that would be invalidated by a configuration change.\n5. Call `start` on the new runtime.\n\nArachne will not provide a mitigation for avoiding the cost of stopping and starting individual components. If this becomes a pain point, we can explore solutions such as that offered by Suspendable.\n \n"}
{"prompt":"We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.\n","completion":"rivers\n\n- **Easy to use:**  We want a framework that is simple to learn and can be integrated into our development workflow.\n\n- **Customizable:**  We want a framework that can be customized to match the client's brand guidelines and theme.\n\n- **Large community support:**  We want a framework that has an active community of developers and designers that can provide help and resources when needed.\n\n- **Time-saving:**  We want a framework that can save time and effort in styling the UI.\n\n## Considered Options\n\n- **Bootstrap:**  A popular CSS framework with a lot of pre-built components and a large community of developers.\n\n- **Foundation:**  Another popular CSS framework that was designed for mobile-first development.\n\n- **Materialize:**  A CSS framework based on Google's Material Design language with pre-built components for UI development.\n\n- **Tailwind CSS:**  A utility-first CSS framework with a focus on customization and efficiency.\n\n## Decision\n\nAfter considering the options, we decided to use Tailwind CSS for the following reasons:\n\n- **Easy to use:**  Tailwind's approach is simple and easy to understand. Its utility classes make building a UI faster.\n\n- **Customizable:**  Tailwind allows for customization by providing a set of configuration files that allow developers to modify the framework's default styles with ease.\n\n- **Large community support:**  Tailwind has a very active community of developers and designers that are consistently creating new resources, plugins, and tools.\n\n- **Time-saving:**  Tailwind can save time, enabling developers to focus on other areas of the project.\n\nWe will use Tailwind CSS's utility classes to build the UI for our project. The development team will use its atomic and modular design system to write efficient and scalable CSS code. We will leverage Tailwind's pre-built templates and visual components to speed up the development process while utilizing custom styles to match the branding and theme of the client."}
{"prompt":"Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.\n","completion":"After a thorough evaluation of various cloud infrastructure platforms, it has been decided that Amazon Web Services (AWS) will be adopted by our organization for our cloud infrastructure needs."}
{"prompt":"We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\nDecision Drivers\nShould be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles\/classes.We want a pattern for responsive props with atomic layout components.","completion":"Chosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\nExample:\n\/\/ Good:<Box padding\"small\" \/><Box padding={{xs: 'small', md: 'medium'}} \/><Input large \/><Text preset=\"heading3\" as=\"p\" \/>\n\/\/ Bad:<Box className={customLayout} \/><Input style={{ height: 50, padding: 16 }} \/><Text className={styles.heading} \/>\nPositive Consequences\nTreat is statically extracted at build time, so it has minimal runtime.Styles load in parallel with JS, also when code splitting.Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.\nNegative Consequences\nWe are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles."}
{"prompt":"We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.\n  \n## Option Considered\n\n1. React\n  \n2. Vue\n\n3. Svelte\n  ","completion":"We will use React as our front-end JavaScript library."}
{"prompt":"We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.\nDecision Drivers\nReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level","completion":"SaaS solution (DataDog) is a clear winner due to its hastle-free service usage, continuous improvement as well as its competitive pricing.\nPositive Consequences\nEasy access to observability for devs and devopsWell known integration target for third-party servicesMonitoring setup is well known in the industryNo special knowledge about setup of the plaftorm\nNegative Consequences\nNone."}
{"prompt":"Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application.","completion":"We have decided to use the Django web framework for the development of the web application. Django provides a robust set of tools and features for building web applications quickly and efficiently."}
{"prompt":"Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java.","completion":"We will adopt Go programming language for building web applications going forward."}
{"prompt":"We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.\n","completion":"onsiderations\n\n- Ease of use and maintenance\n\n- Community support and resources\n\n- Performance and scalability\n\n- Security and reliability\n\n- Cost and licensing\n\n- Compatibility with our technology stack\n\n### Considered Options\n\n- MySQL\n\n- PostgreSQL\n\n- Oracle\n\n- Microsoft SQL Server\n\n- MongoDB\n\n## Decision\n\nAfter evaluating the above options based on our decision considerations, we have decided to choose MySQL as our DBMS for Project X.\n\nMySQL is a popular open-source system with a strong development community and a large pool of resources for problem-solving and knowledge sharing. It is well-known for its excellent performance and scalability capabilities, making it ideal for handling vast amounts of data with high levels of efficiency. The platform is secure, reliable, and has a wide range of features that are essential for our project, including ACID compliance for transactions, flexible data model, and support for various programming languages and frameworks.\n\nMySQL is also compatible with the majority of our technology stack, including our web development framework, hosting solutions, and other essential tools. Plus, its cost and licensing terms are competitive compared to other proprietary systems like Oracle and Microsoft SQL Server."}
{"prompt":"Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application.","completion":"We have decided to adopt the Vue front-end JavaScript library"}
{"prompt":"We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. We\u00e2\u20ac\u2122re looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.\n","completion":"After conducting a thorough analysis of each container orchestration platform, we have decided to adopt Kubernetes as the best option for our enterprise needs."}
{"prompt":"Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended.","completion":"After weighing the pros and cons of allowing employees to work from home, the management team has decided to implement a permanent work from home policy for eligible employees. The policy applies to all full-time employees who can effectively perform their duties from home and have demonstrated success working remotely during the pandemic.\n\nEligible employees can choose to work from home up to three days per week, while in-office work will be required for the remaining two days. To ensure effective communication and collaboration, regular check-ins with team members and managers will be required. Additionally, employees will be provided with the necessary equipment and resources to perform their duties effectively from home.\n\nThe implementation of this policy will be reviewed and revisited regularly to ensure that it continues to benefit both the employees and the company."}
{"prompt":"We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. ","completion":"We have decided to implement the SvelteKit front-end architecture for our web application. SvelteKit provides an advanced front-end development system that is fast, efficient, and flexible. It can be used for optimizing large and small applications."}
{"prompt":"We want to be able to roll out new features gradually, perform A\/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.\nDecision Drivers\nEase of setupEase of maintenanceCostDeveloper experienceUsability\/UXOperational concernsHandling of PII","completion":"Chosen option: \"ConfigCat\", because:\nWe can probably get away with using it for very low costWe can start using it almost right away with little configuration\nIf we decide later that we would like some of the features of LaunchDarkly, we want to be able to quickly swap. Thus, it is vital that we write some kind of service-agnostic wrapper.\nPositive Consequences\nWe can start using feature flags across our stack.\nNegative Consequences\nComplexity of applications will increase"}
{"prompt":"There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.\n\nCurrently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant\/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.\n\nThe Contract API (defined in plutus-apps\/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations\/interpreters for one interface. Currently, there are two implementations for the Contract API:\n\n    one for the plutus-apps emulator (inside plutus-apps\/plutus-contract)\n\n    one for plutus-apps\u2019 Plutus Application Backend (inside plutus-apps\/plutus-pab)\n\nTherefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps.","completion":"We will split the plutus-apps\/plutus-contract package into two parts: the Contract API (plutus-contract) and the emulator (plutus-contract-emulator).\n\n    We will create effects for the constraints-based transaction builder library (plutus-apps\/plutus-ledger-constraints) in the Contract API. Currently, the interface and the implementation in the transaction builder library are tightly coupled. Therefore, we need to decouple them.\n\n    We will create a separate repository with the contract effects and types (the splitted plutus-contract). By moving the Contract API out of the plutus-apps monorepository, any tool could update to newer version to their discretion. Without many dependencies, many tools could utilize the Contract API without having to depend on the whole plutus-apps monorepo.\n\n    We (the Plutus Tools at IO Global) will continue to be the main maintainers of this new repository. However, a new ADR will need to be created if we ever decide to make this a community driven project.\n\n    TODO: What about governance? How do we decide which interface changes are accepted? ADRs? Who ultimately accepts and rejects them?"}
{"prompt":"We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC\/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing.","completion":"We have decided to use Docker Swarm as our container orchestration tool. Docker Swarm provides a simple and intuitive way to manage containerized applications across a cluster of nodes. It also allows us to leverage our existing Docker-based workflows and infrastructure. With Docker Swarm, we can easily deploy, scale, and manage our applications, all while taking advantage of built-in load balancing."}
{"prompt":"We need to store secrets, such as passwords, private keys, authentication tokens, etc.\n\nSome of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.\n\nSome of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.\n","completion":"Bitwarden for user-oriented secrets\n\nVault by HashiCorp for system-oriented secrets."}
{"prompt":"We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.\n\n  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.\n  \n  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.\n","completion":"Decided against Microsoft Azure DevOps."}
{"prompt":"What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.\nDecision Drivers\nSecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices","completion":"Chosen option: \"OAuth 2.0 + OpenID Connect\", because it is secure and well examined and and has support libraries for our tech stack."}
{"prompt":"- The organization is developing a web application with complex business logic and multiple integrations with third-party services.\n\n- The team has experience in building web applications with Ruby on Rails.\n\n- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability.","completion":"- Use Ruby on Rails as the framework for developing the web application."}
{"prompt":"Offer copying files from the source to the files folder\n\nForces:\n\n- Source code needs to be versioned in Winery\n- Support for scripting languages do not need any processing and only need to be copied as they are\n- Support for compiled languages need to be processed before copying\n\n## Considered Alternatives\n\n* Copying the sources to files as they are\n* Require external tooling to go from source to files","completion":" Chosen Alternative: Copying the sources as they are\n* For supporting compiled languages, it is relied on an external IDE (see [ADR-0014]).\n  This IDE stores the files in the \"source\" folder and manages the copying to the files folder.\n  Thus, the only left support is for scripting languages.\n  In that case, the source can be directly used as files (\"binary\") in an artifact template.\n\n"}
{"prompt":"As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.\nDecision Drivers\nProvide consistency when navigating the codebaseThe earlier we decide on this, the better","completion":"Chosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.\nNaming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of islandUi: import { Box } from '@island.is\/island-ui\/core'\nUse PascalCase for directories only containing React components:\ncomponents\/CtaButton\/index.tsimport 'components\/CtaButton'\nor:\ncomponents\/CtaButton\/CtaButton.tsximport 'components\/CtaButton\/CtaButton'\nrather than\ncomponents\/cta-button\/CtaButton.tsx\nIn all other cases, use camelCase.\nPositive Consequences\nEasier to navigate the codebaseFile names are more readable, and developers know what to expectThis approach is the most common practice, and something most JS and TS developers are familiar with."}
{"prompt":"In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.\n","completion":"rivers\n\n- Consistency with existing naming conventions in the project\n\n- Readability and clarity for anyone who may be working on the API\n\n- Alignment with industry best practices for REST API naming conventions\n\n- Ease of implementation and maintenance\n\n## Decision\n\nThe snake_case naming convention will be used for REST API endpoints. This choice is driven by the following factors:\n\n1. **Consistency**: The project already uses snake_case naming convention for all endpoints, and it would be beneficial to maintain this convention to ensure consistency across the entire project.\n\n2. **Readability and clarity**: The snake_case convention is more readable and easier to understand. The underscores provide a clear separation between words, making it easier to parse and understand the meaning of the name.\n\n3. **Alignment with industry best practices**: The snake_case convention is widely used in the industry and is considered to be a best practice for REST APIs, making it a good choice for the project.\n\n4. **Ease of implementation and maintenance**: Keeping with the existing naming convention is easier to implement and maintain as all existing code and documentation would need to be updated if a new convention was chosen.\n"}
{"prompt":"We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.\n\nWe interact with systems that have different timestamp formats:\n\n* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize\/deserialize.\n\n* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.\n\n* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.\n","completion":"We choose the timestamp standard format ISO 8601 with nanosecond precision, specifically \"YYYY-MM-DDTHH:MM:SS.NNNNNNNNNZ\".\n\nThe format shows the year, month, day, hour, minute, second, nanoseconds, and Zulu time zone a.k.a. UTC, GMT."}
{"prompt":"We want to record architectural decisions made in this project. Which format and structure should these records follow?","completion":"Chosen option: \"MADR 2.1.0\", because\nImplicit assumptions should be made explicit.Design documentation is important to enable people understanding the decisions later on.See also A rational design process: How and why to fake it.The MADR format is lean and fits our development style.The MADR structure is comprehensible and facilitates usage & maintenance.The MADR project is vivid.Version 2.1.0 is the latest one available when starting to document ADRs."}
{"prompt":"We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.\n\nJSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.\n\ngRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications.","completion":"After considering the pros and cons of both options, we have decided to use gRPC for our API. Although JSON over HTTP is a simpler option, we believe that gRPC will provide a more efficient and scalable solution for our service. We also anticipate that our API will handle a large amount of data, and gRPC's binary serialization will be more efficient for this use case.\n\nIn addition, we believe that gRPC's support for bidirectional streaming will be beneficial for real-time applications that we may develop in the future."}
{"prompt":"Provide Support for Custom Key-Value Properties\n\nMost properties are key\/value\n\n## Considered Alternatives\n\n* Provide support for custom key-value properties\n* Support XML-only","completion":"* Chosen Alternative: Provide support for custom key-value properties\n* Nice UI experience\n\n"}
{"prompt":"Arachne has a number of goals.\n\n1. It needs to be *modular*. Different software packages, written by\n   different developers, should be usable and swappable in the same\n   application with a minimum of effort.\n\n2. Arachne applications need to be *transparent* and\n   *introspectable*. It should always be as clear as possible what is\n   going on at any given moment, and why the application is behaving\n   in the way it does.\n\n3. As a general-purpose web framework, it needs to provide a strong\n   set of default settings which are also highly overridable, and\n   *configurable* to suit the unique needs of users.\n\n\nAlso, it is a good development practice (particularly in Clojure) to\ncode to a specific information model (that is, data) rather than to\nparticular functions or APIs. Along with other benefits, this helps\nseparate (avoids \"complecting\") the intended operation and its\nimplementation.\n\nDocumenting the full rationale for this \"data first\" philosophy is\nbeyond the scope of this document, but some resources that explain it (among other things) are:\n\n- [Simple Made Easy](http:\/\/www.infoq.com\/presentations\/Simple-Made-Easy) - Rich Hickey\n- [Narcissistic Design](https:\/\/vimeo.com\/77199361) - Stuart Halloway\n- [Data Beats Functions](https:\/\/malcolmsparks.com\/posts\/data-beats-functions.html) - Malcolm Sparks\n- [Always Be Composing](https:\/\/www.youtube.com\/watch?v=3oQTSP4FngY) - Zach Tellman\n- [Data > Functions > Macros](http:\/\/www.lispcast.com\/data-functions-macros-why) - Eric Normand\n\nFinally, one weakness of many existing Clojure libraries, especially\nweb development libraries, is the way in which they overload the\nClojure runtime (particularly vars and reified namespaces) to store\ninformation about the webapp. Because both the Clojure runtime and\nmany web application entities (e.g servers) are stateful, this causes\na variety of issues, particularly with reloading namespaces. Therefore,\nas much as possible, we would like to avoid entangling information\nabout an Arachne application with the Clojure runtime itself.","completion":"Arachne will take the \"everything is data\" philosophy to its logical\nextreme, and encode as much information about the application as\npossible in a single, highly general data structure. This will include\nnot just data that is normally thought of as \"config\" data, but the\nstructure and definition of the application itself. Everything that\ndoes not have to be arbitrary executable code will be\nreflected in the application config value.\n\nSome concrete examples include (but are not limited to):\n\n- Dependency injection components\n- Runtime entities (servers, caches, connections, pools, etc)\n- HTTP routes and middleware\n- Persistence schemas and migrations\n- Locations of static and dynamic assets\n\nThis configuration value will have a *schema* that defines what types\nof entities can exist in the configuration, and what their expected\nproperties are.\n\nEach distinct module will have the ability to contribute to the schema\nand define entity types specific to its own domain. Modules may\ninteract by referencing entity types and properties defined in other\nmodules.\n\nAlthough it has much in common with a fully general in-memory\ndatabase, the configuration value will be a single immutable value,\nnot a stateful data store. This will avoid many of the complexities\nof state and change, and will eliminate the temptation to use the\nconfiguration itself as dynamic storage for runtime data.\n\n"}
{"prompt":"Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.\n\n## Priorities\n\nThe architecture for programming code editors should prioritize the following:\n\n* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.\n\n* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.\n\n* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.\n\n* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.\n\n* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers.","completion":"Based on these priorities, the architecture for programming code editors should be designed with the following components:\n\n* **Core**: This component provides the basic functionality of the code editor, such as syntax highlighting, text editing, and file management.\n\n* **UI**: This component provides the user interface for the code editor, including menus, toolbars, and keyboard shortcuts.\n\n* **Plugins**: This component allows developers to extend the functionality of the code editor by installing third-party plugins. Plugins can provide additional features, such as code completion, linting, or debugging.\n\n* **Integrations**: This component allows the code editor to integrate with other tools and technologies, such as version control systems, build systems, or debugging tools."}
{"prompt":"OAuth with GitHub\n\n**UserStory:**\n\nIn order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.\n  Therefore, the OAuth flow is implented as follows:\n  \n1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.\n    1. Login and authorize the Application to access the private repositories.\n    1. Or, if the application was already been authorized, GitHub automatically continues with step 2\n1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.\n1. Our server responses with the access token.\n\n![OAuth flow diagram](graphics\/oauth-flow.png)  \n\n## Considered Alternatives\n* *[ALTERNATIVE 1]* Perform the whole login process in the browser\n* *[ALTERNATIVE 2]* Perform the whole login process in the backend\n* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token","completion":" *Chosen Alternative: [ALTERNATIVE 3]*\n   because we can easily protect our client secret on the server, store the token safely in the clients local storage and \n   do not need to keep the state at the server. Further, it is possible to save additional user information in the local\n   storage without the need for getting it every time from the server.\n\n"}
{"prompt":"[ADR-002](adr-002-configuration.md) indicates that we will store the\nentire application config in a single rich data structure with a schema.\n\n### Config as Database\n\nThis implies that it should be possible to easily search, query and\nupdate the configuration value. It also implies that the configuration\nvalue is general enough to store arbitrary data; we don't know what\nkinds of things users or module authors will need to include.\n\nIf what we need is a system that allows you to define, query, and\nupdate arbitrary data with a schema, then we are looking for a\ndatabase.\n\nRequired data store characteristics:\n\n1. It must be available under a permissive open source\n   license. Anything else will impose unwanted restrictions on who can\n   use Arachne.\n2. It can operate embedded in a JVM process. We do not want to force\n   users to install anything else or run multiple processes just to\n   get Arachne to work.\n3. The database must be serializable. It must be possible to write the\n   entire configuration to disk, and then reconstitute it in the same\n   exact state in a separate process.\n4. Because modules build up the schema progressively, the schema must\n   be inherently extensible. It should be possible for modules to\n   progressively add both new entity types and new attributes to\n   existing entity types.\n5. It should be usable from Clojure without a painful impedance mismatch.\n\n### Configuration as Ontology\n\nAs an extension of the rationale discussed in\n[ADR-002](adr-002-configuration.md), it is useful to enumerate the\npossible use cases of the configuration and configuration schema\ntogether.\n\n- The configuration is read by the application during bootstrap and\n  controls the behavior of the application.\n- The configuration schema defines what types of values the\n  application can or will read to modify its structure and behavior at\n  boot time and run time.\n- The configuration is how an application author communicates their\n  intent about how their application should fit together and run, at a\n  higher, more conceptual level than code.\n- The configuration schema is how module authors communicate to\n  application authors what settings, entities and structures\n  are available for them to use in their applications.\n- The configuration schema is how module authors communicate to other\n  potential module authors what their extension points are; module\n  extenders can safely read or write any entities\/attributes declared\n  by the modules upon which they depend.\n- The configuration schema can be used to validate a particular\n  configuration, and explain where and how it deviates from what is\n  actually supported.\n- The configuration can be exposed (via user interfaces of various\n  types) to end users for analytics and debugging, explaining the\n  structure of their application and why things are the way they are.\n- A serialization of the configuration, together with a particular\n  codebase (identified by a git SHA) form a precise, complete, 100%\n  reproducible definition of the behavior of an application.\n\nTo the extent that the configuration schema expresses and communicates\nthe \"categories of being\" or \"possibility space\" of an application, it\nis a formal [Ontology](https:\/\/en.wikipedia.org\/wiki\/Ontology). This is\na desirable characteristic, and to the degree that it is practical to\ndo so, it will be useful to learn from or re-use existing work around\nformal ontological systems.\n\n\n### Implementation Options\n\nThere are instances of four broad categories of data stores that match\nthe first three of the data store characteristics defined above.\n\n- Relational (Derby, HSQLDB, etc)\n- Key\/value (BerkelyDB, hashtables, etc)\n- RDF\/RDFs\/OWL stores (Jena)\n- Datomic-style (Datascript)\n\nWe can eliminate relational solutions fairly quickly; SQL schemas are\nnot generally extensible or flexible, failing condition #4. In\naddition, they do not fare well on #5 -- using SQL for queries and updates\nis not particularly fluent in Clojure.\n\nSimilarly, we can eliminate key\/value style data stores. In general,\nthese do not have schemas at all (or at least, not the type of rich\nschema that provides a meaningful data contract or ontology, which is the point\nfor Arachne.)\n\nThis leaves solutions based on the RDF stack, and Datomic-style data\nstores. Both are viable options which would provide unique benefits\nfor Arachne, and both have different drawbacks.\n\nExplaining the core technical characteristics of RDF\/OWL and Datomic\nis beyond the scope of this document; please see the\n[Jena](https:\/\/jena.apache.org\/documentation\/index.html) and\n [Datomic](http:\/\/docs.datomic.com) documentation for more\ndetails. More information on RDF, OWL and the Semantic web in general:\n\n- [Wikipedia article on RDF](https:\/\/en.wikipedia.org\/wiki\/Resource_Description_Framework)\n- [Wikipedia article on OWL](https:\/\/en.wikipedia.org\/wiki\/Web_Ontology_Language)\n- [OWL Semantics](http:\/\/www.w3.org\/TR\/owl-semantics\/) standards document.\n\n### RDF\n\nThe clear choice for a JVM-based, permissively licensed,\nstandards-compliant RDF API is Apache Jena.\n\n#### Benefits for Arachne\n\n- OWL is a good fit insofar as Arachne's goal is to define an\n  ontology of applications. The point of the configuration schema is\n  first and foremost to serve as unambiguous communication regarding\n  the types of entities that can exist in an application, and what the\n  possible relationships between them are. By definition, this is\n  defining an ontology, and is the exact use case which OWL is\n  designed to address.\n- Information model is a good fit for Clojure: tuples and declarative logic.\n- Open and extensible by design.\n- Well researched by very smart people, likely to avoid common\n  mistakes that would result from building an ontology-like system\n  ourselves.\n- Existing technology, well known beyond the Clojure\n  ecosystem. Existing tools could work with Arachne project\n  configurations out of the box.\n- The open-world assumption is a good fit for Arachne's per-module\n  schema modeling, since modules cannot know what other modules might\n  be present in the application.\n- We're likely to want to introduce RDFs\/OWL to the application\n  anyway, at some point, as an abstract entity meta-schema (note: this\n  has not been firmly decided yet.)\n\n#### Tradeoffs for Arachne (with mitigations)\n\n- OWL is complex. Learning to use it effectively is a skill in its own\n  right and it might be asking a lot to require of module authors.\n- OWLs representation of some common concepts can be verbose and\/or\n  convoluted in ways that would make schema more difficult to\n  read\/write. (e.g, Restriction classes)\n- OWL is not a schema. Although the open world assumption is valid and\n  good when writing ontologies, it means that OWL inferencing is\n  incapable of performing many of the kind of validations we would\n  want to apply once we do have a complete configuration and want to\n  check it for correctness. For example, open-world reasoning can\n  never validate a `owl:minCardinality` rule.\n    - Mitigation: Although OWL inferencing cannot provide closed-world\n    validation of a given RDF dataset, such tools do exist. Some\n    mechanisms for validating a particular closed set of RDF triples\n    include:\n       1. Writing SPARQL queries that catch various types of validation errors.\n       2. Deriving validation errors using Jena's rules engine.\n       3. Using an existing RDF validator such as\n      [Eyeball](https:\/\/jena.apache.org\/documentation\/tools\/eyeball-getting-started.html)\n      (although, unfortunately, Eyeball does not seem to be well\n      maintained.)\n    - For Clojure, it would be possible to validate a given OWL class\n      by generating a specification using `clojure.spec` that could be\n      applied to concrete instances of the class in their map form.\n- Jena's API is aggressively object oriented and at odds with Clojure\n  idioms.\n    - Mitigation: Write a data-oriented wrapper (note: I have a\n    working proof of concept already.)\n- SPARQL is a string-based query language, as opposed to a composable data API.\n    - Mitigation: It is possible to hook into Jena's ARQ query engine\n      at the object layer, and expose a data-oriented API from there,\n      with SPARQL semantics but an API similar to Datomic datalog.\n- OWL inferencing is known to have performance issues with complex\n  inferences. While Arachne configurations are tiny (as knowledge bases\n  go), and we are unlikely to use the more esoteric derivations, it is\n  unknown whether this will cause problems with the kinds of\n  ontologies we do need.\n    - Mitigation: We could restrict ourselves to the OWL DL or even\n      OWL Lite sub-languages, which have more tractable inferencing\n      rules.\n- Jena's APIs are such that it is impossible to write an immutable\n  version of a RDF model (at least without breaking most of Jena's\n  API.) It's trivial to write a data-oriented wrapper, but intractable\n  to write a persistent immutable one.\n\n### Datomic\n\nNote that Datomic itself does not satisfy the first requirement; it is\nclosed-source, proprietary software. There *is* an open source\nproject, Datascript, which emulates Datomic's APIs (without any of the\nstorage elements). Either one would work for Arachne, since Arachne\nonly needs the subset of features they both support. In, fact, if\nArachne goes the Datomic-inspired route, we would probably want to\nsupport *both*: Datomic, for those who have an existing investment\nthere, and Datascript for those who desire open source all the way.\n\n#### Benefits for Arachne\n\n- Well known to most Clojurists\n- Highly idiomatic to use from Clojure\n- There is no question that it would be performant and technically\n  suitable for Arachne-sized data.\n- Datomic's schema is a real validating schema; data transacted to\n  Datomic must always be valid.\n- Datomic Schema is open and extensible.\n\n#### Tradeoffs for Arachne (with mitigations)\n\n- The expressivity of Datomic's schema is anemic compared to RDFs\/OWL;\n  for example, it has no built-in notion of types. It is focused\n  towards data storage and integrity rather than defining a public\n  ontology, which would be useful for Arachne.\n    - Mitigation: If we did want something more ontologically focused,\n      it is possible to build an ontology system on top of Datomic\n      using meta-attributes and Datalog rules. Examples of such\n      systems already exist.\n- If we did build our own ontology system on top of Datomic (or use an\n  existing one) we would still be responsible for \"getting it right\",\n  ensuring that it meets any potential use case for Arachne while\n  maintaining internal and logical consistency.\n    - Mitigation: we could still use the work that has been done in\n      the OWL world and re-implement a subset of axioms and\n      derivations on top of Datomic.\n- Any ontological system built on top of Datomic would be novel to\n  module authors, and therefore would require careful, extensive\n  documentation regarding its capabilities and usage.\n- To satisfy users of Datomic as well as those who have a requirement\n  for open source, it will be necessary to abstract across both\n  Datomic and Datascript.\n    - Mitigation: This work is already done (provided users stay\n      within the subset of features that is supported by both\n      products.)","completion":"The steering group decided the RDF\/OWL approach is too high-risk to\nwrap in Clojure and implement at this time, while the rewards are\nmostly intangible \"openness\" and \"interoperability\" rather than\nsomething that will help move Arachne forward in the short term.\n\nTherefore, we will use a Datomic style schema for Arachne's configuration.\n\nUsers may use either Datomic Pro, Datomic Free or Datascript at\nruntime in their applications. We will provide a \"multiplexer\"\nconfiguration implementation that utilizes both, and asserts that the\nresults are equal: this can be used by module authors to ensure they\nstay within the subset of features supported by both platforms.\n\nBefore Arachne leaves \"alpha\" status (that is, before it is declared\nready for experimental production use or for the release of\nthird-party modules), we will revisit the question of whether OWL\nwould be more appropriate, and whether we have encountered issues that\nOWL would have made easier. If so, and if time allows, we reserve the\noption to either refactor the configuration layer to use Jena as a\nprimary store (porting existing modules), or provide an OWL\nview\/rendering of an ontology stored in Datomic.\n\n"}
{"prompt":"Wrap properties in TOSCA properties element\n\nWhen GETting\/PUTting the properties of an entitty template, the content has to be serialized somehow.\n\n## Considered Alternatives\n\n* Wrap properties in TOSCA properties element\n* Use nested XML element (`getAny()`)","completion":"* Chosen Alternative: Wrap properties in TOSCA properties element\n* Receiving an XML element is not possible with JAX-B\/JAX-RS as that setting relies on strong typing.\n\n"}
{"prompt":"Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?\nDecision Drivers\nShould use NodeJS and React as outlined in S\u00cd technical direction\u200bShould be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design","completion":"Chosen option: NextJS + NestJS\nNextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using Remark library.\nNestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing."}
{"prompt":"Let\u2019s start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).\n\n-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address\n--   and a 'TxIn' transaction input that can spend it.\npubKeyContract\n    :: forall w s e.\n    ( AsPubKeyError e\n    )\n    => PaymentPubKeyHash\n    -> Value\n    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)\npubKeyContract pk vl = mapError (review _PubKeyError   ) $ do\n    -- Step 1\n    let inst = typedValidator pk\n        address = Scripts.validatorAddress inst\n        tx = Constraints.mustPayToTheScriptWithDatumHash () vl\n    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx\n               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx\n\n    -- Step 2\n    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)\n\n    -- Step 3\n    let refs = Map.keys\n               $ Map.filter ((==) address . txOutAddress)\n               $ getCardanoTxProducedOutputs ledgerTx\n    case refs of\n        []                   -> throwing _ScriptOutputMissing pk\n        [outRef] -> do\n            -- Step 4\n            ciTxOut <- unspentTxOutFromRef outRef\n            pure (outRef, ciTxOut, inst)\n        _                    -> throwing _MultipleScriptOutputs pk\n\nHere\u2019s an outline of the contract\u2019s steps:\n\n    Creates a transaction and submits it to the node\n\n    Waits for transaction to be confirmed\n\n    Finds the first UTXO of that transaction (return type TxOutRef)\n\n    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef\n\nThe problem is that the ciTxOut variable in step 4 will almost always result in Nothing.\n\nWhy? Here\u2019s some context.\n\nThe PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.\n\nMeanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.\n\nThis brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.\n\nCurrently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing.","completion":"The best solution is probably a combination of the Alternative solutions described below. However, we will mainly choose the Query functions should interact with a single source of truth solution.\n\n    We will replace plutus-chain-index with Marconi as PAB\u2019s indexing component\n\n    We will move out the blockchain information indexed by PAB in Marconi\n\n    We will add new indexers in Marconi in order to replicate the information indexed by plutus-chain-index\n\n    We will adapt the architecture of Marconi (which will become our new indexing component) to support waiting queries\n\n    Since we suppose that indexing component should be in the same machine as the PAB, then we will use Marconi as a library to index and query the indexed blockchain information without relying on an HTTP API"}
{"prompt":"Is there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?\nRequirements\nSince the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student \/ startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for\nRegistration of services with rate limitSelf-service portalAPI key for ConsumerRate limit for Consumer\nProvider\nRegister open service in API gateway.Set rate limit on open service in API gateway.\nConsumer\nRegister as service user.Register application intended to use API (Consideration).Get API key for that application or consumer.Register what API to use in the application.Ability to test the API from API console with application API key (Consideration).\nConsiderations\nAPI Keys \/ Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.\nConsumer registration What will be used to validate \/ approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.\nDecision Drivers\nVendor lock in for runtimeOpen source or notInstallation optionsFunctional abilityMarket presencePricing\nPricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.\nThere is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.\nFunctional ability for decision\nFunctional ability to consider when evaluating the tool. The following list was taken from wikipedia page for API Management.\nGateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.Developer portal\/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","completion":"We recommend using hosted solution from AWS for API gateway. The reason is that if we look at the requirements and other architectural decisions in the project, the AWS solution does both fit in the architecture and pricing based on usage is cheaper than in other options. We made a pricing estimate for five years. Based on 100 million API calls per month the price of using AWS is one third of bought enterprise or homemade solutions. If the usage is 20 million API calls per month, we are looking at one fifth of the enterprise solutions. Note that there is also cost in using the management API and storage cost of logging, but it seems to be fraction of the total cost of using the product. Since the requirement is to host X-Road services that are defined as open and hosted at organization, we need to access the service through open X-Road server. Currently there will be installed X-Road server on AWS environment to use by island.is, that server can also be used by API gateway. Downside is that all request needs to go to the AWS environment, and then go back to Iceland. According to vendor lock in, then the investment cost of this option is not in the range that it will stop us to change to other solution. That could happen if requirement changes or the usage will be more than expected. This decision is based on the requirements and intended usage. If those requirements change, for example we would use the API portal as portal for Viskuausan, or more intense usage is expected, other options could be more relevant.\nFollowing is the decision phases used to get this conclusion.\nSecond phase of decision\nFor Open Source tool, we recommend usage of Kong Community Edition, with custom made API Developer Portal and Analytics. The analytics part could be based on ELK stack through plugin. The Kong community is large, and there are some Developers Portals available in the Open Source community. There is also some plugin available for logging and monitoring available. Kong API Gateway provides rest interface that can be used for customizing API Portal, and ability to create custom plugins for custom implementations. This decision provides more custom code to be developed and we need to rely on that the community can provide plugins. We also rely on that the Kong product will remain open source, but lot of plugins are only available for Enterprise edition.\nWSO2 could be considered, since the whole suite is Open Source, so API Developer Portal and Admin UI is part of their Open Source offering. It is not recommend to use it without paid support plan.\nFor Vendor specific tool we recommend Software AG API Management. It is fully functional with customizable Developer Portal, and analysis tool. It has both partner and customers in Iceland. Current pricing model is based on transaction count and same applies for On-Prem vs. Hosted implementations. There are no cloud provider or runtime lock in. Implementation is that the tool needs to be installed and configured. The Developer Portal needs to be customized. For starter it is also option to host the installation at Advania for further evaluation. Analytics are fully integrated to ELK stack.\nFor hosted solutions, we recommend AWS, since it best fits the architectural decisions made for island.is\nFirst phase of decision\nIn the first decision phase the following tools were initially pinpointed for further analysis. That was based on the option to run the API Management tool on premise. In decision outcome above, the tools have been narrowed to two options.\nIf open source options are not a requirement, it is suggested to evaluate the following tools.\n\u200bSoftware AG API Management\u200b\u200bIBM API Connect\u200b\u200bGoogle Apigee Edge\u200b\u200bMulesoft Anypoint\u200b\u200bAxway Ampify\u200b\nThese are the tools that are most mature, and do not provide lock in for runtime platform. They need to be evaluated based on pricing and technical ability.\nIf true open source is required, it is suggested to evaluate the following tools.\n\u200bWSO2\u200b\u200bKong\u200b\nThese tools provide open source offering, but in most cases the features for Api Developer Portal and Publishing tools are only part of enterprise offering with subscription. Evaluation is needed for validating if the open source offering of the tool contains what is needed for implementation. We need to evaluate pricing of enterprise offering against the price of creating\/implementing required pieces, like custom Api Developer Portal.\nFor hosted solution, the following options is considered\n\u200bAWS\u200b\u200bMicrosoft Azure API Management\u200b\u200bGoogle Apigee Edge\u200b\nDownside is that they are all platform dependent, to the owners proposed platform with more limited on-premise options. These are all top of the line tools according to capabilities.\nFor all considered tools we need to check what underlying software components are required. For example, data storage, queuing, and logging capacity. We need to take into consideration effort and ability to build Developer portal, compared to customizing the tool offering.\nOther tools that we looked at did in our opinion lack functionality or other ability for further considered, even though many of them could be considered. Note that the list provided is not all existing Api Management tools, so other options might apply.\nPositive Consequences\nApi Management tools are listed and grouped based on if they have open source option or not.\nNegative Consequences\nAll considered options are vendor lock in."}
{"prompt":"We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.\nTerms\ncode integration - this is a process that checks the integrity\/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results\npushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.\nDecision Drivers (Policy)\nThe code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations\nBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience\nEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process\nNeeds more knowledge and experience\nBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general\nPlatform independenceRepeatabilitySecurity\nNeeds expertise in Dockerfile and Docker in general\nBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\nBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology\nCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\nBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations\nSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows\nDevs that use non-Linux OS might need to install additional software and customizations\nCI Platform ","completion":"GitHub Actions\nNumber 1 CI platform on GitHub at the time of this writingEasy customization of which parts of the CI process to run depending on branching patterns and pull requestsGood integration of code health with the pull request processAs a GitHub open-source project, we have an unlimited number of \"compute\"-minutes that come as a part of the packageSupports parallelisation of the process which can be pretty important in the context of monorepoSupport using own runners which can be helpful to maximize speed, minimize costs and increase security."}
{"prompt":"Double Encoded URLs\n\nURLs should be human-readable, slashes are not allowed.\nEncoded slahes are not enabled as default due to security reasons.\n\nMore information about encoding is available at [dev\/Encoding.md](dev\/Encoding).\n\n## Considered Alternatives\n\n* Using namespace prefixes in the URLs\n* Single-encoded URLs and forcing the environment to be reconfigered\n* Double-encoded URLs","completion":"* *Chosen Alternative: Double-encoded URLs*\n\n"}
{"prompt":"We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application.","completion":"After considering the benefits and drawbacks of each library, we have decided to use Svelte as our front-end JavaScript library for the following reasons:\n\n1. **Lightweight:**  Svelte is one of the lightest libraries available, which will help us keep our application size small and improve page load times.\n\n2. **Fast:**  Svelte is known for its fast rendering speed, which will make our application feel responsive and snappy.\n\n3. **Easy to use:**  Svelte's syntax is simple and easy to understand, which will make it easier for our team to adopt it and write clean code.\n\n4. **Optimized for UI rendering:**  Svelte is optimized for rendering UI components, which will make it easier for us to create dynamic and reusable UI elements.\n\n5. **Good documentation and community support:**  Svelte has a growing community and good documentation, which will help us get started quickly and resolve any issues that arise."}
{"prompt":"End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger\/node\/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.\n\nThere is also ongoing development work to separate the functionality of cardano-api out into two packages:\n\n    cardano-ledger-api handles the building and balancing of transactions.\n\n    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.\n\nBoth of these packages are in early stage development and will require end-to-end test coverage.\n\nThis document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.\n\nThe exploratory testing approach is not in the scope of this document.","completion":"We will create a new end-to-end testing framework written in Haskell called plutus-e2e-tests that will initially be a package in plutus-apps, see argument 1.\n\n    We will use cardano-testnet for configuring and initialising local test network environments, see argument 2.\n\n    We will initially use cardano-api for building and balancing transactions, and to submit balanced transactions and for querying the ledger, see argument 3.\n\n    When available, we will use cardano-ledger-api instead of cardano-api for building and balancing transactions, see argument 4.\n\n    When available, we will use cardano-node-client instead of cardano-api to submit balanced transactions and for querying the ledger state to make test assertions, see argument 5.\n\n    We will prioritise Plutus test coverage over cardano-node, see argument 6.\n\n    We will start by creating a few tests with the node\/ledger apis without depending on plutus-apps and then assess whether we want to use the Contract API and other off-chain tooling going forwards, see argument 7.\n\n    We will continue adding a subset of Plutus tests to cardano-node-tests, see argument 8."}
{"prompt":"Most applications need to store and manipulate data. In the current state of the art in Clojure, this is usually done in a straightforward, ad-hoc way. Users write schema, interact with their database, and parse data from user input into a persistence format using explicit code.\n\nThis is acceptable, if you're writing a custom, concrete application from scratch. But it will not work for Arachne. Arachne's modules need to be able to read and write domain data, while also being compatible with multiple backend storage modules. \n\nFor example a user\/password based authentication module needs to be able to read and write user records to the application database, and it should work whether a user is using a Datomic, SQL or NoSQL database.\n\nIn other words, Arachne cannot function well in a world in which every module is required to interoperate directly against one of several alternative modules. Instead, there needs to be a way for modules to \"speak a common language\" for data manipulation and persistence.\n\n### Other use cases\n\nData persistence isn't the only concern. There are many other situations where having a common, abstract data model is highly useful. These include:\n\n- quickly defining API endpoints based on a data model\n- HTML & mobile form generation\n- generalized data validation tools\n- unified administration & metrics tools\n\n### Modeling & Manipulation\n\nThere are actually two distinct concepts at play; data *modeling* and data *manipulation*.\n\n**Modeling** is the activity of defining the abstract shape of the data; essentially, it is writing schema, but in a way that is not specific to any concrete implementation. Modules can then use the data model to generate concrete schema, generate API endpoints, forms, validate data, etc.\n\n**Manipulation** is the activity of using the model to create, read update or delete actual data. For an abstract data manipulation layer, this generally means a polymorphic API that supports some common set of implementations, which can be extended to concrete CRUD operations \n\n### Existing solutions: ORMs\n\nMost frameworks have some answer to this problem. Rails has ActiveRecord, Elixir has Ecto, old-school Java has Hibernate, etc. In every case, they try to paper over what it looks like to access the actual database, and provide an idiomatic API in the language to read and persist data. This language-level API is uniformly designed to make the database \"easy\" to use, but also has the effect of providing a common abstraction point for extensions.\n\nUnfortunately, ORMs also exhibit a common set of problems. By their very nature, they are an extra level of indirection. They provide abstraction, but given how complex databases are the abstraction is always \"leaky\" in significant ways. Using them effectively requires a thorough understanding not only of the ORM's APIs, but also the underlying database implementation, and what the ORM is doing to map the data from one format to another.\n\nORMs are also tied more or less tightly to the relational model. Attempts to extend ActiveRecord (for example) to non-relational data stores have had varying levels of success.\n\n### Database \"migrations\"\n\nOne other function is to make sure that the concrete database schema matches the abstract data model that the application is using. Most ORMs implement this using some form of \"database migrations\", which serve as a repeatable series of all changes made to a database. Ideally, these are not redundant with the abstract data model, to avoid repeating the same information twice and also to ensure consistency.","completion":"Arachne will provide a lightweight model for data abstraction and persistence, oriented around the Entity\/Attribute mode. To avoid word salad and acronyms loaded with baggage and false expectations, we will give it a semantically clean name. We will be free to define this name, and set expectations around what it is and how it is to be used. I suggest \"Chimera\", as it is in keeping with the Greek mythology theme and has several relevant connotations.\n\nChimera consists of two parts:\n\n- An entity model, to allow application authors to easily specify the shape of their domain data in their Arachne configuration.\n- A set of persistence operations, oriented around plain Clojure data (maps, sets and vectors) that can be implemented meaningfully against multiple types of adapters. Individual operations are granular and can be both consumed and provided \u00c3\u00a1 la carte; adapters that don't support certain behaviors can omit them (at the cost of compatibility with modules that need them.)\n\nAlthough support for any arbitrary database cannot be guaranteed, the persistence operations are designed to support a majority of commonly used systems, including relational SQL databases, document stores, tuple stores, Datomic, or other \"NoSQL\" type systems.\n\nAt the data model level, Chimera should be a powerful, easy to use way to specify the structure of your data, as data. Modules can then read this data and expose new functionality driven by the application domain model. It needs to be flexible enough that it can be \"projected\" as schema into diverse types of adapters, and customizable enough that it can be configured to adapt to existing database installations.\n\n#### Adapters\n\nChimera _Adapters_ are Arachne modules which take the abstract data structures and operations defined by Chimera, and extend them to specific databases or database APIs such as JDBC, Datomic, MongoDB, etc.\n\nWhen applicable, there can also be \"abstract adapters\" that do the bulk of the work of adapting Chimera to some particular genre of database. For example, most key\/value stores have similar semantics and core operations: there will likely be a \"Key\/Value Adapter\" that does the bulk of the work for adapting Chimera's operations to key\/value storage, and then several thin _concrete_ adapters that implement the actual get\/put commands for Cassandra, DynamoDB, Redis, etc.\n\n### Limitations and Drawbacks\n\nChimera is designed to make a limited set of common operations *possible* to write generically. It is not and cannot ever be a complete interface to every database. Application developers _can_ and _should_ understand and use the native APIs of their selected database, or use a dedicated wrapper module that exposes the full power of their selected technology. Chimera represents only a single dimension of functionality; the entity\/attribute model. By definition, it cannot provide access to the unique and powerful features that different databases provide and which their users ought to leverage.\n\nIt is also important to recognize that there are problems (even problems that modules might want to tackle) for which Chimera's basic entity\/attribute model is simply not a good fit. If the entity model isn't a good fit, <u>do not use<\/u> Chimera. Instead, find (or write) an Arachne module that defines a data modeling abstraction better suited for the task at hand. \n\nExamples of applications that might not be a good fit for Chimera include:\n\n- Extremely sparse or \"wide\" data\n- Dynamic data which cannot have pre-defined attributes or structure\n- Unstructured heterogeneous data (such as large binary or sampling data)\n- Data that cannot be indexed and requires distributed or streaming data processing to handle effectively\n\n### Modeling\n\nThe data model for an Arachne application is, like everything else, data in the Configuration. Chimera defines a set of DSL forms that application authors can use to define data models programmatically, and of course modules can also read, write and modify these definitions as part of their normal configuration process.\n\nNote: The configuration schema, including the schema for the data model, is _itself_ defined using Chimera. This requires some special bootstrapping in the core module. It also implies that Arachne core has a dependency on Chimera. This does not mean that modules are required to use Chimera or that Chimera has some special status relative to other conceivable data models; it just means that it is a good fit for modeling the kind of data that needs to be stored in the configuration.\n\n#### Modeling: Entity Types\n\n_Entity types_ are entities that define the structure and content for a _domain entity_. Entity types specify a set of optional and required _attributes_ that entities of that type must have.\n\nEntity types may have one or more supertypes. Semantically, supertypes imply that any entity which is an instance of the subtype is also an instance of the supertype. Therefore, the set of attributes that are valid or required for an entity are the attributes of its types and all ancestor types.\n\nEntity types define only data structures. They are not objects or classes; they do not define methods or behaviors.\n\nIn addition to defining the structure of entities themselves, entity types can have additional config attributes that serve as implementation-specific hints. For example, an entity type could have an attribute to override the name of the SQL table used for persistence. This config attribute would be defined and used by the SQL module, not by Chimera itself.\n\nThe basic attributes of the entity type, as defined by Chimera, are:\n\n- The name of the type (as a namespace-qualified keyword)\n- Any supertypes it may have\n- What attributes can be applied to entities of this type\n\n#### Attribute Definitions\n\nAttribute Definition entities define what types of values can be associated with an entity. They specify:\n\n1. The name of the attribute (as a namespace-qualified keyword)\n1. The min and max cardinality of an attribute (thereby specifying whether it is required or optional)\n1. The type of allowed values (see the section on _Value Types_ below)\n1. Whether the attribute is a _key_. The values of a key attribute are expected to be globally unique, guaranteed to be present, and serve as a way to find specific entities, no matter what the underlying storage mechanism.\n1. Whether the attribute is _indexed_. This is primarily a hint to the underlying database implementation.\n\nLike entity types, attribute definitions may have any number of additional attributes, to modify behavior in an implementation-specific way.\n\n##### Value Types\n\nThe value of an attribute may be one of three types:\n\n1. A **reference** is a value that is itself an entity. The attribute must specify the entity type of the target entity. \n2. A **component** is a reference, with the added semantic implication that the value entity is a logical \"part\" of the parent entity. It will be retrieved automatically, along with the parent, and will also be deleted\/retracted along with the parent entity.\n3. A **primitive** is a simple, atomic value. Primitives may be one of several defined types, which map more or less directly to primitive types on the JVM:\n   - Boolean (JVM `java.lang.Boolean`) \n   - String (JVM `java.lang.String`)\n   - Keyword (Clojure `clojure.lang.Keyword`)\n   - 64 bit integer (JVM `java.lang.Long`)\n   - 64 bit floating point decimal (JVM `java.lang.Double`)\n   - Arbitrary precision integer (JVM `java.math.BigInteger`)\n   - Arbitrary precision decimal (JVM `java.math.BigDecimal`)\n   - Instant (absolute time with millisecond resolution) (JVM `java.util.Date`)\n   - UUID (JVM `java.util.UUID`)\n   - Bytes (JVM byte array). Since not all storages support binary data, and might need to serialize it with base64, this should be fairly small.\n   \n   This set of primitives represent a reasonable common denominator that is supportable on most target databases. Note that the set is not closed: modules can specify new primitive types that are logically \"subtypes\" of the generic primitives. Entirely new types can also be defined (with the caveat that they will only work with adapters for which an implementation has been defined.)\n   \n#### Validation\n\nAll attribute names are namespace-qualified keywords. If there are specs registered using those keywords, they can be used to validate the corresponding values. \n\nClojure requires that a namespace be loaded before the specs defined in it are globally registered. To ensure that all relevant specs are loaded before an application runs, Chimera provides config attributes that specify namespaces containing specs. Arachne will ensure that these namespaces are loaded first, so module authors can ensure that their specs are loaded before  they are needed.\n\nChimera also provides a `generate-spec` operation which programmatically builds a spec for a given entity type, that can validate a full entity map of that type.\n\n#### Schema & Migration Operations\n\nIn order for data persistence to actually work, the schema of a particular database instance (at least, for those that have schema) needs to be compatible with the application's data model, as defined by Chimera's entity types and attributes.\n\nSee [ADR-16](adr-016-db-migrations.md) for an in-depth discussion of database migrations work, and the ramifications for how a Chimera data model is declared in the configuration.\n\n### Entity Manipulation\n\nThe previous section discussed the data _model_, and how to define the general shape and structure of entities in an application. Entity _manipulation_ refers to how the operations available to create, read, update, delete specific *instances* of those entities.\n\n#### Data Representation\u00e2\u20ac\u00a8\nDomain entities are represented, in application code, as simple Clojure maps. In their function as Chimera entities, they are pure data; not objects. They are not required to support any additional protocols.\n\nEntity keys are restricted to being namespace-qualified keywords, which correspond with the attribute names defined in configuration (see _Attribute Definitions_ above). Other keys will be ignored in Chimera's operations. Values may be any Clojure value, subject to spec validation before certain operations.\n\nCardinality-many attributes *must* use a Clojure sequence, even if there is only one value.\n\nReference values are represented in one of two ways; as a nested map, or as a _lookup reference_.\n\nNested maps are straightforward. For example: \n\n````\n{:myapp.person\/id 123\n :myapp.person\/name \"Bill\"\n :myapp.person\/friends [{:myapp.person\/id 42\n                          :myapp.person\/name \"Joe\"}]}\n````\n\nLookup references are special values that identify an attribute (which must be a key) and value to indicate the target reference. Chimera provides a tagged literal specifially for lookup references.\n\n````\n{:myapp.person\/id 123\n :myapp.person\/name \"Bill\"\n :myapp.person\/friends [#chimera.key[:myapp.person\/id 42]]}\n````\n\nAll Chimera operations that return data should use one of these representations. \n\nBoth representations are largely equivalent, but there is an important note about passing nested maps to persistence operations: the intended semantics for any nested maps must be the same as the parent map. For example, you cannot call `create` and expect the top-level entity to be created while the nested entity is updated.\n\nEntities do *not* need to explicitly declare their entity type. Types may be derived from inspecting the set of keys and comparing it to the Entity Types defined in the configuration.\n\n#### Persistence Operations\n\nThe following basic operations are defined:\n\n- `get` - Given an attribute name and value, return a set of matching entity maps from the database. Results are not guaranteed to be found unless the attribute is indexed. Results may be truncated if there are more than can be efficiently returned.\n- `create` - Given a full entity map, transactionally store it in the database. Adapters _may_ throw an error if an entity with the same key attribute and value already exists.\n- `update` - Given a map of attributes and values update each of the attributes provided attributes to have new values. The map must contain at least one key attribute. Also takes a set of attribute names which will be deleted\/retracted from the entity. Adapters _may_ throw an error if no entity exists for the given key.\n- `delete` - Given a key attribute and a value, remove the entity and all its attributes and components.\n\nAll these operations should be transactional if possible. Adapters which cannot provide transactional behavior for these operations should note this fact clearly in their documentation, so their users do not make false assumptions about the integrity of their systems.\n\nEach of these operations has its own protocol which may be required by modules, or satisfied by adapters \u00c3\u00a0 la carte. Thus, a module that does not require the full set of operations can still work with an adapter, as long as it satisfies the operations that it *does* need.\n\nThis set of operations is not exhaustive; other modules and adapters are free to extend Chimera and define additional operations, with different or stricter semantics. These operations are those that it is possible to implement consistently, in a reasonably performant way, against a \"broad enough\" set of very different types of databases.\n\nTo make it possible for them to be composed more flexibly, operations are expressed as data, not as direct methods.\n\n#### Capability Model\n\nAdapters must specify a list of what operations they support. Modules should validate this list at runtime, to ensure the adapter works with the operations that they require.\n\nIn addition to specifying whether an operation is supported or not, adapters must specify whether they support the operation idempotently and\/or transactionally.\n\n"}
{"prompt":"Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\nRequirements\nThe charting library should:\nsupport rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy\/dynamic loading to minimize js bundlesTypescript support\nDecision Drivers\nMeet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support","completion":"Chosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\nPositive Consequences\nWe can start implementing charts and graphs as needed in island.is\nNegative Consequences\nIt is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase."}
{"prompt":"Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options \u00e2\u20ac\u201c PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities.","completion":"We will use PostgreSQL as the preferred database management system for our organization's upcoming project."}
