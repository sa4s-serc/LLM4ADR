File Name,Context,Decision,tokens,id,Context_tokens,Decision_tokens
cdh-adrs/0011-pentaho.md,"## Context\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\n","Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\n",52,1,52,55
cdh-adrs/0006-etl_cdh_communication.md,## Context\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\n,We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\n![Event Queue](../assets/images/workflows/image3.png)\n,81,4,81,58
modiapersonoversikt/0002-selvstendig-visittkort-som-library.md,"## Context\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som på sikt skal erstatte frontenden i dagens modiabrukerdialog. For å kunne levere fortløpende ny funksjonalitet til saksbehandlerene, ønsker vi å levere ofte og smått.\n",Visittkortet dras inn som en enkeltstående react-komponent inn til modiabrukerdialog.\n,81,43,81,29
modiapersonoversikt/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,44,16,39
mokka/0003-upgrade-to-spring-boot-2-2-1.md,## Context\nCurrently used: 2.1.0\nThe latest stable version of Spring Boot is 2.2.1:\nhttps://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes\nMore frequent but smaller upgrades are recommended.\n,Spring Boot will be upgraded to 2.2.1.\nAccording to release notes no migration needed on Mokka side.\n,63,46,63,28
mokka/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,48,16,39
copilot/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md,## Context\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\n,We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\n,44,52,44,26
copilot/0002-use-event-streaming-model-for-diego-actuallrp-syncing.md,## Context\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\nacross all diego cells at a specified time interval (at the time of writing 10\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\nimpact the performance of the BBS (large response sets coming back).\n,We want to use the [Event package](https://github.com/cloudfoundry/bbs/blob/master/doc/events.md)\nto get the event stream for each ActualLRP. We will also use a bulk sync every\n60 seconds to catch any events that were missed.\n,75,53,75,58
copilot/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,54,16,39
ELEN4010/ADR Reordering Destinations.md,"## Context\nA list of destinations should be reorderable, not fixed\n","A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\n",15,59,15,55
ELEN4010/ADR Entering destinations for a trip.md,"## Context\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site/map websites.\n",We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\n,63,60,63,26
ELEN4010/ADR Login API Choice.md,"## Context\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\n",Using a well known and widely known/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login/registration page through the Google Login API\n,58,61,58,52
ELEN4010/ADR GitHub Project Board as KanBan.md,"## Context\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size/priority/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\n","Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for ""To Do"", ""In progress"", and ""Completed"".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\nhttps://github.com/witseie-elen4010/2019-005-project/projects/1\n",94,62,94,91
dapr/API-006-universal-namespace.md,"## Context\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\n",We should consider adding universal namespace capabilities to Dapr.\n,70,71,70,12
dapr/ENG-004-signing.md,## Context\nAuthenticode signing of binaries.\n,* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\n,11,72,11,27
dapr/SDK-002-java-jdk-versions.md,## Context\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\nSee https://github.com/dapr/java-sdk/issues/17\n,* Java 8 should be the minimum version supported for Dapr's Java SDK.\n* Java 11 should be used in samples and user documentation to encourage adoption.\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\n,71,73,71,71
dapr/ENG-001-tagging.md,"## Context\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\nWe needed a way to construct an accepted and constant way of naming our Docker images.\n","* An image will conform to the following format: \<namespace>/\<repository>:\<tag>\n* A valid tag conforms to the following format: \<version>-\<architecture>, or just \<version>, then arch is assumed Linux\n",51,74,51,47
dapr/API-012-content-type.md,"## Context\nNot adding content-type to state store, pubsub and bindings.\n","* We will not add content-type since it is a persisted metadata and it can cause problems such as:\n* Long term support since metadata persisted previously would need to be supported indefinitely.\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\nOriginal issue and discussion: https://github.com/dapr/dapr/issues/2026\n",17,77,17,80
dapr/CLI-001-cli-and-runtime-versioning.md,"## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n",* We'll keep CLI versioning and runtime versioning separate.\n* CLI will pull down latest runtime binary during the *init()* command.\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\n,28,78,28,50
dapr/API-010-appcallback-versioning.md,## Context\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\n,"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\n- There would be no way for DAPR runtime to find out the app callback version before hand\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\n",47,79,47,97
dapr/API-004-binding-manifests.md,"## Context\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\n","### Dapr\n* All components use the same **Component** manifests, identified by a component **type**.\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\n",40,84,40,57
dapr/API-007-tracing-endpoint.md,"## Context\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\n",We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\n,53,91,53,23
nucleus/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,94,16,39
nozama/adr-001-simulator-webserver.md,"## Context\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\n",As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\n,63,97,63,48
kitsune/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,104,16,39
Nosedive/0006-use-of-migratus-library.md,## Context\nDirect form the [jdbc documentation](http://clojure-doc.org/articles/ecosystem/java_jdbc/home.html)\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\n* Drift\n* Migratus\n* Ragtime\n,"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\n",60,105,60,34
Nosedive/0004-start-with-console-app.md,## Context\nTe console appication it is the more siple app that come to my mind.\n,Creates a console app and send the data via parameters\n,21,106,21,11
Nosedive/0009-use-mount-and-clojure-tools-namespace.md,"## Context\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\n[mount](https://github.com/tolitius/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\n","Use mount libray and clojure tools space.\nThe decision of mount over component is made afer review bouth solutions. My feeling is\n* Mount is more clojure dialect oriented\n* Mount use namespace and component\nRecords, this made that the compliler control the dependencies\n* Mount is [less contagious](https://engineering.riotgames.com/news/taxonomy-tech-debt)\n",100,107,100,80
Nosedive/0003-evolutionary-architecture.md,"## Context\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\n","We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\n",49,108,49,31
Nosedive/0005-sqlite-database.md,"## Context\nOne of the final stack canditate for persistence is [cockroachdb](https://www.cockroachlabs.com/), but the windows version\n","We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\n",33,109,33,29
Nosedive/0002-use-of-clojure.md,"## Context\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\n","This will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\n",37,110,37,22
Nosedive/0007-uso-of-tools-cli.md,## Context\nWe need working with command line arguments.\n,Use of [tools.cli](https://github.com/clojure/tools.cli)\n,12,111,12,17
Nosedive/0008-migrating-by-code.md,"## Context\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\n",Develop the migration part in the code\n,31,112,31,8
Nosedive/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,113,16,39
dotfiles/0004-use-vim-plug-in-favour-of-vundle.md,## Context\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\nread a bit about how vim-plug was pretty good. And it felt like it was time for\na change.\n,[vim-plug](https://github.com/junegunn/vim-plug) will be used to manage Vim\nplugins in favour of [Vundle](https://github.com/VundleVim/Vundle.vim).\n,51,114,51,47
dotfiles/0003-switch-to-use-zsh-in-place-of-bash.md,"## Context\n[Starting with macOS Catalina, zsh will be the default shell](https://support.apple.com/en-us/HT208050).\n",zsh should be used in place of bash.\n,31,115,31,11
dotfiles/0005-use-coc-in-place-of-ycm.md,"## Context\nYCM has been a mostly positive experience over the years of using it, however,\nthere have been many occasions where time has been spent fixing issues. The\ntime has come to look at using an alternative.\n",The decision is to use [CoC](https://github.com/neoclide/coc.nvim).\n,48,116,48,22
dotfiles/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,117,16,63
dpul/0003-synchronization-via-rabbitmq.md,"## Context\nWe want Pomegranate to be a separate application from Figgy, but need some way\nfor Figgy to tell Pomegranate about new resources so that when something is\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\n",Figgy will send create/update/delete messages to a fanout RabbitMQ Exchange.\nPomegranate will register a durable queue which listens to that exchange and\nprocess messages using [Sneakers](https://github.com/jondot/sneakers).\nThe message will contain the following information:\n* Collection slugs the object is a member of\n* Manifest URL of the object\n* change event (create / update / delete)\n,60,119,60,89
dpul/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described at https://adr.github.io/\n",16,120,16,17
dpul/0007-indexing-visibility.md,## Context\nFiggy resources may have any of the following visibilities:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\n- Reading Room\n- Private\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\nWe used to index only public / complete items. But to support the music reserves\ncollection we need pages that would have a viewer for logged-in institutional\nusers only.\n,Resources with the following visibilities should index into DPUL:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\nThis is implemented with a token authentication mechanism in `iiif_resource#def\nurl`\n,98,122,98,52
dpul/0005-resource-per-collection.md,"## Context\nWe needed to be able to display a resource in more than one collection, because\nin Figgy a resource can be a member of multiple collections.\nAt the time of this decision, one IIIFResource could only be a member of one Exhibit.\n",We create one IIIFResource per Collection of which it is a member.\nWe map each IIIFResource to one SolrDocument.\n,55,124,55,29
dddsample-pelargir/0002-isolate-components-into-submodules.md,"## Context\nI want to be able to mix and match different implementations\nof the ddd sample application, without needing to pull in\na large collection of unnecessary dependencies.\nI want all of the code to be together in one place; which\nis to say, I want to treat the entire project as a mono-repo.\nI can't be bothered to maven install/maven deploy each\nlittle piece to propagate the necessary changes between\nisolated libraries.\n",Use a maven reactor project to track the dependencies between\ndifferent libraries within the project\n,97,127,97,19
dddsample-pelargir/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,129,16,39
accessibility-monitoring/ADR001-record-architectural-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,131,16,39
documents/0002-move-to-clang-format-5.md,## Context\nCurrent version of clang-format is aging an becoming unavailable.\n,We will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\non this version across the project.\n,15,135,15,43
documents/0003-move-to-visual-studio-2017.md,## Context\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\nfeature complete and has many C++17 features.\n,As part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\n,42,136,42,40
documents/0004-move-to-devtoolset-7-on-rhel7.md,## Context\nRed Hat 7's default compiler is gcc 4.8. This is not C++14 compliant and prevents access to more modern C++ features.\n,As part of the maintenance cycle after release 3.13 we will move Red Hat 7 compilers over to use\ngcc 7 as part of the [devtoolset-7](https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/) tools provided by Red Hat.\n,35,137,35,65
documents/0001-record-architecture-decisions.md,## Context\nWe need to record the future architectural decisions made.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",14,138,14,39
openmrs-java-client/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,140,16,39
lcarsde/identification-of-special-tool-windows.md,## Context and Problem Statement\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\n,"Chosen option: ""Client Properties / Atoms"", because it is the most reliable and side-effect free way to identify the tool windows.\n",48,145,48,29
PactSwift/ADR-001-Language_choice.md,"# Context\niOS applications can be written in Objective-C or Swift. Objective-C offers greater interaction with C++ code but is considered a legacy language choice in the iOS developer community. The `pact-consumer-swift` framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.\n# Decision\nThe framework is written in Swift.\n# Consequences\n",The framework is written in Swift.\n# Consequences\n,90,148,90,12
eslint-config-nhsuk/0001-replace-istanbul-with-nyc-for-code-coverage.md,## Context\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\nnyc is one of the suggested replacements. Additional information is available\non [npm](https://www.npmjs.com/package/istanbul). This leaves the package open\nto security flaws that will not be patched. Features available in the latest\nversions of node will not be supported.\n,The decision is to migrate from istanbul to nyc.\n,81,151,81,14
AYCHAIN.github.io/adr-006-handling-plural-i18n.md,"## Context\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\n",We chose to follow the second approach which adds less overhead to the bundle.\nIt comes at the cost of having the contributor to add the plural version of the string they want translated. But this explicit approach also reduces the risk of error and inconsistency.\n,57,156,57,51
MoviesAndroid/0002 Kotlin.md,"## Context and Problem Statement\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\ndon't choose and use both, but i want to make it consistent.\n## Decision Drivers\n* New empty project\n* Kotlin now fully supported for Android by Google\n","* New empty project\n* Kotlin now fully supported for Android by Google\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\nGoogle now, which eliminates the risk of being dropped any time soon.\nPositive Consequences:\n* Less code, more fun!\n* Having to learn a new language\n* Great interoperability with Java if needed\nNegative consequences:\n* Having to learn a new language\n",64,159,64,93
MoviesAndroid/0001 ADR.md,## Context and Problem Statement\nI need a way to document design decisions.\n,"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\nPositive Consequences:\n* Choices and reasoning are persisted and versioned\nNegative consequences:\n* Having to document\n",16,160,16,48
MoviesAndroid/0003 The Movie Db.md,"## Context and Problem Statement\nThis app shows how i would build an Android app. Therefore the app architecture is the goal itself,\nthe features this app has, are just a way to show an app architecture.\n## Decision Drivers\n* Availability\n* Realism\n* Content creation\n","* Availability\n* Realism\n* Content creation\nChosen option: ""The Movie Db"", because it is more realistic when the app consumes a real service.\nPositive Consequences:\n* It is realistic\nNegative consequences:\n* When it's down the app cannot consume it\n",59,161,59,59
disco-poc-vue/0002-use-heroku-static-buildpack.md,"## Context\nWe need to decide how to deploy our Vue app in Heroku. We can achieve this either\nby using a Node.js web server, or by deploying the app as a static site with\nthe [Heroku static buildpack](https://github.com/heroku/heroku-buildpack-static).\n","We will use the static buildpack to deploy, as our app will essentially be a\nstatic frontend backed by TIMDEX. This is consistent with [Vue's deployment\nguidelines](https://cli.vuejs.org/guide/deployment.html#general-guidelines)\non deployments.\n",63,162,63,58
disco-poc-vue/0004-use-yarn.md,"## Context\nNPM is causing confusion as to why lock files are changing in local\nenvironments when no changes have been made. We have found explanations and\nworkarounds, but it feels like the type of unexpected default behavior that will\nlead to frustration as new developers join the project.\nYarn is an alternative package manager that seems to have a more expected set\nof default behaviors while maintaining compatibility in case we need to revert.\n",We will use Yarn instead of NPM for this project.\n,90,163,90,14
disco-poc-vue/0003-use-vuejs-3.md,"## Context\nWe reviewed various options for building our front end and decided Vue.js was\nthe best fit. However, Vue is preparing for a major version upgrade. We could\nchoose to use the current stable version (2) and migrate to the new version (3)\nwhen it is released or start with the release candidates for the new version.\n",We will use Vue.js 3 releases candidates.\n,72,164,72,11
disco-poc-vue/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,165,16,39
paas-team-manual/ADR006-rds-broker.html.md,## Context\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n- [18F](https://github.com/18F/rds-service-broker)\n,"We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\n",85,186,85,49
paas-team-manual/ADR020-deletion_of_ci_environment.html.md,"## Context\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\n","We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\n",78,187,78,53
paas-team-manual/ADR023-idle-cpu-alerting-change.html.md,"## Context\nWith the implementation of ADR021 we have reduced the number of cells in\nproduction in order to make more efficent use of our budget. This in turn means\nthat we have increased the load on the individual cells. Originally the idle CPU\nmonitor was set in line with the free memory on cells monitor (for alerting on\na need to scale the cell pool), however CPU usage does not appear to grow\nlinearly with allocated memory for tenant applications.\n","In order to avoid false positives from triggering due to CPU load spiking rather\nthan being a constant level we will increase the monitoring window to 24 hours.\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\nthresholds to warn at 37% and become critical at 33%.\n",98,215,98,69
paas-team-manual/ADR001-manifest-management.html.md,"## Context\nThe alpha initially took the approach of starting with a vanilla set of cloud\nfoundry manifests, and merging new values into it using spiff. This became\ndifficult to reason about, and cf-release was forked because it was easier than\noverriding necessary values using spiff. However, the confusing spiff hierarchy\nremained.\n",We will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\n,74,217,74,83
Maud/0008-package-structure.md,## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,26,234,26,35
Maud/0007-unit-tests.md,## Context\nWe need to make a decision on the testing framework for our project.\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,18,235,18,26
Maud/0006-code-testing.md,## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n,"We use tox to define, configure, and run different test scenarios.\n",21,237,21,15
Maud/0003-python-3-6-only.md,## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n,We make an early decision to only support Python 3.6 and above.\n,38,238,38,17
Maud/0004-python-package-versioning.md,## Context\nWe need a simple way to manage our package version.\n,We use versioneer to do this for us.\n,15,241,15,11
Maud/0009-gpl.md,"## Context\nWe need to choose an appropriate license before making Maud public. The main\ncontenders are the Apache license, which would allow others to use Maud in\nproprietary software, and the GPL v3 license, which would not.\n","We will use the GPL license but may change to a more permissive license in a\nfuture release, depending on the circumstances at the time.\n",52,244,52,30
Maud/0002-version-control-our-code.md,## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n,We use git for version control and GitHub for collaboration.\n,33,245,33,12
Maud/0005-code-quality-assurance.md,## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n,"We will use isort, black, and flake8.\n",35,248,35,14
Maud/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,249,16,39
scientific-thesis-template/0005-use-pdflatex-as-default.md,## Context and Problem Statement\nWhich latex compiler to use?\n* pdflatex\n* xelatex\n* lualatex\n,"Chosen option: ""pdflatex"", because compiles faster and correct ligatures are required at the final typesetting step only.\n### Positive Consequences\n* Faster compile time\n### Negative Consequences\n* Using lualatex just before publication might lead to a different layout and additional effort\n* lualatex's power might not be used at all\n",30,250,30,76
offender-management-architecture-decisions/0008-use-rails.md,## Context\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\nThe team are already very familiar with Rails and it is widely used within MOJ.\n,We will use Rails as our web framework for our new applications.\n,61,266,61,14
docspell/0001_components.md,"# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n","The following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n",75,276,75,25
docspell/0011_extract_text.md,"# Context and Problem Statement\nWith support for more file types there must be a way to extract text\nfrom all of them. It is better to extract text from the source files,\nin contrast to extracting the text from the converted pdf file.\nThere are multiple options and multiple file types. Again, most\npriority is to use a java/scala library to reduce external\ndependencies.\n# Considered Options\n","- MS Office files: POI library\n- Open Document files: Tika, but integrating the few source files that\nmake up the open document parser. Due to its huge dependency tree,\nthe library is not added.\n- PDF: Apache PDFBox. I know this library better than itext.\n",86,282,86,64
docspell/0016_custom_fields.md,"# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n","- values are strings at the database\n- values are strings when transported from/to server\n- client must provide the correct formatted strings per type\n- numeric: some decimal number\n- money: decimal number\n- text: no restrictions\n- date: a local date as iso string, e.g. `2011-10-09`\n- bool: either `""true""` or `""false""`, case insensitive\n",90,285,90,85
core/0003-decommision-qeb-hwt.md,## Context and Problem Statement\n[Qeb-Hwt](https://github.com/marketplace/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\nPull Requests as a check. This functionality could be integrated into https://github.com/marketplace/khebhut and\ncomplexity and maintain costs.\n## Decision Drivers <!-- optional -->\n* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\n,"* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\nChosen option: ""merge function into Khebhut"", because we can reduce the cost of maintaining our software infrastructure\nby reducing redundancy.\n",96,289,96,47
core/0002-allow-dynamic-loading-of-yetibot-plugins-via-config.md,"## Context\nYetibot currently resides across two primary repos:\n- github.com/yetibot/yetibot\n- github.com/yetibot/yetibot-core\nThese code bases continue to grow in size, and consist of a diverse range of\nfeatures, many of which many users won't care to use.\n","Switching to a plugin system allows us to split up the code base into much more\nfine grained, logical units. For example, we may split the `github` command into\nits own plugin.\nThe first plugin is [yetibot-kroki](https://github.com/yetibot/yetibot-kroki).\nWe will continue to extract plugins from both of the above code bases.\n",70,290,70,86
core/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,292,16,39
beadledom/default_dependencies_primary.md,"## **Context**:\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\n","In order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\nWe did this because we don't know if our consumers rely on the previous behavior of meta/health checking dependencies if they were of unspecified importance.\n",51,293,51,52
opg-refunds/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,294,16,39
opg-refunds/0002-default-service-key-for-dynamo-db-sessions.md,"## Context\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\n","We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\nTable names                                       |\n--------------------------------------------------|\nrefunds-sessions-front-<opg_stackname>            |\nrefunds-sessions-caseworker-front-<opg_stackname> |\n",48,295,48,71
wh_covid19_app/ADR-0003 Use Flutter beta channel.md,"## Context\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\n","We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\n",52,298,52,26
nso.aurora/MartketPlace.md,## Context\nDo we need a MarketPlace component?\n,It doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\n,12,299,12,31
nso.aurora/Notifier.md,"## Context\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and/or SMS.\n",A Notifier component is needed to send email and/SMS to customers.\n,38,300,38,16
nso.aurora/LoyaltyProgram.md,## Context\nDo we need a Loyalty Program component?\n,It doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\n,13,301,13,32
nso.aurora/MicroserviceDesgin.md,"## Context\nTo ensure scalability, reliability and performance, microservice architecture should be used\n",All agreed with this decision\n,18,302,18,6
nso.aurora/NumberOfDatabases.md,"## Context\nThe state and data are required for the following components: Inventory, Customer, Order Management, Rating Manager and Recommendation Manager.  We had a database for each of the component initially.  However, it can save on the number of distrubted transactions by storing state and data in one database.\n",It was agreed that one database would be sufficient.\n,63,303,63,11
nso.aurora/SegmentationEngine.md,"## Context\nThe purpose of Segmentation engine are:\n- grouping of customers' goals\n- targeted groups\n- history\n- purchasing behaviour\n- eating habit, location\n- managing coupons and promotions\nWhat is the difference between Recommendation and Segmentation engines?\n",It was decided to not have it.\n,54,304,54,9
nso.aurora/InventoryManager.md,## Context\nShould we have an Inventory Manager and a Fridge Manager where the Inventory Manager would hold the food items and Fridge would manage the physical fridges?\n,Inventory Manager can manage the following:\n- manage fridges\n- manage food items including their pricing\n- manage the number of items in each fridge\n,34,305,34,31
nso.aurora/MultipleQueues.md,## Context\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\n,The decision is to introduce another queue for Order Management\n,25,306,25,11
nso.aurora/AsynchronousMessages.md,"## Context\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\n",Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\n,22,307,22,22
nso.aurora/QueuingForNotification.md,"## Context\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\n",The decision is to introduce a queue for all these messages.\n,30,308,30,13
nso.aurora/FaultTolerance.md,## Context\nWe have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\n,The decision is to have the polling mechanism to regularly obtain Smartfridges' state/inventory.\n,31,309,31,20
nso.aurora/Kiosks-Fridge-customer-vendor-management.md,"## Context\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\n","We will have separate components to manage kiosks (along with point of sale systems), fridges, customers and vendors\n",30,310,30,25
hodder/0002-use-docker-compose-v-2.md,"## Context\nPeople are very used to traditional layered architectures and request/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\n",We will use a docker-compose.yml (v.2) as the primary entry point of the project\n,92,311,92,21
hodder/0003-use-circle-ci-for-ete-tests.md,## Context\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\n,Use CircleCI for automated ETE tests\n,41,312,41,9
verify-event-store-schema/0001-flyway.md,## Context\nWe need a way of running database migrations to the verify event store database.\n,We have chosen to use [Flyway](https://flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\n,19,318,19,36
re-build-systems/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[documenting architecture decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,325,16,44
Brighter/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,327,16,39
docnow-infra/0003-use-terraform-enterprise-for-backend.md,## Context\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\n,Use Terraform Cloud for teams\n,80,329,80,7
docnow-infra/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,330,16,39
openfido-auth-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,333,16,39
youtube-playlist-manager/0002-use-chrome-identity-launchwebauthflow-instead-of-chrome-identity-getauthtoken.md,## Context\nA method to authorize against the youtube API to get the access token needs to be added to the application.\n,I decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\nfor youtube from my chrome account.\n,25,339,25,55
youtube-playlist-manager/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,340,16,39
midashboard-infrastructure/0002-use-aws-bare-metal-rig-approach.md,## Context\nWe need to create a riglet for our new bookit project so that we practice what we preach.\n,"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: Travis\n",25,341,25,85
midashboard-infrastructure/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md,## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,36,342,36,50
midashboard-infrastructure/0004-use-cloudwatch-logs-for-log-aggregation.md,## Context\nContext here...\n,Decision here...\n,7,343,7,4
midashboard-infrastructure/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,344,16,39
twig-infrastructure/0006-use-ecs-fargate-host-type.md,## Context\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\n,"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\n",23,348,23,48
twig-infrastructure/0002-use-aws-bare-metal-rig-approach.md,## Context\nWe need to create a riglet for our new twig project so that we practice what we preach.\n,"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: CodePipeline, with Jenkins as an eventual target\n",24,349,24,90
twig-infrastructure/0003-start-with-aws-codepipeline-and-codebuild.md,## Context\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer/simpler riglet flavor\nand put newer approaches to the test.\n,"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n",40,350,40,57
twig-infrastructure/0004-use-cloudwatch-logs-for-log-aggregation.md,## Context\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\n,We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\n,27,352,27,29
twig-infrastructure/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,353,16,39
Marain.Instance/0003-shared-application-insights.md,## Context\nMarain services need to be able to deliver diagnostic information somewhere.\n,`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\n,17,355,17,26
Marain.Instance/0004-powershell-core-6-deployment-scripts.md,## Context\nDeployment requirements need to be expressed somehow—either declaratively or in a programming language.\n,Individual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\n,21,356,21,29
fxa/0009-testing-stacks.md,"## Context and Problem Statement\nWe have a variety in tools used for unit & functional tests. This variety means developers need to learn & know many different tools. It would be nice to reduce this mental workload, while still using appropriate tools for testing.\n## Decision Drivers\n- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\n",- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\nIdentify a consistent stack of testing tools per aspect of the project\n,81,365,81,40
asb-client-spa/0002-use-netlify-for-deployments-and-hosting.md,"## Context\nGitHub Pages is a perfectly good static site host, however, not having the\nability for preview builds is problematic when wishing to test changes prior to\nthem being merged into the main branch. Netlify has the ability to create\npreview builds for each PR or even each branch along with a number of other\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\n[comparison](https://www.netlify.com/github-pages-vs-netlify/).\n",The decision is to move hosting and deployment onto Netlify\n,100,390,100,12
asb-client-spa/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,391,16,40
structurizr-python/0008-package-structure.md,## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,26,393,26,35
structurizr-python/0007-unit-tests.md,## Context\nWe need to make a decision on the testing framework for our project.\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,18,394,18,26
structurizr-python/0006-code-testing.md,## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n,"We use tox to define, configure, and run different test scenarios.\n",21,395,21,15
structurizr-python/0003-python-3-6-only.md,## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n,We make an early decision to only support Python 3.6 and above.\n,38,396,38,17
structurizr-python/0004-python-package-versioning.md,## Context\nWe need a simple way to manage our package version.\n,We use versioneer to do this for us.\n,15,397,15,11
structurizr-python/0002-version-control-our-code.md,## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n,We use git for version control and GitHub for collaboration.\n,33,398,33,12
structurizr-python/0009-use-pydantic-for-json-de-serialization.md,"## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n","In order to perform these operations we choose\n[pydantic](https://pydantic-docs.helpmanual.io/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n",39,399,39,47
structurizr-python/0005-code-quality-assurance.md,## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n,"We will use isort, black, and flake8.\n",35,400,35,14
structurizr-python/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,401,16,39
amf-core/0003-flattened-json-ld-parsing-emission-for-self-encoded-dialects.md,## Context\nSelf-encoded dialects define dialect instances which share the same ID between the instance document and the dialect\ndomain element encoded in such document. This allows the document and the encoded dialect domain element to be treated\nas the same resource.\nOn the other hand Flattened JSON-LD emission only renders one node for each ID.\n,We merge both nodes (the document and the encoded domain element) and emit the single merged node with the shared ID.\nThe merged node contains both the properties from the document and the encoded domain element.\nWhen parsing the resulting flattened JSON-LD we parse the merged node twice: first as a domain element and then as a\ndocument. Parsing the properties from the domain element ignores the properties from the document and vice-versa.\n,71,402,71,88
amf-core/0005-expose-non-scalajs-types-in-amf-client-remote-content.md,"## Context\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\nit from export.\n",- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\n- Add the `toString()` method in `Content` that returns the content in `stream`\n,67,403,67,54
amf-core/0004-domainelementmodel-isexternallink-field.md,## Context\nLinking nodes between different graphs is a feature not provided by AMF/AML. This feature is required by some adopters to:\n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)\n* Link nodes between different parsed dialect instances (ANG)\n,Add a boolean field to DomainElementModel called IsExternalLink that marks that a domain element is a reference to a domain element defined in another graph.\n,65,406,65,31
documentation/0003-transfere-hash-in-jwt-claim.md,## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the json data model.\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\nThe validity of the hash must be verified.\n## Decision Drivers <!-- optional -->\n* JWT should be used\n,"* JWT should be used\nChosen option: ""Transfer hash in JWT Claim"", because it's the only option when using JWT.\n### Positive Consequences <!-- optional -->\n* multiple hashes for different json documents can be added in one JWT\n",93,411,93,51
documentation/0001-use-json-web-tokens.md,## Context and Problem Statement\nExternal services must authorize web clients to the WebSocket API.\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\nOnly little data should be stored for an open WebSocket connection.\n## Decision Drivers\n* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\n,"* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\nChosen option: ""JWT from pre-shared keys"", because the WebSocket API is loosely coupled and it is well supported on many platforms.\n### Positive Consequences\n* Simple to implement\n* Authorization data can be send in a portable and verifiable way\n### Negative Consequences\n* The shared keys must be handled\n",72,412,72,87
documentation/0002-use-sha256-with-base64url-encoding.md,"## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the data.\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\nThe WebSocket API must able to verify the integrity of the json data.\n## Decision Drivers <!-- optional -->\n* Use standard encodings\n","* Use standard encodings\nChosen option: ""Send SHA256 hash of Base64Url encoded json"", because this method is platform independent and not much session state is required.\n### Positive Consequences <!-- optional -->\n* The JWT really function as a verification token for the other requests.\n* Can be applied to all json data that must be verified.\n### Negative Consequences <!-- optional -->\n* The json must be transferred in Base64Url encoding\n",77,413,77,96
documentation/0004-use-asymmetric-jwt-signing.md,## Context and Problem Statement\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\nJWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\nWhen implementing JWTs one must decide which method to use.\n## Decision Drivers\n* Multi tenant support with own keys for each tenant\n,"* Multi tenant support with own keys for each tenant\nChosen option: ""Asymmetric JWT signing"", because it the only option which allow to use different keys for different tenants.\n### Positive Consequences\n* multiple keys are supported\n### Negative Consequences\n* complex management of keys\n",77,414,77,59
csw-backend/0002-not-cloud-agnostic.md,## Context\nSince Cloud Security Watch specifically aims to monitor for\nmisconfigurations in AWS accounts it does not make sense to\nmake the tool cloud agnositic.\n,Whilst we support the notion of writing cloud agnostic\ncode in general. In this instance it is not appropriate\nor desirable.\n,36,419,36,28
csw-backend/0003-not-paas.md,## Context\nThe first architecture considered was using the PaaS.\nThis tool collects data about misconfigurations in a\ndatabase. Since the data held is quite sensitive we\nwanted to take all reasonable measures to protect that\ndata.\nThere are a lot of advantages to using the PaaS in that\nit limits the amount of work required for common\noperational tasks like deployment and monitoring.\n,At the present time we felt the Paas was not a viable\noption for this tool because of the following:\n* Shared tenancy RDS\n* Shared tenancy VPC\n* No ability to natively control ingress\n* No ability to control egress\n__Some of these issues are being addressed by the PaaS so\nwe may revisit this decision in future.__\n,83,420,83,79
csw-backend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,422,16,39
casa/0002-disallow-ui-sign-ups.md,"## Context\nWe want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\n",We are going to disable Devise 'registerable' for the user model so that there will no longer be a public sign up option on the site. Creation of new accounts will be done on the backend.\n,63,424,63,43
casa/0004-use-bootstrap.md,## Context\nWe would like to have an easy-to-use system for consistent styles that doesn't\ntake much tinkering. We propose using the `bootstrap` gem.\n,Pending\n,36,425,36,2
casa/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,426,16,39
GoStudy/0002-use-c4-for-architecture-diagrams.md,"## Context\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\n",We will use Simon Brown C4 model.\n,26,427,26,10
GoStudy/0002-use-hexagonal-architecture.md,## Context\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\nTo enable different communication ways we need to apply a proper architectural style.\n,Adopt Hexagonal Architecture for project.\n,37,428,37,9
GoStudy/0004-use-ports-adapters-as-application-architecture-style.md,## Contextadjecent\nWe need to adopt application architecture style adjecent to our architectural drivers\n,We will use ports&adapters architecture style\n,21,429,21,10
GoStudy/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,430,16,39
GoStudy/0003-use-modular-monolith-as-system-architecture-style.md,## Context\nWe need to adopt system architecture style adjecent to our architectural drivers\n,We will use modular monolith architecture style\n,18,431,18,9
ADR/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,440,16,39
up-fiscal-data/004-receipts.md,## Context and Problem Statement\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\n,"- Not in the project scope for either of the engagements\n- Good to have and can be incorporated at a later date\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\n",34,441,34,43
up-fiscal-data/005-phases.md,## Context and Problem Statement\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\n## Decision Drivers\n* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\n,* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\n,88,443,88,68
up-fiscal-data/007-rescoping.md,"## Context and Problem Statement\nDuplicacy of data scraping from Koshvani platform.\nFor the following sections, the data has already been extracted from the `DDO-wise expenditure` section.\n- `Grant-wise (Revenue/Capital) expenditure`\n- `Division-wise expenditure`\n## Decision Drivers\nBoth these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\n",Both these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\nOnly the main pages of the aformentioned sections contain new information that will require extraction.\n,89,444,89,42
up-fiscal-data/003-selnium.md,## Context and Problem Statement\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\n,"- Structure of the Koshvani platform\n- Platfrom links do not reflect selection criteria\n- Automation job requirements for data scraping\n- Periodical jobs and access to new data\nUse [Selenium](https://www.selenium.dev/) to create robust, browser-based automation.\n",32,445,32,58
up-fiscal-data/008-grant-wise.md,## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n,The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\n,82,447,82,42
up-fiscal-data/002-koshvani.md,## Context and Problem Statement\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\n- Expenditure\n- Receipts\n,Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\n,39,448,39,19
nisq-analyzer/0002-monorepo.md,## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n",22,450,22,61
nisq-analyzer/0001-use-URI-for-entities.md,"## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n",39,451,39,79
hmpps-interventions-ui/0006-use-jest-for-testing.md,## Context\nWe want a test framework that has good support for TypeScript and Node. Jest is\na fast testing framework with good resources for mocking.\n,We will use Jest as our testing framework.\n,31,452,31,10
hmpps-interventions-ui/0008-use-snyk-to-scan-for-cves.md,"## Context\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\nend up in production, and make sure to block deployments with known high\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\nare any vulnerabilities in the code we've written.\n","We will use Snyk to:\n- Run scans on PRs provide results and fail builds if any known high severity\nCVEs are found.\n- Run scans on main and fail builds on high severity CVEs, posting the results\nto the Snyk platform for monitoring.\n- Run nightly scans on the docker image and app dependencies.\n",72,453,72,74
hmpps-interventions-ui/0002-use-express.md,"## Context\nWe want to build a web-based application in Node. Express is a minimal and\nflexible framework that sets up a lot basic configuration for us like routing\nand middleware, and makes it easy to incoporate the GOV UK design system UI\ncomponents.\n",We will use Express as the basis for this project.\n,57,455,57,12
hmpps-interventions-ui/0007-use-cypress-for-integration-testing.md,"## Context\nWe want to be able to automate testing end-to-end user journeys through our\napplication. Cypress is an alternative to Selenium that runs in the browser to\ndo this for us, and is used across multiple projects at MOJ.\n",We will use Cypress for integration tests.\n,50,456,50,9
hmpps-interventions-ui/0004-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,69,457,69,57
hmpps-interventions-ui/0010-use-stylelint-for-linting-styles.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n",We will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n,75,458,75,41
hmpps-interventions-ui/0003-use-typescript.md,"## Context\nWe want to be confident about the code we write, and for it to be\nself-documenting as much as possible. TypeScript is a compiled language with\noptional typing. It's a superset of JavaScript, so is familiar to developers\nwho know JavaScript. It has wide editor support.\n",We will use TypeScript by default.\n,63,459,63,8
hmpps-interventions-ui/0009-use-helmet-for-http-security.md,## Context\nWe want to make sure we're setting the correct HTTP headers for security e.g.\nContent Security Policy to protect against XSS attacks.\n[Helmet](https://helmetjs.github.io/) is a package that works well with Express\nto make it easy to set various HTTP headers for secutiy.\n,We'll use Helmet to set secure HTTP headers.\n,67,462,67,11
hmpps-interventions-ui/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,463,16,41
openlobby-server/0010-replace-flask-with-django.md,## Context\nFlask turned out to be poorly designed piece of software which relays on too\nmuch magic like manipulations of global objects like `g`.\nSeems like we will also decide to use relational database.\n,"We will switch to Django. It's not only well written server but it has also\n""batteries included"" like a good ORM layer. And some other features like\nmiddlewares will simplify things.\n",47,466,47,42
openlobby-server/0012-use-postgresql.md,## Context\nWe want to add relational database.\n,We will use PostgreSQL. It's a mature database with handy features like JSON\nand hstore data types. It's fully ACID compliant including schema changes. It\nhas very good support in Django's ORM.\nAnother popular option is MySQL/MariaDB. But because it has a major bug `#28727`\n(10 years since it has been reported and it's still not fixed) breaking ACID in\nschema changes it can't be used for any serious project.\n,11,468,11,99
openlobby-server/0006-use-flask.md,## Context\nWe need to choose webserver.\n,We will use Flask. Server should be simple - pretty much just with a GraphQL\nendpoint and GraphiQL.\n,11,469,11,24
openlobby-server/0007-adopt-graphql-relay-specification.md,## Context\nWe need to make API friendly for clients and design pagination.\n,We will adopt GraphQL Relay specification. It solves pagination so we don't\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\nIt has a way to define inputs in mutations.\nGraphene lib has good support for creating API following Relay specifications.\n,16,470,16,59
openlobby-server/0013-black-code-formatter.md,## Context\nWe would like to simplify code reviews and unify code style.\n,Use Black code formatter: https://github.com/ambv/black\n,16,471,16,14
openlobby-server/0002-use-elasticsearch-for-fulltext-search.md,## Context\nWe need a database with fulltext search capable of searching in various\nlanguages especially in Czech.\n,"We will use Elasticsearch. It's well known database with great fulltext search\ncapabilities based on Apache Lucene. It has also aggregations, highlighting of\nresults, and many other useful features.\nWe will use it as database for all data so we have just one database in the\nsystem.\n",23,473,23,64
openlobby-server/0008-pytest.md,## Context\nWe need to choose framework for tests.\n,We will use Pytest. It's much more Pythonic and simplier to use than Unittest\nfrom standrd library.\n,12,474,12,27
openlobby-server/0011-add-relational-database.md,## Context\nNumber of document types which does not use Elasticsearch's fulltext\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\nper index which means management of many indices.\n,We will add relational database as primary database. Elasticsearch will be used\nfor denormalized reports and related data intended for fulltext search.\n,42,475,42,29
openlobby-server/0003-just-api-without-frontend.md,## Context\nOpen Lobby must have open stable API.\n,"Open Lobby will be server just with an API based on API First design. Frontend\nwill be written as separate application (web based, mobile, ...). This will\nensure that there is a working API for anything that frontend application(s)\nwill do.\n",12,476,12,54
openlobby-server/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis article:\nhttp://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,477,16,41
openlobby-server/0009-openid.md,## Context\nWe need an authentication mechanism for users. It must be secure and\nfrontend application independent.\n,We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\nfor a frontend applications. Ideally over the GraphQL API.\n,22,478,22,29
nearby-services-api/0003-use-mongodb-for-data-storage.md,## Context\nInformation about pharmacies is required by the application in order to display\nto users. A Docker image of pharmacies\n([pharmacy-db](https://hub.docker.com/r/nhsuk/pharmacy-db/)) running a MongoDB\ninstance has been created for use by other applications.\nNodeJS uses a single threaded event loop architecture and as such works best\nwhen the work it is doing is non-CPU intensive. Searching through datasets is\npotentially CPU intensive.\n,We have decided to use the existing Docker image rather than spend effort\nacquiring the data again.\n,98,480,98,21
nearby-services-api/0005-use-prometheus-for-exposing-metrics.md,## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n,We will use Prometheus to monitor and alert on the state of the application.\n,86,481,86,16
nearby-services-api/0006-split-nearby-and-open-results-into-separate-endpoints.md,## Context\nThe primary (only) consuming application for this API needs to show both open\nand nearby services on separate pages (and more of them). Previously the\napplication had shown a mix of open and nearby services within a\nsingle page.\nHaving the API so closely aligned to the needs of the consumer is not ideal.\nThere is scope to increase the flexibility of the API along with increasing the\nease with which it can be used both by the current and future consumers.\n,The decision is to add a new endpoint i.e. `/open` alongside the current\n`/nearby` endpoint. The former endpoint will return only services that are open\nwhere the latter will be refactored to return only services that are nearby\nregardless of their opening state.\n,100,482,100,60
nearby-services-api/0004-use-elastic-search.md,"## Context\nElasticsearch is configured as a cluster for reliability and failover, and\nprovides a single point for data updates. MongoDB runs as a single instance and\nis not clustered.\n",nearby-services-api will consume data from Elasticsearch rather than MongoDB.\n,41,483,41,14
nearby-services-api/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,484,16,39
elasticsearch-updater/0002-store-configurations-in-repository.md,## Context\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\n,"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\n",61,485,61,53
elasticsearch-updater/0003-remove-unwanted-indexes.md,"## Context\nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.\nRunning two updaters simultaneously, as happens for pull requests that last several days, can leave\ntwo indexes with the same alias.\nElasticsearch watches are run to export data to Prometheus throughout the day.\nTwo new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\n","Orphaned and indexes with duplicate aliases will be removed as part of the update.\nThe indexes created every day by the watch will also be deleted within the elastic-search updater.\nThis could be done by an Elasticsearch watch, but a license is required to use this functionality.\nTo avoid adding more infrastructure components to the system rather than having a standalone service, the cleanup will be\nperformed in the elastic-search updater. This can be moved into another service in future if required.\n",96,486,96,98
elasticsearch-updater/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,488,16,39
the-zoo/0004-graphql-api.md,"## Context\nWe need API for the Zoo to expose issues, analytics data, etc.\n","We will create API by GraphQL specification. Pagination will be done according\nto Relay server specification. We considered to build REST API, but we decided\nthat for fresh new APIs is the GraphQL right choice.\n",19,489,19,43
the-zoo/0002-use-black-code-formatter.md,## Context\nWe would like to use code formatter to standardize code look so we stop bothering about it\nin code reviews and focus on how code actually works.\n,We will use Black: https://github.com/ambv/black\n,34,490,34,14
the-zoo/0003-save-adrs-into-adr-dir.md,## Context\nWe need dir to save ADRs into.\n,"We will save ADRs into top level `adr` dir. Another option was to put them into `docs/adr`\nalong with Sphinx docs, but they might be bit hidden there.\n",14,493,14,41
the-zoo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,494,16,40
front-end-monorepo/adr-13.md,"## Context\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\n","We will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\n",79,510,79,49
tamr-client/0010-confirm-performance-issues-before-optimizing.md,"## Context\nThere are multiple, equally-effective ways to implement many features.  In some cases, the most\nstraightforward implementation might involve making more API calls than are strictly necessary\n(e.g. `tc.dataset.create` makes an additional call to retrieve the created dataset from the server\nto construct the returned `Dataset`).\n","The simplest and most understandably-written implementation of a feature should be prioritized over\nperformance or reducing the number of API calls.  When real performance issues are identified,\noptimization should be done on an as-needed basis.\n",68,534,68,46
tamr-client/0003-reproducibility.md,"## Context\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\nFor this codebase, we will focus on consistent Python versions and dependency versions.\n",Manage multiple Python versions via [pyenv](https://github.com/pyenv/pyenv).\nManage dependencies via [poetry](https://python-poetry.org/).\nDefine tests via [nox](https://nox.thea.codes/en/stable/).\nRun tests in automation/CI via [Github Actions](https://github.com/features/actions).\n,43,535,43,72
tamr-client/0006-type-checking.md,"## Context\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\n",Type-check via [mypy](http://mypy-lang.org/).\n,25,536,25,16
tamr-client/0007-tamr-client-package.md,"## Context\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\nBut, we want to rearchitect this codebase as a [library of composable functions](/contributor-guide/adr/0005-composable-functions).\n",Implement rearchitected design as a new package named `tamr_client`.\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\n,64,539,64,68
tamr-client/0002-linting-and-formatting.md,"## Context\nInconsistent code formatting slows down development and the review process.\nCode should be linted for things like:\n- unused imports and variables\n- consistent import order\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\n","For linting, use [flake8](https://flake8.pycqa.org/en/latest/) and [flake8-import-order](https://github.com/PyCQA/flake8-import-order).\nFor formatting, use [black](https://github.com/psf/black).\n",55,541,55,59
tamr-client/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,542,16,39
molgenis-r-armadillo/0002-manage-data-in-armadillo-suite.md,## Context\nIn this project we manage the data in the Armadillo suite.\n,We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\n,18,545,18,37
molgenis-r-armadillo/0001-use-adr-to-describe-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n",16,546,16,40
molgenis-r-armadillo/0003-use-s3-api.md,## Context\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\n* There are existing client libraries for the S3 API.\n* It is nontrivial to proxy large file uploads through the armadillo server.\n,The Armadillo client will be written as a high-level library on top of an existing S3 API.\n,56,547,56,23
gladiator_v2/12-9-2017_player_interface.md,## Context:\nNeed to create a way for the player to interact with their character and potential with their allies such that there is choice involved in combat on the player side instead of randomly decided actions.\n,Create a child class of character to give branching functionality to player-specific characters.\n,41,549,41,16
gladiator_v2/12-12-2017_castable_spells.md,## Context:\nWant to give more variation to the combat system beyond physical attacks. This will allow for reasons to create separate builds and more options for player characters and enemies to use during a fight. This will give more of a *game* element to the combat as opposite to a system based on luck alone.\n,Create spells and integrate them into the character and combat systems.\n,64,550,64,13
gladiator_v2/12-6-2017_combat_class.md,## Context:\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\n,There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\n,58,551,58,21
gladiator_v2/12-12-2017_character_creation.md,## Context:\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\n,Create a player module with user interaction to affect the starting statistics of a character.\n,61,552,61,17
gladiator_v2/12-12-2017_useable_items.md,"## Context:\nWhile we have a class for items already, we need a way for them to be used during combat. This way we can add in more items that will change the flow of battle and eventually a money and shop system to add to combat variation.\n",Create a way to use items and integrate into the combat flow.\n,54,553,54,14
gladiator_v2/12-12-2017_random_character_generation.md,## Context:\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\n,Create a random character factory that will pump out characters.\n,57,554,57,12
kuona/0002-evaluate-buddy-auth-for-admin-access-controls.md,## Context\nKuona instances need to be secured - particularly for deployments that have public access.\n,https://funcool.github.io/buddy-auth/latest/#example-session Buddy seems to fit the bill and is compatible with Compojure and Ring. Provides a number of options and possible persistance mechanisme.\n,21,572,21,42
kuona/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,573,16,39
verify-onboarding-prototypes/0003-use-files-to-store-private-keys copy.md,"## Context\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\ndecrypt Response Assertions.\nThey will need to provide these to the verify-service-provider in some, reasonably\nsecure way. Different users may have different opinions on how best to do this.\n",Initially we'll use files for this.\nWe chose not to use environment variables because they're visible to other processes.\nWe chose not to use a more complicated solution because it would be more complicated.\n,62,574,62,42
verify-onboarding-prototypes/0007-we-will-document-a-strawman-api.md,## Context\nThe client and the service provider will have to communicate using some API.\nWe need to decide how the requests and responses will look like.\n,We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\n,32,575,32,35
verify-onboarding-prototypes/0002-build-a-prototype.md,"## Context\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\nconduct some user research, which should involve putting software in front of users and observing them using it.\n","We will build a simple ""prototype"" which we will use to test our assumptions about whether our proposed\nsolution is the best way of meeting our users needs.\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\ndo the SAML interactions with Verify at this stage.\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\n",48,576,48,88
verify-onboarding-prototypes/0006-we-will-build-a-js-client.md,## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n,We will initially build only a node / passport client. We will want to build\nanother client in another language as soon as possible to make sure the API\nis well designed.\nUsers should also be able to interact with the API directly if we haven't built\nan appropriate client for their use case.\n,43,577,43,63
verify-onboarding-prototypes/0002-how-do-we-secure-the-api.md,"## Context\nWe need to secure the interaction between the ""client"" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n",If possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\n,59,578,59,73
verify-onboarding-prototypes/0004-users-will-be-able-to-provide-relay-state.md,"## Context\nIn SAML RPs can provide some extra data along with the request. This is\ncalled RelayState. Some existing RPs use this, but we're not sure what\nthey use it for.\nWe're not aware of any need for the service-provider to use relay state itself.\n",Users will be able to specify whatever relay state they want to and it will be\nprovided in the response.\n,64,579,64,23
verify-onboarding-prototypes/0005-sp-will-generate-request-id.md,"## Context\nAuthnRequests contain an ID attribute the value of which will be sent back in\nthe Response as an ""InResponseTo"" attribute.\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\n",The service provider will generate a random GUID to use as the AuthnRequest ID.\n,62,580,62,18
verify-onboarding-prototypes/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,582,16,39
old-web-frontend/0002-use-aws-s3-as-host-solution.md,## Context and Problem Statement\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\n## Decision Drivers\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\n,"* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\nChosen option: ""AWS"", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\n",53,583,53,66
old-web-frontend/0001-use-react-with-typescript.md,## Context and Problem Statement\nWe have to choose the framework and language to build this web application.\nWhich framework and language should we use?\n,"Chosen option: ""React with Typescript"", because\n* The single developer has experience with React and Ember, but no experience with Vue\n* This developer favors React over Ember, because he thinks the first contributes to a better separation of concerns\n* This developer has experiece with TypeScript and JavaScript, but no experience with Elm\n* This developer favors TypeScript mostly because it's typed.\n",30,584,30,79
abracadabra/0005-use-custom-testeach-instead-of-jest-it-each.md,"## Context\nJest [`it.each` seemed great][jest-it-each], but it turns out to be limited.\nSpecifically, it doesn't allow us to have a `.only` on a single test of the list.\nOf course, this can be done at runtime through Jest runner. But if we use tools like [Wallaby.js][wallaby], we can't do that. We need a way to add things like `.only` on individual tests.\n",We've decided to implement a custom `testEach()` function that will provide a convenient API to run the same test over different data.\nThis function will provide a way to run individual test of the list with `only: true`.\n,98,590,98,49
abracadabra/0006-create-generator-to-bootstrap-new-refactorings.md,"## Context\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\n","To speed up this process, we've decided to use a code generator.\nWe went for [hygen](https://www.hygen.io/) because it's quite simple to use, very fast and flexible.\nWe're abstracting the usage of hygen behind a npm script alias: `yarn new` will ask you few questions to scaffold a new refactoring.\n",77,591,77,77
abracadabra/0010-integration-tests.md,"## Context\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\n","We will now cover adapters with integration tests.\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\n",79,592,79,52
abracadabra/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,594,16,39
terraform-provider-harbor/0001-use-swagger-for-generate-http-client.md,"## Context and Problem Statement\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\n","Chosen option: ""Swagger Based"", because this solution supports the fastes development Start without writting any boilerplate code.\n### Positive Consequences\n* No Painfull HTTP Client Implementation\n### Negative Consequences\n* the API Client Implementation dependents to the Swagger Spec Quality...\n",31,595,31,58
ehoks-ui/0001-record-architecture-and-desing-decisions.md,## Context\nWe need to record the architectural and desing decisions made during this project.\n,"We will use Architecture Decision Records, as described by\nMichael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nAlso https://medium.com/better-programming/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\n",19,597,19,78
nats-architecture-and-design/ADR-11.md,## Context and Problem Statement\nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.\n,"This was driven by the fact that the Go client behaves as described above and some users have shown interest in all clients behaving this way.\nSome users have DNS where the order almost never change, which with client libraries not performing randomization, would cause all clients\nto connect to the same server.\n",29,603,29,62
ols-client/0002-api-result-access.md,"## Context\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\nhaving to know expected keys.\n",To implement.\n,62,604,62,4
ols-client/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,606,16,39
airline-reservation-system/0002-TCH-RES-use-modular-monolith.md,## Context and Problem Statement\n**What type of arichitecture is to be used ?**\nConsidered options:\n1. Monolith\n2. Modular Monolith\n3. Microservices\nDrivers:\n* Project is realized as GreenField\n* A small development team (~10)\n* Little experience in implementing distributed software\n* Load - (50-150 req/sec) - not so much\n,"Option no 2 - Modular Monolith.\nArchitecture suitable for teams with little experience in implementing distributed systems.\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\narchitecture.\n",85,607,85,52
airline-reservation-system/0004-TCH-RES-use-document-database-as-a-aggregate-data-repository.md,## Context and Problem Statement\n**What kind of database should be used to storage aggregates data ?**\nConsidered options:\n1. Document database (MongoDB)\n2. Relational database\nDrivers:\n* Simple objects will be saved.\n* There will be no relationship between objects .\n* A Read Model combining data from different aggregates is required .\n,Option no 1 - document database - MongoDB.\n,78,608,78,11
airline-reservation-system/0005-TCH-RES-use-spring-repository.md,## Context and Problem Statement\n**What technology is to be used for persistence?**\nConsidered options:\n1. Spring Template\n2. JPA + Spring Repositores\nDrivers:\n1. Technology must be simple and well known\n2. Cannot require a lot of code writing\n,Option no 2 - JPA+Spring Repositories.\n,62,609,62,13
airline-reservation-system/0003-TCH-RES-use-ports-and-adapters-architecture.md,## Context and Problem Statement\n**What type of application architecture is to be used ?**\nConsidered options:\n1. Layered architecture\n2. Ports and adapters architecture\nDrivers:\n* The Reservation module is an example of a deep module.\n* A large number of business rules.\n* Some of the rules are quite complicated.\n* Application should be well - tested .\n,Option no 2 - Ports and Adapters architecture. It is dedicated for modules with high business complexity.\nIt separates domain logic from application logic. It allows to create independent tests for each type of logic.\n,83,610,83,43
lockfix/0001-documenting-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,611,16,39
cloud-platform/017-Variable-Naming.md,"## Context\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\ntemplates which need to define and use variables. We want a consistent\nconvention for naming these so that, as we write code in multiple,\ninter-dependent repositories, we can be confident that the names we are using\nare correct.\n",We will always use snake case (e.g. `foo_bar`) for variable names which appear\nin terraform/yaml files and templates.\n,71,616,71,30
cloud-platform/010-live-0-to-live-1-Cluster.md,"## Context\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\n","After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\n",83,617,83,82
cloud-platform/016-Kibana-is-open-to-all-service-teams.md,## Context\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\n,It has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\n,90,623,90,53
io-pagopa-proxy/0002-use-uuid-v1-for-sequential-requests.md,"## Context\nWe have to send requests to a server that requires unique Request IDs for each message.\nSo, we need to generate uuids.\n",We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\nhttps://www.npmjs.com/package/uuid\n,31,634,31,31
kafka/0003-port-protocol-s-types-from-java-implementation.md,## Context\nKafka's protocol defines some primitive types which are used to send requests\nand parse responses: https://kafka.apache.org/protocol#protocol_types\nProviding an easy way to define the schema of the requests and responses is\nquite critical to make this library extensible enough.\n,"We've decided to basically port the Java implementation to the PHP world,\nbecause it was very well written and it simplifies things by a lot.\nSome minor things obviously have to be adapted and for now we'll leave some\ntypes to the upcoming releases - just because they aren't need to implement the\nmessages we're planning to provide at the moment.\n",61,637,61,75
kafka/0007-require-latest-stable-php-version.md,"## Context\nWe've previously decided to require PHP 7.2, however many months have passed and\nPHP 7.3 is quite stable nowadays.\n",Bump up requirement to latest stable PHP version (7.3 at the moment).\n,33,638,33,18
kafka/0006-require-php-7-3.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,639,21,13
kafka/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,640,16,39
cena/0003-adopt-hexagonal-architecture.md,## Context\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\naggregates and services) from the infrastructure (i.e. application clients and persistence).\n,"`menu-generation` application will adopt [hexagonal architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)),\nas it aims to provide this separation.\n",53,643,53,36
cena/0007-execute-build-tasks-with-make.md,## Context\nWe don't want the development tools or Continuous Integration pipeline to be strongly bound to [Gradle](0006-manage-build-with-gradle.md).\n[Make](https://linux.die.net/man/1/make) is an utility agnostic of any language or build management tools.\n,"Make will be used to execute build tasks, abstracting Gradle and potential other tools used during build execution.\n",64,648,64,23
cena/0001-use-java-as-language.md,"## Context\nWe need to choose a programming language to implement the `menu-generation` application.\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\n","Java is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\nprogramming language used to implement the `menu-generation` application.\n",62,651,62,37
cena/0010-implement-acceptance-tests-with-serenity.md,"## Context\n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance\nscenarii. However, to help maintaining an acceptance tests client library, we need to organize this library to be\nextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.\nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about\nwhat wrong happened during API calls.\n",The [Serenity](http://www.thucydides.info/#/) framework will be used to define the acceptance tests library.\n,100,652,100,28
cena/0013-adopt-contract-first-with-openapi.md,"## Context\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\nwell designed for specific use-cases, based on consumers point of view.\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\nthe application.\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\n",[OpenAPI Specification](https://swagger.io/specification/) will be used to enforce contract-first approach.\n,85,655,85,22
uqlibrary-reusable-components/adr-001.md,"## Context\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au/primo-sand-box/reusable-components/`, via the primo BO.\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\nManual processes are to be avoided.\n","Generate the branch to be used, by looking at the host name and the vid parameter on the url\nhttps://github.com/uqlibrary/uqlibrary-reusable-components/commit/4f1c182\n",87,665,87,43
editions/01-🚯-pwa.md,"## Context\nThis needs to work cross platform.\nThe UX will be rendered in browser, which may not appear native.\n","To implement Editions as a progressive web app, delivered in a native wrapper. The native wrapper will contain a webview, that renders the PWA.\nThe wrapper will handle required features that are not supported by PWAs such as background fetch, scheduled downloads and push notification handling.\n",25,666,25,57
editions/05-✅-archiver-s3-event.md,## Context\nThe archiver lambda needs to be able to respond to issue published events from tools.\n,For the archiver to react to S3 file created events from the fronts tool.\n,21,667,21,18
editions/02-✅-react-native.md,## Context\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\n,To develop the editions app as a React Native application.\n,46,670,46,12
editions/04-✅-paths.md,## Context\nThe editions lambda needs to be able to identify specific versions of an issue.\n,"To have two deployments of the backend, one for previewing, and a second for published issues.\nThe published issues deployment will replace the issueid path parameter with source/issueid.\n`source` will identify which file in the published bucket will be retreived to form the issue on.\n",19,671,19,61
editions/06-✅-to-use-step-functions.md,## Context\nThe archiving process is time consuming and consists of many stages.\n,For the archiving process to be run as an amazon step function so we can break the process down into component stages.\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\nThis will also allow the publication process to fire the notification at the desired time.\n,17,672,17,61
Sylius/2021_07_05_api_providing_locales_available_in_active_channel.md,## Context and Problem Statement\nCustomer should have access only to locales available in their channel\n,"Chosen option: Using Data Provider\nShops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated\nquery in Doctrine Collection extension\n",18,676,18,39
Sylius/2021_06_15_api_platform_config_customization.md,"## Context and Problem Statement\nSylius is by design application that will be adjusted to customer needs.\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\n","Chosen option: ""Config merging"", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\n",98,680,98,26
Sylius/2020_11_18_sending_emails_via_api.md,"## Context and Problem Statement\nTo achieve 100% API coverage, we need to handle emails by API.\n","Chosen option: ""Using events"", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\n",23,681,23,35
va.gov-team/0006-scale-vagov-content-gql-not-cms-export.md,## Context\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\n,Halt implementation on CMS Export and focus on scaling GraphQL.\n,69,684,69,13
va.gov-team/0002-deploy-pact-on-aws.md,"## Context\nThe Pact Broker has been on Heroku during development, but its database is running out of rows for the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\n",We will move the Pact Broker onto our AWS infrastructure.\n,51,687,51,12
va.gov-team/0001-record-architecture-decisions.md,## Context\nWe want to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described in this Confluence document](https://vfs.atlassian.net/l/c/58vSL6ZP).\n",16,690,16,34
qc-atlas/0007-junit-for-testing.md,"## Context and Problem Statement\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\n","Chosen option: ""[JUnit5]"", because it is the newer version and therefore has a higher maturity and a wider feature-set.\n### Positive Consequences <!-- optional -->\n* Uniform tests\n* More and newer features\n",37,692,37,46
qc-atlas/0005-use-OpenAPI.md,"## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n","* readable API documentation\n* effort of manually creating client services\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\n### Positive Consequences\n* Standardized documentation of the API\n* Automatic service generation for clients is possible\n### Negative Consequences <!-- optional -->\n* OpenAPI annotations have to be maintained\n",56,693,56,78
qc-atlas/0003-model-mapper.md,"## Context and Problem Statement\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\n","Chosen option: ""[Model Mapper](http://www.modelmapper.org)"", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\n### Positive Consequences <!-- optional -->\n* Less boilerplate code\n",42,694,42,51
qc-atlas/0004-postgres-for-service-tests.md,"## Context and Problem Statement\nTo ensure high test coverage within the project, functionalities that interact with the database must be tested as well.\nThere are different approaches to testing these functionalities that require a database running.\n","Chosen option: ""Use Production-like [Postgres](http://www.postgresql.org) database"", because it ensures a production-like behavior.\n### Positive Consequences <!-- optional -->\n* Almost same configuration for test and runtime.\n* Less database technologies used.\n* Guaranteed that production and tests behave the same.\n",43,695,43,67
qc-atlas/0006-model-assemblers.md,"## Context and Problem Statement\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\nConstructing such objects, as well as adding the desired links to them is a common operation that\nrequires entity-specific boilerplate code.\nHow can duplicate code in nearly all controller methods be avoided?\n## Decision Drivers <!-- optional -->\n* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\n","* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\nand our DTO classes.\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\n",97,696,97,80
qc-atlas/0002-monorepo.md,## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n",22,697,22,61
qc-atlas/0009-remove-hal-links.md,"## Context and Problem Statement\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\n## Decision Drivers\n* readable API documentation\n* development effort to test/gather the HATEOAS links\n","* readable API documentation\n* development effort to test/gather the HATEOAS links\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\n",55,698,55,43
qc-atlas/0008-testcontainers.md,## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n,"Chosen option: [Testcontainers](https://www.testcontainers.org/), because it simplifies the test execution process\n### Positive Consequences <!-- optional -->\n* It is not mandatory to ensure postgres is running before starting the tests\n",34,699,34,49
qc-atlas/0001-use-URI-for-entities.md,"## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n",39,700,39,79
ionic-dummy-repo/ADR-001.md,"## Context\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\n### Who Was Involved in This Decision\n- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\n","- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\n",93,701,93,90
auth-account-koa/0002_use_lodash.md,## Context\nThere is the problem where to store different utilities and write every time the same code on all projects.\n,"Use [lodash](https://lodash.com/docs/) possibilities as main package for utilities on the JS microservices, in order to spend less time and write less code. And use utils directory as a wrapper for it in order to be able to replace it any time.\n",24,705,24,54
read-more-api/0005-use-docker.md,## Context\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\n,We will use Docker for packaging and running the application in a Linux container.\n,24,717,24,16
read-more-api/0009-service-layer.md,"## Context\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\n",We will use a service layer to ensure that Controllers do not contain business logic.\n,51,720,51,17
read-more-api/0002-use-asp-net-core.md,"## Context\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\n",We will use ASP.NET Core.\n,32,722,32,8
read-more-api/0010-feature-toggles.md,"## Context\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\n",We will associate a set of feature toggles with a Pocket Account.\n,67,724,67,15
read-more-api/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,726,16,39
datalab/0019-react-js-for-front-end-ui.md,## Context\nWe need to decide which front-end web framework to use for the Datalabs application.\nThe choices we evaluated were [React.js](https://reactjs.org/) and\n[Angular](https://angular.io/).\n,"We have decided to use the React framework for the following reasons:\n* Preferred the ""batteries not included"" approach of React vs the ""batteries\nincluded"" approach of Angular.\n* Better development tooling available for React.\n* Larger community for React.\n* The team had previous experience with React.\n",48,728,48,68
datalab/0012-public-and-private-subnets.md,## Context\nWe would like to minimise our externally exposed footprint. To achieve this we have\nalready decided that access to cluster servers will be via a Bastion server. We do need\nto have some externally accessible services and need to decide how to achieve this.\n,We have decided to have external proxy servers for each environment that expose ports 80\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\n,55,729,55,53
datalab/0011-cluster-authentication-by-ssh-key.md,## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n,We have decided that all server log on authentication will use SSH key pair\nauthentication. The public keys will be provisioned onto the server at server creation\nfor the default deploy user. A password will be required for sudo.\n,29,731,29,46
datalab/0041-user-driven-dask-and-spark.md,"## Context\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\nfrom notebook environments. However since this decision a number of other options\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\nschedulers.\nWe are now moving to a pattern of users being able to spin up their own clusters\nwhen required.\n","We have decided to collapse the centralised Dask & Spark clusters in favour of writing\ndocumentation/working with users to provision their own clusters using projects such\nas [Dask Labextension](https://github.com/dask/dask-labextension), which is now supported\nwithin the Labs environment.\n",78,732,78,64
datalab/0036-replace-kong-with-nginx-ingress-controller.md,## Context\nWe have reached a point where we need to secure third party web applications that provide\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\nsource version and we haven't had and response from our efforts to contact them.\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\nwas released provides a Kubernetes native route for the same functionality.\n,We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\nThis should provide all of the same functionality as Kong and in addition should provide\na mechanism for token and cookie authentication using the `auth_url` annotation that\nwraps the underlying Nginx `auth_request` module.\n,91,733,91,83
datalab/0032-jest-for-node-js-testing.md,## Context\nWe need to select a testing framework to use for testing node.js applications. There are\nmany choices in this space including [Jasmine](https://jasmine.github.io/) and\n[Jest](https://facebook.github.io/jest/).\n,We have decided to use Jest as our testing framework as it provides mocking and\nexpectation functions which would have to be provided separately to Jasmine. Additionally\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\nparticularly beneficial to front end unit testing.\n,53,734,53,56
datalab/0034-custom-k8s-deployment-tool.md,## Context\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\nenvironments as currently we would have to manually update them in order to apply them to\ndifferent environments. The options available are to either use\n[Helm](https://github.com/kubernetes/helm) or to build a custom tool.\n,We have decided to build a custom tool called Bara to deploy our templates. This will use\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\napplied using the command line `kubectl` tool.\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\neach independent component given our current requirements are very simple and the tool\nwill only take a few hours to write.\n,66,735,66,89
datalab/0042-remove-discourse.md,"## Context\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\nin practise we have found that it is not used as discussion takes place either in\nperson or on Slack, and we can use the documentation page where required.\n",Discourse will be removed from the stack.\n,53,737,53,10
datalab/0035-use-of-microbadger-for-docker-containers.md,## Context\nWe have a growing number of Docker containers and it is useful to have at a glance\ninformation available about them. [MicroBadger](https://microbadger.com/) provides\na way to inspect and visualise Docker containers.\n,We have decided to use MicroBadger for new containers and will update existing containers\nas we make updates to them.\n,51,741,51,25
datalab/0031-kubernetes-namespace-for-environment-isolation.md,## Context\nWe need to run multiple instances of the Datalabs system to allow us to continue to\ndevelop while giving early adopters access to the system. We intend to run both a test\nand production environment and need to decide whether to do this as a completely separate\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\n,We have decided to run both environments on the same Kubernetes cluster but with a\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\ntaken to avoid the maintenance overhead of having two clusters.\n,71,743,71,45
datalab/0009-bastion-for-cluster-access.md,## Context\nIt will be necessary to access the servers that form the Datalabs network but we do not\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\n,"We have decided that all access to the cluster will be via a Bastion server over an SSH\nconnection on port 22. We will restrict access through the firewall to known IP address\nranges including the development workstations, the STFC VPN and the Tessella public IP\naddress.\nThis excludes public facing services that should be available over HTTPS on port 443 via\na different route.\n",44,745,44,81
datalab/0027-thredds-to-serve-netcdf-data.md,## Context\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\nARCHER system. The current usage requires data to be extracted using shell scripts and\nthis process takes a long time. We need to identify a better way to access this\ndataset to allow the Datalabs environment to make best use of it.\n,"We have decided to use a [Thredds](http://www.unidata.ucar.edu/software/thredds/current/tds/)\nserver to present a unified view of the dataset as it should provide significant\nperformance improvements over manual scripting.\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\nto allow us to provision a Thredds server in the JASMIN managed cloud.\n",77,746,77,96
datalab/0007-kubernetes-for-container-orchestration.md,"## Context\nWe are expecting to have to run a large number of containers across several servers\nand in different environments. Given this, we think that we need a Container\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\n",We have selected to use Kubernetes as our container orchestration platform. This is due\nto it being the choice that the JASMIN team have made and also that it has established\nitself as a clear industry favourite.\n,52,748,52,47
datalab/0023-use-of-apparmor-and-bane.md,## Context\nWhile Docker containers now provide good root isolation from the host compared to earlier\nversions of Docker there are still security risks. We intend to provide sudo access\nfor users of the Notebook containers and this significantly magnifies the risks.\n,We have decided to use [AppArmor](https://wiki.ubuntu.com/AppArmor) to improve our\ncontainer security with the intention to make all policies as restrictive as possible.\nTo make it easier to build AppArmor profiles we have also decided to use\n[Bane](https://github.com/jessfraz/bane).\nTo further secure containers we have also decided to run all of our custom build\ncontainers as a non root user.\n,49,749,49,92
datalab/0030-kubernetes-direct-api-access.md,## Context\nIn order to dynamically orchestrate the containers running in the Datalab environment we\nneed to interact with the Kubernetes API. There are several choices for this:\n* Use one of the officially [supported clients](https://kubernetes.io/docs/reference/client-libraries/#officially-supported-kubernetes-client-libraries).\n* Use one of the Node.js community clients\n* Directly interact with the Kubernetes REST API.\n,We have decided to directly interact with the Kubernetes REST API as this presented the\neasiest option for development.\nWe ruled out using a supported client as we didn't want to have to write a service in a\nlanguage we were not familiar with.\nWe trialled all of the Node.js community clients but didn't feel that they were complete\nenough to meet our needs and were poorly documented.\n,88,750,88,84
datalab/0038-yarn-workspaces-for-monorepo.md,"## Context\nWe are using a mono-repository structure for the Datalabs project, this permits\nsharing of components across independent services. In NodeJS sharing of code\nwithout duplication requires linking of modules to the dependant service.\nThere are a few libraries to manages code-sharing:\n* NPM Link\n* Yarn Workspaces\n* Learna\n","We have decided to use Yarn Workspaces as is very lightweight and offers\nmanagement of the links required for module share, including auto discovery of\nother services.\nWe have ruled out using straight NPM Links as they are difficult to set-up and\nshare between development team. We have used Learna on other project and found\nit to be very heavyweight and requires that is adds git commits to releasing new\nversions.\n",74,751,74,87
datalab/0026-auth0-for-authentication.md,"## Context\nUser Authentication is a complex problem, can be time consuming to implement and errors\nin implementation can lead to security vulnerabilities. We feel that authentication,\nwhile critical, is not a differentiating factor and want to offload the work to a\nmanaged service.\n",We have opted to use [Auth0](https://auth0.com/) as our Identify provider. This gives us\na quick way to integrate authentication into our application with minimal effort and as\nan open source project we are able to use the service free of charge.\n,56,752,56,55
datalab/0024-vault-for-secret-store.md,## Context\nIn order to dynamically provide secure Notebook containers it will be necessary to\ndynamically generate and securely store secrets. We want to isolate this from the\ndatabase and use a dedicated solution for this problem.\n,We have decided to use [Hashicorp Vault](https://www.vaultproject.io/) to store secrets.\nIt provides a dedicated system to securely store and manage access to secrets.\n,46,754,46,39
datalab/0022-material-ui-for-component-framework.md,"## Context\nWe need to select a front end component framework to style the web application. The\nchoices available are [MaterialUI](https://material-ui-next.com/),\n[SemanticUI](https://react.semantic-ui.com/introduction) and\n[ReactBootstrap](https://react-bootstrap.github.io/).\n","We have decided to use MaterialUI for the front end component library. Additionally, we\nhave chosen to use the vNext version that is still in alpha as it provides a\nsignificantly different API and we expect it to be released prior to the end of the\nproject.\n",65,756,65,57
datalab/0010-new-default-deploy-user-on-servers.md,## Context\nIn order to use Ansible to provision the servers there needs to be a user with sudo\naccess. The default administrator user configured onto the base VM is not configured\ncorrectly and we are unable to use SSH keypairs with this user. After investigation\nwe are not clear what the issue is with the administrator user and need alternative\noption.\n,We have decided to remove the password authentication from the administrator user having\nfirst provisioned a new deploy user with ssh keys for the team in the authorized keys.\nThis user will require a password for sudo which will be stored in the ansible vault to\nallow automated provisioning.\n,75,757,75,56
datalab/0017-separate-storage-cluster-rather-than-hyper-converged.md,## Context\nWe need to decide whether to run our storage cluster as a standalone cluster or\nhyper-converged by running pods on the Kubernetes cluster.\n,We have decided to run a standalone storage cluster. The reason for using a separate\ncluster is that by keeping the persistent data separate we keep flexibility over the\nKubernetes cluster and can drop and recreate it without having to worry about the data.\n,32,758,32,50
datalab/0003-ubuntu-16-04-as-server-os.md,"## Context\nWe need to select a base operating system to install on all virtual machines that form\nthe Datalabs environment. There are three choices available through the JASIMN portal\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\n",We have selected Ubuntu 16.04 as the base operating system for our servers for several\nreasons:\n* The team are more familiar with Ubuntu over CentOS.\n* Packages are likely to be more easily available on Ubuntu.\n* CentOS 6.9 is no longer being updated (last update 10/5/2017).\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\n,58,759,58,98
datalab/0021-use-create-react-app.md,## Context\nWe need to decide whether to configure our web application code base ourselves or make\nuse of the [Create React App](https://github.com/facebookincubator/create-react-app)\nto provide project configuration.\n,We have decided to use the Create React App project to provide the base for our React\nproject as this brings best practice configuration and build and is actively being\ndeveloped.\n,46,760,46,37
datalab/0020-redux-for-state-store.md,## Context\nBuilding a complex web application brings challenges around how to manage state. The\nRedux website provides an excellent [motivation page](http://redux.js.org/docs/introduction/Motivation.html) that discusses the challenges in detail.\n,"We have decided to adopt the [Redux](http://redux.js.org/) architecture to provide a clean separation between\nour views, actions and state store.\n",49,762,49,32
datalab/0005-docker-containers-for-everything.md,## Context\nThe Datalabs project has challenging vision for dynamic creation of scientific analysis\nenvironments. We need to decide what technology will enable us to meet this vision.\n,We do not think that regular VM orchestration will meet the vision and have instead\nopted to deploy all services (where possible) as Docker Containers. Alternative container\ntechnologies were not evaluated as the team has previous Docker experience and it is the\nclear leader in this space.\n,35,763,35,60
datalab/0039-prometheus-operator-helm-chart.md,## Context\nWe need to decide between [Kube Prometheus](https://github.com/coreos/kube-prometheus)\nand [Prometheus Operator](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nfor use in deploying the prometheus monitoring solution for DataLabs.\n,We have decided to use the [Prometheus Operator Helm Chart](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\noption. The Prometheus Operator Helm chart provides a similar feature set to the\nkube-prometheus option.\n,63,764,63,71
datalab/0013-custom-redbird-proxy.md,## Context\nWe need to select a proxy server to proxy requests to services running in the private\nKubernetes cluster.\n,"We have decided to use [RedBird](https://github.com/OptimalBits/redbird) as a reverse\nproxy. This was selected as it is written in Node.js which we expect our applications\nto be written in and will allow us to extend to support authentication, logging and other\nedge concerns.\n",25,765,25,65
datalab/0033-enzyme-for-react-testing.md,## Context\nWe need to choose a rendering library to allow testing of React components.\n,We have chosen to use [Enzyme](https://github.com/airbnb/enzyme) to provide a library\nfor shallow rendering.\n,18,766,18,31
datalab/0006-dockerhub-for-container-registry.md,## Context\nHaving selected to run all services and applications in Docker containers we need a\nregistry to store them in.\n,We have decided to store the Docker containers in [DockerHub](https://hub.docker.com/u/nerc/dashboard/). Given the project is Open Source this seemed to be the easiest option\nas most tools default to this registry.\n,25,767,25,49
datalab/0008-weavenet-for-overlay-network.md,## Context\nKubernetes does not provide an overlay network out of the box and we need to choose\nwhich one to use from [here](https://kubernetes.io/docs/concepts/cluster-administration/networking/).\n,We have decided to use the [WeaveNet](https://www.weave.works/oss/net/) network as this\nhas already been used by the JASMIN team. It also appears easy to use and there is good\ndocumentation.\n,46,768,46,53
datalab/0015-use-kong-for-reverse-proxy.md,## Context\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\nDashboard. We have tested using an Nginx proxy and have found that this provides the\nsupport required but does not provide an API for configuration.\n,We have decided to use [Kong](https://getkong.org/) for our reverse proxy as it is a\ncustom build of Nginx that provides an API. We have tested with all of our services and\nit appears to meet our needs.\n,53,769,53,55
datalab/0014-use-traefik-for-reverse-proxy.md,## Context\nThe Redbird proxy does not support WebSockets which are required to support the\ninteractive notebooks. We have also had problems with the reliability\nof the proxy and have found it difficult to configure.\n,We have decided to replace the custom Redbird proxy with a [Traefik](https://traefik.io/)\nproxy as this looks easier to configure and claims Web Socket support.\n,44,770,44,40
datalab/0002-ansible-for-provisioning-tool.md,## Context\nWe need a tool to provision servers and software for the datalabs project.\n,We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\nteam have experience using it.\n,20,771,20,30
datalab/0029-dask-for-python-distributed-compute.md,## Context\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\ngreat success using [Dask](https://dask.pydata.org/en/latest/) as their distributed\ncompute environment. Dask appears that it could be easier to use than Spark for users\nwho already know Python and NumPy.\n,We have decided to offer Dask in addition to Spark within the Datalabs platform. This\nenables us to appeal to more users at limited cost.\n,69,773,69,33
datalab/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,774,16,39
ditto/DADR-0001-record-architecture-decisions.md,## Context\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) and also use the proposed\n[template](https://github.com/joelparkerhenderson/architecture_decision_record/blob/master/adr_template_by_michael_nygard.md).\n",26,778,26,79
ditto/DADR-0006-merge-payload.md,## Context\nWe want to allow partial or merge updates of things with a single request.\n,A merge request\n* uses HTTP `PATCH` method.\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https://tools.ietf.org/html/rfc7396).\n* has the request header `content-type` set to `application/merge-patch+json`.\n,19,779,19,65
frontend-library/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,784,16,39
infrastructure-adrs/0004-remove-workflow-datastream.md,"## Context and Problem Statement <!-- required -->\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\n",Remove the datastream.\nThis was done in dor-services v9.0.0 ([commit](https://github.com/sul-dlss/dor-services/commit/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\n,39,792,39,62
form-design-system/remove_mui_theme.md,"## Context and Problem Statement\nThe [material-ui](https://material-ui.com) library is considered ""legacy"". In FDS, we\naim to provide component coverage so that in the future, consumers will no longer need to\nrely on material-ui.\nThe FDS node module however, still provides a `mui-theme` theming object for the\nmaterial-ui library.\n",Removed `mui-theme` from FDS.\n### Negative Consequences <!-- optional -->\n- Consumers will need to handle the breaking change. Consumers are now responsible for\ndefining their own material-ui base theme.\n,78,799,78,45
crispy-dragon/ADR-1-Serverless-Framework.md,"## Context\nThe resources, database, storage and API endpoints needs to be managed as a unit.\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\n",To help package the individual AWS resource into a serverless application we will use [Serverless](https://www.serverless.com/) framework.\nWe also considered the following alternative solutions:\n* Manual creation of resource in AWS throurgh the AWS Console.\n* AWS Serverless Application Model (AWS SAM)\n,43,846,43,64
crispy-dragon/ADR-2-Frontend-Framework.md,## Context\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\n,"The frontend will be build with [React](https://reactjs.org/) and [Snowpack](https://www.snowpack.dev) will be used as build tool.\nThe application will be styled with [Tailwind CSS](https://tailwindcss.com)\nWe also considered the following alternative solutions:\n* Vanilla HTML would work for the scope of this application.\n* Vanilla CSS would work, but take more time to generate nice layouts.\n",27,847,27,92
infra/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,859,16,39
nada-kafkarator/0002-use-python.md,"## Context\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\nAnother option was Rust, which could be a useful learning exercise.\nPython is used in FIAAS, so we have some experience using it for an operator.\n",We will use Python for Kafkarator.\n,61,861,61,11
nada-kafkarator/0007-only-for-aiven.md,"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n","Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n",72,863,72,25
nada-kafkarator/0004-combine-topic-creation-and-credentials-management-in-same-app.md,"## Context\nThe project requires dealing with two relatively separate concerns:\n1. Create topics when needed\n2. Supply credentials for working with topics.\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\nso it makes sense to keep them in the same application.\n","We will ignore the SRP in this instance, and keep the two concerns in the same application.\n",83,866,83,21
nada-kafkarator/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,867,16,39
profiles-etl-combiner/0003-remove-mongodb-update-and-write-to-azure.md,## Context\nThe merge of the data sources is a common operation from which the generated asset can be used by several different processes.\nCurrently the generated asset is only available for the immediately following process within this application i.e. updating MongoDB.\nThere is an immediate need to use the same generated asset for updating Elasticsearch.\nThe [mongodb-updater](https://github.com/nhsuk/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.\n,"The `gp-data-merged.json` file will be written to the team's preferred cloud hosting platform, enabling the merged data to be used as a\nsource for both the `mongodb-updater` and the forthcoming `elasticsearch-updater`.\n",100,870,100,51
profiles-etl-combiner/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,871,16,39
python-library-project-generator/0000-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on Opinionated Digital Center.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",18,875,18,39
modular-monolith-with-ddd/0015-use-in-memory-events-bus.md,"## Context\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some ""events bus"" to do that.\n",Solution number 1 - In Memory Events Bus</br>\nAt that moment we don't see more advanced integration scenarios in our system than simple publish/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\n,35,885,35,54
modular-monolith-with-ddd/0003-use_dotnetcore_and_csharp.md,"## Context\nAs it is monolith, only one language (or platform) must be selected for implementation.\n","I decided to use:\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\n- F# will not be used, I don't have commercial experience with it\n",23,886,23,77
modular-monolith-with-ddd/0014-event-driven-communication-between-modules.md,"## Context\nEach module should be autonomous. However, communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\n","Solution number 2 - Event-driven (asynchronous)</br>\nWe want to achieve the maximum level of autonomy and loose coupling between modules. Moreover, we don't want dependencies between modules. We allow direct calls in the future, but this should be an exception, not a rule.\n",36,887,36,59
modular-monolith-with-ddd/0007-use-cqrs-architectural-style.md,"## Context\nOur application should handle 2 types of requests - reading and writing. </br>\nFor now, it looks like:</br>\n- for reading, we need data model in relational form to return data in tabular/flattened way (tables, lists, dictionaries).\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\n","We applied the CQRS architectural style/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\n",87,888,87,60
modular-monolith-with-ddd/0002-use_modular-monolith-system-architecture.md,## Context\nAn advanced example of Modular Monolith architecture and tactical DDD implementation in .NET is missing on the internet.\n,I decided to create nontrivial application using Modular Monolith architecture and Domain-Driven Design tactical patterns.\n,26,889,26,22
modular-monolith-with-ddd/0008-allow-return-result-after-command-processing.md,"## Context\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always ""void"". However, sometimes we need to return some data immediately as part of the same request.\n","We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\n",55,890,55,44
modular-monolith-with-ddd/0005-create-one-rest-api-module.md,"## Context\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\n",Solution 1.\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\n,34,892,34,45
modular-monolith-with-ddd/0017-implement-archictecture-tests.md,"## Context\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\n",We decided to implement Unit Tests for our architecture. </br>\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\n,56,894,56,48
modular-monolith-with-ddd/0013-protect-business-invariants-using-exceptions.md,"## Context\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\n","Solution number 1 - Use exceptions. </br>\nPerformance cost of throwing an exception is irrelevant, we don't want too many if/else statements in entities, more familiar with exceptions approach.\n",32,895,32,41
modular-monolith-with-ddd/0009-use-2-layered-architectural-style-for-reads.md,"## Context\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\n","We will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\n",50,897,50,66
modular-monolith-with-ddd/0016-create-ioc-container-per-module.md,"## Context\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\n",Solution number 2 - IoC Container per module</br>\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\n,37,899,37,46
modular-monolith-with-ddd/0001-record-architecture-decisions.md,"## Context\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\n","For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\nEach ADR will be recorded using [Michael Nygard template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\n",30,900,30,77
modular-monolith-with-ddd/0010-use-clean-architecture-for-writes.md,"## Context\nWe applied the CQRS style (see [ADR #7](0007-use-cqrs-architectural-style.md)), now we need to decide how to handle writing operations (Commands).\n","We will use **Clean Architecture** to handle commands with 4 layers: **API layer**, **Application Service layer**, **Infrastructure layer** and **Domain layer**. </br>\nWe need to add Domain layer because domain logic will be complex and we want to isolate this logic from other stuff like infrastructure or API. Isolation of domain logic supports testing, maintainability and readability.\n",43,901,43,79
csc-swr-architecture/004-Use-Fake-Data.html.md.erb,"## Context\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\n","The placement alpha prototypes and development work will only process fake data.\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the ""Authority to Operate"" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\n",53,915,53,80
csc-swr-architecture/001-Use-GitHub-Workflow-for-ADRs.html.md.erb,## Context\nArchitectural Decisions are not made in isolation and need to be discussed and agreed amongst the Service Team.\n,"Utilise [GitHub Workflow](https://guides.github.com/introduction/flow/) to manage the acceptance, or not, of ADRs by the Service Team. The Pull Request review process, prior to merging into `main`, provides suitable mechanisms for tracking comments, amendments, approvals and so on.\n",26,916,26,62
csc-swr-architecture/005-Monolith-First.html.md.erb,## Context\nThe CSC Placement service is taking an [Evolutionary Architecture] approach.\n,"Since we are in a highly uncertain phase of the Service, and we want to optimise around developer productivity and feature prototyping, the adoption of a [Monolith First] architecture is considered best.\nAlthough Martin Fowler uses this as a precursor for a [Microservice Architecture], it is not definitely regarded as a transition to Microservices here, but rather a reasonable first step before deciding future architectural quanta.\n",19,917,19,83
universal-remote-control-skill/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,922,16,39
terraform/AMIs.md,"## Context\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build/maintain our own images.\n","We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\n",50,923,50,77
terraform/OS-Patching.md,"## Context\nAny operating systems that we use are likely to be patched for bug-fixes or security reasons at least once per month. Ideally we would pick up the latest stable release all the time, however this implies a great deal of churn. Such change would also come in spikes of activity e.g. around Windows patch tuesdays.\n",We will not patch our operating systems in AWS. Instead we will pin every instance to a specific version of an operating system and move the pin (via a PR) only when necessary.\n,69,924,69,38
terraform/Scale-and-consolidate.md,"## Context\nWe are moving from platforms such as VMWare, where instances are kept small and are encouraged to scale out. If we were to scale up instead, then we would require either\n1. A workload that is able to make efficient use of multi-threading.\n1. Workloads that are able to co-exist peacefully.\nIt cannot be assumed that the workloads we are moving into AWS will fit neatly into either category.\n",Our preference for enterprise applications is to consolidate onto a smaller number of instances.\n,91,925,91,16
terraform/SSH-and-RDP.md,"## Context\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\n","RDP and SSH are allowed for all non-production EC2 istances, but are left blocked for production EC2 instances.\n",66,927,66,26
terraform/Amazon-Linux.md,"## Context\nWe are migrating from several different versions of Windows and Linux in DXC to AWS. The easiest migration path would be to move applications and databases to exactly the same platform in AWS. The lowest TCO for the target solution would be provided by migrating onto a small number of consolidated Linux platforms (assuming we can't go serverless for everything). These two options are mutually exclusive, so we need a decision on the way forward.\n","Our strong preference is to go serverless. Where we can't go serverless we prefer to use the latest AWS Linux, but accept that the choice of OS is often dictated by the application and/or database layers.\n",89,928,89,44
terraform/Automation.md,"## Context\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\n","We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\n",93,929,93,34
postcode-checker/0001-use-postgres-database.md,## Context\nOur postcode checker uses a database to store valid service areas and allowed post codes.\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https://postcodes.io).\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\n,We will use Postgres for our database.\n,91,930,91,10
tech-team/0009-supported-python-versions-containers.md,## Context\neLife has numerous projects written completely and partly with the [Python programming language](https://www.python.org/).\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\nPython official images make it easy to support a new Python version without custom PPAs.\n,We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\n,98,932,98,81
tech-team/0008-civi.md,"## Context\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\n","We will push responsibilities away from CiviCRM, favoring [Separate Ways](http://culttt.com/2014/11/26/strategies-integrating-bounded-contexts/) or API-based integration of an external microservice over developing inside CiviCRM.\n",80,934,80,57
tech-team/0004-supported-python-versions.md,## Context\neLife has numerous projects written completely and partly with the [Python programming language](https://www.python.org/).\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\n,We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\n,79,935,79,53
tech-team/0001-pull-request-workflow.md,"## Context\nSoftware is built incrementally as an accumulation of changes.\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\nShort-lived pull requests allow:\n- visibility of who is changing what.\n- discussion and review from other people in the team.\n- automated testing and other kind of checks to run, offloading work from humans to machines.\n",We will open short-lived pull requests as the primary means for deploying a change.\n,85,938,85,17
summits/0003-use-java-newer-than-java-8.md,## Context\nWe use Java 8 in all projects so far. However the most current version of Java as of now is version 13.\n,Use Java 13 to start the Java version of these services.\n,30,940,30,14
summits/0002-use-docker-as-runtime-for-services.md,"## Context\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\nDocker is the de-facto standard today and is the system used by us for all projects.\n",Use docker to deploy all services.\n,53,941,53,8
summits/0004-move-services-to-top-level.md,## Context\nInitially the directory structure was to have the language as a top folder and have services below. However this makes it harder to use different language versions of the services together.\n,Keep a monorepo for easier handling the project (it will never be grow too large and will not be maintained by different teams).\nMove the services to top level. And have language versions below each service.\n,37,942,37,44
summits/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,943,16,39
edgex-docs/0001-Registy-Refactor.md,- [Context](#context)\n- [Proposed Design](#proposed-design)\n- [Decision](#decision)\n- [Consequences](#consequences)\n- [References](#references)\n<!--te-->\n,"- [Consequences](#consequences)\n- [References](#references)\n<!--te-->\nIt was decided to move forward with the above design\nAfter initial ADR was approved, it was decided to retain the `-r/--registry` command-line flag and not add the `Enabled` field in the Registry provider configuration.\n",51,944,51,70
pharmacy-data-etl/0002-store-etl-output-in-the-cloud.md,## Context\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\n,"When the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\nThe output file will be exposed on a publicly available URL.\n",25,963,25,37
pharmacy-data-etl/0003-generate-file-containing-organisation-data.md,## Context\nThe [NHS Organisation API](http://api.nhs.uk/organisations) holds information about pharmacies.\nThe [Connecting to Services](https://github.com/nhsuk/connecting-to-services) applications needs to make use of Pharmacy data.\n,The NHS Organisation API will be scraped nightly to generate a file containing pharmacy data.\n,53,964,53,17
pharmacy-data-etl/0004-make-etl-re-entrant.md,"## Context\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\n","The ETL will be re-entrant, storing state on a regular basis.\nRestarting the ETL will pick up from the last point.\n",47,965,47,31
pharmacy-data-etl/0005-refresh-data-on-major-version-change.md,## Context\nData structure may change between releases of the ETL.\nThe ETL uses a scrape of previous data to reduce unnecessary work.\n,The major and minor version will be included in the seed data file to identify a change of data structure.\n,29,966,29,22
pharmacy-data-etl/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,967,16,39
golang-git-fritz/0002-implement-with-golang.md,## Context\nWe need to choose a language to in which to code this project.\n,Use Golang.\n,18,968,18,5
golang-git-fritz/0003-follow-conventional-commits.md,"## Context\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\n","Use [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\n",52,969,52,46
golang-git-fritz/0001-record-architecture-decisions.md,## Context\nSome records are consistently followed and should be centralized for consistency.\n,Use jncmaguire/adr.\n,16,970,16,9
xebikart-infra/006-use-password-authentication-for-rabbitmq.md,## Context and Problem Statement\nWe are deploying a RabbitMQ cluster on GKE and people will need to connect to\nit. We want to provide some kind of authentication so it's not totally\nopen-bar.\nWhat authentication mecanism should we use?\n,"Chosen option: **""User/Pawword""**, because:\n- We want to make the RabbitMQ cluster quickly available to teams\n- It will be easy to change to client certificate auth later on\n- It is better than not providing auth at all\nWe **will replace it by client certificates authentication later**.\n",54,981,54,67
dl4j-dev-tools/0003-dealing_with_inconsistencies_in_java_naming.md,## Context\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\n`BernoulliDistribution`.\nTwo possible solutions where suggested:\n1. Add an additional property that provides us with the correct class name\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\n,For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\nname.\n,92,986,92,27
dl4j-dev-tools/0004-auto_initialization_for_inplace_operations.md,"## Context\nSome operations work in-place on the inputs that they are given in ND4J, but in SameDiff the same operations will\ngenerate an array from a given shape. Examples for this include `BernoulliDistribution`, and other random ops, that\neffectively initialize the array that they are given.\nFrom a consistency point of view, it would be nice if both API's would support both ways of using those ops.\n","We introduce an option to mark inputs as `inPlace = true` to make it clear that this input is going to be changed\nin-place. In addition we introduce an option `supportsInPlaceInit = true` to mark an input as initialize-able. If the\n`supportsInPlaceInit` option is enabled, two signatures for the Op will be created, one that takes an input, and one\nthat takes the appropriate shape and data type information in its stead.\n",92,993,92,98
CCS-Architecture-Decision-Records/0014-use-new-Salesforce-organisation-for-CRM.md,"## Context\nSalesforce is currently live within CCS and is used for a number of processes, including managing commercial agreements.  It has not so far been used as a pure CRM tool.\nIt has been agreed by Exec to implement a CRM system.  As Salesforce was originally designed as a CRM tool, it is proposed to utilise Sf in this context.  The existing instance, however, has been significantly customised and contains a large quantity of poor-quality data.\n","The current Sf instance be maintained, but that a new Sf instance specifically for use as a CRM, without significant customisation.\n",96,997,96,28
Data-Platform-Playbook/007-sftp-to-s3-lambda.md,## Context\nAn SQL dump of the liberator database is put in a SFTP server as a zip file by a supplier. This data is important to the parking team so we need to ingest it into the platform. In order to do this we first need to move the data from the SFTP server into the landing zone S3 bucket.\nThere was also the option to pay for the supplier to directly drop the file into S3.\n,"We will use a lambda function to achieve this. The function will run at 06:00 UTC, triggered by a cloudwatch event, finding the file date stamped for that day then streaming it into the given S3 bucket.\nIt finds the file by searching for a filename pattern, if there is more than one file that matches then it will get all of them.\nThe connection details for the SFTP server are stored in ssm parameter store.\n",90,1010,90,92
Data-Platform-Playbook/003-role-based-access-control.md,## Context\nWe will be storing sensitive council data within S3 and therefore need to restrict access to this data based on the\ndepartment the user belongs to.\n,"In order to limit access, we propose to store all S3 buckets in a single AWS account. Users accessing this account\ndirectly will have little or no access to the owned S3 buckets, instead through the infrastructure deployment process\n(terraform) we will share a partition of the S3 buckets to relevant department accounts.\nE.g. s3://s3-bucket/social-care/\* -> Social Care Account\n",35,1011,35,86
Data-Platform-Playbook/005-recovered-data.md,"## Context\nHackney have recovered a number of datasets from prior to the Cybersecurity incident, these datasets will need to be\nmerged with datasets created after the incident.\n","Recovered datasets that are in a consumable format (json, csv, .sql) will be deposited into the landing zone directly,\nusing the same partition storage strategy as other data sets [see ADR 0004](partition-strategy)\n",35,1013,35,50
Data-Platform-Playbook/009-ingesting-data-from-apis.md,## Context\nThe Data Platform currently has no capacity to ingest data from external APIs\n,"![API ingestion](../images/api-ingestion.png)\nDeploy a Lambda that will call the the required API and save the returned data\nobject into S3, once all the records have been downloaded the lambda will\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\n",17,1015,17,77
Data-Platform-Playbook/002-ingest-google-sheets-data.md,## Context\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\nefforts. We need to get this information pulled into the data platform for processing.\n,We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\n,41,1016,41,25
Data-Platform-Playbook/001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1018,16,39
edward/0002-implement-in-go.md,"## Context\nEdward will be provided as a command-line tool, ideally across multiple operating systems. It will need a simple means of installation and updating.\n",Edward shall be implemented using Go.\n,31,1019,31,8
edward/0003-use-cobra-for-command-line-interface.md,## Context\nEdward provides a rich command-line interface intended to use a command structure familiar to most developers.\n,[Cobra](https://github.com/spf13/cobra) will be used as a CLI framework.\n,22,1020,22,22
edward/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1021,16,39
poc-terraform-aws-web-cluster/0001-pos-decisions.md,"## Context\nFor the Proof of Skill assignment several auxillary technology have been used on top of the required terraform. As terraform\nas a infrastructure as source is a given in the assignment, others will be explained here.\n","### Repeatable project creation\nThe use of ansible as a project templater. Ansible is selected to have a reentry method to create projects from a template. In this case this is more used for documentation than template but it is used\nto illustrate its use. The author is aware that ask nicely is using Puppet, but the author is more familliar with ansible and as this is an anxillary\ntool this one is selected over puppet.\n",47,1022,47,94
poc-terraform-aws-web-cluster/0003-packer-as-builder.md,"## Context\nIn AWS the use of AIM images is used to create and customize base line and derived application images from. These can be created with a tool like packer that will take a base image, configure it, and then has a ready to use AIM image ready.\n",Packer is a multi cloud vendor tool specialized in provision images. It has been decided by the author to use this as a quick prototype example of how to use this in a simple CI/CD example.\n,56,1024,56,42
lobiani/0007-use-events-as-communication-mechanism-between-modules.md,"## Context\nWe need a communication mechanism between modules. We can let each module directly consume the API of another module, or\nwe could let each module subscribe to events published by other modules\n","Since in [ADR 2](0002-start-with-modular-monolith-backend.md) we decided to be ready for microservices, we will use the\nsecond approach - events as the communication mechanism\n",39,1030,39,41
lobiani/0005-use-cqrs-architectural-style.md,"## Context\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\nfor reads and writes at the same time in regard to performance and maintainability.\n",We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\ndomain entities each respectively on the Command (write) and Query (read) sides\n,45,1031,45,38
lobiani/0011-develop-main-frontend-shopping-as-a-separate-ssr-app.md,"## Context\nIn [ADR 9](0009-develop-admin-tool-as-spa.md) we decided to develop admin tool as an SPA. Unlike admin tool, shopping\nhas different requirements:\n- User experience matters\n- Should be accessible for unauthenticated users\n- User interface should have unique look and feel, more suited for public use\n- Should be SEO friendly\n",We will develop shopping as a separate frontend application employing Server Side Rendering technique to improve the\nuser experience without compromising interactivity.\n,76,1033,76,26
lobiani/0003-use-jvm-for-backend.md,"## Context\nSince the backend is monolithic, we need to choose a language or at least a platform\n","We will develop on JVM platform, specifically we will use Kotlin for production code and Groovy for tests\n",22,1036,22,21
lobiani/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1038,16,39
testy/0009-asynchronous-code-support.md,"## Context\nIt's necessary for a testing tool to support asynchronous code, the community is using a lot async/await in their code.\nWe want Testy to be competitive with other testing tools, so we need to provide a solution for asynchronous code.\n",Fully implement asynchronous code support ([#106](https://github.com/ngarbezza/testy/issues/106)). This will allow us to\nuse async/await in both tests and hooks (before/after).\n,53,1042,53,44
testy/0003-zero-dependencies.md,"## Context\nKeeping the tool simple, minimal, easy to install and understand.\n",Do not depend on any NPM package unless there's a strong reason to do that. Keep the zero dependence counter as much\nas possible.\n,17,1043,17,30
testy/0004-console-ui-and-formatter.md,## Context\nReducing complexity in the `ConsoleUI` object and make it more reusable and testable.\n,"`ConsoleUI` now only knows when to output things, but not the contents of messages, which is now responsibility of a\n`Formatter`. This object can be replaced by other formatters in the future.\n",23,1044,23,43
testy/0002-place-utilities-in-utils-module.md,## Context\nHaving control of all the utilitary functions the tool might need.\n,"Use the `Utils` module for any responsibility whose domain does not seem to be part of other objects' essence. Every time you feel you need a ""helper"" function, it is a good candidate for `Utils`. If you need to rely on `typeof`, it is also a good sign for needing `Utils`. We want to maximize reuse of these functions by having in a single, ""static"" place. We understand this might be a temporary solution.\n",17,1045,17,93
testy/0005-avoiding-test-doubles.md,## Context\nTests against the real system instead of test doubles was proven to be useful detecting bugs and exercising better our\nsystem.\n,Avoid introducing test doubles as much as possible. The only allowed exception is to stub/simulate external systems we\ncan't control. Library code should never be stubbed.\n,27,1046,27,37
testy/0006-avoid-offensive-vocabulary.md,"## Context\nThere is an initiative about questioning the technical vocabulary we use, and avoiding some words that are widely used\nand may offend people. This is a project that adheres to this initiative, therefore...\n","Remove all existing technical vocabulary that might be offensive, and prevent those terms to be added in the future.\nFor instance, the use of ""master/slave"" replaced by ""main/replica"" (or similar), or ""whitelist/blacklist"" by\n""safelist/blocklist"" (or similar).\n",43,1047,43,65
testy/0007-support-node-versions-with-at-least-security-updates.md,## Context\nMaking clear which versions are supported and how this will be updated as time passes.\n,"Only support Node versions with active and security support. Do not support newer, unstable versions.\nWe can use [endoflife.date](https://endoflife.date/nodejs) as a reference. Example: at the moment\nof this decision, only Node 12, 14 and 16 should be supported.\n",20,1048,20,66
testy/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1049,16,39
aws-sync-routes/0005-uri.md,"## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\nThere isn't an obvious fit for this in the official REST API URI specifications.\n",The `/vpcs/{vpcId}/route-tables/{routeTableId}` URI will be used.\n* `{vpcId}` is the VPC ID.\n* `{routeTableId}` is the main route table ID\n,64,1050,64,49
aws-sync-routes/0003-http-patch-method.md,"## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\nOf the available HTTP methods, there isn't a perfect fit for this use case.\nPUT & PATCH were generally recommended for similar scenarios.\n",The HTTP PATCH method will be used.\n,76,1051,76,9
aws-sync-routes/0002-aws-amplify-cli-toolchain.md,"## Context\nThe requested functionality was an API endpoint that would synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event.\nAll resources should be managed programmatically for an optimal possible user experience.\n","The AWS Amplify CLI toolchain will be used for programmatically creating, updating, and destroying project resources.\nThe endpoint will be defined in an AWS API Gateway, and the synchronization functionality will be defined in a Lambda function.\n",55,1052,55,46
aws-sync-routes/0006-specificity.md,## Context\nFeature requests for both summary routes and dynamic route discovery have been proposed so that new routes are automatically discovered and synchronized in order to reduce the risk of new routes being missed.\n,Programmatically making changes to a production routing table is serious business that has the potential to cause network outages.\nOnly specified routes will be synchronized to prevent unintentional changes.\n,38,1054,38,37
aws-sync-routes/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this [article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,1055,16,41
caritas-onlineBeratung-nginx/0002-log-rotation-for-nginx.md,"## Context\nWe need a log rotation for the nginx webserver. As the nginx does not support log rotation out of the box, we need our own solution.\n","We implement the log rotation using the programm logrotate inside the docker container for the nginx webserver. We do not trigger the log rotation from outside of the docker container, because we want a solution that is independed from the host system.\n",34,1056,34,50
caritas-onlineBeratung-nginx/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1057,16,39
reading-todo/003-postgres-as-database.md,## Context\nFor the project a RDBMS need to be chosen and adopted.\n,- [PostrgreSQL](https://www.postgresql.org/) will be the relational database management system of choice.\n,18,1058,18,24
reading-todo/002-react-and-jest-for-front-end.md,## Context\nFor the project a front end technology need to be chosen and adopted coupled with a unit test framework.\n,"[React](https://reactjs.org/) as a front end development library, and [Jest](https://jestjs.io/) as a unit testing framework.\n",24,1060,24,34
reading-todo/004-kubernetes-with-docker-for-deployment.md,## Context\nFor the project a deployment and development mechanism is need to be chosen and adopted.\n,"- [Kubernetes](https://kubernetes.io/) will be the deployment technology of choice.\nThe project will be deployed into a kubernetes cluster using [helm](https://helm.sh/).\n- [Docker](https://www.docker.com/) will be the container building technology of choice.\nEvery thing will be directed through docker containers, from development to deployment. CI Pipeline will run inside a docker container as well.\n",20,1061,20,87
log4brains/20201103-use-lunr-for-search.md,"## Context and Problem Statement\nWe have to provide a search bar to perform full-text search on ADRs.\n## Decision Drivers <!-- optional -->\n- Works in preview mode AND in the statically built version\n- Provides good fuzzy search and stemming capabilities\n- Is fast enough to be able to show results while typing\n- Does not consume too much CPU and RAM on the client-side, especially for the statically built version\n","- Works in preview mode AND in the statically built version\n- Provides good fuzzy search and stemming capabilities\n- Is fast enough to be able to show results while typing\n- Does not consume too much CPU and RAM on the client-side, especially for the statically built version\nChosen option: ""Option 2: Lunr.js"".\n",87,1064,87,70
log4brains/20200926-use-the-adr-number-as-its-unique-id.md,"## Context and Problem Statement\nWe need to be able to identify uniquely an ADR, especially in these contexts:\n- Web: to build its URL\n- CLI: to identify an ADR in a command argument (example: ""edit"", or ""preview"")\n","Chosen option: ""ADR number"", because\n- It is possible to have duplicated titles\n- The filename is too long to enter without autocompletion, but we could support it as a second possible identifier for the CLI in the future\n- Other ADR tools like [adr-tools](https://github.com/npryce/adr-tools) already use the number as a unique ID\n",56,1068,56,80
cnp-design-documentation/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1077,16,39
monolith/0004-asset-integrity-check.md,"## Context\nIn HTML5, `link` and `script` nodes have an attribute named `integrity`, which lets the browser check if the remote file is valid, mostly for the purpose of enhancing page security.\n","In order to replicate the browser's behavior, the program should perform integrity check the same way it does, excluding the linked asset from the final result if such check fails.\nThe `integrity` attribute should be removed from nodes, as it bears no benefit for resources embedded as data URLs.\n",45,1080,45,59
monolith/0003-network-request-timeout.md,## Context\nA slow network connection and overloaded server may negatively impact network response time.\n,"Make the program simulate behavior of popular web browsers and CLI tools, where the default network response timeout is most often set to 120 seconds.\nInstead of featuring retries for timed out network requests, the program should have an option to adjust the timeout length, along with making it indefinite when given ""0"" as its value.\n",18,1081,18,66
monolith/0006-reload-and-location-meta-tags.md,## Context\nHTML documents may contain `meta` tags capable of automatically refreshing the page or redirecting to another location.\n,"Since the resulting document is saved to disk and generally not intended to be served over the network, it only makes sense to remove `meta` tags that have `http-equiv` attribute equal to ""Refresh"" or ""Location"", in order to prevent them from reloading the page or redirecting to another location.\n",25,1082,25,62
monolith/0007-network-errors.md,"## Context\nServers may return information with HTTP response codes other than `200`, however those responses may still contain useful data.\n","Fail by default, notifying of the network error. Add option to continue retrieving assets by treating all response codes as `200`.\n",26,1085,26,27
monolith/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1087,16,39
FindMeFoodTrucks/Hosting Model.md,## :dart: Context\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\n,The application will be hosted on Microsoft Azure Cloud datacenter.\n,43,1089,43,13
FindMeFoodTrucks/CosmosDB API.md,## :dart: Context\nAzure Cosmos DB provides different APIs to access and interact with the data it stores.\n* Core(SQL) API\n* Mongo DB API\n* Cassandra API\n* Azure Table\n* Gremlin (graph) API\nChoosing the right API model will play a role in integration complexity and data access performance.\n,The recommended approach is to use Core(SQL) APIs.\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/3-analyze-the-decision-criteria\n,68,1090,68,51
FindMeFoodTrucks/Messaging choice.md,## :dart: Context\nThe Web APIs can be implemented as Synchronous or Asynchronous.\n,"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\nThe decision is based on the guidance provided by Microsoft here: https://azure.microsoft.com/mediahandler/files/resourcefiles/api-design/Azure_API-Design_Guide_eBook.pdf\n",20,1091,20,60
FindMeFoodTrucks/Choice of service for Gatekeeper.md,## :dart: Context\nThe application APIs have the following requirements around the interfaces\n* Should support a rich developer experience\n* Should support Gatekeeper pattern for security\nThe following options for service are considered for this:\n* App Gateway\n* API Management\nChoosing the right service to implement the gatekeeper patter will help optimize the development experience and operational cost\n,The recommended approach is to use Azure API Management considering the following points:\n* Support for a developer portal\n* Ability to offload security\n,73,1092,73,29
FindMeFoodTrucks/Architecture Style.md,## :dart: Context\nFollowing Architecture styles were considered for the application\n1. n-tier\n1. Web-Queue-Worker\n1. Microservices\n1. Big data/compute\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\n,A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https://martinfowler.com/bliki/CommandQuerySeparation.html\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/\n,58,1094,58,76
frontend/001-spa.md,"## Context\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\n",Build a SPA.\n,89,1096,89,5
zsh/0002-use-drop-in-pattern-for-extensibility.md,"## Context\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\n",Create a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\n,37,1102,37,47
zsh/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1103,16,39
island.is/0005-error-tracking-and-monitoring.md,"## Context and Problem Statement\nKnow it before they do! We need a tool to discover, triage, and prioritize errors in real-time.\n","Chosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\n",30,1121,30,42
registraties/003-translations.md,"## Context\nTo allow this project to be used by other parties (mostly non-Dutch speaking), it will be necessary to be able to translate all content in the application that is not coming from the APIs.\n",The application's content will be stored in translation files. The code will make use of [React-Intl](https://github.com/formatjs/react-intl). Also see the [i18n documentation](https://github.com/react-boilerplate/react-boilerplate/blob/master/docs/js/i18n.md) in the [react-boilerplate repository](https://github.com/react-boilerplate/react-boilerplate).\n,43,1124,43,85
mongodb-updater/0002-store-common-database-configuration-in-the-repository.md,"## Context\nThe application loads data from a JSON file into a mongodb database. Along with the raw data the mongo database\nmay also need to create indexes to improve search performance or provide geolocation searches.\nThe configuration of a database such as database name, collection name, indexes and ID key remain the same\nacross environments.\n","As the index may be fundamental to the operation of the consuming application, such as the geolocation search in the pharmacy database,\nthese configurations should be stored in version control.\nGiven the small number of databases (currently only Pharmacy, and GP Profiles data are held in mongodb) it is pragmatic to co-locate the configuration files\nalongside the `mongodb-updater` code, rather than creating a new repository and file hosting for each database configuration files.\n",67,1126,67,93
mongodb-updater/0003-host-two-mongodb-updaters-in-stack.md,## Context\nTwo different MongoDBs need to be updated on a daily basis.\n,"Given the small number of databases to be updated both services will be hosted in the same stack, rather than manually create a stack for each database updater.\n",17,1127,17,31
mongodb-updater/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1128,16,39
time-diagnostics/0003-macro-for-creating-new-timer.md,## Context\nCreating and holding a timer object can introduce errors and noise in the code that's using it. It's also hard to remove for release code to optimize performance.\nExample:\n```c++\n#include <time_diagnostics.h>\nTimeDiagnostics td<2>;\nstatic const TIMER_ID = 0;\n{\nTimer t = td.get_timer(TIMER_ID);\n// ... user code\n}\n```\n,"Provide a macro that can be used to start a new timer and can be disabled during recompile.\nExample:\n```c++\n// uncomment this to remove timers for release code\n// #define DISABLE_TIME_DIAGNOSTICS\n#include <time_diagnostics.h>\nTimeDiagnostics td<2>;\nstatic const TIMER_ID = 0;\n{\nSTART_TIMER(td, TIMER_ID);\n// ... user code\n}\n```\n",93,1138,93,95
time-diagnostics/0002-using-int-for-timer-id.md,## Context\nTimers need a way of being uniquely identified in order to be processed after several are recorded. The ID should be something easy for the user to work with.\n,"Integers will be used over the alternatives of strings, enumerations, enum classes, or custom object.\n",36,1139,36,22
time-diagnostics/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1140,16,39
runbooks-mkdocs/0002-use-sphinx-as-our-knowledge-base-system.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,1143,21,13
runbooks-mkdocs/0005-use-plantuml-for-diagramming-with-use-of-stdlib.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,1144,21,13
runbooks-mkdocs/0003-use-plantuml-for-diagramming.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,1145,21,13
runbooks-mkdocs/0004-use-mkdocs-as-a-knowledge-base-system.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,1146,21,13
runbooks-mkdocs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1147,16,39
core-bundle/001-support-deserialization-plain-text.md,"## Context\nThe built-in deserializers included in JMS are sufficient for most cases, but sometimes the provided payloads are not compatible with them. Situations like this include strings in plain text, where no deserialization is expected. Feeding this type of data to JMS raises an exception that stops the execution of the application.\nIn these cases, the serialization needs to be bypassed in a clean and transparent way that will allow the calling class to continue its execution.\n","We decided to implement a custom deserializer **that accepts plain text**.\nThis deserializer is plugged **as a custom deserializer** in JMS' configuration and called whenever the format type ""plain_text"" is passed to the Serializer class.\n",97,1148,97,51
pul_library_drupal/0002-use-load-balancer.md,## Context\nThe ability to update/upgrade our servers for security or testing purposes is\nkey. We will have at least two virtual machines sharing a mounted directory on\neach that will contain files that all servers can view. The biggest benefit will\nbe the Web Application Firewall that will be applied to all traffic that is\ntargeted to the `library.princeton.edu`. Using deep packet inspection malicious\npatterns will be diverted away.\n,We will create\n* a smb/cifs and/or nfs mount that will be present on all nodes behind the\nloadbalancer\n* we will create a service (Kemp terminology) called `library.princeton.edu`\n* we will create nodes that will mount the aforementioned drives\n* we will implement SolrCloud on a remote endpoint that all nodes can read and\nwrite to\n* we will implement remote database (mariadb) that all nodes can read/write to\n,90,1149,90,98
pul_library_drupal/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1150,16,39
once-ui/0002-build-a-high-quality-ui-components-library.md,"## Context\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\n",- We will build a library of atomic ui components that will be used throughout the applications.\n- Developers should be able to easily import only the components they need and use them in their applications.\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\n,40,1151,40,62
once-ui/0004-add-readme-for-each-component-for-documentation.md,## Context\nComponents should be well documented so that the users of the library will be able to quickly start using the component.\n,"For now, we will create a README.md file inside each component folder, adjacent to the code for the component. The readme file should have the same structure and highlight some use cases, code examples and have api reference.\n",26,1152,26,45
once-ui/0007-use-semver-for-versioning.md,## Context\nWe need a versioning scheme for our project.\n,- We will use [semver](https://semver.org/) which is the most prevalent versioning scheme for node.js libraries and is easy to use.\n- We will follow the rules of updating `MAJOR.MINOR.PATCH` version numbers and their semantic meaning.\n,14,1153,14,57
once-ui/0011-semantic-beta-versioning.md,## Context\nWe have some issues with the versioning because sometimes the code updated regardless of the fix version in the once-ui library. So for this we used the beta versioning mechanism.\n,- In beta versioning mechanism we publish the prerelease version only for the team environment for testing purpose that would be from the once-ui QA branch and all the release version will be updated in the Master branch of once-ui library.\n,39,1155,39,47
once-ui/0005-publish-the-library-on-npm.md,## Context\nNeed to find a way for developers to import this library into their own projects.\n,We will use npm to publish the library privately in [MyGet](https://www.myget.org) (a 3rd party npm compatible registry)\n,20,1156,20,32
once-ui/0008-run-the-code-in-continuous-integration-server.md,"## Context\nIn addition to writing tests for each component, we need to run these tests continuously.\n",We will use Jenkins to run all the tests in the project and also run linter and Prettier.\n,21,1157,21,23
once-ui/0006-keep-the-api-surface-as-minimal-as-possible.md,"## Context\nSince many developers are going to use this library for many different features, we need to have some rules of thumb for what goes into the component and what does not go.\n","- We will choose to keep the api surface as minimal as possible, adding only the most necessary features in.\n- We will prefer leaving some features to the user of the library.\n- If we are not sure about something, it's better to leave it out and wait before we make the decision to add it.\n",38,1158,38,66
once-ui/0009-create-a-theming-system.md,"## Context\nMultiple applications are going to use the ui components, each one with slightly different theme (color). We need the library to be flexible enough to support such themes.\n",- We'll create a theming system based exactly on how Angular Material does themes.\n- Users will be able to override theme for specific component instance if really needed.\n,36,1159,36,35
once-ui/0010-add-keyboard-accessibility.md,## Context\nWe have to provide the accessibility feature in the application. Right now most of the places we are using the once-ui components so we won't be able to access the application through the keyboard. So for this we need to enhance the accessibility in the application.\n,- We are adding the cdk focus monitor in all the components to enhances the accessibility which it self managing all the focus states.\n,55,1160,55,27
once-ui/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1161,16,39
Head-Start-TTADP/0008-api-documentation.md,"## Context\nThere are a lot of tools available for documenting APIs. These tools generate easy to read documentation that can be used by frontend engineers and even third party developers. Setting up a documentation workflow will allow those engineers to integrate with the TTAHUB API faster and with more confidence. In addition, these API tools open a path for further testing and automation in the future, since the definition of the API is machine readable.\n",We will document our API with [OpenAPI specification 3](https://swagger.io/specification/) (formerly known as swagger).\n,87,1162,87,27
Head-Start-TTADP/0012-malware-scanning.md,"## Context\nIn order to satisfy the [RA-5](https://nvd.nist.gov/800-53/Rev4/control/RA-5)\ncontrol around vulnerability scanning, we wish to scan all user-uploaded files\nwith a malware detection service. These files are user-supplied, and thus\ncannot fit into our other security controls built into the CI/CD pipeline.\n","We will run a ClamAV daemon, fronted by a REST API that we will pass all user-uploaded\nfiles through as part of the upload process. The ClamAV app is maintained in a\n[separate government-controlled repository](https://github.com/HHS/Head-Start-clamav-api-cg-app)\n",82,1164,82,69
Head-Start-TTADP/0014-web-analytics.md,"## Context\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\n","We will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https://docs.newrelic.com/docs/browser/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https://docs.newrelic.com/docs/browser/new-relic-browser/browser-agent-spa-api/) and [APM](https://developer.newrelic.com/collect-data/custom-attributes) methods for capturing custom data.\n",49,1165,49,95
Head-Start-TTADP/0005-hses-oauth2-integration-backend.md,## Context\nTo integrate with the HSES oauth2 a package providing that functionality can be used to help the development. Two packages were considered for this purpose:\n1. passport - one of the more popular and extensive Javascript packages covering various authorization flows with multiple strategies. Passport provides a lot of functionality of out the box and was initially explored.\n2. client-oauth2 - a smaller package allowing a straight-forward implementation.\n,"Even though passport was originally considered, client-oauth2 will be used.\n",87,1170,87,16
Head-Start-TTADP/0011-monitoring.md,## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n,"TTA Smart Hub will utilize New Relic for Monitoring.\n* Integrates easily with front end and backend code\n* Track performance metrics and errors\n* FedRAMP approved\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\n",20,1171,20,60
Head-Start-TTADP/0003-configuration-by-environment-variable.md,"## Context\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\n",We will use environment variables to configure applications.\n,35,1173,35,10
Head-Start-TTADP/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1176,16,39
clone_difitalcitizenship/0002-use-openapi-to-defined-the-api-specs.md,## Context\nWe need to define the API specifications of the services we're going to implement.\n,We use the [OpenAPI 2.0](https://swagger.io/specification/) specification (aka Swagger spec) as standard for our REST API definitions.\n,20,1180,20,33
clone_difitalcitizenship/0004-minimize-cloud-lock-in.md,"## Context\nThere are no formal standards for describing or managing most types of cloud resources. This is because the cloud hosting market moves so rapidly. Organisations are very quick to adopt new cloud products that offer significant opportunities, but this reduces the opportunities to work with standards.\nFor this reason, when choosing to develop a product on top of a non-standard cloud component there's a risk of lock-in.\n",We should take steps to avoid lock-in with cloud hosting providers.\n,82,1183,82,14
clone_difitalcitizenship/0007-choice-of-azure-region.md,"## Context\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\n","We decide to deploy our services on the [West Europe](https://azure.microsoft.com/en-us/regions/) region, located in the Netherlands.\n",29,1186,29,30
clone_difitalcitizenship/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1189,16,39
hee-web-blueprint/0003-use-github-actions-for-automation.md,"## Context\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\n","Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\n",27,1196,27,57
hee-web-blueprint/0006-use-testcontainers-for-providing-ephemeral-test-environments.md,"## Context\nHaving determined to use Gherkin and Selenide to manage the generation of our automated tests, we need to determine the platform on which these tests will run. The platform should be open source, if at all possible and portable between hosting platforms.\n",We have determined to use the testcontainers.org project to manage the infrastructure for our automated tests. This platform provides a way of generically describing the containers that we use to run our tests and can be executed on platforms including Github Actions.\n,54,1198,54,48
hee-web-blueprint/0004-use-gherkin-for-describing-acceptance-tests.md,## Context\nWe should determine how to develop and document our accptance tests. This decision describes the choice around the langauge (DSL) that we use to describe them.\n,We have determined to describe our acceptance tests using Gherkin.\n,37,1201,37,14
hee-web-blueprint/0008-use-npm-to-host-hee-frontend-framework.md,## Context\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\n,We have chosen to store our packages on NPM.\n,47,1202,47,12
hee-web-blueprint/0002-use-github-for-source-control.md,## Context\nWe needed to determine where to store and manage the source code for the HEE National Website Platform. HEE currently have a number of projects managed on GitHub so already have a presence on the platform.\n,We will store the platform source code on Github\n,44,1203,44,10
hee-web-blueprint/0005-use-selenide-for-creating-and-managing-acceptance-tests.md,"## Context\nHaving determined to use Gherkin as the way that we describe our acceptance tests, we need to choose a Java based (because BloomReach is Java based) framework to write our acceptance tests in.\n",We have determined to use Selenide as the framework to implement our automated tests.\n,44,1204,44,18
hee-web-blueprint/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1206,16,39
hee-web-blueprint/0011-use-federated-authentication-provider.md,## Context\nWe need to describe an approach to authentication that supports the use of multiple authentication providers. Azure AD provides the ability to configure direct federation with a range of authentication providers and includes the ability to provide managed authorisation for users without the ability to authenticate with Azure AD\n- https://docs.microsoft.com/en-us/azure/active-directory/b2b/delegate-invitations\n,We will choose an authentication provider/platform that supports the federation of authentication.\n,76,1207,76,15
TruBudget/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1226,16,39
TruBudget/0007-execution-architecture-overview.md,## Context\nWe need an overview for the execution architecture\n,TruBudget can be deployed either locally or in the cloud using Docker. Interfaces are exposed via web interface and via blockchain connection.\n![Overview of architecture](./img/0007-execution-architecture-overview.png)\n,12,1227,12,46
WorkoutLog/0004_fragment_specific_menu.md,## Context\nWe need a consistent way to add menus within fragments.\n,Every fragment handles its menu independently. This is achieved through calling setHasOptionsMenu(true)\nand overriding the specific methods needed for creating the menu and handling selection events.\n,15,1229,15,34
WorkoutLog/0007_license_info.md,"## Context\nOpen Source Software should be honored and apart from our obligation to show the used open source libraries, we think\nthe great people have to be named to honor their contribution.\n",We need a new view showing all relevant open source libraries we are using including a link to the project-page.\nFor simplicity we should use a simple webview showing a pre created HTML page.\n,38,1230,38,40
WorkoutLog/0009_one_time_events_livedata.md,## Context\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\nafter returning to a fragment from another navigation path. Errors or special events like automatic\nnavigation should only be executed once.\n,A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\n,49,1231,49,21
WorkoutLog/0001_appbar_interface.md,## Context\nWe need a consistent way to access the Application Bar to update the Caption or change the Hamburger menu to homeAsUp\n,"We create an interface which will be implemented by the Activity. Due to the single activity architecture, the base\nfragment will cast the context to which the fragment is attached to the interface and set a member variable. Also some\nmethods will be exposed by the fragment so that extended fragments can make use of these methods.\n",28,1232,28,64
WorkoutLog/0003_edit_decision_from_fragment.md,"## Context\nThe ViewModel is responsible for the data, so i tried an attempt to notify the ViewModel from within the view that the\nuser clicked the ""edit"" menu item. The ViewModel exposed a LiveData object which was observed by the view to trigger the\nnavigation to a edit fragment.\nAfter editing the workout and clicked saved, the edit fragment was opened again instantly. This was caused by the\nViewModel which triggered the edit event again.\n","The view (DetailviewWorkoutFragment) no longer notifies the ViewModel about the edit action but directly calls the\nnavigation component to open the EditWorkoutFragment, and therefore decides what should be done after the edit action\nwas clicked (normally the view should not make this kind of decision).\n",90,1233,90,60
WorkoutLog/0008_event_method_prefix.md,## Context\nOnboarding new engineers on a software project should be fast. Also our future self should be back on track on a project fast.\nConsistent and logical naming can be a benefit here.\n,"All ""event""-methods within a ViewModel class should have a prefix ""on"". Events are methods which are called by the view.\n(ie. onClickFavorite, onSearchChanged).\n",42,1234,42,38
WorkoutLog/0002_remove_state.md,## Context\nI think exposing the whole state from the ViewModel to the fragment moves some logic to the fragment and is error prone\nin my opinion.\n,"Instead of exposing whole state objects, we expose only single properties which can individually handled by the fragment\n",31,1235,31,20
WorkoutLog/0006_viewmodel_exposing_state.md,"## Context\nAfter saw the results of ADR 0002 (update stream mess) and read some blog posts regarding ""redux like"" Android apps,\nwe should move back to exposing whole state objects from the ViewModel.\n",ViewModels exposing a single state object (sealed class) which is handled by the fragment.\n,46,1237,46,19
stamper/0002-kotlin-as-language.md,## Context\nWe need to choose main language for the app.\n,"We use Kotlin because it is the main language in JetBrains. When the summer practice is ended, this will simplify maintenance and contributions from other JetBrains employees.\n",14,1238,14,31
stamper/0007-docker.md,## Context\nWe need to choose how we will deploy our app.\n,We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\n,15,1239,15,24
stamper/0004-bootsptrap-as-frontend-framework.md,## Context\nWe need to choose a frontend framework.\n,"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\n",12,1240,12,28
stamper/0005-junit-as-test-framework.md,## Context\nWe need to choose test framework.\n,"We use JUnit because it is a standard framework for JVM, and we don't have any advanced requirements yet.\n",11,1241,11,24
stamper/0008-gradle.md,## Context\nWe need to choose how we will build our app.\n,"We use Gradle because it is very flexible, has some plugins for Docker integration and @mkuzmin know this technology very well.\n",15,1242,15,28
stamper/0003-spring-boot-as-web-framework.md,## Context\nWe need to choose a web framework for the app.\n,"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\n",15,1243,15,28
stamper/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1244,16,39
stamper/0006-thymeleaf.md,## Context\nWe need to choose view representation\n,We use Thymeleaf because it provides:\n- clean HTML view\n- templates that can be run without server\n,10,1245,10,25
buy-for-your-school/0005-use-bullet-to-catch-nplus1-queries.md,## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n,Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n,37,1246,37,23
buy-for-your-school/0022-use-awsS3-for-storage-requirements.md,## Context\n* Case management system requires secure cloud storage to store case related documents.\n* GOV.UK  Ruby applications often use [Amazon S3 storage and Rails Active Storage](https://docs.publishing.service.gov.uk/manual/conventions-for-rails-applications.html#header) together and as it is preferred technical choice.\n* A private aws-s3-bucket would be provisioned on Gov PaaS and only accessible via rails app  bind with the service.\n,* Use Amazon S3 storage service on Gov PaaS and Rails Active Storage\n,97,1247,97,16
buy-for-your-school/0004-use-a-changelog-for-tracking-changes-in-a-release.md,"## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",45,1248,45,57
buy-for-your-school/0003-use-standard-rb.md,"## Context\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\n,36,1249,36,35
buy-for-your-school/0012-create-infrastructure-on-gpaas-cloud-foundry-with-terraform.md,## Context\nThe early beta was originally hosted on dxw's Heroku to get delivering quickly whilst access to GPaaS could be set up.\nAccess to GPaaS with approved billing has now been confirmed so we are migrating the service to its longer term home.\n,- Move the service from Heroku to GPaaS for all environments except the ephemeral pull request review environments.\n- Use Terraform to define the infrastructure as code.\n,54,1250,54,35
buy-for-your-school/0017-use-codeclimate-in-pipeline.md,"## Context\nCodeClimate is an automated code review tool that looks at test coverage, code smell and tech debt.\n",We will use CodeClimate as an additional level of quality assurance on our pull requests.\n,24,1253,24,18
buy-for-your-school/0020-use-accessible-autocomplete-for-autocomplete-fields.md,## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n,We will use [accessible-autocomplete](https://github.com/alphagov/accessible-autocomplete) to provide the autocomplete capability in our pages.\nThis package has been chosen because accessibility has been carefully considered when developing the package.\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\nand not be jarring to the user.\n,26,1255,26,83
buy-for-your-school/0016-use-rubocop-govuk-for-code-linting.md,## Context\nGov.uk projects should maintain consistent [code formatting](https://gds-way.cloudapps.digital/manuals/programming-languages/ruby.html#code-formatting)\n,"Change from [Standard.rb](https://github.com/testdouble/standard), which is a wrapper around [Rubocop](),\nto the [GovUK maintained version](https://github.com/alphagov/rubocop-govuk).\n",36,1256,36,51
buy-for-your-school/0015-store-user-activity-in-app.md,"## Context\n- We need to keep a record of activities taken by users so that we can gather quantitative research data\n- We need to avoid third-party tracking services, since these carry with them privacy concerns, lack of clarity around retention rules, additional user consent, and the need for additional approvals\n",Store all records of activities taken by users in-app.\n,60,1257,60,12
buy-for-your-school/0009-use-simplecov-to-monitor-code-test-coverage.md,## Context\nWe want to keep our test coverage as high as possible without having to run\nmanual checks as these take time and are easy to forget.\n,Use Simplecov with RSpec to monitor coverage changes on every test run\n,32,1258,32,15
buy-for-your-school/0010-use-redis-for-a-read-cache.md,## Context\nThe service needs a way to build resilience against the external Contentful API becoming unexpectedly unavailable. This service is a single point of failure.\ndxw use often use Rails and Redis together and recommend it as a technical choice.\n,Add and use Redis for a read cache.\n,48,1259,48,10
buy-for-your-school/0011-use-sidekiq-for-asynchronous-tasks.md,"## Context\nWe need a way for the service to automatically and regularly run a task.\nWe already have Redis available as our caching layer and Sidekiq works great with it.\nAn alternative could be to use Cron for scheduled tasks and a postgres backed asynchronous jobs, perhaps even run inline. We know how to get Sidekiq running with Docker and reusing Redis (rather than Postgres) for job data that is ephemeral feels a better fit given we already have Redis.\n",Use Sidekiq for processing asynchronous tasks.\n,99,1260,99,10
buy-for-your-school/0002-use-pull-request-templates.md,"## Context\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\n",Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n,89,1261,89,22
buy-for-your-school/0018-use-notify-api-for-sending-emails.md,## Context\nIt is necessary for the service to confirm various actions with users via email messages.\n,"Gov.UK Notify is the recommended solution, using the `notifications-ruby-client` library.\n",20,1262,20,20
buy-for-your-school/0008-use-brakeman-for-security-analysis.md,## Context\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n,Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n,20,1263,20,29
buy-for-your-school/0020-use-contentful-for-page-content.md,"## Context\n* Several content-only pages are served by our `Pages` table;\n* `Pages` within the app mirror `Pages` within Contenful;\n* This allows content designers (via Contentful) the ability to edit, add and\ndelete Pages without help from developers.\n",* Remove [high_voltage](https://github.com/thoughtbot/high_voltage);\n* Rely on a final generic route that directs to `PagesController`;\n* Use the slug to retrieve the Page from `t.pages`.\n,62,1264,62,49
buy-for-your-school/0019-use-loaf-for-breadcrumb-links.md,## Context\nIt is necessary for breadcrumb navigation to be supported and configured in a consistent way across the application\n,The recommended solution is to use the `loaf` library - which is already used in several other DfE repositories as the go-to solution for breadcrumb management:\n- https://github.com/DFE-Digital/academy-transfers-frontend\n- https://github.com/DFE-Digital/get-into-teaching-app\n- https://github.com/DFE-Digital/npd-find-and-explore\n,22,1265,22,87
unruly-puppet/0002-standalone-nrpe-custom-check-module.md,## Context\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\nThe `base` module already contains NRPE specific code.\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\n,"We considered three possible approaches to adding the new NRPE functionality:\n- Add it to the `base::nrpe`\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\n",75,1268,75,98
unruly-puppet/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1269,16,39
libelektra/empty_files.md,## Problem\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\nrepresentation?\n,Remove files on empty KeySet.\n,23,1271,23,8
libelektra/highlevel_help_message.md,## Problem\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\n,"Ignore missing `require`d keys (in help mode), but fail for every other error.\n",31,1274,31,20
libelektra/elektra_web_pubsub.md,"## Problem\nTo develop a [Web UI](https://github.com/ElektraInitiative/libelektra/issues/252),\nwe need to be able to remotely configure Elektra via a network socket.\nThe idea is to use a Pub/Sub concept to synchronize actions which describe\nchanges in the Elektra state.\n",REST Api is used.\n,69,1275,69,6
libelektra/spec_expressiveness.md,"## Problem\nCurrently, you can easily come into wrong assumptions\nthat something would work in the specification.\nWe need to find minimal requirements to implement a sane spec plugin.\n",- no defaults for `sw/_/key` specifications\n(default will not work for `ksLookup(/sw/sthg/key)`)\n- plugins are not allowed to create keys (may change in future; depends on plugin positions)\nThe spec plugin should yield errors when it detects such situations.\n,36,1277,36,60
libelektra/cmake_spec.md,## Problem\nThe compilation variants of plugins blow up the number of plugins.\nAdditionally there is the concept of default storage + resolver that is\nneeded for bootstrapping plugins.\n,Rejected: keep default plugins as-is\n,37,1279,37,8
libelektra/multiple_file_backends.md,"## Problem\nIn some situations a single mountpoint refers to more than one file per namespace:\n- For XDG in the `system` namespace may contain several files (XDG_CONFIG_DIRS).\n- A fallback file if some data cannot be stored in some format (Idea from @kodebach:\nwriting the same content to several files, merging when reading)\n","Multiple File Backends are not supported for now in the case of writing files.\nMultiple sources in one namespace only work, if the fallback KeySet is\npart of the mountpoint config. That way any change to the fallback\nKeySet would essentially make the whole thing a different mountpoint\nand thereby invalidate all guarantees.\n",76,1281,76,67
libelektra/elektra_web_recursive.md,"## Problem\nAfter deciding how to remotely manage instances and groups of instances\n(clusters) with Elektra Web, there is still the issue of recursively nested\nclusters (clusters of clusters).\n",Managing the hierarchy in a single clusterd instance.\n- [Elektra Web Structure decision](elektra_web.md)\n,40,1282,40,27
libelektra/global_plugins.md,"## Problem\n- Notification does not happen once after final commit, but for every\nplugin\n- Problems in spec plugin\nThese problems can be traced back to the placement of the plugins.\nWe need to clean up and simplify the placement.\n","Have hooks and API specific to the list of global plugins in assumptions.\nThese hooks are not shared, so no `list` plugin is needed.\nInstalled plugins will be used.\n- [Array](array.md)\n- [Ensure](ensure.md)\n",50,1284,50,54
libelektra/relative.md,"## Problem\nThere is a different behavior of various plugins whether their name is\nabsolute or relative, including:\n1. mounting the same file somewhere else does not work\n2. importing somewhere else (other than from where it was exported) does not work\n(See [here](https://github.com/ElektraInitiative/libelektra/issues/51))\n",Key names shall be relative to parent Key name\nNone\n,77,1285,77,12
libelektra/library_split.md,"## Problem\nOnly libelektra-core is supposed to access private data but this contradicts the goal to keep the library minimal.\n`kdbprivate.h` was too generic, it contained many other parts next to the struct definitions of Key/KeySet.\n","Also allow `libelektra-operations` library to access private Key/KeySet.\nPut struct definitions of Key/KeySet in a separate header file, which gets\nincluded by parts that need it\n- none\n",55,1286,55,47
libelektra/vendor_spec.md,"## Problem\nVendors (distributors, administrators) might want to modify the specification.\ngsettings has a similar feature.\n","As found out during implementation of [specload](/src/plugins/specload), only a very limited subset can be modified safely, e.g.:\n- add/edit/remove `description`, `opt/help` and `comment`\n",27,1287,27,47
libelektra/internal_cache.md,"## Problem\nWhen doing kdbGet() possible more keys are returned which might be\nconfusing. When doing a second kdbGet() with a new keyset\nno keys might be returned, because it is up-to-date.\nWhen doing kdbSet() a deep duplication is needed.\nIdea: keep a duplicated keyset internally. Return (non-deep?)\nduplications exactly of the parts as requested.\n",Not yet decided.\n- [Global Validation](global_validation.md)\n,90,1289,90,15
libelektra/error_handling.md,## Problem\nThere are ambiguous cases where the same return value can refer to multiple problems:\n- name modifications which can be either invalid name or locking the key name\n- getting values of (non-)binary keys\n,"- Update documentation in `doc/dev/error-*` and link to them in the documentation\nfor the module `kdb`\n- Add second channel for getting information about errors\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\n- Harmonize return values from all functions and move error reporting to second channel\n- Binary metadata vs flag #4194\n",44,1292,44,86
libelektra/script_testing.md,## Problem\nWriting portable shell code for testing command-line tools is difficult.\n,Develop [shell recorder](/tests/shell/shell_recorder) and [tutorial wrapper](/tests/shell/shell_recorder/tutorial_wrapper).\n,16,1294,16,31
libelektra/key_string_return_value.md,## Problem\nWhen using keyString() on empty / binary values the return values are the literal strings (null) / (binary). This seems very awkward and unintuitive from a user's perspective.\n,"- `key == NULL` return 0, error code via second channel\n- `key->value == NULL` return 0, error code via second channel\n- `key == <binary>` return 0, error code via second channel\n- everything else as is\n",41,1295,41,57
libelektra/base_name.md,"## Problem\nA key name is made out of a sequence of key part names, and can be constructed with `keyAddBaseName/keySetBaseName`.\nBoth applications and configuration file formats might need arbitrary strings to be encoded within a key name part.\nFor example:\n- an application uses names of internal components as sections within the configuration.\n- a parser reads an empty string, to be encoded as base name.\n","`keyAddBaseName/keySetBaseName` never fail with any argument, so any character sequence can be escaped except of NULL bytes.\nThe argument goes unmodified to the unescaped key name.\nFor arrays there is no escaping needed because an array is only an array if the metadata `array` is appended to the direct parent key.\nSee [array](array.md).\n- [Array](array.md)\n",89,1297,89,86
libelektra/rest_api_documentation.md,"## Problem\nA standard way of describing REST APIs offered by tools and plugins for Elektra is required to ease development for and usage of these. Because many good standards for describing APIs are out there already, an existing one shall be used.\n",The decision is to use [API blueprints](https://apiblueprint.org/) together with additional tools from its ecosystem.\n,50,1298,50,28
libelektra/commit_function.md,"## Problem\nWhen `kdbSet()` is called, plugins implementing the commit role need to\ntrack their state to distinguish between carrying out that role and\ncarrying out potential other roles (commit and setresolver for the\nresolver plugin, for example). This limits the possibilities of plugin\nreuse and the ways plugins can be combined.\n","Committing will no longer be done by `kdbSet()`. Instead, the functionality\nwill be implemented by its own function, `kdbCommit()`.\n",70,1300,70,34
libelektra/default_values.md,## Problem\n- KeySet might get modified on access (hash rebuilds)\n- Expectation that already all keys are there after `kdbGet()`\n- No default value calculation\n,- spec-plugin does a lookup for values (Maybe also resolving missing fallback/override links?)\n,40,1301,40,20
libelektra/warning_array.md,## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n,"The format should be aligned with the correct array notation,\nstarting with `#0`. The maximum number of warnings will stay at\n100 entries (`#0` - `#_99`).\n- [Array](array.md)\n",46,1303,46,49
libelektra/unit_testing.md,## Problem\nThe previous unit testing framework started as hack to have a bit more\nthan simple asserts. It is not easy to use (needs explicit enumeration\nof all test cases) and lacks really important features (e.g. output of\nthe assertion that failed).\n,"- Keep C framework for C tests and ABI tests\n- Google Unit testing framework `gtest` with code downloaded by CMake for\nsystems where no source is packaged (Debian Wheezy, Arch Linux,\nFedora,...) for C++ tests\n- [Script Testing](script_testing.md)\n",55,1304,55,63
libelektra/ensure.md,## Problem\nApplications want to ensure that some functionality (=global plugin)\nis present in Elektra.\n,"Integrate `kdbEnsure` in `kdbOpen(Key *errorKey, KeySet *contract)` but only allow global plugins.\n- [Global Plugins](global_plugins.md)\n",22,1305,22,39
libelektra/elektra_web.md,"## Problem\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\nof instances (clusters). The remote configuration of a single instance is\nsimple. However, to manage multiple instances, we need to store the information\nto access the daemons, as well as information about the grouping (clusters) of\ndaemons.\n",Use one cluster daemon (clusterd) to manage all clusters and instances.\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\n,75,1306,75,34
libelektra/holes.md,## Problem\nConfig files ideally do not copy any structure if they only want to\nset a single key.\n,"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\nIf the underlying format does not support it and there is also not an obvious\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\nvalues in non-leaves can be supported with key names starting with ®elektra.\n",23,1307,23,75
libelektra/ingroup_removal.md,"## Problem\nCurrently, an error or warning message in elektra causes the following line to be shown:\n```\nIngroup: <group>\n```\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\n",The `ingroup` message will be removed as it does not yield any notable benefit.\nSee [Error concept](error_codes.md)\n,83,1308,83,28
libelektra/lookup_every_key.md,"## Problem\nOn structures like maps or [arrays](array.md) there are different\npossibilities which keys are looked up in the KeySet and which\nare simply iterated.\nWithout any guidelines, applications would provide arbitrary inconsistent\nbehavior.\n","Every key that an application wants to use, must be looked up\nwith `ksLookup` using a cascading lookup key.\n- [Arrays](array.md)\n",51,1310,51,35
libelektra/functions_with_buffers.md,"## Problem\nCurrently the way functions like keyGetName() work is by passing a buffer with\na maxSize and if the buffer is large enough, the value gets copied into the\nbuffer. This leads to the user having to write a lot of surrounding boilerplate\ncode, checking for the size of every value / name they want to copy into a buffer.\n","- Remove Functions:\n- keyGetName()\n- keyGetUnescapedName()\n- keyGetBaseName()\n- keyGetString()\n- keyGetBinary()\n- add documentation in API docu about life-time and add in release notes that you should use strncpy() / memcpy() instead:\n```c\n// str values\nstrncpy(..., keyName (k), ...)\n// binary values\nmemcpy(..., keyValue (k), ...)\n```\n",74,1313,74,99
libelektra/global_validation.md,## Problem\nLinks and validation constraints might point to keys not loaded\nwith the respective `kdbGet`.\n,"Not supported, admins/maintainers need to stay with their spec within what applications request by `kdbGet`.\n- [Internal Cache](internal_cache.md)\n",24,1315,24,35
libelektra/null_pointer_checks.md,## Problem\nCurrently all functions do proper argument checking which might degrade\nperformance.\n,Rejected (keep checks) due to time constraints\n,17,1317,17,10
libelektra/capabilities.md,"## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos/status`, others were not.\n","Add `infos/features/storage` to document limitations of storage plugins.\nIdeally, storage plugins should throw an error in `kdbSet` for\nunrepresentable KeySets.\nElektra cannot guarantee that any configuration file format can\nbe mounted anywhere.\nDevelopers, maintainers and administrators are responsible for what\nthey mount. They need to test the setup.\n- [Base Name](base_name.md)\n",66,1318,66,88
cfo/0002-use-yarn-install-instead-of-npm-install-for-web.md,## Context\nI see found both `yarn.lock` and `package-lock.json` in the repository.\n,Sticking to `yarn` for `web` for now.\n,23,1320,23,15
cfo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1321,16,39
molgenis-service-armadillo/0006-use-rdata-format-as-data-input.md,"## Context\nWe want to make the MOLGENIS ""Armadillo"" service data provider agnostic. There are a couple of reasons why we are doing this\n- the service is usable for other parties as well\n- the service can still integrate with MOLGENIS.\n- the release cycle of the service is data provider independent\n- the service can be developed by other parties as well\n","We implement an endpoint to upload and load RData files in the MOLGENIS ""Armadillo"" service to manage data for the use in DataSHIELD.\n",83,1326,83,35
molgenis-service-armadillo/0004-implement-asynchronicity-in-requestflow.md,## Context\nClients can have operations running on multiple DataSHIELD servers concurrently.\n,We need to support asynchronous requests in the R client. We implemented it using completable futures.\nWe must keep the last execution result for each R session until it gets retrieved or until a new execution is started.\n,17,1327,17,44
molgenis-service-armadillo/0001-use-adr-to-describe-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n",16,1328,16,40
molgenis-service-armadillo/0005-deploy-on-both-vm-and-kubernetes.md,## Context\nBecause of the diversity in landscape we need to be able to deploy on different environments.\n,We are going to create deployments for virtual machines based on CentOS (>=8) using Ansible and Vagrant.\nBesides that we are going to create a chart which allows you to deploy on Kubernetes.\n,21,1329,21,42
molgenis-service-armadillo/0009-use-bean-scope-for-profiles.md,## Context\nWe need to be able to switch between profiles.\nBut many singleton beans have profile information in them.\n,"Create a custom `@ProfileScope` to store the different versions of the profiles and inject proxies\nto look up the correct instance.\nWe already did this for the ArmadilloSession, a `@SessionScope` bean, to store the connection to\nthe R server which is different for each user session.\n",25,1330,25,66
molgenis-service-armadillo/0002-implement-authentication-using-openid.md,## Context\nWe need a way to help users authentication in our system\n,"We implement openid in our application so we use ID-providers to authenticate in our services.\nWe intent to use JWTs which are machine-readable bearer tokens, to integrate external services.\n",15,1331,15,38
molgenis-service-armadillo/0008-load-multiple-rdata-files.md,## Context\nWe want to give researchers as much freedom as possible when they're selecting and loading data.\n,The `/load-tables` endpoint supports loading multiple .RData files at once. These files can be in different folders.\n,22,1333,22,26
astarte/0002-astarte-channels-are-not-meant-to-be-reliable.md,"## Context\nAstarte Channels are a way to receive events of specific Astarte entities via\nWebsockets, leveraging Phoenix Channels and volatile Triggers.\nDue to the way volatile Triggers are implemented, it would be necessary to\nintroduce a large overhead to guarantee that all the data is correctly\ndelivered to Astarte Channels.\n","Since we already have a reliable way to receive the events of a given entity\n(persistent Triggers), we will not guarantee that Astarte Channels are a\nreliable source of events.\n",73,1334,73,40
astarte/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1335,16,39
dilay/0002-framework-project-directory-examples.md,## Context\nContext here...\n,Decision here...\n,7,1336,7,4
dilay/0003-use-compile-api-to-transfter-class.md,## Context\nhttps://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API\nhttps://github.com/dsherret/ts-morph#readme\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\n,Decision here...\n,54,1337,54,4
static/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1362,16,39
monitoring-doc/0010-packaging-node-exporter-as-deb-for-verify.md,## Context\nNode Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered.\nVerify runs Ubuntu Trusty which does not have an existing node exporter package.\nVerify has an existing workflow for packaging binaries which can be leveraged to package node exporter.\n,Node exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.\n,58,1367,58,19
monitoring-doc/0008-use-of-verify-egress-proxies.md,"## Context\nVerify employ egress proxies to control access to external resources.\nThese are a security measure to help prevent data from being exfiltrated from within Verify.\nThe Prometheus server will need access to external resources, notibly an Ubuntu APT mirror during the bootstrap process.\nThe Prometheus server should not setup it's own routes to bypass the egress proxy i.e. use a NAT gateway or Elastic IP, as this will potentially open up a route for data exfiltration.\n",The Prometheus server should use Verify's egress proxies and choice of APT mirror.\n,98,1370,98,18
monitoring-doc/0001-environments.md,## Context\nWe want to have separate environments for running our software at different stages of release.\nThis will be used to provide a location where changes can be tested without impacting\nproduction work and our users.\n,"We have decided to have N+2 separate environments: development,\nstaging and production. In development, we can create as many separate\nstacks as we want. The staging environment will be where the tools\nteam will test their changes. The production environment will run all\nof our users monitoring and metrics and poll each of their\nenvironments.\nAny code can be deployed to development environments.  Only code on\nthe `master` branch can be deployed to staging and production.\n",43,1373,43,100
monitoring-doc/0009-use-cloud-init-to-build-prometheus-server.md,"## Context\nThe Prometheus Server needs to be built in a reproducible way within AWS.\nReproducible in this context means that the server can be built, destroyed and rebuilt.\nThe rebuilt server will be identical to the original server and the is no external intervention required (i.e. logging into the server to make changes to configuration)\n",Cloud init will be used to build a reproducible server.\n,69,1377,69,13
wikiindex/adr-009-hosting.md,## Context\n* We have a Clojure application\n* We do not know the security requirements for hiding the application\n* We would rather not pay for hosting\n,* We host on Heroku. A free PAAS provider.\n,33,1379,33,14
wikiindex/adr-004-xml_freshness.md,## Context\n* Our application works by indexing an XML file given by WikiMedia\n* WikiMedia manually create a new XML file twice a month on average\n* We have not been given an indication on how fresh the data our service provides needs to be\n* We want the software to be deployable without relying on a shared file system\n,* We store the XML dump in this git repo and update it when we need to\n* The file will be accessed over http to allow us to move to a different storage solution in the future\n,69,1380,69,40
wikiindex/adr-001-search_and_indexing.md,"## Context\n* We need to index and search a 32Mb XML file.\n* The search is across multiple fields.\n* Only one search word is used.\n* We currently don't have any external platform tools like Mongo, ElasticSearch, an RDMS\n* We do not know how often the XML file will be updated.\n","* We will parse the file on startup, store it as a clojure collection that is available throughout the system.\n* We will do this before the web server http port is available.\n* Searches will be performed against this in memory collection.\n* The app would need to be restarted to parse a new XML file, unless an additional endpoint is added.\n",71,1381,71,74
wikiindex/adr-007-search_and_indexing.md,## Context\n* As per ADR-001\n* In memory search is unacceptably slow\n,* We will use Clucy which is a Clojure library on top of Lucene\n,21,1382,21,19
wikiindex/adr-008-search_and_indexing.md,## Context\n* As per ADR-007\n* Clucy takes over an hour to process the index\n,* We will use ElasticSearch as a frontend to Lucene.\n,24,1383,24,14
wikiindex/adr-002-routing_library.md,"## Context\n* We are building a web app that has one official url that uses query params\n* The url is /search?q=search-term\n* We believe that future endpoints will be added, or more complicated search requests\n",* We will use Bidi to parse the routes.\n,46,1384,46,12
wikiindex/adr-005-logging.md,## Context\n* Running a service is easier if you know what it is doing\n,* We will follow the logging principles of the 12 factor app: http://12factor.net/logs\n* We will only ever send output to console. Never write logs to file.\n* For example in production we could redirect the app output using: ```lein run | logger``` then we can see the output by using  ```tail -n 0 -f /var/log/system.log```\n* Semantic logging should be used where appropriate\n,17,1385,17,91
wikiindex/adr-006-config.md,## Context\n* We need to set the port and other config values for our application\n* We do not know the deployment environment yet\n,"* We will use environment variables to configure the app, rather than a file\n",28,1386,28,16
wikiindex/adr-003-testing_library.md,## Context\n* We want to write programmer tests to support a TDD workflow.\n* We want to be able to mock out functions.\n,"* We will use Midje to test our code.\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\n",30,1387,30,32
amf/0005-api-hierarchy-web-async.md,"## Context\nAsyncApis are not semantically the same that other kinds of apis, like Web Apis. Some adopters need to know if they are looking specifically an async api, regarding the structure is the same or no.\nAlso, we need to prepare our model to support different kinds of Apis.\n",Create a hierarchy based on Apis. A Document unit will encodes an Api. This Api is abstract. There will be different types of Apis depending on the specifications.\n,64,1388,64,36
amf/0002-async-uses-its-editing-pipeline-as-validation-pipeline.md,"## Context\nCurrently the async validation pipeline is missing many core resolution stages present in the default and editing pipeline, such as JsonMergePatch and AsyncExamplesPropagation. This is causes missing validations, as well as exceptions in resolution. In many cases fixes are applied to the default and editing pipelines but the validation pipeline is left behind.\n",The async editing pipeline will be used as the validation pipeline.\n,66,1389,66,13
amf/0004-async-message-examples-are-saved-in-separate-fields.md,"## Context\nAfter a clarification in the async specification, the examples facet of a message object contains a list of pairs of examples,\nwhere one is associated to the payload while the other to the header. Due to time restricitions, this change has to be introduced without a breaking change in the model.\n",Within the message model a new field has been added to store header examples. The payload examples are being stored in a separate field.\n,63,1391,63,27
amf/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md,"## Context\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\n","In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\n",61,1393,61,91
amf/0006-message-headers-schema.md,"## Context\nAsync Api 2.0 spec details that message headers property must be a Schema Object in which his properties will be the headers for that message.\nAMF currently is parsing that object as the schema of a parameter object. This is confusing and semantically incorrect, because that property represents a set of parameters, not just one\n","Create a new field to the Message model, similar to the query string property of Raml 1.0 operations, that will contains the Schema Object. Any user looking at the model, will be able to read that schema as a list of headers based on the field in which is contained:\n####ApiContract:HeaderSchema\nThe header example property of the message will be validated against that schema.\n",69,1396,69,82
lbh-frontend-react/0006-use-jest.md,"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n",We will use Jest as our testing framework.\n,40,1403,40,10
lbh-frontend-react/0005-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,69,1404,69,57
lbh-frontend-react/0007-use-dependabot-to-keep-dependencies-up-to-date.md,## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n,We will use Dependabot to monitor dependency updates.\n,38,1407,38,12
lbh-frontend-react/0003-use-rollup-to-build-distributables.md,## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n,We will build distributables using Rollup.js.\n,62,1411,62,12
lbh-frontend-react/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1415,16,40
architecture/0002-apply-clean-code-guidelines.md,"## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n","I will always focus on simplicity, readaility and the ease of understanding\nany code or structure.\n",57,1416,57,22
architecture/0007-use-static-code-analysis.md,## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n,Use Static Code Analysis to find violations of standards.\n,45,1418,45,11
architecture/0003-have-the-master-documentation-in-the-readme.md,"## Context\nWhen joining a project everybody needs some point where he can start stepping\ninto the system, building it, running it, understanding it. The main entrypoint\nis always the README.\n",I will consider the README as the most valuable place of documentation. The\nREADME of any project shall give a new team-member everything he needs to\njoin the team.\n,41,1424,41,35
architecture/0004-waiting-is-waste.md,"## Context\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\n",Any processes and workflows need to be very fast and fully automated.\n,43,1434,43,14
architecture/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1436,16,39
verify-frontend/0003-new-ab-testing-approach.md,## Context\nPrior AB testing involved changing existing model/view/controller files using if/else blocks to distinguish between A or\nB test behaviour. This also involved changing the various tests to accomodate the new B route. This approach made it\ndifficult for a future team to identify and remove the B code. This approach was developed in response to a need to\nspeed up the process of setting up and tearing AB tests in anticipation that a lot of them would be run.\n,"Rather than trying to modify files to accommodate B changes, this approach applies changes to cloned copies of files\nthat relate to the B behaviour.  A and B routes are defined through a new piece of code in routes.rb, which takes\nadvantage of Rail's constraints.  Cloning any JavaScript files is helped by code(ab_test_selector.js) used in\napp/javascripts/application.js.erb\nSimilarly, test files are also cloned and specific B tests are added to the copy.\n",97,1448,97,98
verify-frontend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1450,16,39
boxtribute/adr_auth0.md,"## Context or Problem Statement\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\n## Decision Drivers\n1. Security / Risk for us\n2. Ease of Use\n3. Cost\n","1. Security / Risk for us\n2. Ease of Use\n3. Cost\nWe are going for Auth0 since\n- we have prior experience with Auth0 in the team,\n- a first test in an afternoon coding session were satisfying and\n- Auth0 offers a free plan for Open-source projects.\nWe are not building our own authentication solution to reduce the security risks coming with handling g passwords.\n",84,1456,84,85
inner-source_kotlin_spark_template/20180522-kotlin.md,"## Context\nWe are free to pick any JVM language with static typing.\nDevelopers are not expected to have any background with jvm languages except java.\nKotlin looks like a ""better Java"" option with painless on-boarding.\nScala is a radical move towards functional paradigm.\n",We will adopt Kotlin as a project language.\n,61,1459,61,10
inner-source_kotlin_spark_template/20180617-docker_postgre.md,## Context\nDevelopers need to be able to run application on their local machines in isolated environment.\nDockerized DB provides required service without the need to setup\nfull-blown DB server on each local machine.\n,We will provide docker compose setup for Postgre SQL server.\n,45,1460,45,13
inner-source_kotlin_spark_template/20180502-spark.md,## Context\nWe need some framework with fast startup time and low memory footprint.\nSpark looks like a good fit due to project's maturity and good reception by the industry.\n,We will build app temple based on Java Spark framework\n,36,1461,36,11
inner-source_kotlin_spark_template/20180617-flyway.md,## Context\nDevelopers need some tool for DB versioning and safe migrations.\n,We will introduce dockerized flyway setup for local development.\nMigration scripts will also be provided.\n,17,1462,17,21
inner-source_kotlin_spark_template/20180525-gradle-kotlin.md,## Context\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\nIntellij IDEA has great support for Kotlin based builds.\n,We will adopt Kotlin as a Gradle build's definition language.\n,32,1463,32,14
aws_infrastructure/0005-use-terraform-as-infrastructure-as-code.md,"## Context\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\n",Use Terraform\n,21,1464,21,4
aws_infrastructure/0004-create-billing-alarm.md,"## Context\nThere is a fixed amount of budget for this project, since this is supposed to be a micro scale project\nand the costs of the resources used can go up pretty quickly.\n","Create an alarm for billing, if the predicted billing for the month is more than $5.\n",39,1465,39,20
aws_infrastructure/0003-use-eu-west-1-as-default-region.md,## Context\nChoosing the default region to start the resources\n,Using eu-west-1 Ireland as default\n,12,1466,12,9
aws_infrastructure/0006-use-aws-budgets-to-create-budget-alarm.md,## Context\nCreate an alarm for budgets for free\n,Use AWS budgets\n,11,1467,11,4
aws_infrastructure/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1468,16,39
aws_infrastructure/0002-use-initial-test-user-manually-to-deploy-terraform-state-buckets.md,"## Context\nTo use terraform one needs to setup s3 bucket and dynamo lock for multi user access which creates the chicken and egg problem, where the bucket and dynamodb resources have to be present first to start managing Infrastructure with terraform.\n",The requried resources would be bootstrapped using a cloudformation stack and this requires a initial admin user which will be deleted right after the creation\n,51,1469,51,31
branchout/0003-testing.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,1470,21,13
branchout/0004-use-multiple-dot-files-to-make-it-simpler-to-contribute.md,## Context\nUsing once shell file makes for one very big complicated file\nUsing smaller self contained files means new developers can more easily make useful changes and adding new files has less risk\n,Each file should do one thing and do it well\n,37,1471,37,11
branchout/0002-language.md,"## Context\nA language should be universal, simple and easily testable\nOptions\n* Shell\n* Go\n* Java\n* JavaScript\nThere should be very few dependencies\n",Shell\n* No dependencies\n* Installed pretty much everywhere developers are\n,36,1472,36,14
branchout/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1473,16,39
uniprot-rest-api/0001-programming-language.md,"## Context\nWith the goal of producing services (e.g., REST APIs) to support the UniProt website, a programming\nlanguage should be chosen so that the requirements can be implemented. The choice will\nleverage the skills of the team of developers working on this project.\n","This particular UniProt REST API project will use Java, and currently version 8.\nEven though Java 11 is already available, many companies still have not made the leap from 8 to 9\ndue to the possible amount of work required. In future, we do seek to upgrade to Java 8+.\n",57,1474,57,67
uniprot-rest-api/0004-lombok.md,"## Context\nJava projects often contain a large amount of boilerplate code, e.g., defining data/value classes, builders, etc. All\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\n",We will use the [Lombok](https://projectlombok.org/) library to reduce the amount of boilerplate code we need to write.\n,73,1475,73,30
uniprot-rest-api/0005-solrcloud.md,"## Context\nWe need a search engine to which we will send user queries, and from which we will receive their results.\nMoreover, we need an engine that can scale with our data and be resilient to faults (network, filesystem, etc.).\n","The current UniProt website uses lucene as the search engine. This is very fast. However, the drawback is that it does not easily scale.\nThis can be provided by Solrcloud. We have used this for the Proteins API with success, and therefore have 4 years of experience with it.\n",52,1476,52,64
uniprot-rest-api/0003-spring-framework.md,"## Context\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\nadditional features (than one would otherwise write themselves), etc.\n","We have used the [Spring framework](https://spring.io/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\n",35,1477,35,39
uniprot-rest-api/0002-build-setup.md,"## Context\nA build tool is necessary for various tasks such as compile, test, packaging and deploying our code.\n",We will use [Maven 3.2.5+](https://maven.apache.org) to build the back-end UniProt REST API components.\n,24,1478,24,33
uniprot-rest-api/0006-voldemort.md,"## Context\nWe need a store from which entry data will be retrieved. Moreover, this store should scale as our data grows,\nand should also be resilient to issues it might face (e.g., network, filesystem, etc.).\n","We will use Project Voldemort, as we have 4+ years of experience with this already with the Proteins API and UniProt's Java API.\n",49,1479,49,31
evidence-api/0004-build-an-abstracted-resident-store.md,"## Context\nA critical part of building these services is being able to track who evidence belongs to, and therefore we must be able to identify residents in a platform-wide manner.\nAt the time of this decision, the HackIT team is in the process of building the Platform APIs necessary to provide access to this sort of core data, but they are not available yet.\n","To build a resident data source in this application, but make the extra effort to abstract it appropriately so that when an external resident data source and accompanying API is available.\n",75,1482,75,34
evidence-api/0003-use-asp-net-for-platform-apis.md,"## Context\nPrevious APIs at Hackney have been written in a variety of languages and frameworks, including Ruby on Rails, C# and ASP.NET, and Node/Typescript.\n","Use C#, ASP.NET and Hackney's Clean Architecture framework, as specified in the [API Playbook](https://github.com/LBHackney-IT/API-Playbook-v2-beta). These applications are intended to be a robust part of HackIT's platform and iterated upon and the HackIT team has mostly C# skillset.\n",37,1483,37,71
evidence-api/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nThis tool was previously used by the HackIT team (for example in the [React component library](https://github.com/LBHackney-IT/lbh-frontend-react#architecture-decision-records)).\n",16,1484,16,83
evidence-api/0005-use-text-files-for-simple-configuration.md,"## Context\nThere are certain pieces of configuration which do not change often but need to be easy to change for colleagues who are not developers. Some examples include:\n- the document types that the API recognises and their metadata (like ther ID, title and description)\n- the council services which the API recognises and their metadata (like their name and google group ID).\nThe choice came down to either storing these configurations in the database, or storing them in text files.\n",Use text files for this configuration.\n,97,1485,97,8
holochain-rust/0005-alpha1-is-last-major-go-release.md,## Context\nA complete Rust rewrite is planned (see ADR #0002)\nGo code has code debt -> substantial effort to refactor\nGo implementation would not be reusable in Holo front-end (whereas portions of rust compilable into WASM could be)\nGo code has dependencies that make it hard to compile for mobile\n,"- Alpha1 go release is last major go release of holochain because energy will be focused on the new Rust version.  One team, one code base for now, may revisit this later\n- We will call the rust release Alpha2, and will have at least the functionality of Alpha 1 plus World-model & new network-transport\n",68,1491,68,70
holochain-rust/0003-redux-architecture-pattern.md,"## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n","The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\nWe will apply nested state objects which represent a state tree, with sub states for each module.\nWe use reference counting smart pointers for the sub\nstates such that it is possible for each module's\nreducer to decide if the sub state is to be mutated or reused.\n",92,1495,92,80
holochain-rust/0009-support-64bit-architectures-only.md,"## Context\nGoing forward with the rust implementation, we recognize that some 32bit architectures exist that people may want to run Holochain on.  Supporting 32bit architectures may have particular consequences in the realm of cryptography.  We have limited resources.\n",For now we will assume availability 64bit CPUs and not use our resources testing against 32bit targets.\n,53,1496,53,23
holochain-rust/0008-mobile-first-development.md,"## Context\nMobile is the main platform today. We want to reach as many users as possible.\nMobile use has two big constraints:  battery life (and consequent sleeping of apps), and bandwidth because of costs\nGo platform development ignored mobile from the start and we found out late about the problems of compiling to mobile.\n","Target a mobile build from the start.\nDo not initially worry about battery/bandwidth constraints, assuming that ADR 0006 will handle solve this issue in the medium term, and that advance in technology will handle it in the long-term.\n",67,1499,67,51
holochain-rust/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1502,16,39
elife-base-images/0001-commit-tags.md,## Context\nTraceability of a Docker image to a source code repository is valuable to debug any problem that comes up during testing and deployment.\nDependency pinning on project images pinning a particular version of the base image they are using is also valuable for build reproducibility.\n,"We will tag every new, tested version of an image using the commit SHA value that produced it.\n",57,1503,57,21
elife-base-images/0005-provide-standard-bin.md,"## Context\nApplications often need to drop in binaries or scripts, either downloaded or picked up from other images such as `proofreader-php`.\nThe binaries needed do not need `root` permissions or system-wide installation.\nThe binaries may need to modify the application files.\nA container image may be an alien environment, as it makes it difficult to find out which of the files were provided by the Dockerfile build process.\n","Every image should provide a standard `/srv/bin` folder, on the PATH, containing utilities owned by `elife`.\n",86,1505,86,26
elife-base-images/0002-image-labels.md,"## Context\nAn image is a powerful mechanism for deployment, as it is portable across environments.\nSome images need to specify the version of their dependencies (such as other images) that should be run as sidecars.\n",Use image labels of the form `org.elifesciences.dependencies.api-dummy` to store the version of a dependency.\n,45,1507,45,26
elife-base-images/0003-extract-scripts.md,"## Context\nDockerfiles make no provision for extracting duplication of steps unless they are rendered from a template. Similar `RUN` steps can multiply across different files.\n`RUN` steps also allow no logic or encapsulation, and promote long chains of commands due to the necessity of producing a single layer.\n`RUN` steps are not testable in isolation or re-runnable inside an image for debugging.\n","Extract long `RUN` steps (or sequences of steps) into a bash script. If the script is only to be used by `root`, place it in `/root/scripts`.\n",84,1508,84,38
molgenis-frontend/0004-molgenis-core-apps.md,## Context\n*See [frontend architecture sessions](https://docs.google.com/document/d/1VW3ah5VAvAz2KnqNZlNmVqCzFhBMlIcjPPUlsHMFRIY/)\nfor background information on micro-frontend/SPA architecture.*\n,We are going to merge the following apps together in a *@molgenis-ui/core* app:\n* navigator\n* data explorer 2\n* data-row-edit\n,66,1511,66,35
molgenis-frontend/0005-index-file-delivery.md,## Context\n*See [frontend architecture sessions](https://docs.google.com/document/d/1VW3ah5VAvAz2KnqNZlNmVqCzFhBMlIcjPPUlsHMFRIY/)\nfor background information on micro-frontend/SPA architecture.*\nWe need a consistent method to serve the index file for our frontend application.\n,We are going to use a clientside index file for the *@molgenis-ui/core* app.\n,82,1512,82,22
molgenis-frontend/0001-use-adr-to-describe-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n",16,1513,16,40
molgenis-frontend/0002-use-typescript.md,## Context\nWe need a way to describe the structure of Javascript objects for readers of the code to be able to understand the structure.\nAnd we need a way to detect null references at build time Javascript.\nOur Javascript passes though a build step.\nOur Javascript is maintained by people who did not write the code.\n,We will use Typescript as a tool to describe object structure.\n,66,1515,66,14
docker-compose-rule/0002-move-waiting-code-into-a-seperate-concern.md,## Context\nWe started with waiting code spread throughout the different layers of the DockerComposition stack:\n* DockerComposition\n* Container\n* DockerPort\nThis makes our custom waits depend on specific functionality rather than general functionality that external devs can also use. Now we're making all `waitForService` calls go through just a couple of API calls with flexible Healthchecks this is going to bite us.\n,"Wait code (for the purposes of powering `waitForService`) does not go in the Composition stack. Instead, waits depend on external observations of this stack.\n",81,1538,81,32
docker-compose-rule/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1539,16,39
opg-metrics/0002-opg-metrics-has-it-s-own-vpc.md,"## Context\nOPG Metrics will live in the opg-management account, which is home to other products and services.\nWe want to keep OPG Metrics infrasture separate from those other products so that it can be more portable across accounts and avoids impacting the performance or configuration of those other products.\n",OPG Metrics will have it's own VPC.\n,62,1542,62,12
opg-metrics/0002-aws-infrastructure.md,"## Context\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\n",To run in a fully managed AWS cloud environment using Terraform to manage it.\n,31,1545,31,17
opg-metrics/0003-use-timestream-for-time-series-data-store.md,"## Context\nData collected within the solution is stored in a Time Series format. This data can be stored in any type of database if needed, however we should use, if possible, a database that natively supports it. This will result in a more optimised system and therefore cheaper to run.\nThe amount of data that gets sent to the database will grow over time and so it will need to be scalable and managable over this time.\n","We will use Amazon Timeseries for our data store. It is a fast, scalable and serverless time series database that fits well with our existing use of AWS infrastructure.\n",91,1546,91,35
opg-metrics/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1547,16,39
webtty/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1550,16,39
origin/0004-use-openzeppelin.md,"## Context\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\n","Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\n",39,1551,39,27
origin/0007-rename-utils-demo-package-to-migrations.md,"## Context\nWe use the `utils-demo` package to deploy the whole Origin system. After receiving some feedback, we noticed that the name `utils-demo` didn't reflect the true intention of that package.\n",We've decided to rename the `utils-demo` package to `migrations` and publish it on NPM.\n,43,1553,43,24
origin/0006-rename-assets-to-devices.md,## Context\nThere has been confusion on why we call generating and consuming devices Assets in Origin. We have noticed that most Origin users would refer to Assets as Devices and not Assets.\n,"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\n",37,1555,37,26
origin/0009-migration-to-exchange.md,"## Context\nCurrent on-chain approach to handle demands and certificates matching is not scalable enough, prone to front-running and hard to extend with new features.\n",The change provides a new way of trading/matching certificates using off-chain order book matching engine.\n,31,1557,31,20
origin/0005-simplify-off-chain-storage.md,## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n,"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\n",52,1558,52,39
origin/0002-use-lerna-for-package-versioning.md,## Context\nOrigin project consist of multiple packages which are the part of Origin SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n,Migrate code base to monorepo structure and use `lerna` for versioning management.\n,34,1559,34,21
origin/0003-create-backend-client-lib.md,## Context\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\n,Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\n,29,1560,29,39
origin/0010-more-complete-database-type-and-adjust-migrations.md,"## Context\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\n","A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\n",64,1561,64,49
origin/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1562,16,39
mediawiki-extensions-WikibaseLexeme/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1565,16,39
aws-lambda-benchmark/0003-use-amazon-dynamodb-to-mimic-real-world-use-cases-of-lambdas.md,## Context\nTo make the benchmark more meaningful we will want to make them to be more like real world Lambdas. A typical pattern we see with lambdas is fetching data from Amazon DynamoDB.\n,With the deployment of the Lambdas and API gateway we will also deploy a DynamoDB table.\n,41,1567,41,20
aws-lambda-benchmark/0002-use-aws-codepipeline-with-aws-codebuild-to-build-and-deploy-project.md,## Context\nThe project needs a way to be built and deployed. It needs to be quick and easy to use so has to be able to pick up changes to the git repo. It should be easy for anyone recreate the workflow used here and flexible enough to handle building multiple languages. At the end of successful build the artifact should get deployed.\n,As this is project is benchmarking AWS Lambdas it makes sense to use the services AWS has for building (AWS CodeBuild) and deploying (AWS CodePipeline) code.\n,70,1568,70,36
aws-lambda-benchmark/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1569,16,39
mlmc/0005-use-api-gateway.md,"## Context and Problem Statement\nFor Microservices architecture it is a good idea to have API Gateway service to manage interservices communication and handle cross-cutting concerns (logging, security, caching, etc.).\n## Decision Drivers\n* Easy to use\n* Easy to learn\n* Can be hosted on Azure\n* Can be hosted in Docker container\n",* Easy to use\n* Easy to learn\n* Can be hosted on Azure\n* Can be hosted in Docker container\nNo outcome yet. Have to investigate both a bit and then will decide.\n### Positive Consequences\n* I will be able to manage communication between services more effectively and securely with API Gateway.\n### Negative Consequences\n* n/a\n,71,1573,71,74
SoundCloudVisualizer/0004-use-gulp-to-build-source-code.md,"## Context\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\n",[Gulp](https://gulpjs.com/) will be used to bundle client-side code.\n,81,1576,81,19
SoundCloudVisualizer/0003-use-angularjs-1-x.md,## Context\nThis project was first implemented in plain JavaScript. Its only third-party dependency was on SoundCloud's JavaScript SDK.\nI had recently discovered frontend web frameworks and had been learning about AngularJS. I was interested in applying it to a larger project.\n,The application will be rewritten in AngularJS 1.x.\n,53,1577,53,13
SoundCloudVisualizer/0005-manage-dependencies-using-yarn.md,## Context\nWhen this project was first starting out [Bower](https://bower.io/) was the tool of choice for managing frontend dependencies.\nBower has since been deprecated and its creators have [advised its users to migrate to other solutions](https://bower.io/blog/2017/how-to-migrate-away-from-bower/).\n,Dependencies will be managed using [Yarn](https://yarnpkg.com/).\n,70,1578,70,18
SoundCloudVisualizer/0008-use-css-modules-to-create-scoped-css-for-components.md,## Context\nThe CSS declarations for all components was declared in a single file. This has become difficult to manage as the relevant CSS is located separately from its component and there are a lot of declarations in one file.\n,We will enable [CSS Modules](https://github.com/css-modules/css-modules) and co-locate each component's CSS declarations with its code.\n,44,1580,44,30
SoundCloudVisualizer/0002-manage-client-side-dependencies-using-bower.md,"## Context\nThis project was first implemented in plain JavaScript. Its only third-party dependency was on SoundCloud's JavaScript SDK, which was included via a `script` tag.\nI had come across package managers while learning AngularJS 1.x. Bower was one of the popular tools of choice at the time.\n",Client-side dependencies will be managed using [Bower](https://bower.io/)\n,65,1582,65,18
SoundCloudVisualizer/0006-use-webpack-to-build-source-code.md,## Context\nWhen this project was first starting out [Gulp](https://gulpjs.com/) was one of the tools of choice for running tasks such as bundling and minifying code.\nSince then task runners seem to have fallen out of popularity and bundlers such as Webpack and Parcel are the preferred approach.\n,Webpack will be used to bundle source code.\n,66,1583,66,10
SoundCloudVisualizer/0001-record-architecture-decisions.md,"## Context\nThis has been a long-running side project. I am interested in capturing what design decisions have been made, and why.\n","This repo will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",28,1585,28,40
iampeterbanjo.com/0009-place-tests-next-to-source-files.md,"## Context\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\n",Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\n,60,1588,60,36
iampeterbanjo.com/0012-parallel-database-tests-mongodb-server.md,"## Context\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\n- [define databases in tests][defined-test-database]\n- [randomly create databases for each test][random-test-database]\n","In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\n",80,1589,80,77
iampeterbanjo.com/0008-use-ava-test-runner.md,## Context\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\n,Jest seems like the way to go:\n- Typescript support\n- Large community\n- Familiar\n,91,1594,91,23
iampeterbanjo.com/0013-cypress-e2e-tests-in-typescript.md,## Context\nI'm getting Typescript errors in the e2e tests. If I'm relying on Typescript for linting then keep errors clean is good hygiene. I'v tried ignoring the `e2e/` or `cyperss/` folders and that is not working. Why? Not sure.\n,"In the context of getting Typescript errors in my e2e tests and facing the concern to keep errors clean, then use Typescript for e2e tests. Especially since it seems to be [well documented][cypress-ts] with examples. Therefore the burden should be light.\n",67,1600,67,58
iampeterbanjo.com/0015-frontend-backend-architecture.md,"## Context\nFacing the concern that by serving views from the backend API, I have less opportunity for reusing APIs, creating new apps and staying on trend, I propose to decouple the frontend and backend architecture of the site.\nThe opportunities that this creates would be to learn new technologies such as\n- GraphQL: new approach to APIs to solve the problem of creating different endpoints for different frontend requirements.\n- Server-side rendering: important for Search engine optimization and performance.\n",- Re-write the backend in Fastify for better Typescript support.\n- Create a server-side rendered frontend.\n,95,1601,95,24
iampeterbanjo.com/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"In the context of making several architecture and design decisions for this site, I was concerned that I would not have a way to audit and learn from them. So I decided to use Architecture Decision Records, as described by Michael Nygard in [this article][1], to achieve better learning outcomes in the future. I accept it's more work to do so.\n",16,1603,16,73
dogma/0010-handler-timeout-hints.md,"## Context\nWe need to decide on a mechanism for engine implementations to determine\nsuitable timeout durations to apply when handling a message.\nFor aggregate message handlers, which are not permitted to access external\nresources, a fairly constant timeout duration should be discernable by the\nengine developers.\nFor all other handler types, which may make network requests or perform CPU\nintensive work, there is no one timeout duration that makes sense in all\ncircumstances.\n","We have decided to allow process, integration and projection message handlers\nto provide a timeout ""hint"" on a per-message basis by way of a\n`TimeoutHint(dogma.Message) time.Duration` method.\nBy returning a zero-value duration, the handler indicates that it can provide no\nuseful ""hint"" and that the engine should choose a timeout by other means.\n",96,1613,96,79
dogma/0005-routing-of-commands-to-processes.md,## Context\nWe need to decide whether command messages can be routed directly to processes.\n,"We have decided to disallow this behavior - processes may only handle events\nand timeout messages.\nIf we were to allow processes to accept commands directly it may be tempting to\nimplement domain idempotency in the process instead of in an aggregate where\nsuch logic belongs.\nFurthermore, it's easier to allow commands to be routed to processes in a future\nversion than it is to remove it once it's in use.\n",18,1615,18,88
dogma/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1621,16,39
dogma/0007-location-of-examples.md,"## Context\nWe need to decide whether Dogma's examples should reside in the `dogma`\nrepository itself, or a separate `examples` repository.\n","We've decided to move the examples to a separate repository, so that we can\nprovide fully-functional examples that depend on modules/packages that we would\nnot want to have as dependants of `dogma` itself, such as `mysql`, etc.\n",33,1622,33,52
akka-monitor/0005-use-dropwizard-metrics.md,## Context\nWe want to provide metrics about the status of a monitored service.\n,We will use _Dropwizard_ for creating the metrics.\n,17,1623,17,13
akka-monitor/0002-use-akka.md,## Context\nWe need to be able to monitor network services separately.\n,We will use _akka_ for managing the task to monitor a particular network service.\n,15,1624,15,19
akka-monitor/0004-use-micrometer-prometheus.md,## Context\nWe want to provide metrics about the status of a monitored service.\n,We will use _Micrometer_ for creating the metrics and make them available through a _Prometheus_ endpoint.\n,17,1625,17,24
akka-monitor/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1627,16,39
terraform-aws-iam-module/0003-dual-support-for-terraform-version-0-11-and-0-12.md,"## Context\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\n",This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\n,46,1628,46,73
terraform-aws-iam-module/0002-create-a-collection-of-iam-related-modules.md,"## Context\nThere are many IAM resources of which most, if not all, are relatively small in size & complexity. In order to minimise repo churn, the module will contain a collection of modules which can be invoked as required.\n",The module will be a mono repo of specific IAM resources.\n,47,1629,47,13
terraform-aws-iam-module/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1630,16,39
admin-react-components/0003-component-bundling.md,"## Context\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\n",We'll package components with their own inline styles. No external dependencies needed.\n,38,1631,38,16
admin-react-components/0002-bulma.md,"## Context\nWe needed a straight-forward, customizable UI style framework that was somewhat opinionated (we don't have a design team), customizable, intuitive in naming convention, and one which would not interfere with our JavaScript React component implementations.\nWe reviewed a few UI frameworks including Ant Design, Bootstrap, Bulma, and TailwindCSS.\n","We've chosen [Bulma](https://bulma.io/) for it's blend of simplicity, ease of use, and CSS-only approach, and current market share. It has a modern look & feel, seems to be gaining in popularity, and doesn't look like a lot of Bootstrap applications.\n",69,1632,69,62
admin-react-components/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1633,16,39
qiskit-vscode/ADR-001.md,## Context\nWe need a parser to be able to validate syntactically and semantically an Open QASM file. And a previous one with its grammar is already available from QX.\n,We will use Jison and the original Open QASM grammar from the QX project.\n,39,1634,39,19
qiskit-vscode/ADR-004.md,"## Context\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\n","The recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\n",49,1635,49,49
qiskit-vscode/ADR-003.md,"## Context\nSome parts of the code were quite different and some ""bad practices"" started to appear. Because no person in the team is an expert in TS, we believe a stricter static analysis process could help.\n","Because the community mainly accepts Google's TSLint rules as a good practice, we will use them.\n",44,1636,44,22
qiskit-vscode/ADR-002.md,"## Context\nWe already have a parser for Open QASM files, but we are having problems to create the suggestions library because we haven't a valid AST available.\n",We will use ANTLR4 and ANTLR4-C3 library to generate a parser which can return an AST. With the AST the ANTLR4-C3 library can return the valid tokens at the caret position.\n,34,1637,34,47
linshare-mobile-android-app/0012-decoupling-upload-logic-from-uploadworker.md,"## Context\nLinShare App not only upload to `MySpace`, but also to `WorkSpace`.\nCurrently, `UploadWorker` is tightly with the logic of Uploading to MySpace, so we can not reuse it with other new uploading logic\n","#### Current Implementation\n```\nUploadWorker -> UploadInteractor\n```\n#### New Implementation\n```\n|-------> UploadToMySpaceCmd\n|\nUploadWorker -> UploadController -> UploadCommand\n|\n|-------> ***Cmd\n```\nWe will add `UploadController` to `UploadWorker`, it will execute the `Command` as we want to execute upload logic.\n",53,1638,53,83
linshare-mobile-android-app/0011-upload-document.md,"## Context\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\nBut, an `Uri` has `UriPermission` protect the data which is represented.\nThis permission will be revoked by shared application when the received Activity no-longer run.\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\n","- We extract all requirement information of the Document at the time we receive the Intent.\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\n- We deliver temporary file path to Worker\n",94,1643,94,47
linshare-mobile-android-app/0009-downloadingrepository-to-manage-downloading-tasks.md,## Context\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\nApplication need to store this Id to do further stuff with this task:\n- Query status\n- Get the completed state\n- Get error details\n- Cancel a download task\n,Creating a `DownloadingRepository` to manage downloading tasks\n,62,1645,62,11
linshare-mobile-android-app/0008-download-with-downloadmanager-service.md,"## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n","Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\n",99,1647,99,25
linshare-mobile-android-app/0013-destination-picker-s-flow-for-upload.md,## Context\nUse-case:\n```\nAlice want to share a file to a workgroup\nAlice select file and select destination to upload to\n```\nWe have many UI flows of destination picker in Uploading case.\n,The accepted flows are implemented:\n![destination_picker](./images/destination_picker_in_uploading.png)\n,48,1648,48,22
linshare-mobile-android-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1649,16,39
DunkMe/0001-front-end.md,"#### Context and Problem Statement\nA front end, client facing technology stack / language is needed.\n#### Considered Options\n- ASP.NET MVC (C#)\n- ASP.NET Web Forms (C#)\n- Angular (with TypeScript)\n#### Decision Outcome\nChosen option: ""Angular"", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n","Chosen option: ""Angular"", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n",93,1659,93,38
DunkMe/0003-relational-database.md,"#### Context and Problem Statement\nA relational database is required to persist the applications data.\n#### Considered Options\n- Microsoft SQL\n- MySQL\n- PostgreSQL\n- SQLite\n#### Decision Outcome\nChosen option: ""Microsoft SQL"", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n","Chosen option: ""Microsoft SQL"", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n",78,1661,78,38
DunkMe/0004-game-database-fields.md,"#### Context and Problem Statement\nAll game records need to be persisted to the relational database with reporting in mind.\n#### Considered Options\n- dbo.game for each game record, dbo.game_line for each shot attempt.\n#### Decision Outcome\nChosen option: dbo.game with dbo.game_line\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\n",Chosen option: dbo.game with dbo.game_line\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\n,97,1662,97,48
ODT_OnlineBank/ARD-0002.md,"## Context\n*This section describes the forces at play, including technological, political, social, and project local.\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\nIt is simply describing facts.*\nShould the application be implimented as a monolith or microservices.  While a monolith approach would work for this simple\napplication, CAH builds sophisticated, complex applications composed of multiple logical components.\n","*This section describes our response to these forces. It is stated in full sentences, with active voice.\n""We will ...""*\nGiven these facts, a multi-repository microservices approach will be used\nSCM Repos:\n- UI\n- Microservice #1\n- Microservice #2\n- Microservice #3\n",100,1664,100,68
simple-server/009-single-user-model.md,## Context\nWe currently have two separate models for Administrators (Admin) and Nurses (User)\nto distinguish users of the dashboard and the app respectively. But there is an increasing\noverlap between the two models.\n- Some Administrators are using the app\n- Access to dashboard needs to be audited similarly to syncing\n,"Combine User and Admin into a single User model, and have different authentication mechanism for\nboth these models.\n- Authenticate syncing api using phone number authentication\n- Authenticate dashboard access using email authentication\n",67,1699,67,39
simple-server/010-user-permissions.md,"## Context\nWe are currently using a role based access control mechanism with the following roles\n- Owner\n- Organization Owner\n- Supervisor\n- Analyst\n- Counsellor\nAs the application grows, we foresee an explosion in the roles and their abilities. This\nwould be very difficult to track with static roles. This also causes confusion to users\nas we add more nuanced roles.\n",We propose moving to a permissions based access control model that can give granular access\nto dashboard features. Each feature on the dashboard is backed by its on permission and user\naccess can be granted and revoked accordingly.\n,79,1700,79,45
commercetools-adyen-integration/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1717,16,39
viplab-websocket-api/0003-transfere-hash-in-jwt-claim.md,## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the json data model.\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\nThe validity of the hash must be verified.\n## Decision Drivers <!-- optional -->\n* JWT should be used\n,"* JWT should be used\nChosen option: ""Transfer hash in JWT Claim"", because it's the only option when using JWT.\n### Positive Consequences <!-- optional -->\n* multiple hashes for different json documents can be added in one JWT\n",93,1718,93,51
viplab-websocket-api/0001-use-json-web-tokens.md,## Context and Problem Statement\nExternal services must authorize web clients to the WebSocket API.\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\nOnly little data should be stored for an open WebSocket connection.\n## Decision Drivers\n* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\n,"* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\nChosen option: ""JWT from pre-shared keys"", because the WebSocket API is loosely coupled and it is well supported on many platforms.\n### Positive Consequences\n* Simple to implement\n* Authorization data can be send in a portable and verifiable way\n### Negative Consequences\n* The shared keys must be handled\n",72,1719,72,87
viplab-websocket-api/0002-use-sha256-with-base64url-encoding.md,"## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the data.\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\nThe WebSocket API must able to verify the integrity of the json data.\n## Decision Drivers <!-- optional -->\n* Use standard encodings\n","* Use standard encodings\nChosen option: ""Send SHA256 hash of Base64Url encoded json"", because this method is platform independent and not much session state is required.\n### Positive Consequences <!-- optional -->\n* The JWT really function as a verification token for the other requests.\n* Can be applied to all json data that must be verified.\n### Negative Consequences <!-- optional -->\n* The json must be transferred in Base64Url encoding\n",77,1720,77,96
viplab-websocket-api/0004-use-asymmetric-jwt-signing.md,## Context and Problem Statement\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\nJWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\nWhen implementing JWTs one must decide which method to use.\n## Decision Drivers\n* Multi tenant support with own keys for each tenant\n,"* Multi tenant support with own keys for each tenant\nChosen option: ""Asymmetric JWT signing"", because it the only option which allow to use different keys for different tenants.\n### Positive Consequences\n* multiple keys are supported\n### Negative Consequences\n* complex management of keys\n",77,1721,77,59
bridge/002-code-organisation.md,## Context\n,"bridge.* - top level ns\nbridge.ENTITY.* - everything about ENTITY\nthese could be directly in bridge, in which case it's the engine, or in an ENTITY, in which case it's the actual implementation for that ENTITY.\nfrom 'bottom' to 'top':\n.schema - datomic schema\n.spec - specs\n.data - all data transformations, all Datomic interactions\n.api - controller layer for client\n.ui - all client code\n",3,1722,3,93
arborescence/0002-configure-line-endings-in-gitattributes.md,"## Context and Problem Statement\nTo avoid problems in your diffs, you can configure either IDE or Git to properly handle line endings, so you can collaborate effectively with people who use different operating systems.\n## Decision Drivers\n* Cross-platform editing with different native line endings\n* Configuration should apply automatically by cloning repository\n","* Cross-platform editing with different native line endings\n* Configuration should apply automatically by cloning repository\nChosen option: “`* text=auto` entry in a .gitattributes file”, because this doesn't depend on EoL-settings on the developer's machine, even if they are somewhat enforced using the EditorConfig file.\n### Positive Consequences\n* All text files have native line endings after check-out without any friction between collaborators on different operating systems.\n",63,1725,63,94
external-service-operator/0003-the-name-of-the-endpoints-service-and-ingress-gets-inherited-by-the-controlling-externalservice.md,"## Context\nTo easify finding the according Endpoints, Services and Ingress Ressources, they are named exactly the same as the Externalservice Ressource.\nNethertheless, of course the Owner will be set correctly as well as every ressource gets the label:\n```\napp= <external-servicename>\n```\n",The change that we're proposing or have agreed to implement.\n,76,1733,76,13
external-service-operator/0004-ports-are-never-stored-as-named-ports.md,"## Context\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\n",The change that we're proposing or have agreed to implement.\n,70,1735,70,13
external-service-operator/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1737,16,39
digitalrig-metal-aws/0002-use-aws-bare-metal-rig-approach.md,## Context\nWe need to create a riglet for our new bookit project so that we practice what we preach.\n,"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: Travis\n",25,1738,25,85
digitalrig-metal-aws/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md,## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,36,1741,36,50
digitalrig-metal-aws/0004-use-cloudwatch-logs-for-log-aggregation.md,## Context\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\n,We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\n,27,1743,27,29
digitalrig-metal-aws/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1744,16,39
TOSCAna/0018-cloudfoundry-no-deployment-in-runtime.md,## Problem\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\n,* Chosen Alternative: creating scripts\n,29,1745,29,8
CAFE5/0004-use_median-rather-than-mean-in-gamma-discretization.md,"## Context\nIn order to simulate a smooth gamma curve, we discretize values from the gamma curve and\nset categories based on them.\n",It was decided to use the median value rather than the mean for calculating the discrete values.\n,29,1748,29,19
CAFE5/0005-remove_chisquare_option.md,## Context\nThe original intent was to run both the base and gamma models and compare them with a chi-squared test.\n,Currently it seems more effective to allow the user to run base and gamma models separately and compare them\nusing their own favorite method.\n,26,1749,26,27
CAFE5/0003-do-not-perturb-lambdas-in-simulations.md,"## Context\nSimulations are generated based on the lambda value provided by the user. It seemed that the simulations\nwe generated were not realistic enough. It was thought that slightly modifying the lambda value every\nso often would create more realistic simulations, and this code was implemented.\n",We will simply base all simulated families on the given lambda and not perturb the lambda.\n,56,1750,56,19
CAFE5/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1751,16,39
ksch-workflows/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1759,16,39
tdr-dev-documentation/0016-govuk-notify-local-development-stack.md,## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\n,"No Full Local Development Stack for GovUK Notify.\nGovUK Notify does not interact with any other TDR systems, therefore setting up a full local development stack is not necessary.\n",33,1776,33,37
automated-workstation-setup/0002-ansible-do-not-use-local-connections.md,"## Context\nSince this project targets a local workstation and is aimed to be run by the same\nexact localhost, should we use ansible's connection local for ansible.cfg and localhost\nas a host in all the playbooks?\n","We will model the ansible playbook hosts as separate groups and form a full inventory\nof them. The inventory will be assigned to localhost or 127.0.0.1 ip.\nThe logical explanation of this is having a set of playbooks that can be run from everywhere\nfor two reasons\n* learn and expirement with the main ansible goal, i.e. setup remote hosts\n* re-use this project in the future for more than our workstation\n",46,1790,46,92
automated-workstation-setup/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1791,16,39
NorthwindCore/0001-record-architecture-decision.md,"## Context\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\n","For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\nEach ADR will be recorded using [Michael Nygard template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\n",33,1792,33,77
NorthwindCore/0003-arm-templates.md,## Context\nApplication will be hosted in Azure & validated after each pull request to master/main branch.\n,ARM template will be created to quickly create test environment to execute all unit tests.\n,21,1793,21,17
NorthwindCore/0002-simplified-monolith.md,## Context\nTo quickly check database connection and created objects as a result of Northwind database reverse engineering.\n,Simple call from handler to DbContext is used without any middle layer.\n,22,1794,22,14
fixcity/0002-sys-pick-documentation-language.md,"## Context\n- We have polish development team, so all business related communication is in polish,\n- All team members prefer to write code in english,\n- There is not much business facing documentation (BFD).\n","- All BFD and artifacts will be in english (ES sessions, System Context diagram, e.t.c),\n- All developer facing documentation and artifacts will be in english (Code, ADR, C2-C3 diagrams, e.t.c).\n",45,1796,45,52
fixcity/0003-sys-use-modular-monolith.md,## Context\n- Constraints\n- Greenfield project\n- Quality attributes\n- Expected fast load\n,"We will not separate components in to separate deployment units,\nwe will use modular monolith approach with single database.\n",20,1797,20,24
fixcity/0001-sys-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1798,16,39
wiki-treasure-hunt/001-hosting.md,"### Context\nThe current version of the game is not user friendly. It requires knowledge\nand acces to a Linux command line.\n### Decision\nCreate a web interface to host the hunts. To keep costs low, lets host it\non GitHub Pages.\n### Consequences\n- Easier User Experience\n- Wider player base\n- GUIs are complex to build\n","Create a web interface to host the hunts. To keep costs low, lets host it\non GitHub Pages.\n### Consequences\n- Easier User Experience\n- Wider player base\n- GUIs are complex to build\n",77,1805,77,47
bibdata/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,We will need to record the architectural decisions made on this project to orient new developers.\n,16,1806,16,18
bibdata/0003-index-merged-holdings-info.md,## Context\nTo speed up indexing we need holdings information included in the dump.\n,"* Removes bib 852s and 86Xs, adds 852s, 856s, and 86Xs from holdings, adds 959 catalog date\n* https://github.com/pulibrary/voyager_helpers/blob/37b62ad83ed2f1af6185e54534d901e23e5fdf30/lib/voyager_helpers/liberator.rb#L940\n",17,1807,17,84
bibdata/0002-index-catalog-date.md,## Context\nThe need is to identify newly available materials in a facet.\n,"* If the item is an electronic resource, capture the date of the bib's creation.\n* If the item is a physical resource, capture the date of the earliest item's creation; return nil if there are no items.\n* https://github.com/pulibrary/voyager_helpers/blob/37b62ad83ed2f1af6185e54534d901e23e5fdf30/lib/voyager_helpers/liberator.rb#L959\n",16,1808,16,96
publisher/0002-use-component-level-micro-frontends-for-the-header-and-footer.md,"## Context\nHaving [separate micro frontends for pages] requires duplication of the header and footer.\nWe expect users will want to change the content of both, as well as their styling and behaviour.\nWe investigated the use of micro frontends in [#371][Spike].\n","We will create dedicated services for the header and footer.\nWe will use [Edge Side Includes (ESIs)], limited to `<esi:include>`, to transclude them into pages.\nEach component will have 2 ESIs: 1 for the `<head>` and 1 for the `<body>`.\n",60,1810,60,65
register-a-food-business-service/0002-record-architecture-decisions.md,"## Context\nNotify Template IDs were stored in Environment variables, and this would have to be updated for every template change. The fields required by the Notify templates were also hardcoded. This meant that any change to the data fields required a new version of the app to be deployed.\n","We will store Notify template IDs in the Config DB, associated with a specific data schema version number. Requests to the back end must include a data schema version number. Required template fields are fetched via the Notify API and optional fields are created as empty strings if required.\n",56,1811,56,54
register-a-food-business-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1812,16,39
experimenter/0007-doc-hub-url.md,"## Context and Problem Statement\nWe'd like to have a permanent URL for the experimenter docs hub that is:\n- Memorable\n- Stable\n- Ideally, part of the rest of our data information architecture\n","We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\n",45,1823,45,52
cloud_controller_ng/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1833,16,39
sdbmss/0008-rdf-sparql-support.md,"## Context\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\n[mmm]: http://mappingmanuscriptmigrations.org ""Mapping Manuscript Migrations project site""\nAt the time the export was built a single unified data model had not been agreed upon.\n","The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\n",94,1839,94,90
sdbmss/0004-name-authority-component.md,## Context\nA promise of the SDBM grant proposal was to add a name authority component to the SDBM. Most important are the names of previous and current owners of manuscripts. Some are found in existing authorities; other are not.\n,"The decision was to use the Virtual International Authority File <https://viaf.org>, which aggregates many national library authority lists, including the Library of Congress. A helper module `lib/viaf.org` is used by Name forms to allow users to search VIAF for VIAF IDs. Newly added names are approved by admin users of the application.\n",50,1840,50,71
sdbmss/0006-de-ricci-integration.md,## Context\nA deliverable for the SDBM NEH rebuild grant was integration of scanned notes from Seymour de Ricci's unfinished survey of British Manuscript. The value in the cards lies in the notes on the manuscripts of named individuals and organizations that could be linked to or added to the SDBM name authority.\n,"The decision was to create a game that allows users to attach randomly selected cards to existing SDBM name entries or identify new names to be added. The data is stored in the SDBM database, but the PDFs of the notes are hosted by the Senate House Library at the University of London. For example,\n- <https://dericci.senatehouselibrary.ac.uk/web_pdf/dericci_a_f_s.pdf>\n",66,1841,66,91
sdbmss/0005-user-maintenance-of-page-content.md,"## Context\nThe project team wanted to able to edit frequently different varieties of textual page content--welcome messages, instructional text, read-me text, tool tips, and so forth.\n",A system of HTML pages was created to be loaded via an AngularJS plugin so that admin users can edit the static files directly. The edited files are saved to disk and included in the pages at load time.\n,37,1842,37,43
sdbmss/0009-sdbm-user-forum.md,## Context\nThe SDBM staff desired a method for receiving and responding to user feedback and questions in a public forum.\n,The Thredded gem was chosen to enable threaded forum discussions.\n,26,1843,26,14
sdbmss/0003-provide-search-results-downloads.md,## Context\nUsers wanted to be able to download sets of search results for use apart from the application.\n,"The CSV format was chosen for downloaded search results. Because these sets can be quite large, the creation of search\nresults is backgrounded using delayed and users or informed of this through a dashboard and in-browser notification. Delayed job is used for background orchestration.\n",22,1844,22,54
sdbmss/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1845,16,39
ipsec-tester/0003-only-work-with-configured-vpns.md,## Context\nThis system needs storage in the data store for every VPN it deals with.\nSince it shall be used to analyse IPsec tunnels it could create the needed storage on the fly when interesting traffic reaches the system.\nThis would make it easier to get started with a new VPN but it would make the system vulnerable to resource exhaustion.\nAdditionally it could be triggered with spoofed source addresses to send traffic to arbitrary systems.\n,"For every VPN gateway that this system shall interact with, at least the storage in the data store has to be initialized.\n",88,1846,88,25
ipsec-tester/0004-use-sockets.md,## Context\nThe IP stack automatically handles UDP and IP datagrams\nwhen there is no service listening for the port or protocol\nby sending ICMP type 3 messages to the sender.\nThis would interfere with the IPsec exhange\nunless there is a process listening\nfür UDP port 500 or 4500 and for ESP or AH.\n,The program uses sockets for network access.\n,72,1847,72,9
ipsec-tester/0002-use-libpcap-and-libnet.md,## Context\nThis project is about learning and debugging IPsec.\nTherefore it has to analyse every received IPsec datagram by itself and can't rely on the OS kernel.\nLikewise it has to craft every IPsec datagram it will send by itself.\n,Use *libpcap* to receive datagrams and *libnet* to send datagrams by the IPsec translator.\n,55,1848,55,26
ipsec-tester/0005-use-zlog.md,## Context\nThis is a debugging tool.\nIt is vital to have meaningful logging that can be customized.\n,[Zlog](https://github.com/HardySimpson/zlog)\nseams to be a reasonable library for this purpose.\n,23,1849,23,28
ipsec-tester/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records (ADR), as described by Michael Nygard in this article: <http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions>.\n",16,1850,16,44
LogLady/0005-use-of-fabric-ui-as-design-system.md,## Context\nCan development be made easier and design be improved by using a design system with pre-made components and design guidelines?\n,"We will use [Fabric UI](https://developer.microsoft.com/en-us/fabric#/) as our design system.\nThe closest competitor to Fabric UI was Material UI, but Fabric UI was chosen due to it having an already bundled effective list renderer, good support for theming, amny components and a good license (MIT). Each component in Fabric UI is also accompanied by a series of do's and don'ts, making it easier to make informed design decisions as a developer.\n",26,1853,26,97
LogLady/0003-implement-redux.md,## Context\nState handling in React becomes very complicated when you have too many states\n,We implement Redux to handle state\n,17,1854,17,7
LogLady/0004-implement-styled-components.md,## Context\nComplicated with uses of different css rules\n,Only uses styled-components for design\n,12,1855,12,7
LogLady/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1856,16,39
content-data-api/adr-010-track-content-items-in-all-languages.md,"## Context\nIn [adr-008][1] we agreed on focusing only on English pages.\nWe have noticed during the last few months that this restriction is causing more\nissues than benefits, because:\n- We have Content Items with the same `content_id` and different `locale`s.\n- We needed to handle edge cases when retrieving the information from Publishing API in order to work around only retrieving English language content.\n",Track Content metrics of all Content Item regardless of their locale.\n,88,1862,88,13
content-data-api/adr-006-track-metrics-via-time-dimension.md,"## Context\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\n","Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\n",80,1863,80,53
content-data-api/adr-000-document-architectural-decisions.md,## Context\nWe aim to:\n- Make it easier to understand the codebase and its status\n- Reduce the number of meetings to handover information across teams\n- Facilitate team rotations across GOV.UK\n,"Track architectural decision that impact the status of the CPM, [following a lightweight format: ADR][1]\n",46,1864,46,24
content-data-api/adr-016-rename-content-performance-manager.md,## Context\nThe data warehouse is no longer an application with an UI. It is an API that is\nconsumed by other applications in order to get metrics through time for content\nitems of GOV.UK.\nThe name of the application is not compliant with GOV.UK guidelines for naming.\n,Rename Content Performance Manager to Content Data Admin\n,62,1865,62,9
content-data-api/adr-009-track-metrics-by-basepath.md,"## Context\nWe started tracking performance and quality metrics by `content_id`, but after\nthe first 3 months it was clear that the user needs to track metrics at the\nbase_path level in Guides and Travel Advice.\n",Track metrics at the base_path level\n,46,1866,46,8
content-data-api/adr-008-focus-on-english-content.md,"## Context\nSome Content Items are written in different languages, so [Publishing-api][1] will return the `content_id` along with all locales assigned to the Content Item.\n","Focus on content written in English.\nThe main reason is that we would need different algorithms and libraries to make our application consistent among all the languages / locales.\nIf this is a real need, we will support it in future iterations of the Data Warehouse.\n### Benefits:\nThis makes the codebase simpler.\n",38,1868,38,64
rfcs/0000-backup-strategy.md,"## Context\n[context]: #context\nAll contents of *kartevonmorgen.org* are stored in an SQLite database, i.e. a single file\non the server. Currently this file is backed up manually, every few days or sometimes\nweeks. The backup is stored both on the server and on private, external storage media.\n","[decision]: #decision\n- A daily backup is generated automatically, compressed and stored on the server in a dedicated folder\n- The backup folder is synchronized periodically (daily/weekly?) via rsync or syncthing with external storage\n- NTH: Old backups are selectively deleted, e.g. only te most recent 30 daily backups\nare kept, for previous month only the last daily backup ist kept.\n",71,1876,71,85
rfcs/0000-template.md,"## Context\n[context]: #context\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\n","[decision]: #decision\n> This section describes our response to these forces. It is stated in full sentences, with active voice. ""We will ...""\n",60,1879,60,33
toc-poc/1577574698834_architecture_choice_for_template_generation.md,"## Context\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\n",Use ports and adapters architecture style.\n,94,1883,94,8
toc-poc/1577573517559_language_choice.md,"## Context\nCurrently I'm capable of using two languages to develop what I'm intending to do - Ruby and Javascript. It didn't make sense to use both, since functionality is not that big. The main thing I wanted to check was related to presentation layer, so it kinda limited my options as well.\n",Use Javascript to develop proof of concept.\n,63,1884,63,9
toc-poc/1577576739261_infrastructure_choice_for_deployment.md,"## Context\nThe final effect of the POC will be static HTML page, with css and javascripts included in the file. Therefore, it will be easy to host it online for demonstration purposes, with configuration for multiple environments.\n",Use S3 static webpage hosting as deployment infrastructure.\n,47,1885,47,11
toc-poc/1577570614084_document_each_decision_in_standarised_way.md,"## Context\nUsually development comes at full pace as soon as there is some work to do.\nDuring development, plenty of decisions are being made.\nSome of them are even made before development itself, based on drivers.\nThis is only proof of concept, but at the same time, even in development of POC some decisions are made.\nWhen I come back to it on some day, I want to know why I made this and not other decision.\n","Document each decision with ADR (Architecture Decision Record). Each decision will constitute a markdown document with title, date of decision, its status, context providing background and preconditions for the decision, and listed positive/negative consequences. Filename of each decision will have included timestamp and title in `snake_case` convention. All decision will be store in `doc` directory. They will constitute Architecture Decision Log. Each record will be immutable.\n",95,1886,95,87
wsgi-base/0003-install-all-base-dependencies.md,"## Context\nIt is possible to install any dependencies for child services in this base\nimage. Doing so increases the image size, but speeds up child image builds.\n",We will install all common base dependencies in the base image. This includes:\n* WSGI libraries\n* Flask and related plugins\n* Development requirements\n,34,1887,34,32
wsgi-base/0002-use-non-deterministic-docker-builds.md,"## Context\nIn principle, we agree that deterministic bulids are a great idea. In practice,\nfirst creating a lock file (using pipenv or pip-tools) via a Docker based\nworkflow would require us to use a separate image and thus seems inefficient.\n",We build our Docker images directly from the requirements and let the resulting\ncontainer be the deterministic unit of distributing our code. We feel that the\nimproved build workflow outweighs the negative consequences.\n,54,1888,54,40
wsgi-base/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1889,16,39
aspan-server/0003-replacing-ramda-with-lodash.md,## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n,Decision here...\n,22,1890,22,4
aspan-server/0002-derived-attributes.md,## Context\nDerived attributes are read-only.\n,"Following attributes are derived from file system:\n1. name - file name without extension\n2. contentType - file extension without dot, lowercased\n",10,1891,10,30
aspan-server/0001-aspan-will-support-multiple-repositories.md,## Context\nContext here...\n,Decision here...\n,7,1892,7,4
aspan-server/0004-info-graphql-import.md,## Context\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\n,Decision here...\n,20,1893,20,4
talktofrank-beta/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[documenting architecture decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,1895,16,44
terraform-aws-elasticsearch-module/0003-provisoning-service-link-role-is-outside-the-scope-of-this-module.md,## Context\nAn Elastic Search Service Linked Role is required when provisioning an ES Domain in a VPC. It allows the ES Domain to bind and configure ENI's on the VPC.\n,"There can be only 1 ES Service Linked Role per account, and so this module will not create the Service Linked Role. The ES Service Linked Role resource must already be provisioned in the AWS account when using this module.\n",39,1902,39,46
terraform-aws-elasticsearch-module/0002-do-not-specify-maximum-version-constraints-for-required-providers.md,## Context\nTerraform allows to pin the spcific versions of providers required for this module.\n,This module will not enforce maximum versions contraints for all required providers\n,22,1903,22,15
terraform-aws-elasticsearch-module/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1904,16,39
drt-v2/0012-serialize-akka-persistence-with-protobuf.md,## Context\nFor akka persistence to be resilient to message schema changes you can't/shouldn't rely on java  serialization.\nThe big 3 serialization approaches would be:\njson\nprotobuf\nthrift\nhttp://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/\n,Use google's protobuf for serialization\n,65,1905,65,7
drt-v2/0009-use-akka-streams.md,"## Context\nThe bulk of the application is a processing pipeline for data feeds. We need a way to process streams of information.\nWe considered apache-spark but we didn't think we  have enough data yet to justify the overhead of a spark installation.\nConsidered Java/ScalaRX, but we're not quite reactive - also due to the processing overhead of the crunch, the backpressure which\nyou can get with Akka Streams seemed like a win\n",Use akka streams\n,94,1906,94,4
drt-v2/0005-use-diode.md,## Context\nhttps://github.com/suzaku-io/diode\nDiode provides a\n,Decision here...\n,19,1908,19,4
drt-v2/0010-use-akka-persistence.md,"## Context\nWe're storing state in actors (A flights actor, a staffAssignments actor), to decrease lag on startup, use akka persistence.\nWe don't have a need (yet) for a database for the live system.\n",Use akka persistence with serialization\n,49,1909,49,6
drt-v2/0002-use-scala.md,"## Context\nUse scala for the server side processes as it's an approved language for use in the Home Office,\n",Use scala\n,24,1911,24,3
drt-v2/0006-physical-deployment-one-jvm-per-port.md,## Context\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\nor structures. Also it\n,Use a jvm per airport\n,33,1912,33,7
drt-v2/0008-bring-advance-passenger-info-code-into-main-service.md,"## Context\nWhen we began the Advance Passenger Info (API) work, we had been going to stand it up as a webservice, so that\nit could provide values to both PHP drt and drtv2. So it was a separate project. With the decision to focus on v2, we don't need that separation\n",Bring the API service codebase into the main project\n,68,1913,68,11
drt-v2/0004-use-scala-reactjs.md,## Context\nhttps://github.com/japgolly/scalajs-react\n,Use reactjs with scalajs wrapping\n,14,1914,14,8
drt-v2/0011-use-play.md,"## Context\nWe need a webserver. Obvious choices between spray (or akka-http) or Play. Given we're doing very little that uses the\nfull power of play, I'd normally have gone with akka-http, but this reactjs tutorial started with Play, so we stuck with it\nfor the spik\n",Play\n,66,1915,66,2
drt-v2/0003-use-scalajs.md,"## Context\nTo reduce the cognitive load on developers, and obviate the need for javascript, we will use a single\nlanguage front and back.\n",Use scalajs on the client-side\n,32,1916,32,8
drt-v2/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1917,16,39
SiebenApp/0003-dsl-for-creation-and-representation-of-goal-trees.md,## Context\nWe have to work with a lot of goal tree examples during SiebenApp testing.\nCurrent API allows only to create goaltree step-by-step.\nIt makes hard to point a border between test setup and test actions.\n,"Create a declarative [DSL][DSL] that allows to define a goal tree that ""exists before test actions"".\nUse it in all unit tests.\n",49,1919,49,33
SiebenApp/0004-document-old-decisions.md,"## Context\nI've started to use ADR in 2017, but declined after a while.\nNevertheless a lot of important decisions were made since that.\nWithout proper explanations they surely will be unclear to the fellow reader.\n",Write down the most significant architecture decisions that were made in the project.\n,48,1921,48,15
SiebenApp/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1922,16,39
green_log/0003-decouple-generation-and-handling.md,"## Context\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\n","De-couple generation of log message/entries from how they are handled.\n* A `logger` object provides an API that can be used to generate log entries.\n* Log ""entries"" are strongly typed structures.\n* Log entry ""handlers"" provide a simple, consistent interface.\n```mermaid\nsequenceDiagram\nApp ->> Logger:       info(""Message"")\nLogger ->> Handler:   <<(entry)\n```\n",35,1924,35,93
green_log/0005-restrict-data-types-allowable-in-log-entries.md,"## Context\nGreenLog allows data to be attached to log entries, either as ""context"", or ""data"" associated with the logged event.\nLog entries may be consumed by a variety of ""handlers"", which\n","To make the job of ""handlers"" easier, GreenLog will restrict the type of value usable as log ""context"" or ""data"" to:\n- `true`/`false`\n- `Numeric`\n- `String`\n- `Time`\n- `Hash` (with `String` or `Symbol` keys)\n",44,1925,44,71
green_log/0006-use-lock-free-io.md,"## Context\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\nBut also, we want logging to perform well.\n","_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https://www.jstorimer.com/blogs/workingwithcode/7982047-is-lock-free-logging-safe) approach. That is, we will:\n- avoid using of mutexes to serialise output\n- perform atomic writes to `IO` streams (using `<<`)\n",37,1927,37,79
green_log/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,1928,16,39
site-stitcher/000-use-adrs.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\narticle: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1929,16,40
tendermint/adr-062-p2p-architecture.md,"## Context\nIn [ADR 061](adr-061-p2p-refactor-scope.md) we decided to refactor the peer-to-peer (P2P) networking stack. The first phase is to redesign and refactor the internal P2P architecture, while retaining protocol compatibility as far as possible.\n","The P2P stack will be redesigned as a message-oriented architecture, primarily relying on Go channels for communication and scheduling. It will use a message-oriented transport to binary messages with individual peers, bidirectional peer-addressable channels to send and receive Protobuf messages, a router to route messages between reactors and peers, and a peer manager to manage peer lifecycle information. Message passing is asynchronous with at-most-once delivery.\n",62,1935,62,84
tendermint/adr-036-empty-blocks-abci.md,"## Context\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\n","> This section explains all of the details of the proposed solution, including implementation details.\n> It should also describe affects / corollary items that may need to be changed as a part of this.\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\n> (e.g. the optimal split of things to do between separate PR's)\n",43,1943,43,85
tendermint/adr-038-non-zero-start-height.md,"## Context\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\n","> This section explains all of the details of the proposed solution, including implementation details.\n> It should also describe affects / corollary items that may need to be changed as a part of this.\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\n> (e.g. the optimal split of things to do between separate PR's)\n",43,1968,43,85
visit-plannr/0006-collapse-to-one-cloudformation-template.md,"## Context\nPreviously it was decided to have two cloudformation templates. One of which would hold ""long lived infrastructure"". This was because adding a cloudfront distribution takes a long time.\nIn practice it isn't possible to only have the cloudfront distribution in the file so the distinction between the two files becomes unclear. And the second template needed some information from the first.\n",To collapse to one template.\n,76,1986,76,7
visit-plannr/0003-cloudfront-distribution-means-there-is-long-lived-infrastruture.md,"## Context\nThe original intention was to be able to build the entire application stack from a single template. However, adding a [cloudfront distribution](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html) takes 15-25 minutes to complete.\n",To have two infrastructure templates. One with slow to provision infrastructure. A second with the fast to provision infrastructure.\n,58,1987,58,23
visit-plannr/0005-read-model-tables.md,"## Context\nQuerying dynamodb requires a hash key.\n> in order to query dynamodb, you need at least to query on your 'hash' key, and you will get ordered results on your range key (if you have range key)\nfrom [StackOverflow](https://stackoverflow.com/a/34463999/222163)\nSo it isn't possible to have a `FooReadModel` table and read the most recent items from it.\n",Instead of a `FooReadModel` table there will be a `ReadModel` table that any readmodel is written too. It will have a `type` as a hash key and a timestamp as a range key.\n,95,1988,95,46
visit-plannr/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,1991,16,39
handbook/0005-error-tracking-and-monitoring.md,"## Context and Problem Statement\nKnow it before they do!\nWe need a tool to discover, triage, and prioritize errors in real-time.\n","Chosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\n",31,2002,31,42
pixel-art-gallery/0003-use-relatively-modern-tech-stack.md,"## Context\nThis application only has two functions:\n- Display past pixel art entries\n- Display details for selected pixel art entry\nBut as a side project this is a space where I can over-engineer things, both to experiment with new technologies and techniques and to either wow or scare off friends, family, colleagues, potential recruiters and anyone else who may stumble across this repo.\n",This project will leverage the following technologies:\n- React.js\n- PostCSS\n- Webpack\n- Jest\n- React Testing Library\n- Cypress\n,78,2003,78,32
pixel-art-gallery/0002-store-pixel-art-in-github-repository.md,## Context\nI've recently gotten into pixel art. I wanted a place to host these so I could share them later if I felt so inclined. But I didn't want to create a new account or use an existing account that had any PII associated with it.\nI also figured I could make another side project out of this.\n,- Pixel art will be stored in a git repository that will be hosted on Github.\n- Link to image and associated details will be stored in a text file so they can be displayed in a UI later.\n,69,2006,69,43
pixel-art-gallery/0006-record-aliases-for-pixel-art-entries.md,"## Context\nCurrently we use an image's filename as a URL slug so that we can link to individual pixel art entries.\nOccasionally we may want to change these filenames as there may be a typo in the name, or we may decide that we want to change our naming convention. As these URLs may have been bookmarked or shared elsewhere we still want the old URLs to remain valid.\n",Pixel art entries may define multiple aliases. These can be thought of as previous filenames (sans file extension).\nWhen we generate pages for each pixel art entry we will also generate a page for each alias listed which will include the same link preview but redirect to the canonical URL instead.\n,80,2008,80,57
pixel-art-gallery/0001-record-architecture-decisions.md,## Context\nThis is a new side project. It would be useful to capture the context behind past decisions.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",23,2009,23,39
uniprot-website/0005-testing.md,## Context\nTesting is necessary to ensure code quality:\n- Unit tests\n- Integration tests\n- Visual regression tests\n,"Use Jest+Enzyme and snapshot testing.\nJest is used as the main testing framework. Snapshot testing, which is a feature of Jest, is also used and helps prevent visual regression.\nEnzyme is used as a utility of Jest to test React components.\nThe Redux layer is also tested, which provides a certain amount of integration tests.\n",25,2010,25,73
uniprot-website/0002-frontend-code.md,"## Context\nThe front end needs to provide a rich user experience, be modular and performant with large community support.\n","React.js is used as the frontend framework for building the new website. React.js is used with TypeScript, which is a superset of JavaScript.\n",25,2011,25,30
uniprot-website/0003-design-system.md,"## Context\nIn order to provide a consistent UI, we need to use a design system. This design system also helps reduce the amount of code required, and ensures designers and developers speak the ""same language"".\n","We have created our own design system/pattern librarly, [Franklin](https://ebi-uniprot.github.io/franklin-sites). It is built on top [Foundation](https://foundation.zurb.com/) (Atomic level components) and uses React.js. The library is published to `npm` as [`franklin-sites`](https://www.npmjs.com/package/franklin-sites) and can be used by any React.js website.\n",44,2012,44,98
uniprot-website/0006-coding-style.md,"## Context\nAs multiple developers are working on the same codebase, it is important for them to share the same coding style, to improve readability.\n",Using `tslint` with Airbnb rules.\n,31,2013,31,10
uniprot-website/0004-state-management.md,"## Context\nThe state of data and UI will grow in the application as the development goes on. There needs to be good state management mechanism to handle this, particularly when components will require interaction across the component hierarchy.\n",[Redux](https://redux.js.org/) will be used for state management. It is a centralised state management container that can handle data and UI state.\n,44,2014,44,33
uniprot-website/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2015,16,39
helix-authentication-service/0003-use-passport-library.md,"## Context\nThis application is largely about supporting authentication protocols. To achieve a quick time to market, use of a third party library is desirable. Initially the application will support the OpenID Connect and SAML 2.0 authentication protocols, but it is quite possible that other protocols may be desired in the future.\n","A (JavaScript) library that offers a plethora of authentication protocol implementations is named passport.js, licensed under a permissive license. While this library does not implement the particular protocols itself, it facilitates and integrates with Node.js in a way that makes it very easy for developers to use. There are currently over 400 Passport modules available, including OIDC and SAML.\nAs such, this application will leverage the Passport library and the OIDC and SAML modules.\n",64,2018,64,94
adr-playground/0002-implement-as-unix-shell-scripts.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,2025,21,13
adr-playground/0003-remove-unix-scripts.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,2026,21,13
adr-playground/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2027,16,39
fare-django/0001-dockerized-architecture.md,## Context\nThe first draft release has to be highly configurable with few efforts.\n,All the deployment phases are done using docker (compose). Local and prod yml\nfiles govern such a deployment.\n,17,2028,17,24
super-eks/0002-use-cdk-for-implementation.md,## Context\nBuilding the installer requires choosing an appropriate infrastructure as code tool that allows configuring AWS EKS.\n,"We can either use vanilla CloudFormation, Terraform or CDK.\nWe choose CDK. CDK seems like having the most traction right now, it's easy to develop with.\n",22,2029,22,39
super-eks/0003-use-yarn-workspaces.md,"## Context\nWe need to organize our code. We also want to run integration tests, and code examples that use the code as a 3rd party library.\n",We want a monorepo style setup and settle for yarn workspaces. Lerna seems to complicated at the moment.\n,34,2030,34,25
super-eks/0005-use-namespaces-grouped-by-functionality.md,"## Context\nWhen installing manifests or Helm charts you need to pick a namespace.\nOften the choice is made in a drive by fashion, leading to inconsistent configuration.\n",We want to install all the shipped components into namespaces grouped by functionality.\nFor example `external-dns` goes into the namespace `dns`.\n`fluent-bit` goes into the namespace `logging`.\n,34,2031,34,45
super-eks/0006-use-projen.md,## Context\nUsing Yarn Workspaces as decided in [ADR-0003](./0003-use-yarn-workspaces.md) turned out to be impractical and produced too many problems.\n,We're going to switch to [projen](https://github.com/projen/projen).\nRelated to that we're going to switch to npm 7 and drop yarn.\n,40,2032,40,37
super-eks/0004-use-in-tree-ebs-storage-driver.md,## Context\nWe need to decide which storage driver(s) we want to provision by default.\n,"For now we're going with the intree EBS provisioner. If the need arises we're going to add the EBS or EFS CSI Driver, but we're speculating that they are going to be available as managed addons at some point.\n",20,2033,20,52
super-eks/0007-backup.md,## Context and Problem Statement\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\n,"### Positive Consequences\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\ncatastropic failure.\n### Negative Consequences\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\nIaC and manually set up resources (e.g., secrets).\n",41,2034,41,78
super-eks/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2035,16,39
framework/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2038,16,39
ReportMI-service-manual/0007-use-json-api-standard-for-api-endpoints.md,"## Context\n[ADR-0002][adr-0002] mentions the need to develop the API using well defined\nJSON APIs.\nIt is good practice to adhere to a recognised open standard.\nIf there are well-maintained software libraries for serving APIs in a common\nformat, as well as the software which consumes them, then development time is\nalso reduced.\n",We will use the [JSON API][JSON-API] when defining our API endpoints.\n,78,2040,78,18
ReportMI-service-manual/0020-name-the-service-and-domain.md,"## Context\nUntil recently, we have been referring to this service by a temporary name - the\nData Submission Service - to distinguish it from MISO.\nFollowing user research and the GDS [guidance][gds-guidance] on naming services,\nwe have now chosen the final name for the service.\nIn line with [ADR-0009][adr-0009], we also need to pick an appropriate domain\nname to host the service.\n",The service will be called: **Report Management Information.**\nThe service will use the domain name: **reportmi.crowncommercial.gov.uk**.\n,95,2044,95,32
ReportMI-service-manual/0009-user-interface-look-and-feel.md,"## Context\nWe are building a new digital service for CCS.\n[CCS-ADR-0002][ccs-adr-0002] says that all new services should have a common\nuser experience, based on the GOV.UK Design System.\nThese patterns should be amended to use the CCS colours and brand, but should\nremain consistent with GOV.UK where possible.\n","We will follow this CCS Architectural Decision and use the GOV.UK Design System\nand GOV.UK Frontend, amended to use CCS colours and brand.\nThe service will be hosted on a subdomain of `crowncommercial.gov.uk`.\n",82,2047,82,53
ReportMI-service-manual/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2059,16,39
nodejs-clean-architecture-app/0001-add-service-locator.md,"## Context\nDomain centric architectures implementations are based on ""Object Inheritance"" and ""Dependency Injection"" concepts, mainly because of the port & adapters pattern and the dependency flow rules (ouside-in only).\nSuch mechanisms do not exist by default in JavaScript. But they can be implemented.\n",One solution is to use the [Service Locator pattern](https://en.wikipedia.org/wiki/Service_locator_pattern) (a.k.a. Service Resolver pattern).\n,60,2069,60,32
ng-kaart/0002-locatie-adrs.md,## Context\nGeoloket 2 en ng-kaart worden in tandem ontwikkeld.\n,"ADRs voor ng-kaart worden genoteerd in het Geoloket 2 project. De context moeten duidelijk maken of de ADR eerder op Geoloket 2, ng-kaart of beide van toepassing is.\n",23,2075,23,55
ng-kaart/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nADR tools: https://github.com/npryce/adr-tools\n",16,2077,16,54
vscode-slides/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2079,16,39
ToPS/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md,## Context and Problem Statement\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\nrunning at the executing system.\n,Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\n,38,2080,38,55
portfolio/0005-add-style-guides.md,"## Context\nWriting code can be messy, and there's a lot of trade-offs to consider. Especially on a project like this where it's a testing ground to try new techniques.\nFurthermore, I want to make sure that I keep a consistent tone in my writing as I document my learning journey. This will help with remembering things from guiding principles to helpful writing tips.\n","There will be a new folder of `docs/guidelines` that captures guiding principles. These guides should be living documents, not set in stone.\n",76,2081,76,31
portfolio/0010-remove-custom-json-serialization.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,2083,21,13
portfolio/0012-remove-custom-json-serialization.md,"## Context\nTurns out, custom functionality on top of JSON serialization wasn't the greatest.\n- 🧱 this added some weight to the FE bundle. Not a ton, but shaving every byte counts for something!\n- 🤯 complexity. Right now the goal is decreasing the complexity and number of ""custom"" stuff in the stack powering my blog.\n",Remove `superjson` and use transform `Date` objects into strings\n,75,2089,75,15
portfolio/0011-use-preact.md,"## Context\nSince this app doesn't rely heavily on React libraries or things like suspense, switching to [`preact`](https://preactjs.com/) offers a significantly smaller bundle size (~70kb => ~40kb).\nDue to preact's compatability package very little should change in day-to-day development.\n",Use `preact` instead of `react`.\n,66,2090,66,12
claim-additional-payments-for-teaching/0006-use-dependabot-for-dependent-library-vulnerability-checking.md,## Context\nWe want to ensure we are made aware of vulnerabilities that are discovered in\nthird-party open source libraries the application uses and can quickly update\ndependent libraries to secure versions so that we can be confident we are not\nopen to known security threats.\n,"We will run [Dependabot](https://dependabot.com), a third-party service that\nautomatically checks for known vulnerabilities and automatically creates Pull\nRequests against the codebase to update vulnerable dependencies.\n",55,2092,55,44
claim-additional-payments-for-teaching/0004-deployment-on-heroku.md,"## Context\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\nthey would like digital services to use. Access to Azure is heavily restricted\nfor production, and slightly restricted for lower environments.\nWe need to be able to work quickly, particularly in the early stages of this\nproject.\nWe need to be able to deploy prototypes and experimental features and versions\nof the service for user research.\n",We will use Heroku to deploy the application.\nWe will use Heroku's pipeline feature to run CI and deploy the application.\n,84,2093,84,28
claim-additional-payments-for-teaching/0002-use-rspec-for-testing.md,"## Context\nWe need to test the code we're writing, the team has previous experience working\nwith these frameworks.\n",We will use RSpec and Capybara for testing our code\n,25,2094,25,14
claim-additional-payments-for-teaching/0008-use-govuk-verify-for-claiment-identity-assurance.md,## Context\nWe need to be confident that a claimant is who they say they are to ensure\npayments are made to eligible claimants only and to minimise fraudulent claims.\n,"We will integrate with the [GOVUK Verify](https://www.verify.service.gov.uk/)\nservice, a ‘secure way to prove who you are online’.\nGOVUK Verify recommend any services that ‘gives users money or benefits’ use\n‘Level of Assurance 2’ or ‘LOA2’, we will follow this guidance.\n[Understand Level of Assurance](https://www.verify.service.gov.uk/understand-levels-of-assurance/)\n",36,2095,36,99
claim-additional-payments-for-teaching/0007-use-dfe-sign-in-for-admin-authentication.md,## Context\nWe want to authenticate DfE staff members so they can access an admin area of\nthe service for checking and processing claims.\n,We will use DfE's single sign-on service\n[DfE Sign In](https://services.signin.education.gov.uk/) and authenticate\nthrough OpenID Connect.\n,30,2096,30,38
claim-additional-payments-for-teaching/0009-capture-teacher-reference-number.md,"## Context\nA claimant’s eligibility is, in part, determined by their qualifications. We\nwant to be able to validate that a claimant’s qualifications match those of the\neligibility criteria.\n","To aid DfE in the process of validating a claimant’s qualifications, we will\ncollect the claimant’s ‘Teacher Reference Number’ or TRN.\nWith the TRN, DfE can use the Database of Qualified Teachers\n([DQT](https://teacherservices.education.gov.uk/SelfService/Login)) to validate\na claimant’s qualifications.\n",42,2098,42,79
claim-additional-payments-for-teaching/0003-use-rubocop-for-linting.md,"## Context\nWe need to lint our Ruby code, the team has previous experience working with\nRuboCop.\n",We will use RuboCop for linting our Ruby code\n,24,2101,24,13
claim-additional-payments-for-teaching/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis article:\n[http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,2102,16,65
claim-additional-payments-for-teaching/0005-use-brakeman-for-vulnerability-checking-on-ci.md,"## Context\nWe want to ensure we're creating a secure application from the start, by adding\nsome vulnerability checking to our CI we can have confidence that we aren't\nintroducing known insecure code.\n",We will run the Brakeman vulnerability check as part of our CI pipeline.\n,42,2103,42,17
capi-k8s-release/0002-use-spec-for-bdd.md,## Context\nWe wanted a test framework to provide some scaffolding for testing new k8s code.\nSome folks on the team had explained Ginkgo's problems with test pollution and\nconcurrency to many-a-pair. @christarazi wanted to use tools with broader\nacceptance in the k8s/go community. Testify vs Gomega also came into play.\n,"We will try Stephen Levine's [""spec"" library](https://github.com/sclevine/spec)\nWe will try [testify](https://github.com/stretchr/testify) while we're at it.\n",79,2106,79,42
capi-k8s-release/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2108,16,39
eq-author-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2114,16,39
james/0033-use-scala-in-event-sourcing-modules.md,"## Context\nAt the time being James use the scala programming language in some parts of its code base, particularily for implementing the Distributed Task Manager,\nwhich uses the event sourcing modules.\nThe module `event-store-memory` already uses Scala.\n","What is proposed here, is to convert in Scala the event sourcing modules.\nThe modules concerned by this change are:\n-  `event-sourcing-core`\n-  `event-sourcing-pojo`\n-  `event-store-api`\n-  `event-store-cassandra`\n",50,2116,50,60
james/0011-remove-elasticsearch-document-source.md,"## Context\nThough very handy to have around, the source field does incur storage overhead within the index.\n",Disable `_source` for ElasticSearch indexed documents.\n,22,2117,22,11
james/0005-distributed-task-termination-ackowledgement.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\n","* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\n",54,2121,54,54
james/0012-jmap-partial-reads.md,"## Context\nJMAP core RFC8620 requires that the server responds only properties requested by the client.\nJames currently computes all of the properties regardless of their cost, and if it had been asked by the client.\nClearly we can save some latencies and resources by avoiding reading/computing expensive properties that had not been explicitly requested by the client.\n","Introduce two new datastructures representing JMAP messages:\n- One with only metadata\n- One with metadata + headers\nGiven the properties requested by the client, the most appropriate message datastructure will be computed, on top of\nexisting message storage APIs that should remain unchanged.\nSome performance tests will be run in order to evaluate the improvements.\n",73,2132,73,71
james/0009-disable-elasticsearch-dynamic-mapping.md,"## Context\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\nThis causes a serie of functional issues:\n- Maximum field count can easily be exceeded\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\n",Rely on nested objects to represent mail headers within a mapping\n,96,2135,96,13
james/0009-java-11-migration.md,"## Context\nJava 11 is the only ""Long Term Support"" java release right now so more and more people will use it exclusively.\nJames is known to build with Java Compiler 11 for some weeks.\n",We adopt Java Runtime Environment 11 for James as a runtime to benefits from a supported runtime and new features\nof the languages and the platform.\n,44,2136,44,30
james/0004-distributed-tasks-listing.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\n",* Read a Cassandra projection to get all `Task`s and their `Status`\n,32,2140,32,17
james/0008-distributed-task-await.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to `await` a `Task` running on any node of the cluster.\n",* Broadcast `Event`s in `RabbitMQ`\n,35,2148,35,12
james/0007-distributed-task-cancellation.md,## Context\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\n,* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\n,32,2158,32,65
udagram/ADR-1-Kubernetes.md,## Context\nThe API endpoints needs to be managed as a unit.\nTo allow for operational scaling the containerised application will be built to be able to run in Kubernetes.\n,To help package the individual AWS resource into a serverless application we will use [Kubernetes](https://kubernetes.io) framework.\nWe also considered the following alternative solutions:\n* Manual creation of resource from multiple terminal sessions\n* Docker Compose\n,36,2163,36,52
stentor/0002-use-toml-for-config-file.md,"## Context\n`stentor` needs to pick a config file format.\nThe options under consideration are yaml and toml.\nBoth options are human readable and writable,\nallow for easy parsing of structured data,\nand are supported by well maintained libraries.\nyaml has the benefit of being more straightforward to write,\nespecially for nested structures.\nHowever, toml is intended for config files,\nand provides stricter parsing out of the box.\n",`stentor` will use toml for its config file.\n,93,2164,93,15
stentor/0003-all-cli-options-also-respect-environment-variables.md,"## Context\nOne of the goals of `stentor` is to be integrated into a CI system.\nThis requires flexibility in how users set command-line options.\nIn particular, in some CI systems it can be easier to change environment variables than it is to change command-line flags.\n",All `stentor` command-line options can be set via a corresponding enviroment variable.\n,60,2165,60,21
stentor/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2166,16,39
easi-app/0002-use-terraform.md,## Decision Drivers\n* Is a standard tool that has a community to support it.\n* General experience with the tool.\n* Must work on AWS services.\n,* Is a standard tool that has a community to support it.\n* General experience with the tool.\n* Must work on AWS services.\nTerraform\n,34,2169,34,34
blackspots-frontend/0001-create-react-app-as-template.md,## Context\nUsing a boilerplate when starting a new project saves you a lot of time writing boilerplate code.\n,"At Datapunt we use our own [React Boilerplate](https://github.com/Amsterdam/react-boilerplate-amsterdam) as template. However, this project is based on [Create-React-App](https://github.com/facebook/create-react-app). At the time of creating, there was not yet a department-wide decision on which boilerplate to use. At the time Create-React-App was a valid choice.\n",24,2172,24,88
social-care-architecture/0002-use-iso-8601-format-for-dates.md,## Context\nThe system is composed of a number of API and related datastores. There are currently a number of differing date formats in use.\n,We will use the ISO 8601 format for dates: yyyy-mm-dd and times.\n,30,2173,30,19
social-care-architecture/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2175,16,39
opg-data/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2190,16,39
macondo/0005-remove-unnecessary-commands.md,"## Context\nNow that the repo subcommand is able to add, remove, update and list\nrepositories... maybe the barebones `list` command should be removed in favor of\n`repo list`. That may make things simpler.\nAlso the `--with-manifest` command is less useful now that the repository\nmanagement is easier.\n",- Remove the explicit `list` command\n- Remove the `--with-manifest` argument\n- Simplify listing the commands when none is provided (removing versions)\n- List commands when listing repositories\n,70,2194,70,43
macondo/0006-allow-printing-the-contents-of-the-repositories.md,"## Context\nNow that the repositories are behind a cache (http ones), it might be handy to\nsee their contents as YAML wihtout having to fetch its content manually\n",Not worth it. It's easy enough to inspect a repository.\n,36,2195,36,14
macondo/0004-allow-easily-publishing-of-commands-and-repositories.md,"## Context\nNow that the [repository story is a bit\nclearer](0003-remote-repositories-management.md), `macondo build` command should\nbe enhanced to also generating repository YAML files.\n",Add a new flag to the `macondo build` command that also generates a YAML file\nwith all the commands generated.\n,45,2196,45,27
macondo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2197,16,39
connecting-to-services/0015-add-info-page.md,"## Context\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n",The application will provide an info page.\n,55,2198,55,9
connecting-to-services/0013-remove-google-sitelinks-searchbox-metadata.md,## Context\nMetadata had previously been added to the site with the intention of having a search box appear within the results on Google to allow people to perform a search on our site directly from Google. This never ended up working and no search box has been placed by Google within the search results.\n,The decision is to remove the code from within the application as it is effectively redundant.\n,59,2199,59,18
connecting-to-services/0006-use-prometheus-for-exposing-metrics.md,## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n,We will use Prometheus to monitor and alert on the state of the application.\n,86,2200,86,16
connecting-to-services/0016-add-service-search-to-site-root.md,## Context\nSitting directly on the `/find-a-pharmacy` path is not in keeping with the organisations plans for\ninformation architecture going forward. When the service moves from beta.nhs.uk to nhs.uk we want\nit to sit on path consistent with other finder services.\n,The site root will be changed to be `/service-search/find-a-pharmacy`\n,58,2202,58,17
connecting-to-services/0011-add-google-sitelinks-searchbox-metadata.md,## Context\nMetadata can be added to a web site's home page to enable a search box directly within Google.\n,Adding the required metadata is straight forward as the sites' search URL is the correct format for the Sitelinks Searchbox.\nAn additional term will be added to the query string to allow analytic metrics to distinguish Google searches from searches initiated from the main site.\n,24,2206,24,53
connecting-to-services/0005-add-cache-control-headers.md,"## Context\nCache control headers can be used to prevent a client's browser from\nre-requesting a page that has not changed, and may be leveraged by proxies to\nreturn the same cached pages for multiple clients.\n",Cache-control headers will be added to all valid requests. 500 and 404 errors\nwill not be cached.\n,46,2208,46,24
connecting-to-services/0017-update-links-to-pims-profiles.md,## Context\nWe have removed the services link from the results page. We can now navigate to the PIMS profile Overview\npage via a link on the Organisation name in the search results page.\n,Update the results page to use Org name as link to PIMS overview page\nRemove the services link from the search results page\n,40,2209,40,26
connecting-to-services/0007-remove-cache-control-headers.md,## Context\nSearch results are being cached for 30 minutes giving stale opening time information\n,The caching will be removed from the site\n,18,2210,18,9
connecting-to-services/0010-use-postcodes-io-for-reverse-geocoding.md,## Context\nThe coordinate provided by the browser needs to be reverse geocoded in order to\nidentify the country of origin.\n,The decision is to use the same lookup service as is already employed to lookup\npostcodes and places. That service is postcodes.io.\n,27,2211,27,29
connecting-to-services/0014-use-azure-search.md,## Context\nAzure Search is the organisation/programme's strategic vision for serving both\norganisation and service data. This removes the need to maintain an instance of\nElasticsearch and related infrastructure.\n,The application will use Azure Search API to provide pharmacy data.\n,41,2212,41,13
connecting-to-services/0009-search-by-place.md,## Context\nThere are a lot of automated checks performed against the application. These\nchecks are being counted as part of the normal traffic and there is no way to\ncurrently filter them out. There is a need to remove the bot traffic so real\nuser behaviour can be identified.\n,The decision is to count any request that includes a query string parameter `check`.\n,60,2213,60,18
connecting-to-services/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,2214,16,43
fare-platform/0002-permission-crud-operations-on-records-config.md,"## Context\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\n","To avoid the situation where a record is created without file, we put `deny_all` permission in fare/records/config.py that denies to all the possibilities to create, update and delete this kind of records.\n",30,2215,30,43
fare-platform/0004-log-messages.md,"## Context\nBecause of we want to know what is happening in some circumstances, such as granting roles, we introduced message log.\n",In this way every time a successful or unsuccessful operation is done it is recorded.\n,27,2216,27,17
fare-platform/0003-delete-of-a-record-database-tables.md,## Context\nCompletely delete a file is not an easy thing to do in invenio. To achieve this goal we tried to delete as many info as possible.\n,"Delete a record means delete it from the disk, from the database tables and not indexing it anymore. We decided to delete it from all the tables except `record_metadata_version`\n",35,2217,35,36
fare-platform/0005-deploy-strategy.md,"## Context\nThe staging and deployment strategy has to change. Right now many different\nservices are loaded, there is confusion regarding what to use and when. We need\na simpler way of handling such deployment.\n","Change the deployment. We want to have only the services in use exposed.\nMoreover, we should automatize the overall process.\n",43,2218,43,27
fare-platform/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2219,16,39
decodeweb/0002-use-vue-for-front-end.md,"## Context\nWe need to build a UI that allows users to ""login"" by entering a PIN - the UI\nshould allow access to pages if the user logs in. In addition we want to run\nZenroom code in the browser using the Zenroom WASM build. To build this\nmoderately complex UX we could use vanilla HTML with sprinklings of jQuery or\nsomething more structured.\n",To use Vue (https://vuejs.org) to build the front end UI as a SPA that sends\nmessages to Phoenix backend via sockets in order to make use of the required\nDECODE services.\n,82,2220,82,42
decodeweb/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2223,16,39
terraform-aws-msk-module/0004-configurable-internal-vpc.md,"## Context\nThis module has an internal VPC module which has default CIDR and Subnet settings. This enables users of the module to focus on the MSK cluster. It also means that if these default settings are used by another VPC in the users AWS environment the module will not work. We expect people may want to use this module, without having to create a separate VPC, but without using the default settings\n",To enable non-default settings to be used the internal VPC CIDR and Subnet settings are exposed for configuration. This provides a simple way to create an additional VPC for the MSK cluster without using an external VPC module.\n,86,2227,86,48
terraform-aws-msk-module/0008-key-pair-utility.md,## Context\nWhen creating a Client Instance for test use in the MSK Cluster a Key Pair is\nneeded for the EC2 instance to be accessed.\n,We have decided to provide a KeyPair utility that will enable users to import an\nexisting key into AWS for use in this scenario. We will not provide a key\ngeneration mechanism.\n,32,2228,32,39
terraform-aws-msk-module/0005-feature-flags.md,## Context\nTo make this module user-friendly we need to support a number of identified use cases\n1. MSK Cluster / Default Internal VPC\n2. MSK Cluster / Configured Internal VPC\n3. MSK Cluster / External VPC\n4. MSK Cluster using Custom Configuration\n5. MSK Cluster using Client Authentication\nWe should make it simple for users to create an MSK cluster in each of these configurations.\n,To enable this the module will utilise feature toggles. The proposed feature toggles are\n1. Create VPC\n2. Use Custom Configuration\n3. Use Client Authentication\nThe configured internal VPC will utilise the Create VPC feature flag with additional configuration settings.\n,91,2229,91,55
terraform-aws-msk-module/0003-internal-vpc.md,## Context\nManaged Streaming for Apache Kafka (MSK) cluster in AWS requires a VPC to run in. This means that before provisioning an MSK cluster a VPC needs to be provisioned in AWS. The VPC should be set up to deal with High Availability (HA) and fault tolerance\n,To make it simpler to use this module we have included an internal VPC module. This module will be used by default when creating an MSK cluster. This removes the need for users to have to provide their own VPC before using this module.\n,62,2230,62,51
terraform-aws-msk-module/0010-use-session-manager-for-shell-access-to-instances.md,## Context\nProviding shell access to a node in AWS is time consuming to set up in a secure manner.\n,Use Session Manager to establish shell connections to the client instance.\n,24,2232,24,13
terraform-aws-msk-module/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2233,16,39
cf-k8s-networking/0013-rename-master-branch.md,## Context\nA [proposal](https://lists.cloudfoundry.org/g/cf-dev/topic/75070528#9059) was\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\nthe CF community a more welcoming and inclusive environment.\n,We are going to rename our `master` branch to `develop`. `develop` was chosen\ninstead of `main` by team consensus because it better describes the use of the\nbranch.\n,63,2251,63,40
cf-k8s-networking/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2252,16,39
princeton_ansible/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described at https://adr.github.io/\n",16,2253,16,17
princeton_ansible/0003-cicognara-letsencrypt-path.md,"## Context\nThere are instances where we will be unable to use our InCommon Federation SSL certificate framework for certs. In instances where a dot org, etc., In those instances Let's Encrypt is an adequate solution.\nThe Lets Encrypt expects the SSL certs to be under `/etc/letsencrypt/` which differs from where we place the rest of our SSL certificated\n",make sure certificates are under `/etc/letsencrypt/`\ncreate a directory under `/var/www/letsencrypt/<servicename>` This directory which is configured as the `http://tld.domain/.well_known` will be empty but is needed during the Let's Encrypt verification step.\n,75,2254,75,60
princeton_ansible/0002-group-vars.md,"## Context\nThere are a lot of different ways to set variables in Ansible and we should have\na consistent way to do it in all playbooks for each group.\nWe want to make sure that there's a smaller vault.yml file per group, instead of\na gigantic vault.yml file for everything. This is because one huge vault.yml\nfile causes merge conflicts in multiple Github Pull Requests.\n",Unique group variables will be placed in `/group_vars/<groupname>`\nEncrypted group variables will be placed in `/group_vars/<groupname>/vault.yml`\nShared group variables will be placed in `/group_vars/all/`\n,82,2255,82,49
princeton_ansible/0004-key-scope,"## Context\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\n","Keys should be minted on a per-feature, per-application basis.\n",61,2256,61,15
sre-adrs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made for our team.\n,"We will use Architecture Decision Records, as discussed in our original proposal: https://docs.google.com/document/d/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU/edit#\n",16,2261,16,60
wordpress-template/0003-theme-namespace-naming-convention.md,"## Context\nWe need to pick a namespace for themes, and renaming can be tedious.\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\n",We'll use the `\Theme` namespace for every theme we make except for child themes which will have the namespace `\ChildTheme`.\n,45,2262,45,28
wordpress-template/0002-use-acf-pro.md,"## Context\nThe themes and plugins we make often need to add fields to the admin interface.\nWe've been using ACF PRO on nearly every new site we've made in the past few years, and we've used the free version of ACF for perhaps 5-7 years. We're comfortable with it and it's proven its stability and usefulness.\n","Install ACF PRO on every site we create. If we don't need it, we can remove it before deployment.\nTo add a field to a theme or plugin we should copy the PHP code and paste it into the codebase.\n",74,2263,74,49
wordpress-template/0007-validate-whippet-files-in-CI.md,"## Context\nWhippet.lock and whippet.json files can sometimes become out of sync, or malformed. For instance, this can happen when attempting to remove a plugin. When this happens, we often are unaware until the deploy fails.\nWhippet now has a `whippet deps validate` command that will check if the two files are properly aligned, and well-formed, that we could use to spot such errors earlier.\n",We'll run `whippet deps validate` in CI via a re-usable workflow.\n,86,2268,86,19
wordpress-template/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2269,16,39
spin-archetype-base/0002-unified-terminology.md,## Context\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\n,"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\n",34,2270,34,42
spin-archetype-base/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2271,16,39
where-away/0004-test-with-jest.md,## Context\nI am familiar with Jest and am consistently happy with it. Also it's pretty\npopular and widely used so there's good support.\n,Use Jest for testing.\n,31,2273,31,6
where-away/0010-use-a-different-html-parser-for-testing.md,"## Context\nIt's useful to parse snippets of the rendered html to assert on them easier\n(e.g. in transform.js unit tests).\nThe xml parser we bundle with the app can technically parse html, but it doesn't\nseem to support basic operations like `innerHTML`. That's a major nuisance.\nJSDom is a mature and popular html parsing library.\n",Add [JSDom](https://www.npmjs.com/package/jsdom) as a dev dependency to parse html in test.\n,76,2276,76,28
where-away/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md,"## Context\n- semantically, an html element that takes you to a new web page when you click\nit is an anchor -- `<a>`.\n- I'd like tab navigation and the Enter key to work for navigating between\nbookmarks.\n- buttons can be accessed via Tab and activated via Enter\n- not so with an anchor tag. See\nhttps://stackoverflow.com/questions/41476292/how-to-give-focus-to-anchor-tag-without-href,\netc.\n",Model the links as `<button>` elements to make the interaction more normal.\n,100,2280,100,16
where-away/0005-minimize-the-amount-of-code-that-must-be-tested-in-the-browser.md,"## Context\nThere are three main things the package must do:\n1. give useful feedback if the input is in an invalid format\n2. transform the input into an HTML file (including embedded CSS & JavaScript)\n3. that HTML file must show the links and respond correctly to user input.\nItems #1 and #2 are very easy to test, item #3 is not.\n",Minimize the amount of code that must be tested in the browser. Do as much as\npossible in the build step and as little as possible in the browser.\n,81,2282,81,34
where-away/0001-record-architecture-decisions.md,## Context\nI'd like to clarify my plans and intentions for this project. I'd like to record\nthem as architectural records.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",28,2283,28,39
texas-holdem-code-challenge/0005-use-a-linter.md,"## Context\nThis code will be reviewed in a job interview setting; the instructions specify\n""production"" level code.\nStylistically consistent code is nicer to read and more fitting with the\n""production"" level code objective.\n",Use a linter.\n,49,2292,49,6
texas-holdem-code-challenge/0002-use-node-js.md,"## Context\n- The problem instructions state ""C++ or node.js are preferred"".\n- I am most familiar with node.js out of those two.\n- I am completing this problem in my spare time and have a limited amount of\ntime to spend on it: it's convenient to work faster if possible.\n",Use Node.js to complete this problem.\n,67,2294,67,9
texas-holdem-code-challenge/0003-use-familiar-tools.md,"## Context\nAs described in [2. Use Node.js](0002-use-node-js.md), the goal is to make\nreasonable choices without taking too much time.\n",Use familiar tools whenever possible.\n,35,2295,35,7
texas-holdem-code-challenge/0001-record-architecture-decisions.md,"## Context\nIn the in-person technical interview to follow, I will need to discuss why I\nmade the decisions I made.\n","I will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",27,2297,27,39
skunkworks/styling.md,## Context\n,CSS-in-JS via [@vanilla-extract/css](https://vanilla-extract.style/).\n,3,2300,3,22
cpd-adr/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2302,16,39
JustRooms/0002-use-clean-architecture.md,## Context\nWe need to decide how to structure our application so as to lower coupling and improve cohesion.\n,"We intend to use a clean architecture, particularly a hexagonal style\n",22,2304,22,14
JustRooms/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2306,16,39
lbh-adrs/Record-Versioning.md,## **Context**\nIn order to\n,What is the change that we're proposing and/or doing?\n,10,2312,10,13
lbh-adrs/First-Choice-Microservice-Datastore.md,"## **Context**\nAlthough each microservice should decide the most appropriate datastore, generally speaking, it would be good to choose a “first choice” datastore. There are two options considered in Hackney:\n1. AWS PostgreSQL (relational database)\n2. AWS DynamoDB  (NoSQL/Document database)\n",**AWS DynamoDB**\nDynamoDB is the fully managed NoSQL db in AWS. A NoSQL database is the “first choice” of datastore for the following reasons:\n- A microservice is quite small so that the majority of the times we do not need complicated relations models.\n- NoSQL has been designed for horizontal scalability.\n- For most cases APIs don’t need to support a lot of queries on different entity’s properties.\n,67,2315,67,93
lbh-adrs/Feature-Toggling.md,"## **Context**\nIn order to be able to release code safely as part of a CI/CD pipeline, it is necessary to have a feature management capability to externalise the ability to enable features.\nThis can be achieved by including feature toggles in code, and by versioning APIs to account for breaking changes.\nThe options available are:\n- Environment Variables\n- AWS AppConfig\n",**AWS AppConfig**\nWhat is the change that we're proposing and/or doing?\n,80,2319,80,19
lbh-adrs/Observability.md,"## **Context**\nProviding rich logging information will make it easier to investigate issues without making use of intrusive approaches (i.e: debug, memory dump), also making visible the behaviour of services by using monitoring tools to extract and/or query these logs.\nThe idea is to utilize services offered by AWS as they are comprehensive and can operate at scale with minimal administrative overhead.\n",**AWS x-ray**\nAWS X-Ray is an AWS managed service that provides the functionality to debug and analyze distributed applications.\n,76,2320,76,27
akvo-lumen/adr-001-react.md,"## Context\nWe need a framework that serve as base for the front-end development of\nAkvo Lumen. There are several options on the market, some of them are\nJavaScript based others languages that compile to JavaScript\n(transpilers) like ClojureScript.\n","After discussing with the team and taking into account the skill set at\nhand, we have decided that JavaScript and\n[React](https://facebook.github.io/react/) it's a safer approach to\nbuild the UI. It has a large community and we can use some available\ncomponents as the base of our UI.\n",55,2322,55,65
easyvista_toolkit/0001_Using_globalVariable.md,## Context\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\n,We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\n,79,2329,79,49
easyvista_toolkit/0004_Filtering_on_requests.md,"## Context\nWe'd like to be able to pull requests by recipient and/or requester but the */requests* endpoint does not return those values straigh away. Instead they are embedded in JSON format within the fields *REQUESTOR* and *RECIPIENT*.\nThis means that we can't use */search=recipient~""recipientname""* to get the requests linked to a particular user.\n",We will use the *where-object* from powershell to filter on requestor and/or recipient. This will be implemented like so :\n- Using a *maxrows* parameter to force easyvista to return more than the default 100 results when requesting */requests*\n- Using a powershell scritpblock built in the relevants cmdlets to filter the result by requestor and/or recipient.\n,83,2331,83,84
generator-latex-template/0006-use-single-quote-to-enquote.md,"## Context and Problem Statement\nIn a document, some words have to be put in quotes. How to direct latex to enquote a word?\n## Decision Drivers\n* Automatic correct typographical layout\n* Less effort for the user\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\n* Supported by standard LaTeX environments\n","* Automatic correct typographical layout\n* Less effort for the user\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\n* Supported by standard LaTeX environments\nChosen option: ""Use single quote ("") to enquote text"", because resolves all forces.\nWe accept that special hyphenation instructions such as `application""=specific` do not work anymore.\n<!-- markdownlint-disable-file MD013 -->\n",71,2333,71,90
generator-latex-template/0001-use-yeoman.md,## Context and Problem Statement\nWe want to generate the template automatically.\n,"Chosen option: ""Yeoman"", because it seems to be the most easy to use generator.\n",15,2334,15,21
generator-latex-template/0007-use-glossaries-package.md,## Context and Problem Statement\nHow to manage abbreviations\n,"Chosen option: ""glossaries-extra"", because seems to be best.\n",12,2335,12,17
generator-latex-template/0002-use-manuth-TSGeneratorGenerator.md,## Context and Problem Statement\nWhat to use as basis for the generator?\n,"Chosen option: ""vojtechhabarta/typescript-generator"", because\n* actively maintained\n* typescript\n",16,2336,16,24
generator-latex-template/0005-custom-example-environment.md,## Context and Problem Statement\nWe want to present LaTeX examples.\nThey should be shown both as LaTeX source and as rendered output.\n,"* Write once - have two outputs (source and rendered)\n* Good interaction with all pacakges\n* Should support floats\n* Should support compilation without special configuration (such as `--shell-escape`)\nChosen option: ""custom solution"", because resolves all forces.\n### Positive Consequences\n* Works with floats and umlauts\n### Negative Consequences\n* A little bit more than minimal code is presented to the user\n",28,2337,28,93
teacher-training-api/0005-service-entry-points.md,"## Context\nServices help us by wrapping and presenting functionality. Typically services\npresent one piece of functionality, and use a single method to invoke this\nfunctionality. For the sake of consistency, we want to align this ""entry point""\nso that services can be called the same way.\n","Use option 2, `call`. Additionally, provide both the instance method `#call` and a class\nmethod `.call`, of which the later instantiates the service and calls `#call`.\n",60,2349,60,43
teacher-training-api/0007-javascript-linting.md,## Context\nWe need tooling to ensure that our Javascript is in consistent code style and for us to be notified when\nit is not.\n,We decided as a team that option 2 would be the easiest and quickest option to implement. It also follows the ways that GDS work.\nIf we found it too restrictive at a later date we could then switch to option 3 and customise the rules if need be.\n,30,2352,30,58
teacher-training-api/0008-use-skylight-for-performance-monitoring.md,## Context\nWe need to decide between using skylight versus sentry for performance monitoring.\n,"[There was an attempt](https://github.com/DFE-Digital/teacher-training-api/pull/1860) to use sentry, but we managed to fill up our quota in less than a day, and while Sentry does offer tracing a subset of requests, this would only allow us to trace less than 1% of our requests.\nAs such, we have decided to continue using skylight for performance monitoring.\n",20,2354,20,89
teacher-training-api/0003-integration-tests.md,"## Context\nOur deploys to production are currently not very well tested beyond the built-in\nfeature tests, which while providing good feature coverage, do not test how the\nservice works as a whole in the production environment.\n",We've decided on option 2 above.\n,46,2355,46,10
teacher-training-api/0001-record-architecture-decisions.md,"## Context\nWe need to record the architectural decisions made for the manage-courses and\nfind-courses projects. However, this project has multiple repositories, so we\ncould keep a record here, or in it's own repo. I don't think creating another\nrepo is good, if anything we've been considering moving repos together in the\nfuture, so how about we create an ADR here.\n","We will keep the ADR in this repo. Additionally, we'll create ADRs in the other\nrepos that are part of this project with pointers to look in this repo recorded\nin the ADR.\n",84,2356,84,44
agentframework/0012-use-tsmon-instead-of-ts-node.md,## Context\nBecause this project has `path-alias` in tsconfig.json which is a must for bootstrapping. `TS-NODE` not support `path-alias`.\n,Use `tsmon`\n,39,2358,39,6
agentframework/0008-minify-build.md,## Context\nAgentFramework is also use for Browser\n,We need minify build because package.json don't have a property called `minMain`\n,11,2359,11,19
agentframework/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md,"## Context\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\nIt may cause compatibility issue with other frameworks.\n",We will add ES6 Proxy for a transparent layer which not modifying user's class/object.\n,31,2360,31,19
agentframework/0011-re-implement-decorate-metadata-param-to-improve-performance.md,"## Context\n`__decorate`, `__metadata` and `__param` may contain more than half of total generated js in a large enterprise project.\nImprove the performance will benefit large project.\n",Re-Implement the above function with better performance. By remove the comparison inside loop.\n,42,2361,42,18
agentframework/0007-cancel-adr-0002-and-use-codegen-to-generate-interceptor.md,## Context\nUsing Proxy will not as fast as native code. so we use CodeGen to generate a transparent layer on top of user's class to provide interceptor function.\n,Use CodeGen\n,35,2364,35,4
agentframework/0010-use-agent-to-describe-an-instance-of-class.md,## Context\nAt very beginning the Domain API to get an instance of class is called `Domain.getAgent`.\nBut later i was confused and change to `Domain.getInstance`.\nNow I realized this mistake and change back to `getAgent` again.\n,An instance of class should called agent in AOP\n,54,2365,54,11
agentframework/0001-use-rollup-instead-of-webpack-to-generate-release-build.md,"## Context\nFor commonjs use at nodejs, Webpack will introduce some helper functions which we don't want.\n",Use rollup to replace webpack to generate release build\n,25,2367,25,11
agentframework/0005-use-for-instead-of-callbacks-helpers.md,"## Context\n\_.forEach(), Array.map() which is good, but not friendly to debug the code.\n",Use for\n,23,2369,23,3
file-server/ADR.md,"## Context\nWe want to server files via HTTP, without a framework or too much external packages.\n","The functionality of this server is very limited, so this project uses a more functional approach over object orientation.\n",21,2370,21,22
timdex/0002-use-jwt-for-api-authorization.md,## Context\nThe API portion of this application will require authentication.\nJSON Web Token (JWT) is an open standard described by [RFC 7519]( https://tools.ietf.org/html/rfc7519).\n[Additional Information](https://en.wikipedia.org/wiki/JSON_Web_Token).\n,We will use JWT for authentication.\n,62,2375,62,8
timdex/0003-follow-twelve-factor-methodology.md,## Context\nDesigning modern scalable cloud based applications requires intentionally\ndesigning the architecture to take advantage of the cloud.\nOne leading way to do that is\n[The Twelve Factor](https://12factor.net) methodology.\n,We will follow Twelve Factor methodology.\n,48,2376,48,8
timdex/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2377,16,39
beis-report-official-development-assistance/0024-use-scripts-to-rule-them-all.md,## Context\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\nAll[1].\nThis repository should include reference and document this decision.\n[1]\nhttps://github.com/dxw/tech-team-rfcs/blob/main/rfc-023-use-scripts-to-rule-them-all.md\n,We will follow the Scripts to Rule Them All pattern for common tasks in this project.\n,67,2379,67,18
beis-report-official-development-assistance/0021-use-lograge-gem.md,"## Context\nRODA uses Papertrail for logging. Our Papertrail account has a log data\ntransfer limit of 50 MB. Extending this limit means moving to another tier\non Papertrail's platform, which we would prefer to avoid.\n","- Reduce the default logging level on production to `info` instead of `debug`\n- Use the [lograge gem](https://github.com/roidrage/lograge) to turn Rails'\ndefault multiline logs into a single line, without losing information or\ncontext\n",51,2380,51,56
beis-report-official-development-assistance/0014-use-coveralls-for-monitoring-test-coverage.md,## Context\nWe have started to miss test coverage for a few methods. We noticed this problem where we had delivered a feature with a feature test but had forgotten to add enough unit coverage. When the feature test later changed coverage was lost.\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\n,Use the free tier of Coveralls to give us statistics and to give our pull requests feedback.\n,72,2381,72,20
beis-report-official-development-assistance/0003-use-standard-rb.md,"## Context\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\n,36,2382,36,35
beis-report-official-development-assistance/0002-use-bullet-to-catch-nplus1-queries.md,## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n,Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n,37,2383,37,23
beis-report-official-development-assistance/0032-remove-unused-skylight-performance-monitoring-app.md,"## Context\nWe've had the Skylight profiler app installed since May 2020. However, it's not\nbeen used for a long time and in fact has been misconfigured for as long as any\none can remember. Should we invest further, at this point, in a service which is\nnot being used, or should we remove it for now?\n","We opt to remove the Skylight service, for the following reasons:\n- using the git history we can readily re-instate it\n- it's not currently used and is an overhead to maintain\n- it's currently misconfigured\n- we are currently moving out of our development phase into an\noperational/support phase and are transferring ownership of all third party\nservices to BEIS\n",75,2385,75,79
beis-report-official-development-assistance/0011-use-govuk-design-system-form-builder.md,"## Context\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\nmanually declaring the correct HTML structure, class names and ARIA attributes,\nwhich is time-consuming and hard to get right.\nAdditionally, our validation errors currently use Rails' default pattern, rather\nthan the one recommended for use as part of the design system, which is designed\nwith accessibility in mind.\n","We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\nGOV.UK Design System-compliant forms.\nAs we are currently using Simple Form rather than Rails' default form builder\nfor our other forms, the two form builders can co-exist for the time being,\nwhilst we transition the forms over.\n",83,2386,83,75
beis-report-official-development-assistance/0010-decision-to-not-use-docker-in-dev.md,"## Context\ndxw do not set a preferred default for working in development. dxw do set a default stance of using docker in all live environments and CI.\nDocker in development can be slow and it can require some experience to resolve situations where things go wrong, like a full disk space.\n","Use non-docker tools and packages to run the application in development. This includes web servers, rails consoles, tests, local dependencies eg, Sidekiq, NPM, etc.\n",62,2388,62,38
beis-report-official-development-assistance/0026-use-omniauth-to-standardise-authentication.md,"## Context\nThe application needs a way to authenticate users, we use a third party service\nfor this (Auth0), see ADR 0006 [1]. As authentication to third party providers is a\nwell understood problem, it makes sense to leverage an existing gem to handle\nthis for us.\n","Use the omniauth gem to standardise authentication to third party providers,\nthis also supports changing provider if deemed necessary.\n",63,2389,63,26
beis-report-official-development-assistance/0023-use-docker-hub-in-deployments.md,"## Context\nOur CI/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\nPaaS), we need a way to store built images from our pipeline so our hosting\nplatform can access and deploy them.\nDocker hub is one solution offered by the maker of Docker itself.\n",Use [Docker hub](https://hub.docker.com/) to store built deployment container\nimages to facilitate continuous delivery.\nHost the built container images on the dxw Docker hub account.\n,65,2392,65,39
beis-report-official-development-assistance/0005-use-travis-for-ci.md,"## Context\nTravis is actively used for many other projects within the BEIS GitHub account.\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\n",Use Travis to build and test the application.\nUse Travis to automatically deploy the applications to live.\n,45,2393,45,21
beis-report-official-development-assistance/0009-use-pundit-to-manage-permissions.md,"## Context\nThe service will be used by a variety of different types of users, and we need\nto ensure that those users are only able to access the parts of the service that\nwe want them to.\nOur current chosen authentication provider, Auth0, has support for assigning\nroles to users, but this couples the service tightly to their service, so we\nshould avoid this.\n",We will use the 'Pundit' gem to manage users' access to the service. The\npermissions will be grouped into roles that can then be assigned to users\nrequiring a particular level of access.\n,81,2397,81,44
beis-report-official-development-assistance/0013-travis-only-tests-with-docker-containers.md,"## Context\nWe noticed that deployments to staging were silently failing since a gem update that succeeded in Travis did not succeed in our docker environment, where docker is used on our live environment.\ndxw have a default stance to test with containers where we host with containers: https://github.com/dxw/tech-team-rfcs/blob/main/rfc-013-use-docker-to-deploy-and-run-applications-in-containers.md\n",Travis uses Docker containers to test the application on every build and deploy.\n,86,2398,86,16
beis-report-official-development-assistance/0012-use-wicked-for-multi-step-forms.md,"## Context\nThe create activity form is large and complex, with arguable too much for a\nsingle page form. Therefore, we need to break this up into more manageable\nchunks.\nAdditionally, the GOV.UK service manual recommends starting with a [one thing per\npage](https://www.gov.uk/service-manual/design/form-structure#start-with-one-thing-per-page)\napproach.\n","We will use the ""Wicked"" gem to build a multi-step form for activity.\n",85,2399,85,19
beis-report-official-development-assistance/0001-use-pull-request-templates.md,"## Context\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\n",Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n,89,2400,89,22
beis-report-official-development-assistance/0028-add-cdn-route.md,"## Context\nThe apps need to be hosted on a custom domain, behind CloudFront\n","We will create a cdn-route on GPaas, which will create a CloudFront endpoint with the custom domain\nThis will also generate the TLS certificates\n",18,2406,18,32
beis-report-official-development-assistance/0016-use-brakeman-as-a-static-code-analyser.md,## Context\nWe need to be confident that our application is secure and remains so during development and the life time of the service.\n,Install the Brakeman gem as a static code analyser to help us identify security issues.\nAdd an additional check to CI that will check we still have no warnings on every new pull request that is tested (Pull requests must pass before they can be merged given out GitHub configuration).\n,27,2407,27,58
beis-report-official-development-assistance/0019-use-public-activity-gem.md,## Context\nPart 7 of the Service Standard on security and privacy[1] includes a section on non-repudiation. The service should be logging what changes were made by each user.\n,We should use the `public_activity` gem to track user actions in the application. This has been proven to provide the right information and context to pass the Service Standard.\n,41,2408,41,35
beis-report-official-development-assistance/0022-use-govuk-paas-for-hosting.md,"## Context\nAs a government entity, BEIS want to host their service on [GOVUK\nPaaS](https://www.cloud.service.gov.uk/). This was always the service teams\nintention.\n",Host all environments excpet local development on BEIS own GOVUK PaaS account.\n,44,2410,44,19
beis-report-official-development-assistance/0007-use-a-changelog-for-tracking-changes-in-a-release.md,"## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",45,2411,45,57
beis-report-official-development-assistance/0009-use-sidekiq-and-redis-to-send-emails.md,## Context\nThe service needs to send invitation emails through Notify. The service will likely need to send more notifications in future to send reminders or notify of approvals etc.\nWe could use a Postgres backed queue using DelayedJob instead of Redis to remove a dependency.\nSidekiq and Redis are a well used and trusted combo for dxw delivery teams and are familiar in the community.\n,Use Sidekiq to send emails and other asynchronous tasks.\nUse Redis as the queue for jobs to improve fault resilience.\n,80,2412,80,26
beis-report-official-development-assistance/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2413,16,39
beis-report-official-development-assistance/0018-use-a-server-side-session-store.md,"## Context\nWe have a CookieOverflow error that is prompting this work [4]. It occurs when the service is storing more than 4KB of data within the cookie. This includes information on the user, auth tokens and the content of any flash messages.\nA current work-around is to delete your cookie and sign in again.\nRails acknowledges [5] that the default cookie storage is fast but prone to this error.\n",Use Redis [1] as a server-side session store.\n,87,2414,87,13
arch/0015-code-review.md,## Context\nContext here...\n,使用 Github 进行 Code Review\n,7,2416,7,8
arch/0011-apply-github-workflow-to-our-team.md,## Context\nContext here...\n,"### Github workflow\nThere are various workflows and each one has its strengths and weaknesses. Whether a workflow fits your case, depends on the team, the project and your development procedures.\nTBD\n",7,2421,7,40
arch/0017-incident-classify-and-recovery.md,## Context\n对服务的可用等级不清晰，处理问题的优先级不足，临时分析现象及寻找解决方案，耗时过长\n,划分服务等级，做好预案\n,48,2426,48,13
arch/0018-continuous-delivery-ios.md,## Context\n目前使用测试同学的工作电脑做 iOS 的打包与分发，对测试同学有一定的影响。\n,![][image-1]\n基本流程不变，将打包工作交由第三方服务 buddybuild 处理并分发。\nbuddybuild 本可以直接分发，但由于其分发网络在国内比较慢，所以继续使用 fir.im 进行分发\n,39,2428,39,77
arch/0009-make-django-project-with-the-same-structure.md,"## Context\nWe have 20+ projects which based on Django, but we don’t have the same structure which make our project hard to read.\nWe need handle those things:\n1. store all apps in one place;\n2. support different environments;\n3. store project related configs.\n",make a skeleton for django: [https://github.com/huifenqi/django-project-skeleton][1]\n,63,2430,63,24
arch/0013-the-sense-of-done.md,"## Context\nWe face this situation very often?\n1. I’m already fixed this, it’s in testing;\n2. I’m already add this monitoring plugin in the repo, it’s deployed on staging, let’s watch for a few days, then deploy on production;\n3. I’m already move this service to new machine, I will clean the old machine later.\nSounds familiar? Yes, very often.\nIs that all done? No, it doesn’t.\n","1. talk this all the time, let team member have this sense;\n2. make specifications for each domain.\n",100,2450,100,25
arch/0012-think-about-micro-service.md,## Context\n1. 已拆分为各种服务（1. 接口不统一、无监控、无统一日志管理）；\n2. API 文档管理不够；\n3. 服务的部署纯手动；\n,Decision here...\n,68,2453,68,4
openfido-app-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2472,16,39
CrossyToad/adr-0002-use-haskell.md,"## Context\nThe reason I am building this project is to experiment with different architectures for video games\nand I'm particularly interested in how functional programming applies to games.\nSince this is a personal project, it should be fun. Right now I find functional programming more\nfun the other paradigms, and I find static types more fun then dynamic types.\nAdditionally, I'm trying to get better at Haskell.\nTherefore, Haskell seems like an obvious choice.\n",I'm using Haskell!\n,95,2476,95,6
CrossyToad/adr-0005-use-hpack.md,"## Context\nCabal files are annoying to specify, they tend to have a fair bit of redundancy and I found\nthem annoying to use in the past.\nPain points include:\n- Having to specify `ghc-options` for each target\n- Having to specify common dependencies for each target\n- Having to manually specify _every exposed module_\n[hpack](https://github.com/sol/hpack) solves these problems, so let's use it!\n",We're using [hpack](https://github.com/sol/hpack)!\n,95,2481,95,18
pomi-data-etl/0002-store-etl-output-in-the-cloud.md,## Context\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\n,"When the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\nThe output file will be exposed on a publicly available URL.\n",25,2486,25,37
pomi-data-etl/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2487,16,39
educational-platform/0006-rich-domain-model.md,## Context\nPossible solutions related to domain model implementation:\n- anemic domain model without any logic inside;\n- rich domain model with encapsulated logic;\n,Rich domain model solution will be used. Domain model will encapsulate internal structure and logic.\n,33,2496,33,19
educational-platform/0001-bounded-contexts-communication.md,"## Context\nSome common data should be used by several bounded contexts (modules, in the case of monolith application).\n","Communication between bounded contexts asynchronous. Bounded contexts don't share data, it's forbidden to create a transaction which spans more than one bounded context.\n- https://www.infoq.com/news/2014/11/sharing-data-bounded-contexts/\n- http://www.kamilgrzybek.com/design/modular-monolith-primer/\n- https://github.com/kgrzybek/modular-monolith-with-ddd#37-modules-integration\n",25,2497,25,93
educational-platform/0010-use-axon-framework.md,"## Context\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\n",Axon Framework will be used as DDD library.\n,32,2498,32,12
educational-platform/0002-integration-events-implementation.md,"## Context\nFor implementing event-driven application, the platform should be available for communication with integration events.\n","We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\n- https://devblogs.microsoft.com/cesardelatorre/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures/\n",21,2499,21,100
educational-platform/0011-use-axon-event-publishing-mechanism.md,"## Context\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\n",Axon Framework will be used for integration events implementation.\n,79,2500,79,12
educational-platform/0005-identifier-between-modules.md,"## Context\nDuring communications between bounded contexts, global identifiers for entities are needed. Also, these identifiers are needed for possible future integrations with external systems.\n",Natural keys or uuids should be used. Primary keys are forbidden for communications between modules or with external systems. If entity has good natural key - it's the most preferable choice for identifier between modules.\nUseful links:\n- https://tomharrisonjr.com/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439\n,32,2501,32,74
educational-platform/0009-architecture-tests.md,## Context\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\n,Architecture tests with using Archunit should be implemented.\n,21,2502,21,11
educational-platform/0008-result-from-comand-handlers.md,"## Context\nThe idea from CQRS - do not return anything from command processing. But in some cases, we need to get generated identifiers of new created resources.\n",Command handlers can return generated identifiers after processing if it's needed.\n,34,2503,34,14
GOG-Galaxy-Suggester/0002-removed-python.md,## Context and Problem Statement\nCurrent tech stack is very complicated and requires users to download a tool to use it. It takes away from the main idea of this project.\n## Decision Drivers\n* sql.js is a library that allows opening up an SQLite database on the website\n* desktop applciation stack with Python backend is complciated\n,"* sql.js is a library that allows opening up an SQLite database on the website\n* desktop applciation stack with Python backend is complciated\nDropped the backend/frontend solution. Moved to a pure website.\n### Positive Consequences\n* Easier to use - just open a website, upload a file and use the results.\n* Less complex tech stack.\n### Negative Consequences\n* Launching games, and other advanded features are out of scope.\n",70,2505,70,98
pottery/0005-sherd-files-have-unique-ids-to-make-merging-histories-easy.md,"## Context\nSherd files are named after the time at which they were recorded (or relate to).  However, there is a (tiny) chance that two people might record a sherd at the same time, and merging the two commits will be a pain.\n","Include a [large (96 bits), random, base-64][] encoded bit string in the file name to ensure uniqueness.\n",55,2507,55,26
pottery/0004-store-shards-in-time-stamped-directories-and-files.md,"## Context\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\n","Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\n",56,2508,56,26
pottery/0006-provide-a-hook-for-different-types-of-sherd.md,"## Context\nAt the moment, sherds are merely a short bit of free text.\nTo extract useful information, it will be useful to have some standard event types, such as team members joining or leaving.  These event types would have a well defined structure -- maybe still be Markdown, but with constraints on the content and markup used -- and be generated by convenient subcommands of the `pottery` tool.\n","Add the event type to the sherd filename, after the date but before the unique ID.  The parts of the filename are separated by underscores.\nThe event type is a short string that cannot contain an underscore.\nFree text sherds have the event type ""post"".\n",85,2509,85,57
pottery/0003-pottery-shards-are-markdown-text.md,"## Context\nEach event recorded by Pottery (aka ""shard"") is a short piece of text.  Maybe with links, or basic formatting.\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\n",The content of each shard will be in Markdown format.\n,65,2510,65,12
pottery/0002-bootstrap-by-copying-the-adr-tools-source-code.md,"## Context\nI'm writing this very fast as part of a hackday.\nI want the app to be like [adr-tools](https://github.com/npryce/adr-tools): a command line tool that stores data in a project's version control system.  I want it to have built-in help, like adr-tools.\n","Copy the source of adr-tools.  Delete the scripts that don't make sense for Pottery.  Then do a bulk find-and-replace of ""adr"" to ""pottery"".  Bish bosh!\n",70,2511,70,45
pottery/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2512,16,39
volley-management/0006-use-azure-table-storage-for-persistence.md,## Context and Problem Statement\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\n## Decision Drivers\n* Cost\n* Backup capabilities\n,"* Cost\n* Backup capabilities\nChosen option: ""Azure Table Storage"", because cost.\n### Positive Consequences <!-- optional -->\n* Persistence bill should be down from ~$5/month to less than $1/month\n### Negative Consequences <!-- optional -->\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\n",68,2520,68,99
volley-management/0005-use-azure-app-services-for-hosting.md,## Context and Problem Statement\nWe need a hosting provider to host application. It should be cloud capable of hosting Docker containers.\n## Decision Drivers <!-- optional -->\n* cost\n* ease of deployment\n* custom domains\n* https\n,"* cost\n* ease of deployment\n* custom domains\n* https\nChosen option: ""Azure App Service"".\nHonestly this was chosen because I had experience running VolleyManagement V2 on Azure App Service. Price is acceptable and I did not have time to evaluate other options I'd want:\n* Digital Ocean\n* GCP App Engine Flexible environment\n",49,2521,49,74
volley-management/0002-use-sendgrid-as-mailing-system.md,## Context and Problem Statement\nWe need a way to send emails to users for several workflows.\n,"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\n### Positive Consequences <!-- optional -->\n* Fast time to develop\n### Negative Consequences\n* We need to think about setting up development environment\n",20,2523,20,54
volley-management/0003-use-angular-for-spa.md,## Context and Problem Statement\nVolleyManagement.SPA provides a way for users to interact. We need to select a framework which will be used to develop.\n## Decision Drivers\n* TypeScript support\n* Unit Test support\n* Skill availability\n* UI components availability\n,"* TypeScript support\n* Unit Test support\n* Skill availability\n* UI components availability\nChosen option: Angular, because skill availability is better.\n### Positive Consequences\n* Easier to get work done as it will be easier to find person eager to contribute\n### Negative Consequences\n* Angular has a reputation of being complex. As a workaround we can always build Vue.JS app later\n",55,2525,55,82
base-react-typescript/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2528,16,39
gp-data-etl/0003-store-host-output-file-in-etl-container.md,"## Context\nThe ETL process takes a long time to run, approximately 6 hours.\nRunning the ETL process makes close to 30,000 requests to the Syndication API.\nSeveral applications need access to the JSON file created by the ETL.\n","Rather than each application running its own copy of the ETL to obtain the Syndication data in JSON format,\na single instance of the ETL will run and provide access to the resultant file via an nginx web server\nrunning in the ETL container.\nThe output JSON is hosted in the container to remove reliance on external solutions, such as Azure.\n",55,2529,55,71
gp-data-etl/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2531,16,39
gp-data-etl/0004-replace-file-hosting-with-cloud-storage.md,"## Context\nThe container currently runs an nginx web server to host the output file for consuming applications.\nIf the container is not running, the file is not available.\n",The nginx service will be removed and the `gp-data.json` file will be written to the team's preferred cloud hosting platform.\n,35,2532,35,27
pride-london-app/0002-introduce-redux.md,## Context\nWe were interested in pushing the project until we felt we needed redux to see how necessary it really was.\n,We added redux to the project relatively early\n,25,2533,25,9
pride-london-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2535,16,39
terraform-aws-dynamodb/0004-terraform-requirements-in-module.md,"## Context\nTerraform enables you to constrain the version of Terraform able to run a\nmodules, as well as the minimium allowed versions of providers. Many of LIC\nexisting modules do not leverage these configurations.\n",The Terraform version able to run this module will be set in `main.tf`.\nThe miniumum allowed versions of providers will be set in `main.tf`\n,46,2536,46,35
terraform-aws-dynamodb/0006-feature-toggle-defaults.md,## Context\nThis module utilises Feature Toggles. It has toggles to determine:\n* If any resources should be created\n* If a DynamoDB table resource should be created\n* If an Autoscaler should be enabled\nThese features need some default values.\n,"All of the Feature Toggles are defaulted to on (""true"") at this time.\nWe believe that in most cases people will want to create a DynamoDB table with\nAutoscaling at this time.\n",55,2537,55,43
terraform-aws-dynamodb/0010-terraform-backend-in-module.md,## Context\nWhen LIC teams begun using this module it became apparent that the current\nimplementation pattern does not meet their needs. Without a backend in the\nmodule teams would need to add a Terraform `backend` configuration into there\nlocal implementation for it to be picked up.\n,Restored the `backend` into the module for the time being.\nWe still feel this should be removed at some time and teams become familiar with\nhow to use Terragrunt/Terraform configuring there own `backend`.\n,57,2539,57,48
terraform-aws-dynamodb/0009-examples.md,## Context\nConsumers need to know how to use the Terraform Module. This module has a number\nof configurable options.\n,"The module will have several examples of how to use the module. Initially 2\nexamples named `base` and `complete` will be created. These should show how to\nprovide a minimal and a complete configuration.\nThis module may be used inside another module or in isolation, so these two\naspects should also be part of the example implementation.\n",27,2540,27,73
terraform-aws-dynamodb/0003-no-terraform-backend-in-module.md,"## Context\nTerraform requires a `backend` for state file storage. Most existing Terraform\nmodules within LIC have this `backend` set in the Terrform configuration in\n`main.tf`.\nHaving the `backend` configured in the module means that consumers can't\noverwrite the configuration. When building and testing modules it is useful to\nbe able to use a ""local"" `backend`.\n",We have not set the `backend` configuration in the `main.tf`.\n,85,2541,85,17
terraform-aws-dynamodb/0002-autoscaler-terraform-sub-module.md,## Context\nDynamoDB can utilise the [AWS Application Auto\nScaling](https://docs.aws.amazon.com/autoscaling/application/APIReference/Welcome.html)\nto dynamically adjust provisioned throughput capacity on your behalf. Not all\nuses of DynamoDB will use this autoscaling service.\nThere is a plan to create a Terraform `Data Storage Module` which will include\nthe DynamoDB module.\n,To support autoscaling a Terraform sub module will be utilised.\nThis sub module will be included in the DynamoDB module using Terraform `module`\nsyntax. An `enabled` toggle will be used to determine whether or not the\nautoscaling resources will be created.\n,83,2542,83,58
terraform-aws-dynamodb/0007-map-list-variables.md,"## Context\nDynamoDB provides the ability to supply additional attributes, a local\nsecondary index and a global secondary index. These additional attributes\nconsumed by the DynamoDB AWS Provider as maps/lists.\nIn addtion to the consumption as maps/lists there are additional requirements\nthat the range/hash keys be added to the additional attributes if declared. They\nare not added if undeclared.\n","The module will use a `null_resource` to manage the secondary indexes. Creating\nthem from the existences of appropriate maps/lists.\nProperties related to these resources will consume a list of maps as input.\nThese will them be mapped to the appropriate resource within in the module.\nThe range/hash key will be added automatically to the additional attributes by\nthe module, reducing the load on the consumer with implementation detail.\n",80,2543,80,85
terraform-aws-dynamodb/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2544,16,39
terraform-aws-dynamodb/0008-terraform-label-module.md,## Context\nIt can sometimes be hard to name resources in AWS so that you can identify them.\nThe clever guys from [CloudPosse](https://github.com/cloudposse) have created a\n[terraform-terraform-label](https://github.com/cloudposse/terraform-terraform-label)\nmodule aimed at helping to generate consistent label names and tags for\nresources.\n,This module will utilise the\n[terraform-terraform-label](https://github.com/cloudposse/terraform-terraform-label)\nmodule.\n,77,2545,77,29
dalmatian-frontend/0003-use-standard-rb.md,"## Context\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\n,36,2552,36,35
dalmatian-frontend/0002-use-bullet-to-catch-nplus1-queries.md,## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n,Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n,37,2553,37,23
dalmatian-frontend/0007-use-coveralls-for-monitoring-test-coverage.md,## Context\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\n,Use the free tier of Coveralls to give us statistics and to give our pull requests feedback.\n,26,2556,26,20
dalmatian-frontend/0001-use-pull-request-templates.md,"## Context\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\n",Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n,89,2557,89,22
dalmatian-frontend/0005-use-brakeman-for-security-analysis.md,## Context\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n,Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n,20,2559,20,29
dalmatian-frontend/0002-use-a-changelog-for-tracking-changes-in-a-release.md,"## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",45,2561,45,57
dalmatian-frontend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2562,16,39
api-raml/0003-use-raml-annotation-deprecated.md,"## Context\nVersioned APIs need to retire older versions of a schema, in order to keep complexity manageable.\nIt is not possible to deploy different services atomically so that a new version can be switched on in a short time frame. The changes needed have to be propagated to both clients and servers.\nWe add `Warning` headers to responses that are using an old version.\n","Whenever a new version is added, unless it's `@experimental`, we will annotate with `@deprecated` API the previous version as it won't be served by default anymore and will soon disappear.\n",78,2563,78,41
api-raml/0002-use-raml-annotation-experimental.md,"## Context\nDownstream libraries such as API client, SDK, validators, bot-lax-adaptor rely on the schema provided by api-raml.\nIt is difficult and error-prone to point the downstream libraries to a particular branch; in particular given this branch would need to rebuild dist/ after checkout.\nIt is forbidden to update in place a schema exposed by an API without versioning: the content types follow semantic versioning.\n","We will annotate with `@experimental` APIs that are being worked upon, and merge the into `develop` and `master` (indirectly).\n",90,2564,90,32
api-raml/0001-use-square-brackets-for-query-string-parameters-with-multiple-values.md,"## Context\nQuery string parameters to drive searches into an API often need to pass in multiple values, used in OR with each other.\n",We will use the PHP-style query string parameter format:\n```\n/search?type[]=blog-article&type[]=collection\n```\n,28,2565,28,27
titania-os/0009-docker-state-directory-on-data-volume.md,## Context\nDocker has a state directory that contains images and containers. Docker has a separate update mechanism for the images to that of the root filesystem and expects its state to be mutable. The dApps and their updates should logically be disconnected to the updates to the system itself.\n,We will put the Docker's state directory on the read-write data volume.\n,57,2566,57,16
titania-os/0010-wiki-as-submodule.md,## Context\nThe new project guidelines prescribe the usage of the Wiki for documentation. This disconnects the documentation from the repository checkout which may be undesirable. GitLab wikis are in essence git repositories of their own that hold markdown.\n,We will link the project's GitLab wiki as a submodule in `doc` folder.\n,47,2567,47,19
titania-os/0006-systemd-units-for-dapps.md,"## Context\nDocker has a daemon that oversees running containers. It does not however have much functionality to it, can't automatically restart the containers, can't set up containers that must be run automatically and has a separate logging system. It's possible to write a ""unit template"" for systemd to run containers as system services.\n",We will use systemd for running the containers. We will write a template unit that can run an arbitrary dApp with systemd templating mechanism. We will use systemd `enable/disable` symlinking functionality to allow the user to configure which services must auto-start. We will advise dApp developpers to use standard output for logging.\n,67,2568,67,67
titania-os/0007-layers-as-git-submodules.md,## Context\nYocto has a notion of layers which combine heterogenous code allowing to remix already usable code repositories (e.g. RaspberryPi support). There are alternative approaches on how to keep those in the project. Some advocate just forking the third party code in one's own repo. Others suggest linking their respective repos as git submodules to create dependencies.\n,We will include third party layers in the repository as git submodules. We will not modify the third party layer code directly. We will use the `pyro` branch for OpenEmbedded/Yocto as a reference point.\n,74,2569,74,46
titania-os/0003-yocto-as-a-distro-base.md,"## Context\nThere are several competing mainstream solution for generating embedded Linux firmware images. Yocto (umbrella for OpenEmbedded, BitBake and a few tools) offers modular Gentoo-like distribution generator which is extensible. Buildroot offers easier `Kconfig` based setup process. Buildroot can be forked and modified, but does not have a level of abstraction/flexibility Yocto has.\n",We will use Yocto/OpenEmbedded as a base for building the firmware image.\n,84,2570,84,18
titania-os/0002-docker-for-dapps.md,## Context\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\n,"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\n",31,2573,31,29
titania-os/0005-networkmanager-for-networking.md,"## Context\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\n",We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\n,69,2574,69,20
titania-os/0011-preinstallable-images-on-data-volume.md,## Context\nSeveral images must come pre-installed. Currently we ship IoP stack and Nginx out of the box. There must be a way to put the respective images on the device before the network is connected.\n,Images to preinstall are built in Docker format and shipped in the data volume as `/datafs/docker/preinstall/*.tar.gz`. Image names are mangled in order to remove slashes and colons. Each bundled dapp runs depends on `image-preinstall@.service` with the filename (except for `.tar.gz`) of the image to preinstall.\n,46,2575,46,73
titania-os/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2576,16,39
slingshot/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2579,16,39
thing/0003-deploy-via-heroku-pipelines.md,"## Context\nInitially this project was not appropriate for deploy on Heroku because it\nneeded MIT Touchstone authentication. However, based on\n[ADR-0002 Authentication via Touchstone SAML](0002-authentication-via-touchstone-saml.md)\nwe are now able to remove the `mod_shib` requirement that initially prevented\nus from using Heroku.\n",We will use Heroku Pipelines for Staging / Production and PR builds.\n,77,2580,77,17
thing/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2583,16,39
geem/0002-django-rest-framework-api.md,## Context\n,TBD\n,3,2586,3,3
geem/0004-oauth2-social-login.md,## Context\n,TBD\n,3,2587,3,3
geem/0000-record-architecture-decisions.md,"## Context\nRecording the architectural decisions made on this project will help future developers to understand the context of those decisions. By reviewing architecture decision records (ADRs), developers can see which options were considered and how decisions were made.\nFurther details on Architecture Decision Records are described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",TBD\n,84,2588,84,3
geem/0003-postgresql-database.md,## Context\n[PostgreSQL](https://www.postgresql.org/) provides a [native JSON field type](https://www.postgresql.org/docs/10/static/datatype-json.html) which could be useful for storing package contents.\n,TBD\n,47,2589,47,3
geem/0001-use-django-framework.md,## Context\n[Django](https://www.djangoproject.com/) is a popular and mature web framework with comprehensive [documentation](https://docs.djangoproject.com/) and a large ecosystem of app plugins to provide extra functionality.\nDjango provides an object-relational mapping (ORM) and schema migration system that simplifies persistent storage of application data. A simple and convenient admin panel allows developers to see and interact with application data through a web browser.\n,TBD\n,88,2590,88,3
document-evidence-store-frontend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2597,16,39
cljdoc/0003-utilize-grimoire-for-storage.md,"## Context\nWe need to store documentation (API, articles, etc.) for many different projects, versions\nand platforms.\n",[Grimoire](https://github.com/clojure-grimoire/lib-grimoire) has a storage design that\nsupports those requirements and seems to be a well designed project overall.\nGrimoire also supports implementing different storage backends which may be useful later.\nFor now we will try to build a filesystem based storage for documentaton based on Grimoire's\nformat and ideas.\n,26,2608,26,82
cljdoc/0002-don-t-build-on-top-of-codox.md,## Context\nWe want to derive data about Clojure projects that can be used to render API documentation\nas well as plain text documentation (e.g. tutorials).\nCodox is a popular tool to create this kind of documentation as HTML files.\n,Since Codox renders to HTML instead of some more well defined data format it will be hard to\nturn Codox' output into other formats. Due to this problem building on top of Codox is not\na viable path forward.\n,51,2614,51,49
cljdoc/0014-add-support-for-examples.md,## Context\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\n,Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\n,28,2616,28,45
cljdoc/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2618,16,39
containers/2-.md,"## Context\n======\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\n",======\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\n,34,2619,34,39
containers/1-.md,"## Context\n======\nThe existing container images use a variety of base images, including Fedora and Alpine Linux. These base images have different sizes, and different behavior that can cause various issues.\n",======\nWe should use a common base image for building container images. The base image that looks the most promising is Ubuntu minimal.\n,39,2620,39,28
laa-court-data-adaptor/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2621,16,39
laa-court-data-adaptor/0002-store-common-platform-data-locally-for-event-replay.md,"## Context\nThe Court Data Adaptor application relies heavily on CJS Common Platform provided APIs to be able to function. As the Common Platform is owned by a third party organisation, we have limited control over the kind of data sent through to CDA.\n","In order to be able to inspect the data (if needed) for debugging, it has been agreed that we will store each request coming through to CDA in a data store (RDS provided postgres instance).\n",52,2622,52,43
ftd-scratch3-offline/0005-use-cpp-to-interface-with-the-ftduino.md,## Context\nThe scratch blocks need to be run on the ftduino.\nTo run a scratch block a small runtime is needed.\nThe ftduino supports both C and C++.\nThe Arduino and ftduino library use C++.\nInterfacing from C to C++ is possible but tricky.\nC++ also offers constructs C doesn't e.g. type safe enums.\n,We will use C++ to interface with the ftduino.\n,78,2624,78,13
ftd-scratch3-offline/0004-use-multiple-projects.md,## Context\nThe software could be developed in one big (Gradle) project.\nThis would make integration easier.\nAt the same time this would make re-use of the code outside of this project harder.\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\n,We will try to modularize the software and will use multiple projects to achieve this goal.\n,67,2626,67,19
ftd-scratch3-offline/0002-use-java-as-language.md,## Context\nWe need to use a language to implement the software. Scratch itself is written in Javascript and runs in the browser.\nSo Javascript would be a natural choice.\nBut the authors are more proficient in Java than Javascript. Also to compile the C code we always need a way to execute\nnative processes. Thus we always can execute Java programs.\nThis means that we only need a small bridge from Javascript to Java and can write most code in Java.\n,We will use Java to implement the software.\n,95,2628,95,10
ftd-scratch3-offline/0011-implement-complex-scratch-functions-in-a-helper-function.md,"## Context\nSome scratch/ftduino functions take complex arguments that need to be verified.\nSome scratch/ftduino functions are complex to implement.\nThe code for these functions could be directly generated in the java files for the specific function.\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\nWriting these functions in a helper function also makes changes and code reuse easier.\n",Complex scratch/ftduino functions will be implemented in helper functions and those will be called by the generated code.\n,99,2629,99,23
ftd-scratch3-offline/0018-conversion-of-unknown-scratch-functions-to-c-will-fail.md,## Context\nOnly a subset of all scratch functions has been implemented.\nIt might make sense to ignore unsupported functions.\nThis would make sense when encountering sound/image related functions.\nSo the user could still use these functions when using the web version and not using the offline version.\nOn the other hand the user will be surprised that some functions won't actually work.\nFunctions silently being ignored would make troubleshooting harder.\n,Unknown scratch functions will cause the conversion to C to fail.\n,86,2630,86,13
ftd-scratch3-offline/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md,## Context\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\nThe ftduino only supports 32-bit floats.\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\n,We will use 32-bit floats to store any numbers.\n,60,2631,60,13
ftd-scratch3-offline/0016-don-t-support-scratch-sounds-and-sound-related-blocks.md,## Context\nScratch supports sounds.\nThe ftduino has no way to play sound.\n,Scratch sounds and sound related blocks are not supported.\n,19,2632,19,12
ftd-scratch3-offline/0019-allow-direct-input-of-project-json-without-a-sb3-file.md,"## Context\nWe need a way to transfer the scratch save to the local web server.\nScratch provides a function that serializes the scratch program, but directly returns a json file which would normally be the project.json in a .sb3 file.\nRather than searching for a method that provides a .sb3 file, we chose to accept the json directly.\n",project.json can directly be used as input for conversion.\n,75,2636,75,12
ftd-scratch3-offline/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md,## Context\nScratch supports images/sprites.\nThe ftduino has no way to show images/sprites.\n,Scratch images/sprites and image/sprite related blocks are not supported.\n,23,2637,23,16
ftd-scratch3-offline/0010-use-travis-ci-for-continuous-integration.md,"## Context\nWe want to use continuous integration to make sure that at any time the build is working.\nCI will check every commit and PR.\nPossible choices are: Travis CI, CircleCI or AppVeyor.\nTravis CI offers Linux and Mac builds. Windows is in beta.\nCircleCI supports all 3 platforms.\nAppVeyor supports Linux and Windows.\nThe authors have already used Travis CI.\n",We will use Travis CI for continuous integration.\n,89,2638,89,10
ftd-scratch3-offline/0020-use-json-for-scratch-to-local-server-communication.md,## Context\nWe are using POST HTTP requests to communicate with the local server.\nTo pass multiple parameters we would use requests with `multipart/form-data` or `text/plain` type.\nCreating correctly encoded multipart requests is tricky.\n,"We will use json for scratch to local server communication.\nI.e. responses from server to scratch will be json objects similar to this:\n````json\n{\n""status"" : """", | ""SUCCESS"" or ""FAILED""\n""errorMessage"" : """", | null when no error happened, otherwise an error message\n""result"" : """" | null when there is no output (e.g. in case of an error), otherwise the result\n}\n````\n",48,2639,48,98
ftd-scratch3-offline/0012-generate-simple-scratch-statements-in-java-and-don-t-call-a-helper-function.md,"## Context\nAdr 0011 says that complex scratch functions will be implemented in helper functions.\nFor statements like if/else/while this is harder to do.\nAn ""if"" function would have to take a function that is executed on success.\nThis would be more complex than simply using a c ""if"" statement.\n",The code for simple scratch statements will be directly generated in the java file.\nSimple scratch statements are e.g. if/else/while.\n,70,2641,70,30
ftd-scratch3-offline/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2642,16,39
dotcom-rendering/023-dynamic-imports.md,## Context\nThe newest versions of Javascript support `import` as a way to dynamically\nimport modules. Modules live on a URL and can be loaded cross origin.\nThey are therefore useful in a variety of contexts as a mechanism to lazy-load\ncontent.\nBrowser-support is high (~90%) but not enough to forgo a polyfill.\n,"Support `import` via polyfills for browsers that need them.\nAs it is not possible to directly override `import`, dynamic import is exposed\nvia `window.guardianPolyfilledImport`.\n",71,2643,71,41
dotcom-rendering/016-client-side-computation.md,"## Context\nWhen preparing data for the rendering components we currently have up to three possible locations to do so: (1) the frontend Scala backend, (2) the dotcom rendering backend and (3) the end user's client side.\nIn the interest of the user, we should avoid postponing computation to the client side and precompute data and state on either of the two backends whenever possible.\n",- Favour computation in frontend over computation in dotcom-rendering\n- Favour computation on dotcom-rendering server than computation on the client\n,84,2647,84,30
dotcom-rendering/020-react-hooks.md,"## Context\nWe've avoided using React hooks for some time in order to ensure un-desired complexity is avoided in the code base. But as hooks are now standard fare in React applications, it makes sense to review our usage of them in DCR.\n",- Prefer non-stateful components if possible\n- Prefer React's official hooks to custom hooks\n- Avoid abstractions that could lead to hooks within hooks within hooks.\n- Prefer hooks to classes with component lifecycle methods\n- Try to build hooks that are generic and reusable\n,52,2654,52,55
dotcom-rendering/010-storybook.md,"## Context\nGUUI intends to be, at some level, a library of components.\nDotcom rendering will require a mapping of CAPI element to React Component.\nStorybook is a widely used library which allows a series of demos and examples to be easily constructed. It also has support for typescript.\n","It is possible to envision a split in components:\n- those which form our design system\n- those which render individual elements of content from CAPI\nEach of these should have an independant storybook, allowing the design system ones to express the variety of ways each component can and should be used. And allowing, as they are developed, each CAPI element rendering component to demonstrate the variety of content they can encapsulate.\n",64,2657,64,87
dotcom-rendering/013-new-elements-models-in-frontend.md,"## Context\nThe frontend project currently parses capi block elements into a list of BodyBlock objects as part of the\nfrontend model. This is currently used (in frontend) by the liveblog, and we also forward this structure to\ndotcom rendering.\n","We have created a duplicate set of models in frontend to be used only by the dotcom rendering datamodel.\nDoing this will allow us to iterate on the dotcom rendering models more quickly and make enhancements such\nas better image support, without the potential to impact liveblogs.\n",54,2663,54,57
dotcom-rendering/019-remove-monorepo.md,"## Context\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\n","Remove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\n",51,2666,51,28
dotcom-rendering/018-react-context-api.md,# React Context API\n## Context\nWe don't use any state management in dotcom at the moment and this means props have to be\nexplicitly passed around; this can lead to 'prop drilling'.\n[This PR](https://github.com/guardian/dotcom-rendering/pull/801) was a spike to demonstrate using\nthe react context api to extract the `edition` property to prevent this.\n,-   Our component tree is shallow so we shouldn't implement any 'magic' that\nsteps away from having explicit props showing where and what data a component is using.\n-   This decision should be revisited as the codebase grows.\n,88,2667,88,50
archcolider/005 Service readiness checks.md,"## Context\nCurrently there are only several services planned, but most of them require time for initialization (warming cache for instance) and they can't start processing requests from other services immediately. The situation might arise when restarting a service, introducing a new instance, or some other situations.\nIf a service is not ready for processing a request, the timeout might happen and the supervisor might decide that there should be a new instance that leads to a cascading effect and higher bills in the end.\n",Health check endpoints provide information about readiness of processing requests and other services should respect this information.\n,100,2677,100,19
archcolider/009 Rely on payment service provider.md,## Context\nThe _ordering system_ should accept a wide variety of payment methods to satisfy the user's expectations and increase the user base. The number of payment systems is significant and each of them have nuances in the API and ways of communication.\n,Use a 3rd party payment provider to delegate dealing with different payment systems.\n,50,2680,50,17
archcolider/010 Feedback System separation.md,## Context\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\nTwo different feedback methods:\n- surveys (occasional questionnaires about general aspects of the app/service)\n- feedback (is an opinion about an order or app/service)\n,"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\n",74,2681,74,54
archcolider/006 Zero trust architecture.md,## Context\nThe _ordering system_ communicates with upstream and downstream systems in a cloud environment and we can't rely on an assumption that requests from the same subnetwork are safe.\nThe _ordering system_ is implemented as a modularized monolith and those modules will eventually be extracted to become dedicated services. We have to secure such calls at an early stage to make it easier to migrate from module to service.\n,Internal calls from modules should contain security info with auth and claims info from the very beginning.\n,83,2682,83,19
functionaut/0002-use-google-closure-compiler.md,## Context\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\n,"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\n",38,2687,38,68
functionaut/0003-test-distributed-files-only.md,## Context\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\n,Testing will be made against the production bundle to catch compilation errors before they reach our users.\n,67,2689,67,19
functionaut/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2690,16,39
hackathon2020-prominent-colors/0001-use-node-vibrant.md,## Context\n* Need to pick a library to use for finding the prominent colors in an image\n* The scope of this project is subject to change in the future\n* Unsure yet where this project will fit into Condé Nast platform architecture\n* Needs to be deterministic for quality purposes\n,* Use node-vibrant because it's popular and therefore well maintained and tested\n,59,2691,59,17
operational-data-hub/0026-solution-facilitates-a-coherent-set-of-business-functions.md,"## Context\nA lot of different functions are required to support the business. These functions can be grouped into coherent sets, referred to as a solution. The functionality defined by the set of functions can then be implemented on the platform, fulfilling the business requirements.\nThe solution can also implement domain crossing functionality, e.g. a service provided from one domain that is consumed in another domain.\n",We define a solution as the implementation of a coherent set of business functions on the platform.\n,77,2694,77,19
operational-data-hub/0015-automatic-backup-of-each-data-component.md,"## Context\nTo protect against data loss, a backup of all storage components should be implemented. As all data components are specified in the [data catalog](0012-data-catalog-specifies-all-data-components.md), automated backup can be implemented from this specification.\n",We will implemented automated backup based on the [data catalog](0012-data-catalog-specifies-all-data-components.md) for each component that stores data.\n,53,2695,53,32
operational-data-hub/0060-lock-pip-requirements.md,## Context\nCode Injection is a specific type of injection attack where an executable program statement is constructed involving user input at an attack surface that becomes vulnerable when it can be manipulated in an unanticipated way to invoke functionality that can be used to cause harm.\n,To prevent dependency injection attacks we decided to have both a requirements.in file and a [pip-tools/pip-compile](https://github.com/jazzband/pip-tools) generated requirements.txt\n,51,2698,51,39
operational-data-hub/0041-deployment-through-pull-request.md,"## Context\nDeployment of new software to the production environment should be done in a controlled way. The possibility of unauthorized deployment to production is a security vulnerability. Therefore, deployment is [automated](0049-gitops-deployment.md) in CI/CD pipelines. Next to that, a pull request with review procedure can be used to implement 4 eyes principle through review, either by peers or other functions.\n",We will use pull requests to initiate a deployment of new software to production.\n,83,2699,83,16
operational-data-hub/0036-ssl-certificates-are-always-verified.md,"## Context\nWhen using SSL, certificate verification ensures the identity of the other party we're communicating with. Using unverified certificates makes the communication more vulnerable to man-in-the-middle attacks. Certificate verification can be done using a trusted Certificate Authority (CA) or by pinning the certificate (importing a host's certificate in your trust store).\n",We will only use verified SSL certificates.\n,69,2702,69,9
operational-data-hub/0030-life-cycle-management.md,"## Context\nUsing 3rd party components, like libraries, modules or executables, introduces the risk of importing security vulnerabilities in those components. Hackers might target these components in a software supply chain attack. A basic precaution to this is to keep all components up-to-date, a practice referred to as life cycle management.\n",We will keep 3rd party components up-to-date to mitigate software supply chain attack risk.\n,66,2707,66,20
operational-data-hub/0047-shell-scripting.md,"## Context\nA Unix shell is a command-line interpreter or shell that provides a command line user interface for Unix-like operating systems. The shell is both an interactive command language and a scripting language, and is used by the operating system to control the execution of the system using shell scripts.\n",Within the Linux exnvironment Bash (Bourne Again Shell) is the most popular which will be used within our environment. Shell scripts should comply with the CheckCheck linter.\n,58,2708,58,38
operational-data-hub/0032-oauth2-for-authentication.md,"## Context\nUsing http protocol and APIs, authentication and authorisation is mostly done by passing some token with every request. Fixed secrets are easy to understand and configure, but somewhat weak in protection. Instead, oauth2 is a stronger mechanism to create tokens, though more complicated. In oauth2 authentication and authorisation are decoupled and the security critical authentication is delegated to the authorization server. Still, oauth2 allows distributed token validation next to validating the token with the server.\n",We will use oauth2 for authentication and authorization.\n,96,2709,96,11
operational-data-hub/0019-single-schema-per-topic.md,## Context\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\n,"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\n",32,2710,32,50
operational-data-hub/0037-secrets-are-stored-in-gcp-secret-manager.md,"## Context\nSecrets must be protected according to the least-privilege principle. To reduce the trusted computing base, preferrably a 3rd party secret management tool is used to manage and use secrets. Google Secret Mananager is a managed service on GCP, integrated into the platform. This makes it a suitable tool to manage our secrets.\n","We will use GCP Secret Manager to manage, store and use secrets.\n",72,2711,72,16
operational-data-hub/0042-python-version-3.md,"## Context\nIn the past, there was a bit of a debate in the coding community about which Python version was the best one to learn: Python 2 vs Python 3 (or, specifically, Python 2.7 vs 3.5). Now, it's more of a no-brainer: Python 3 is the clear winner.\n",We will use python3  exclusively for all our python code.\nMake sure you use version active maintained version (currently 3.6 or higher)\n,72,2713,72,32
operational-data-hub/0018-single-writer-for-a-topic.md,## Context\nTopics are used to distribute events to other applications. Systems subscribing to these events will be dependent on these events.\n,Every topic is limited to a single writer process.\n,26,2715,26,11
operational-data-hub/0031-uniform-bucket-level-access-for-storage.md,"## Context\nAs motivated in [14. Single confidentiality level per data component](0014-single-confidentiality-level-per-data-component.md), access level granularity is kept at data component level. For buckets, this means uniform bucket-level access will be used, the more fine grained object-level ACLs will not be used.\n",We will use uniform bucket-level access for storage.\n,66,2720,66,11
operational-data-hub/0040-hpa.md,## Context\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\n,We will use a high privilege access procedure to secure access to production systems for support.\n,60,2721,60,18
operational-data-hub/0009-cloud-function-entrypoint.md,## Context\nWe feel the need to define a uniform way of setting entrypoints for Cloud Functions within the Google Cloud Platform\n,- The entrypoint should have the same name as the function directory.\n- The entrypoint name should be clear and descriptive.\n- The entrypoint should be in `main.py`.\n- The entrypoint should not contain business logic.\n,25,2722,25,51
operational-data-hub/0002-use-serverless-infra-components.md,"## Context\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\n",We will use serverless infrastructure components where possible.\n,68,2723,68,11
operational-data-hub/0024-refer-to-blob-at-the-source-project.md,"## Context\nAlthough inter-domain communication should be done using ODH topics, this technology is less suitable to transfer large binary objects. Therefore, references of large binaries are communicated on the ODH topics instead of the large binary itself. This reference can be used by consumers to retrieve the binary object at its source.\n","Instead of the large binary object itself, we will pass references to it on ODH topics, which can be used by consumers to retrieve the object at its source.\n",63,2727,63,34
operational-data-hub/0033-two-identity-providers-development-run.md,"## Context\nAccess to the solutions (applications) as a user, referred to as the _run_ environment, is based on the company identity provider. Access to the development/operations environment, the GCP platform, is based on a separate, DevOps identity provider. This allows strict separation between Run and DevOps and makes automation of DevOps practices somewhat easier.\n","We will use a separate identity provider to access the platform for DevOps practices, disconnected from access to the _run_ environment.\n",75,2729,75,27
operational-data-hub/0021-messages-are-in-json-format.md,## Context\nIt is preferred to use a single message type for the business events. This makes it easier to handle messages on the pub/sub system in a standerdized way.\n,All business events on the ODH platform topics are formatted as [JSON](https://tools.ietf.org/html/rfc7159)\n,37,2730,37,27
operational-data-hub/0048-testing-framework.md,"## Context\nThe variety of JavaScript testing frameworks made available for automation testers have become a cause of confusion for many. It is only natural, the more options you have in front of you, the more time it will take for you to decide which JavaScript automation testing framework fits the best for you.\n",We have decided to use the cypress testing framework for frontend and e2e testing.\n,60,2733,60,19
operational-data-hub/0008-data-is-location-and-time-aware.md,## Context\nAll data that is related to geographic structures is added to the ODH in a standardized way.\n,GeoJSON is a format for encoding a variety of geographic data structures (https://tools.ietf.org/html/rfc7946)\nGeoJSON-events extends RFC 7649 GeoJSON with instants and intervals of time (https://github.com/sgillies/geojson-events)\n,23,2734,23,58
operational-data-hub/0025-domains-correspond-to-business-departments.md,"## Context\nThe organizational model of a company has a great influence on the communication structure in the company. Within a department, people tend to use the same terminology and definitions. This relates closely to the definition of ubiquitous language. Therefore, the domain model used on the platform should be closely related to the organizational model of the company. This is also supported by Conway's law.\n",We will structure the domain model on the platform around the departments organizational model of the company.\n,76,2739,76,19
operational-data-hub/0045-code-config-separation.md,"## Context\nBoth Code and Configuration reside in source code control (Github in our case). This makes it very easy to mix-up code and configuration. However, these 2 should be clearly separated. Where possible code can be reused, but configuration is most of the times instance specific.\n",Code and Configuration is clearly separated. At deployment time the CI/CD tools are responsible for bringin code and config together and deploy the code together with the correct configuration.\n,58,2741,58,35
operational-data-hub/0035-all-communication-uses-ssl.md,"## Context\nEncryption of communication is used to ensure that only the intented recipient can read the information. Even when communication is intercepted, the information will still be protected by the encryption. SSL is a widely used protocol to secure communication over Internet protocols (tcp/ip).\n",We will secure all communication using SSL.\n,54,2742,54,9
operational-data-hub/0001-record-coding-guidelines.md,## Context\nWe need to record the coding standard decisions made on the ODH platform.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",19,2747,19,39
operational-data-hub/0003-create-cloud-native-solutions.md,"## Context\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\n",We will build cloud-native solutions.\n,83,2752,83,8
operational-data-hub/0020-topic-messages-have-gobits.md,"## Context\nData lineage includes the data origin, what happens to it and where it moves over time. Data lineage gives visibility while greatly simplifying the ability to trace errors back to the root cause in a data analytics process.\nBy adding tracing information to every single message it is possible to trace back a single business event to its source (and all the systems inbetween).\n",Every event published on a pub/sub topic has a gobits record added. Every applications handling/modifing or relaying the event should add a gobits record to that single business event.\n,76,2753,76,39
operational-data-hub/0014-single-confidentiality-level-per-data-component.md,"## Context\nGranularity of access determines the boundaries of control regarding protection of confidential information. For example, a bucket has [uniform bucket level access](0031-uniform-bucket-level-access-for-storage.md). Therefore, all information in the bucket should have the same confidentiality level to make sure the right access permissions are applied.\n",We will only store data of a single confidentiality level on a data component.\n,66,2754,66,16
operational-data-hub/0023-iso-8601-to-specify-date-and-time-with-timezone.md,## Context\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https://tools.ietf.org/html/rfc3339) describes the usage of the ISO-8601 standard.\n,"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store/firestore timestamps.\nAll date objects must have a time-zone included.\n",66,2755,66,69
operational-data-hub/0027-a-gcp-project-belongs-to-a-single-domain.md,"## Context\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\n",The set of functions implemented in one GCP project belongs to a single domain.\n,75,2756,75,17
operational-data-hub/0029-components-are-named-according-to-naming-conventions.md,## Context\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\n,The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\n,52,2757,52,24
operational-data-hub/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2761,16,39
operational-data-hub/0022-locations-are-specified-in-geojson.md,## Context\nA large numer of the events on the ODH contain location specific data. It is usefull to standardize the usage for location events.\n,[GeoJSON](https://tools.ietf.org/html/rfc7946) will be used for XYZ (locations) on the ODH. When XYZt data is needed [GeoJSON-events](https://github.com/sgillies/geojson-events) should be considered.\n,32,2762,32,57
operational-data-hub/0057-naming-convention-defines-uri-tags.md,"## Context\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\n",We will use [the 'tag' URI Scheme](https://tools.ietf.org/html/rfc4151).\n,55,2763,55,23
CICD-pipeline/006-homogeneous-build-across-environments.md,## Context\n* In order to develop and test their build and deployment pipeline the teams need be able to run the same build tools as the CI server offline on their machines.\n* In order to be able to trace user acceptance down to code and to enable efficient and fast pipelines we must not build the same code multiple times for different environments / configurations.\n,"* We make a **pipeline development environment** available to teams. See #34\n* The CI server only **build once** per pipeline run, so that the commit hash of the system can be used as reliable reference.\n",72,2764,72,46
CICD-pipeline/003-library-testing.md,"## Context\nWhat is our testing strategy in terms of tooling, coverage and test types.\nhow to test\n- unit tests\n- testing libs / frameworks\n- testing approaches\n- what to test\n- how to e2e test before merging / releasing\n- architecture check\n",- For every public groovy Class method there is at least one unit test\n- We aim for 100% branch coverage\n- we use JUnit 4\n- we use AssertJ\n- we use ArchUnit\n,59,2765,59,46
CICD-pipeline/004-depend-on-linux.md,## Context\nShould we depend on running in an unix environment with support for e.g. `sh` commands.\n,We depend on linux and therefore reduce the overhead of maintaining linux and windows support.\n,24,2767,24,17
CICD-pipeline/001-open-source-shared-jenkins-library.md,"## Context\n1) We are [open by default](https://github.com/baloise/baloise.github.io/blob/master/design/OpenByDefault.adoc)\n1) The myCloud API spec [is not confidential](https://github.com/baloise/CICD-pipeline/issues/15)\n1) Openness is a driver for clean, secure design and good documentation\n1) There are currently no obvious drivers for confidentiality\n",We release the Shared Jenkins Library under Apache 2.0.\n,90,2768,90,14
CICD-pipeline/008-backwards-compatibility.md,## Context\n1) We have commonly used existing Jenkins libraries\n1) We want to be able to migrate a large number of projects with minimal effort\n,We provide a compatibility layer for the current library\n,31,2770,31,10
exercise3-group-6/0004-make-repository-iterable.md,## Context\nObservationRepository and WhaleRepository both implement `Iterable<T>` to enable searching in their\nrespective lists. Since ObservationRepository and WhaleRepository both implement interface `Repository<T>`\nit may make sense to extend `Iterable<T>` in Repository rather than it's subclasses.\n,Extend `Iterable<T>` in Repository and remove `Iterable<T>` from ObservationRepository\nand WhaleRepository.\n,58,2773,58,22
exercise3-group-6/0001-create-whale-comparator.md,"## Context\nThere are a variety of whales with many attributes, thus it may be necessary to sort these whales into\nvarious different groupings. In order to sort these objects, a _function object_ must be implemented using\none of three possible designs: nested classes, anonymous classes, and lambda expressions.\n","Implement `Comparable<Whale>`, create `compareTo` default method for field `species` (Species) and a nested comparator\nclass for field `whaleId` (long) in Whale.\n",64,2774,64,43
exercise3-group-6/0003-create-two-observation-comparators.md,## Context\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\n,"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\ncomparator class for field `sightingTime` (Date) in Observation.\n",39,2775,39,44
exercise3-group-6/0002-create-repository-interface.md,## Context\nThe system has various data types that users may want aggregated together so that they are easily accessible and sortable.\ne.g. Types `Whale` and `Observation`. The system should have a consistent interface so that the user may access various\ntypes of records.\n,We decided to implement a `Repository <T>` interface that can be realised by `Whale` or `Observation` objects. Users\nwho need to access a large list/repository of Whale's or Observation's will do so through the `Repository <T>` interface.\n,58,2777,58,56
life-dashboard/20191019 GUI lib.md,## Context\nMust be free! Python GUI libs to make a fancy clean lovely UI.\n,"Questioning if it's actually worth setting up a GUI or if the python project should just look at spitting out data to be consumed by some kind of dashboard system like ""thingsBoard"". Arch thinking in next decision record. Postponing until the architectural overview is thought out!\n",19,2782,19,57
life-dashboard/20191026 arch.md,## Context\nNeed to decide on a high level structure for this app to take. It's goals are:\n- Me to learn\n- Fun times / be aspirational (as a selling point if it ever gets published seriously)\n- be a potential thing I'll publish\n- make some elements of life easier / be an enabler to life automation.\n,"""Federated syncing"". It's aspirational, one hell of a challenge, awesome for the CV. The main con for this is complexity... but this _is_ a learning project so: bring it on! Next up is to sketch out the layout of the pieces.\n",74,2783,74,56
life-dashboard/20191017 Language.md,## Context\nShould be a new language to me for learning.\nShould be an appropriate one for native desktop apps (if it works cross platform that's a bonus!)\nShould be one that improves future job options / pay ()\n,"Python. The size of the community (it's _crazy_ popular! That pretty much wins it alone), it works with GIS (Chrissy), ml/ai potential. May not be as (possibly) future focused with the whole function thing, but that'll likely be decades away and it's still doable with JS and python (I think).\n",49,2784,49,73
delayer-aws/0006-use-build-and-code-coverage-tools.md,## Context and Problem Statement\nUse build and code coverage tools to automate continuous integrated tests.\n## Decision Drivers\n*   Test automation\n*   Build automation\n*   Display badges in project repository - brings more\n,*   Test automation\n*   Build automation\n*   Display badges in project repository - brings more\n(( to be taken ))\nPositive Consequences:\n*\nNegative consequences:\n*\n,44,2787,44,41
govuk-kubernetes-discovery/0005-consistent-structure-for-terraform-files.md,"## Context\nWe should have a consistent structure for working with our Terraform files. We\nshould be able to look into a file and understand it's function is and what it\ncreates, what variables it needs to run and what it outputs after it's finished.\nWe should not have to spend time picking apart the code to understand what is\ncreated by Terraform.\nWe should not have to work with multiple files to understand a single component.\n",Create style guidelines based upon the following:\n- Header with title and description\n- Variables listed\n- Outputs listed\n- Full descriptions of what variables and outputs do in the code\n- `terraform fmt` across each file\nCreate a style guideline document explaining this structure.\n,93,2793,93,56
govuk-kubernetes-discovery/0003-using-templates-for-manifests.md,## Context\nKubernetes requires someone to write very verbose manifest files in YAML. Templating\ncan allow someone to separate the manifest and data and allow a tool to generate\nand populate manifest files with ease.\nIt allows easily understanding the values of a specific application.\nIt requires us to use a tool on top of plainly writing manifests ourselves.\n,"We will not use templates for the time-being, but will re-evaluate our use of\npopulating common values in the future.\n",72,2795,72,28
govuk-kubernetes-discovery/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2797,16,39
gorouter/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2799,16,39
akvo-product-design/ADR-001.md,"## Context\nWe need a framework that serve as base for the front-end development of\nAkvo DASH. There are several options on the market, some of them are\nJavaScript based others languages that compile to JavaScript\n(transpilers) like ClojureScript.\n","After discussing with the team and taking into account the skill set at\nhand, we have decided that JavaScript and\n[React](https://facebook.github.io/react/) it's a safer approach to\nbuild the UI. It has a large community and we can use some available\ncomponents as the base of our UI.\n",55,2800,55,65
profiles/0002-store-oauth2-clients-in-json.md,"## Context\nProfiles acts as a proxy for ORCID's OAuth2 service, and has its own list of (eLife) clients, eg Journal. This list is small and reasonably static.\n",The list of clients will be maintained in a JSON file.\n,40,2805,40,13
profiles/0001-proxy-orcids-oauth2-service.md,## Context\nWe need to allow applications (eg Journal) to use ORCID's OAuth2 service. We will need to combine data provided by ORCID with other sources in the future.\n,"We will proxy the ORCID OAuth2 service, which allows Profiles to see the access token and use ORCID's API.\n",39,2806,39,26
ski-resort-manager/0002-use-modular-monolith-architecture.md,## Context\nThis application will have severals functionnalities on severals domains. They must be independent to increase maintainability.\n,I decided to choose the modular monolith for some reasons:\n- The time to develop is shorter than microservices\n- It can evolve to microservices in the future\n- The [Modular monolith DDD](https://github.com/kgrzybek/modular-monolith-with-ddd) project is a great inspiration for this approach\n,27,2809,27,70
ski-resort-manager/0003-use-identity-server.md,## Context\nThis application need a user authentication and a fine granularity for roles.\n,"I decided to choose IdentityServer 4 because it is a mature, use by many people and well documented project.\nIt implement OpenID connect and support external authentication (Google, Facebook, ...)\n",17,2810,17,41
ski-resort-manager/0001-record-architecture_decisions.md,"## Context\nThis project is design to explore architecture, so I have to record my decisions.\n","For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\nEach ADR will be recorded using [Michael Nygard template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\n",20,2811,20,77
ski-resort-manager/0004-backend-for-frontend.md,## Context\nThis application will have 2 front end and possibly a mobile app.\nThese differents access will use common and specifics API. Somes datas will be use differently.\n,Every front end will have a specific backend to manage the differents needs.\n,38,2812,38,16
bananatabs/0005-using-environment-variable-instead-of-config-singleton.md,"## Context\nWe had a `config.ts` that exported a single `Config` object with a single `debug` boolean property.\nThis was being used in DebugUtils.tsx to control whether or not to render some extra debug information.\nOne problem was that in order to activate this we had to change the value in a file that was under version control, and then remember to revert it and never to commit it with `debug: true`.\n",We removed the `config.ts` file and used an environment variable instead. This environment variable is set is set when running a new script defined in `package.json`.\n,93,2815,93,35
openchs-adr/0004-use-spring-boot.md,## Context\nThe choice of using spring-boot is not related to any specific purpose. It just happened to be lying around.\n,OpenCHS server will use Spring boot\n,26,2819,26,9
openchs-adr/0008-use-monorepo-for-client.md,## Context\nDevelopment in health-modules often requires tandem work in openchs-android as well. It is much easier to have them in a single repo for ease of development. The repos will be set up using Lerna.\n,All client side libraries will be set up as a monorepo using Lerna.\n,45,2821,45,18
openchs-adr/0009-dos-and-donts-for-building-health-module-api-responses.md,## Context\nSome rules regarding usage of health-module apis.\n,"Input for health-modules can be anything from openchs-models\nOutput of health-modules is loose right now. There is no reason at present to fix this.\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{""highRisk"": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\n",13,2823,13,92
openchs-adr/0002-openchs-client-will-be-offline-first.md,"## Context\nOpenCHS client will be used in places of low or no connectivity. This means the application should be usable at any point in time without internet. However, data needs to be pushed to a central server for reporting, as well as for backup. This means no functionality other than sync to server should require connectivity to the server.\n",OpenCHS client should be usable offline.\n,70,2825,70,10
openchs-adr/0003-use-react-native.md,"## Context\nWhen we started OpenCHS, we wanted to choose between native development and React Native. The reasons to choose RN are\n- React-Native is mature\n- It has a comparatively lower learning curve for the team\n- Development cycles are faster compared to native\n",OpenCHS client will use react-native.\n,56,2826,56,10
openchs-adr/0011-moving-forward-forms-api-will-not-be-used-to-create-update-concepts.md,## Context\nConcepts can be created/updated using Concepts API or along with FormElement through Forms API.\nThis doubles the test cases and increases maintanence.\nWe consistently saw bugs in Forms API when creating or updating coded concepts along with it.\n,We will create Concepts through Concepts API. Forms API will refer concepts.\n,53,2827,53,15
openchs-adr/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2830,16,39
openchs-adr/0010-all-implementation-rules-will-temporarily-stay-in-the-main-health-modules-repository.md,"## Context\nWe currently don't have a good solution of having the code for implementation specific rules separate. The current solution is to add them into the health-modules repository, and switch them on or off through switches in the server.\nWe will eventually move away from the solution, but until then, this stays.\n",Implementations specific rules to stay in the health-modules repository with switching on or off of rules provided by the server\n,64,2831,64,23
kafkarator/0006-future-adrs-in-pig-repository.md,"## Context\nADRs are important documents detailing how we do things in our domain.\nOften an ADR will point to a new feature or even an entirely new service to create.\nWhen an ADR initiates the creation of a new service, where does the ADR belong?\nIn the new repo, or in the repo where the idea originated?\nWould it simply be better to collect ADRs in a central location?\n","When writing new ADRs, they will be in the [PIG repo](https://github.com/navikt/pig).\n",90,2832,90,27
kafkarator/0002-combine-topic-creation-and-credentials-management-in-same-app.md,"## Context\nThe project requires dealing with two relatively separate concerns:\n1. Create topics when needed\n2. Supply credentials for working with topics.\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\nso it makes sense to keep them in the same application.\n","We will ignore the SRP in this instance, and keep the two concerns in the same application.\n",83,2835,83,21
kafkarator/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2836,16,39
kafkarator/0004-only-for-aiven.md,"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n","Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n",70,2837,70,25
pupperware-party/0002-use-semver-for-versioning.md,"## Context\nHow software is versioned conveys a lot of meaning to the end user. If you have a consistent, known way of versioning softeare, the end users can plan for upgrades and have a better overall expereince, and not be suprised by breaking changes.\n",Use [SemVer 2.0.0](https://semver.org/spec/v2.0.0.html) for the guidelines of version numbers for this project\n,61,2846,61,35
pupperware-party/0006-installation-and-config-file-locations.md,"## Context\nSoftware needs to be installed in order to be useful, and also normally needs config files. Where to put these and where to install to can have an impact on user experience.\n","We will follow the example set by Puppet, since this software is part of that ecosystem\n",39,2847,39,18
pupperware-party/0003-use-readme-driven-development.md,"## Context\nDocumentation for a project is critical - without it, you don't know if a project is ""complete"", or as a user, how to use it.  However, too much documentaion can delay the project, or lead to a good implementation of the wrong thing. This process, and the reasoning is much better explained at [this blog post](http://tom.preston-werner.com/2010/08/23/readme-driven-development.html)\n",This project will be developed using the readme driven decelopment process.\n,97,2848,97,16
pupperware-party/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2851,16,39
register-with-a-gp-beta-web/0005-add-synthetic-max-age-to-session-cookie.md,## Context\nin [ADR#4](0004-cookies-encryption.md) we decided to use session cookies but\ndidn't address the problem of sessions lasting an infinitely long time if the\nuser doesn't ever close their browser (all tabs).\nA user could abandon their registration and then return months or years later to register again and we'd have their information pre-populated. This will be extremely disconcerting for the user.\n,"Using middleware, add an extra key to the session cookie called max-age. If the middleware sees a cookie with a max-age < now then it will delete that cookie. We can set to a long time like 4 hours for now and reduce as we continue to learn about our users' behaviour.\n",92,2854,92,61
register-with-a-gp-beta-web/0002-use-hapi-js-framework-for-http.md,## Context\nAs we start to standardise on what our services look like we want to use the\nbest tools for the job when it comes to node web frameworks. Hapi is more of a\nframework than [express] and is more modular allowing many developers to work on\na single project without trampling over each others' routes.\n,We have decided to use [hapi] as our http server.\nThere are many other [alternatives] but none are as actively developed as Hapi\n,70,2855,70,33
register-with-a-gp-beta-web/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,2856,16,39
protagonist/0001-composite-handler-design.md,"## Context and Problem Statement\n### Context\nA design for the implementation and delivery of a component of the DLCS\nwhich can process ""composite"" artifacts. Such artifacts typically take\nthe form of a single object containing multiple embedded resources, each\nof which can be rasterized and treated as an individual resource within\nthe DLCS.\n","Delivery of a standalone component which consumes a DLCS ingest payload\ntailored to ""composite"" artifacts, and triggers an ingestion workflow\nwhich rasterizes each of the constituent parts and invokes the existing\nDLCS ingestion workflow.\n![0001-flowchart.png](images/0001-flowchart.png)\n",69,2859,69,63
protagonist/0000-api-project-design.md,## Context and Problem Statement\n### Context\nHow best can we structure the API project to make it easy for developers to navigate and comprehend.\n,"""Organise by Feature and use MediatR"", because we can use MediatR to encapsulate the various use-cases of the API. Together with feature folders this should allow developers to more easily identify the capabilities of the API and target the section they need to.\n",29,2860,29,56
james-project/0033-use-scala-in-event-sourcing-modules.md,"## Context\nAt the time being James use the scala programming language in some parts of its code base, particularily for implementing the Distributed Task Manager,\nwhich uses the event sourcing modules.\nThe module `event-store-memory` already uses Scala.\n","What is proposed here, is to convert in Scala the event sourcing modules.\nThe modules concerned by this change are:\n-  `event-sourcing-core`\n-  `event-sourcing-pojo`\n-  `event-store-api`\n-  `event-store-cassandra`\n",50,2862,50,60
james-project/0011-remove-elasticsearch-document-source.md,"## Context\nThough very handy to have around, the source field does incur storage overhead within the index.\n",Disable `_source` for ElasticSearch indexed documents.\n,22,2863,22,11
james-project/0005-distributed-task-termination-ackowledgement.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\n","* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\n",54,2867,54,54
james-project/0012-jmap-partial-reads.md,"## Context\nJMAP core RFC8620 requires that the server responds only properties requested by the client.\nJames currently computes all of the properties regardless of their cost, and if it had been asked by the client.\nClearly we can save some latencies and resources by avoiding reading/computing expensive properties that had not been explicitly requested by the client.\n","Introduce two new datastructures representing JMAP messages:\n- One with only metadata\n- One with metadata + headers\nGiven the properties requested by the client, the most appropriate message datastructure will be computed, on top of\nexisting message storage APIs that should remain unchanged.\nSome performance tests will be run in order to evaluate the improvements.\n",73,2881,73,71
james-project/0009-disable-elasticsearch-dynamic-mapping.md,"## Context\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\nThis causes a serie of functional issues:\n- Maximum field count can easily be exceeded\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\n",Rely on nested objects to represent mail headers within a mapping\n,96,2884,96,13
james-project/0009-java-11-migration.md,"## Context\nJava 11 is the only ""Long Term Support"" java release right now so more and more people will use it exclusively.\nJames is known to build with Java Compiler 11 for some weeks.\n",We adopt Java Runtime Environment 11 for James as a runtime to benefits from a supported runtime and new features\nof the languages and the platform.\n,44,2885,44,30
james-project/0004-distributed-tasks-listing.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\n",* Read a Cassandra projection to get all `Task`s and their `Status`\n,32,2891,32,17
james-project/0008-distributed-task-await.md,"## Context\nBy switching the task manager to a distributed implementation, we need to be able to `await` a `Task` running on any node of the cluster.\n",* Broadcast `Event`s in `RabbitMQ`\n,35,2900,35,12
james-project/0007-distributed-task-cancellation.md,## Context\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\n,* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\n,32,2911,32,65
banking-cqrs-es-go/0002-use-adr-tools-to-manage-the-adrs.md,## Context\nA tool to make creating these ADRs helps reduce friction around creating documentation.\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https://github.com/npryce/adr-tools)\n,Install `adr-tools` locally and use it to help quickly generate consistent ADRs\n,56,2914,56,18
banking-cqrs-es-go/0003-drive-development-with-tdd.md,"## Context\nThe goal of this project is to build out a small, demonstration CQRS and Event Sourced application.\nTest Driven Development is an excellent way to keep the development cycle short and on track whilst naturally keeping test coverage high.\nThis project is also a learning tool that is helping me get familiar with the Go programming language.\n",Allow the tests to drive the development of this application.\n,70,2915,70,12
banking-cqrs-es-go/0005-avoid-short-variable-names.md,"## Context\nIdiomatic Go calls for ""short, descriptive"" variable names, which is fine, but I abhor needlessly short variable names.\nCode is for humans to read and understand. Having single character variable names is rarely helpful. Reducing the cognitive load on software engineers trying to understand what code is doing should be one of the top priorities of any shared codebase.\n","In this project, single character variable names will typically only be used for temporary loop variables (e.g. i, j for indexes).  Variable names will be as short as possible but not at the cost of not being descriptive enough.  Exceptions are fine.\n",79,2916,79,53
banking-cqrs-es-go/0004-explore-using-oop-and-ddd-with-go.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nI am comfortable writing Object Oriented code and using the Domain Driven Design tactical design patterns.\nI would like to leverage these approaches for this Go project and see how much my thinking has to change to adapt to the strengths of the Go programming language.\n",Use OOP and DDD tactical patterns where possible.  Recognise when to adapt and change if necessary.\n,73,2917,73,23
banking-cqrs-es-go/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2918,16,39
origin-frontend-challenge/0002-testcafe-for-end-to-end-tests.md,## Context\nThe project needs some framework to test end-to-end cases.\n,Decied to use [Testcafe](https://devexpress.github.io/testcafe/) as a main framework to end-to-end tests.\n,16,2919,16,30
aspan/0002-aspan-router.md,## Context\nRouting is required for Apspan Client applicatoin. I am not using a location bar to navigate between pages. Threfore React Router is not an option.\n,I will implement custom router with GraphQL based state.\nRouter will control type of the main screen component based on state variables.\nApplication starts with a Folder component showing root folder content. Possible successors - Image and MetaData. From Image user can go further to MetaData or back to Folder containing that image.\nTransition from Folder to Image:\n{\nscreen: Folder\nid: ID\n}\n{\nscreen: Folder\nid: ID\n}\n,37,2920,37,94
aspan/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2921,16,39
SAP-Cloud/ignore-end-to-end-tests-on-non-produtive-branches.md,"## Context\nSAP S/4HANA Cloud SDK Pipeline can execute end-to-end tests, which simulate how a human would test the application.\nEnd-to-end tests tend to run quite long, which might impede how fast pull requests can be merged.\n### Decision\nWe allow to skip running end-to-end tests on non-productive branches.\nWe do not allow skipping them on the productive branch.\nThis feature is disabled by default.\n",We allow to skip running end-to-end tests on non-productive branches.\nWe do not allow skipping them on the productive branch.\nThis feature is disabled by default.\n,93,2929,93,36
deeplearning4j/0003-dealing_with_inconsistencies_in_java_naming.md,## Context\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\n`BernoulliDistribution`.\nTwo possible solutions where suggested:\n1. Add an additional property that provides us with the correct class name\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\n,For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\nname.\n,92,2934,92,27
deeplearning4j/0004-auto_initialization_for_inplace_operations.md,"## Context\nSome operations work in-place on the inputs that they are given in ND4J, but in SameDiff the same operations will\ngenerate an array from a given shape. Examples for this include `BernoulliDistribution`, and other random ops, that\neffectively initialize the array that they are given.\nFrom a consistency point of view, it would be nice if both API's would support both ways of using those ops.\n","We introduce an option to mark inputs as `inPlace = true` to make it clear that this input is going to be changed\nin-place. In addition we introduce an option `supportsInPlaceInit = true` to mark an input as initialize-able. If the\n`supportsInPlaceInit` option is enabled, two signatures for the Op will be created, one that takes an input, and one\nthat takes the appropriate shape and data type information in its stead.\n",92,2946,92,98
pcmt/adr-004.md,"## Context\nWhile PCMT's basic unit of deployment is Docker and Docker-Compose\n([ADR #2](adr-002.md)), and Terraform is able to provision a cloud environment\n([ADR #3](adr-003.md)), we still need to address a gap where the computing\nenvironment needs to be provisioned and client configuration needs to be loaded.\n","We will use ready-made Ansible roles to install the latest versions of\nDocker and Docker-Compose, utilized through Terraform.\nWe will build PCMT Ansible role's that will serve as a template and are\navailable in the Ansible Galaxy repository.\nConfiguration of a PCMT instance will be managed as part of an Ansible Playbook.\n",75,2954,75,74
pcmt/adr-002.md,"## Context\nThe need to deliver PCMT in such a way that it's possible for interested\norganizations to provide it at a reasonable cost in a SaaS model, as well\nas for government organizations to have control over their tools and data by\ninstalling it on-prem calls for a containerization tool that has wide adoption\nand which can run in the major host operating systems.\n",We will provide each component of PCMT as a production ready docker image.\nWe will utilize docker-compose to orchestrate the docker containers that a\nsingle PCMT instance requires.\n,80,2955,80,37
manuela/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2972,16,39
aoide-rs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,2984,16,39
sre-challenge/0004-using-aws-ecs-as-platform.md,## Context\nI want to use docker at AWS but I don't want to provision a docker\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\na cluster.\n,Done.\n,42,2997,42,3
sre-challenge/0006-use-prometheus-as-monitoring-service.md,"## Context\nThis was a clear goal of the challenge: to observe the solution.\nAs etcd has a metrics endpoint with the structure supported by prometheus,\nit's a logical solution to use it.\nPrometheus has several discovery services that can help in a cloud environment.\n- static target file\n- service discovery modules\n- sd_ec2: plugin to discover AWS instances.\n",I have tested the sd_ec2 plugin and it works fine.  I prefer to implement it\nrather than trying to implement a provisioner in the configuration management\nsystem that modifies the static target file to add and remove instances.\n,80,3001,80,47
sre-challenge/0007-use-a-dashboard-for-etcd.md,"## Context\nTo visualize the metrics of the etcd cluster, I would like to implement a\ndashboard using the USE and RED methods.\n- USE to measure the performance of the system hosting the etcd cluster\n- RED to measure the performance of the gRPC side of the etcd cluster\nBut I haven't found anything like this and I have no time to waste.  So\nI have found a dashboard on the grafana site that has some metrics.\n","As I'm out of time, I'm just going to implement it at the configuration\nmanagement system, to automatically provision the dashboard and the\ndatasource.\n",97,3004,97,34
sre-challenge/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3006,16,39
embvm-core/0009-event-driven-framework-design.md,"## Context\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\n* We can reduce the number of threads used by relying on event-driven behavior\n","The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register/unregister for events).\nDispatch queues will be provided to assist with the event driven model.\nPlatform examples will default to dispatch-based processing models.\n",66,3007,66,52
embvm-core/0010-dispatch-callbacks.md,"## Context\nBecause we are building an [event-driven framework], we need to think about how to handle callback functions. We want to use them without causing threads/functions to block unexpectedly while executing callbacks.\n",We will dispatch callbacks to a global dispatch queue which will execute them as processing time is available.\n,41,3009,41,20
embvm-core/0005-provide-non-blocking-interfaces.md,"## Context\nBlocking interfaces are those that only return when an operation has been completed.\nNon-blocking interfaces initiate a request and return when the request is submitted. Completion is annouced via a callback or by checking the status through another interface.\nNon-blocking interfaces provide a more flexible implementation route.\nBlocking implementations are typically avoided on embedded systems. Typically, a thread should sleep while waiting for an action to complete, rather than hogging processor resources.\n",Interfaces provided by the framework will be non-blocking. Users can write their own blocking wrappers if blocking code is needed.\n,92,3014,92,24
embvm-core/0003-no-dynamic-memory-allocation-in-core.md,"## Context\nMany systems, teams, and designs require that the system will not utilize dynamic memory allocation. We should maintain a flexible design by allowing users to use dynamic memory allocation if they desire. However, we should be able to support the strictest operating model for maximum flexibility and potential use of the framework.\n",No dynamic memory allocation will be utilized by framework core components\n,63,3015,63,12
embvm-core/0015-use-embvm-top-level-namespace-for-core-interfaces.md,## Context\nAll of the framework code was placed into the global namespace. We need to find a home to group our code in.\n,The `embvm` (Embedded Virtual Machine) namespace will be used for framework constructs.\n,28,3016,28,19
embvm-core/0019-virtual-platform-takes-in-thwplatform-type.md,"## Context\nAs a consequence of [ADR 0018](0018-driver-registration-in-hw-platform.md), we moved the [Driver Registry](../components/core/driver_registry.md) definition to the hardware platform and removed the global singleton from the platform. We also want the platform APIs to forward to the hw platform. However, we needed a way to access the hw platform object for successful forwarding. This requires the platform base class to know about the type.\n","The hardware platform type is now a template parameter for the Virtual Platform base class. A local variable will be declared (`hw_platform_`), and a `protected` API will be provided to access that variable as well.\n",94,3017,94,45
embvm-core/0012-switch-to-catch-for-unit-testing.md,"## Context\nWe started unit testing with doctest. However, when trying to compile with exceptions disabled, it seems to be a problem.\nCatch provides support for compiling with -fno-exceptions, and even lets us configure the termination handler.\n",We're proposing to use Catch for unit testing. But we need to actually try it out firs.t\n,51,3018,51,22
embvm-core/0013-use-templates-to-switch-between-dynamic-and-static-memory.md,## Context\nWe want to build the framework components to work with both dynamic and static memory allocation schemes.\n,"Rather than duplicating implementations, we will use Template Metaprogramming to use a single structure which supports both static and dynamic memory allocation. Classes can be templated with a size parameter. This size is evaluated to determine the underlying data structure that will be used. If the size is equal to 0, dynamic memory is assumed. Otherwise, static memory allocation will be used.\n",22,3020,22,77
embvm-core/0006-differentiate-drivers-and-hal.md,"## Context\nProcessor peripherals often require additional interfaces and information that generic peripheral interfaces don't. For instance, we need to initiate our ARM chip's SPI driver with an SFR base address and some configuration options. If I am writing an accelerometer attached to the SPI bus, I only care about read/write and providing my CS address. I don't need any knowledge of the processor details for the SPI bus.\n",We will provide two sets of interfaces:\n* HAL (drivers) which manage low-level processor hardware devices\n* Drivers with generalized interfaces which abstract away the low-level details\n* The software layer doesn't need to know about the clock settings or location of the SPI0 SFRs\nThe platform and software layers will interact with the generic drivers for application features. The platform layer will create the generic drivers with references to the processor HAL.\n,82,3024,82,89
embvm-core/0011-generic-startup-library.md,"## Context\nSystems go through a predictable set of boot behaviors, which we can commonize:\n* Relocate\n* zero data in the .bss section\n* Call constructors\n* Initialize RTOS\n* Set default hardware pin states\n* Create drivers\n* Configure system\n","We will provide a generic startup library which manages the initial boot sequence in a predictable manner.\nThe library will gain control from the processor startup sequence and transition the boot control process to the platform layer. It will call pre-defined platform APIs to initialize the RTOS, board, etc.\nWe will call the same order ever time, but allow the platform to define actions that occur at each stage.\n",58,3025,58,80
embvm-core/0020-hardware-platform-options-file.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","We will create a new required `hw_platform_options.hpp` file. This file is defined in the platform level, and can be used to configure any necessary compile-time parameters without the use of templates.\n",21,3026,21,41
embvm-core/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3028,16,39
reaction-component-library/0005-test-components.md,"## Context\nOur React Components need to be well tested. At a minimum:\n- Snapshots of what is rendered for the most common props, to see when the output changes and confirm that it was intentional.\n- Test that all function props are called at the proper time with the proper arguments, as documented.\n- Generate a coverage report to prove that everything is tested.\n","Use Jest.\nIt is popular, backed by Facebook, runs tests in parallel and reruns only changed tests, has built-in coverage, mocking, and `expect` patterns, has a snapshot feature, runs well on CI, and is built on Jasmine, which is battle tested.\n",78,3034,78,58
register-a-food-business-front-end/0004-use-create-react-app-and-npm-eject-to-create-the-basis-of-the-project-structure.md,"## Context\nReact projects can be created from scratch, by setting up webpack scripts, installing dependencies etc. or they can be set up to a predetermined configuration using the create-react-app CLI.\n","We will use create-react-app to generate a project scaffold using Facebook-defined best practices.\nWe will use the `npm run eject` command to make full project configuration available to us, such as Webpack.\n",39,3036,39,43
register-a-food-business-front-end/0002-implement-open-source-govuk-react-components-library.md,"## Context\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\n",We will implement the open source govuk-react npm package hosted at [https://github.com/penx/govuk-react](https://github.com/penx/govuk-react) wherever it has a component that fits our needs.\nWe will create custom components and layouts where necessary to follow the GDS design language.\nWe will contribute back to the project to ensure it fulfils all of our needs.\n,41,3037,41,86
register-a-food-business-front-end/0003-implement-cssinjs-as-default-approach-to-styling.md,"## Context\nThe govuk-react components library uses CSSinJS with Emotion for styling. Our approach to styling needs to be efficient and maintainable, and this could be achieved either through a preprocessor like SCSS/SASS, or by using CSSinJS.\n","We will use CSSinJS with the [Emotion library](https://emotion.sh/), with SCSS available as a fallback should it be necessary.\n",55,3038,55,32
register-a-food-business-front-end/0007-switch-from-create-react-app-to-next-js.md,"## Context\nReact can be implemented from scratch, using the `create-react-app` CLI, or by using a 3rd-party framework such as Next.js.\n",We will re-start the scaffold.\nWe will use Next.js as the basis for this project.\nWe will extend and customise Next.js to fit our more bespoke requirements.\n,34,3039,34,37
register-a-food-business-front-end/0006-implement-eslint-with-default-create-react-app-and-prettier-rules-and-scope-for-additional-rules.md,"## Context\nJavaScript linting is required to enforce consistency of code style and best practices. There are different options for linting tools but ESLint is the most common. Within ESLint, there are many options for rules, plugins, etc.\n",We will use ESLint as our linting tool.\nWe will extend the `react-app` and `prettier` linting rules.\nWe will use the Prettier 'recommended' config to disable any conflicting rules and enforce Prettier as the standard.\n,50,3040,50,57
register-a-food-business-front-end/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3041,16,39
register-a-food-business-front-end/0005-implement-yarn-as-a-cli-for-node-package-management.md,## Context\nYarn and NPM can both manage the Node packages for a project. Recent updates to NPM mean that Yarn only has a negligible performance advantage over NPM.\n,"We will use `yarn`, `yarn start`, `yarn add`, `yarn remove` etc. for the management of Node packages in our project.\n",38,3042,38,35
rufus/0000-rufus-decision-records.md,"## Context and problem statement\nThere are many decisions to make in the design of Rufus, and each one has many\ntradeoffs to consider. We want to record the context around important language\ndecisions, both as part of the design process and as documentation for users.\n","* A decision record must clearly describe the context that motivates a solution,\nthe options considered, and the decision made.\nChosen option: a subset of the MADR template, because it's the most easily\nadaptable to our needs. Rufus decision records are assigned an RDR-_nnnn_ ID\nwhen they're accepted. See [RDR-0001](0001-template.md) for a template.\n",58,3043,58,90
signals-frontend/0007-typescript.md,## Context\nCurrently TypeScript is not being used. For typing of component props PropTypes is being used.\n,Making use of typing will make our code more readable and more reliable. Furthermore we don't need to use the PropTypes dependency anymore. TypeScript's interfaces and types are more reusable and adaptable than PropTypes.\nTransition to TypeScript will be done gradually.\n,21,3052,21,49
otm-docs/simulator_description.md,## Context\nApi need to receive vehicles locations informations. Simulator will generate vehicle movement for testing purposes.\n,Team decide to make separate simulator service.\n,21,3056,21,9
otm-docs/api_description.md,## Context\nApi will be responsible for administering vehicle timetables. It will use map service for real time vehicle tracking and will recive informations about it localizations from any source.\n,"Team decide to make API wich allows to organize timetables, receive vehicles localizations and send informations to front-end api.\n",38,3057,38,26
otm-docs/api_architecture.md,"## Context\nBy using the correct architecture, the application will be flexible and legible.\n",REST Api\n,19,3058,19,3
otm-docs/map.md,## Context\nThe choice of database should depend on the information that the database will store.\n,Mapbox\n,19,3059,19,3
otm-docs/containerization.md,## Context\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\n,Docker\n,24,3060,24,3
otm-docs/database.md,## Context\nThe choice of database should depend on the information that the database will store.\n,PostgreSQL\n,19,3061,19,3
otm-docs/api_framework.md,## Context\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\n,Laravel 7\n,27,3062,27,5
otm-docs/api_language.md,## Context\nChoosing proper programming language is the primary choice for new project.\n,PHP 7.4\n,16,3063,16,6
otm-docs/simulator_language.md,## Context\nSimulator should do multiple calculations in a short time. Choice should be dictated by the speed of code execution.\n,Golang\n,26,3064,26,3
terraform-aws-s3-static-website-module/0003-optionally-create-route53-resource.md,## Context\nIt cannot be assumed that users of this module would manage DNS in AWS or that users may have a cross account design that prevents access to the hosted zone.\n,Provide a means to create the custom Route53 resource as an option\n,35,3065,35,14
terraform-aws-s3-static-website-module/0004-dual-support-for-terraform-version-0-11-and-0-12.md,"## Context\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\n",This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\n,46,3066,46,73
terraform-aws-s3-static-website-module/0002-designed-to-require-a-tls-certificate-dependency.md,## Context\nAs https connections have become the standard for web connections this module needed to implement a TLS connection. An abstraction of how certificates and DNS is managed be users of this module needed to be abstracted away.\n,"While this module may have created a certificate resource, it was decided the use cases of any DNS management and certificate providers was too vast and that this module will take a certificate arn as a dependency.\n",44,3067,44,41
terraform-aws-s3-static-website-module/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3068,16,39
moneycount-api/003-Document_API_With_Swagger.md,"## Context\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\n","I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\n",67,3069,67,43
moneycount-api/004-Use_Heroku_As_Deployment.md,"## Context\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\n","I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\n",30,3070,30,54
moneycount-api/002-Choose_Persist_Currency_In_Json.md,"## Context\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\n","I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\n",44,3071,44,60
moneycount-api/001-Choose_Spring_Boot.md,"## Context\nI have to choose a framework to implement moneycount-api project. It could be a familiar framework, such as Spring Boot, Spark, Servlets + Jersey or even Python + Flask, or I could try another different framework to learn something new.\n","I decided to use Spring Boot 1.5.10, the last stable 1.X release at the time, because I want to have a first version of the software in a well known framework that allows me to implement it fast, with easy integration with other features that I can choose in future improvements.\n",53,3072,53,63
atlasdb/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3098,16,39
racing-tips/0002-use-kubernetes-and-docker.md,"## Context\nA lot of options exist to create applications nowadays - serverless, PaaS, SaaS etc. The purpose of this project is to hone Kubernetes and Docker skills in particular which may mean use of these technologies seem over engineered.\n",* Use Kubernetes and Docker\n* Use Amazon EKS\n,49,3118,49,12
racing-tips/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3119,16,39
publicservice-registry/0009-use-more-value-objects.md,"## Context\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\n",Use a Value Object wherever possible.\n,96,3124,96,8
publicservice-registry/0012-use-dutch-resource-names.md,"## Context\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\n",We will use Dutch terms because it is an external requirement dictated by the organisation.\n,56,3126,56,17
publicservice-registry/0010-do-not-use-clr-type-names-for-event-types.md,"## Context\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\n",Use a dictionary/map between your message types and the CLR type you want to deserialize your message into.\n,57,3129,57,21
publicservice-registry/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n### Changes to Nygard's approach\nWe will keep ADRs in the project repository under `docs/adr/NNN-explanation-of-adr.md`.\n",16,3130,16,74
pul_solr/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3131,16,39
pul_solr/0002-increase-zookeeper-timeout.md,"## Context\nA ZooKeeper timeout last night brought several Solr servers down. Factors that might make servers unresponsive\nare garbage collection, network traffic/latency, and high amounts of disk I/O.\n",Increase the ZooKeeper timeout from 15 seconds to 60 seconds.\n,43,3133,43,15
pul_solr/0005-catalog-configset-rotation.md,"## Context\nSometimes we make a change to the catalog config set that will break search results if it's deployed without a new index already in place. In these cases the config set in use cannot be updated with this change and we have to deploy a different config set for the reindexing collection, which is then swapped in once it's fully populated.\n","For most config changes we will continue updating and reloading the config set in production.\nIn the cases described above we will copy a new config set from the most recent production config set, incrementing the version suffix. Once the index is created and swapped in, the previously-used config set should be deleted.\n",72,3134,72,62
modernisation-platform/0012-use-tgw-route-analyzer-to-check-desired-state-for-route-tables.md,"## Context\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence..\n","[Transit Gateway Route Analyzer](https://docs.aws.amazon.com/vpc/latest/tgw/route-analyzer.html) is an AWS tool allowing the analysis of routes in Transit Gateway Route tables. It analyzes the routing path between a specified source and destination, and returns information about the connectivity between components. It is useful in validating and troubleshooting configuration. As such, it could be used to assess the desired state for transit gateway route table configuration, providing feedback on issues.\n",65,3137,65,94
modernisation-platform/0002-use-iam-federated-access.md,"## Context\nThe Modernisation Platform will be used by a large group of people across the Justice Digital and Technology estate, and each person will need their access to the Modernisation Platform managed.\n","Rather than managing the administrative burden of the Joiners, Movers and Leavers (JML) process ourselves, we can automate this for the team by utilising IAM Federated Access to allow access management through an identity provider that is already managed within Ministry of Justice. For example, we can use GitHub, which is included as part of the department's JML process, as an identity provider for access to our AWS account.\n",39,3140,39,87
modernisation-platform/0008-use-kms-in-shared-services-for-cross-account-encryption.md,## Context\nMember account users will need to share encrypted things such as backups and snapshots between AWS accounts.\n,"We've decided to use [AWS Key Management Service (KMS)](https://aws.amazon.com/kms/) for cross account encryption.\nKeys will be created per business unit as standard, if users require application level keys we will create these as and when needed.\n",22,3141,22,55
modernisation-platform/0006-use-a-multi-account-strategy-for-applications.md,"## Context\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\n","We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https://user-guide.modernisation-platform.service.justice.gov.uk/concepts/environments/).\n",44,3147,44,48
modernisation-platform/0005-use-github-actions.md,"## Context\nBefore we start automating any part of the Modernisation Platform, we should define our CI/CD runner.\n","We have decided to use GitHub Actions as our CI/CD runner due to the following:\n- we don't have to roll our own infrastructure for it\n- we get unlimited free running minutes on all of our public repositories\n- it offers centralised CI/CD alongside our code storage\n- it helps us meet our goal of working in the open\n- other teams within Ministry of Justice are moving toward GitHub Actions themselves, so we can align ourselves\n",26,3148,26,92
modernisation-platform/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md,"## Context\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\n","[VPC flow flows](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\n",65,3150,65,98
test-assignments/0001-id-type.md,## Context\nWe need to decide what data structure to use as the identifier for domain entities.\n,We will use a [GUID](https://msdn.microsoft.com/en-us/library/system.guid%28v=vs.110%29.aspx?f=255&MSPPError=-2147217396) type.\n,20,3157,20,44
test-assignments/0002-command.md,## Context\nWe need to decide the format and semantics of a command type within the actor system.\n,We will use a regular class for a command. The data of a command is added as properties with public getters and setters. A command should be handled as an immutable type.\n,21,3158,21,36
hmpps-interventions-service/0002-represent-validation-errors-at-field-level.md,"## Context\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\ninvalid client requests.\n","We will use field-level error validation.\nWe will use meaningful codes per field.\nExample:\n```json\n{\n""status"": 400,\n""error"": ""validation error"",\n""message"": ""draft referral update invalid"",\n""validationErrors"": [\n{\n""field"": ""serviceUser.crn"",\n""error"": ""FIELD_CANNOT_BE_CHANGED""\n}\n]\n}\n```\n",31,3159,31,91
hmpps-interventions-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3160,16,39
opg-data-deputy-reporting/0002-reports-unique-reference.md,"## Context\nIn this integration, Digideps requires Sirius Public API to expose a unique identifier for a successfully created report.\nThis ID is saved by Digideps and used when creating supporting documents against a report\nReports are generally one-per-year-per-donor, but in certain circumstances multiple reports may be generated which cove rthe same reporting year. Both versions are saved in Sirius and need to be uniquely referencable.\n",Sirius Public API will expose a UUID for each report that is created via the Deputy Reporting Integration\n,88,3163,88,20
opg-data-deputy-reporting/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3164,16,39
heroes-board-game/0003-broadcast-messages.md,"## Context and Problem Statement\nHeroes can kill nearby enemies on a certain range, but there is no central place in the game that knows every hero position on the board at any time. Each one of them is the source of truth.\nIn order to do that a hero must broadcast the intent to the rest of players.\nHow to send messages to all heroes at once?\n## Decision Drivers\n* Less possible changes\n* Performance is not a concern\n","* Less possible changes\n* Performance is not a concern\nChosen option: ""`Supervisor.which_children/1`"", because it comes out best.\n",93,3165,93,32
heroes-board-game/0002-tests-definition-and-scope.md,## Context and Problem Statement\nAcceptance tests are embedded into game server application.\nShould they be written one layer above (UI)?\n## Decision Drivers\n* Acceptance tests should be meaningful for end users\n,"* Acceptance tests should be meaningful for end users\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https://www.simpleorientedarchitecture.com/defining-test-boundaries/).\n* Good, because it promotes different tests for different audiences\n* Good, because it sets a clear terminology\n* Good, because end users don't care about layers below UI\n",44,3166,44,83
heroes-board-game/0001-temporary-heroes.md,"## Context and Problem Statement\nHeroes are now permanent, which means they are always restarted.\nShould they be?\n## Decision Drivers\n* A Hero must not exist on the system once the player leaves the game (normal exit)\n* Restoring them from failures is not a requirement but nice to have\n* Project is not finished, lacks most important features\n","* A Hero must not exist on the system once the player leaves the game (normal exit)\n* Restoring them from failures is not a requirement but nice to have\n* Project is not finished, lacks most important features\nChosen option: ""Restart `:temporary`"", because it has the minimum impact.\n",74,3167,74,65
heroes-board-game/0000-decouple-game-from-web-server.md,## Context and Problem Statement\nI want to create a multiplayer web game.\nWhat structure this project should have?\n## Decision Drivers\n* Software architecture is important\n* Clear separation of concerns\n* Testable code\n,"* Software architecture is important\n* Clear separation of concerns\n* Testable code\nChosen option: ""Stand-alone OTP application for game business logic"", because\n* Good for business domain isolation\n* Ignoring web concerns at the beginning helps thinking about game API\n* Possible different clients (command line, web) makes complex interaction tests easier\n* Different releases: bundled together with the server or one game application for many servers\n",45,3168,45,87
tul_cob/0001-use-feature-flags-to-toggle-features.md,"## Context\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\n",We've decided to implement very simple feature flags that can be toggled with environment variables.\n,49,3171,49,19
service-stac/2020_10_01_authentication.md,## Context\n`service-stac` will be accepting machine-to-machine communication and will have an admin interface for operations/debugging. Authentication methods for this two use cases need to be defined.\n,"Machine-to-machine communication will be using token authentication, access to the admin interface will be granted with usernames/passwords managed in the Django admin interface. At a later stage, this might be changed to a more advanced authentication scheme.\n",39,3172,39,46
service-stac/2020_10_21_static_asset.md,"## Context\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\n","Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\n",91,3173,91,80
court-hearing-event-receiver/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3176,16,39
xebikart-dashboard/001-use-SSE-as-exchange-data-technology-between-back-and-front.md,"## Context and Problem Statement\nWe would like to create a reactive Dashboard for the xebiKart. As a leaf component we will be noticed of event occurred on other component, like the car.\nIn order to be reactive, we must find a way to notify the front of event come from back.\n## Decision Drivers\n* An open source standart\n* Esay to use in Javascript or in a backend side\n* Limit change to done on network\n","* An open source standart\n* Esay to use in Javascript or in a backend side\n* Limit change to done on network\nChosen option: ""[WebSocket]"", because it's a standart provide by HTTP2, supported in major language, easy to provide with network.\n",96,3178,96,58
react-transcript-editor/2018-11-20-save-to-server.md,## Context and Problem Statement\nHow should the system handle saving the data to a server API end point?\nshould it be a responsibility fo the component or not?\n## Decision Drivers\n* easy to reason around\n* clean interface\n* flexible to integrate and use component on variety of settings\n* un-opinionated in regards to the API end point and how to make that request\n,* easy to reason around\n* clean interface\n* flexible to integrate and use component on variety of settings\n* un-opinionated in regards to the API end point and how to make that request\n- component returns content in default draftJS format or in variety of supported adapters/converters\n- saving to server is done outside of the component to allow more flexible integration within other contexts\n,79,3189,79,79
aspan_micro_front_ends/0002-using-react-v16-8-0.md,## Context\nSome libs from Apollo stack require react 16.8.0.\n,To be consistent and maintain one react version accross application I will use react 16.8.0.\n,18,3190,18,23
aspan_micro_front_ends/0003-replacing-ramda-with-lodash.md,## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n,Decision here...\n,22,3191,22,4
aspan_micro_front_ends/0002-derived-attributes.md,## Context\nDerived attributes are read-only.\n,"Following attributes are derived from file system:\n1. name - file name without extension\n2. contentType - file extension without dot, lowercased\n",10,3192,10,30
aspan_micro_front_ends/0001-aspan-will-support-multiple-repositories.md,## Context\nContext here...\n,Decision here...\n,7,3193,7,4
aspan_micro_front_ends/0004-info-graphql-import.md,## Context\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\n,Decision here...\n,20,3194,20,4
aspan_micro_front_ends/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3195,16,39
py-ddd-cqrs-microservice-boilerplate/004-license_checker.md,"## Context\nWorking with OpenSource software for commercial purposes requires\nevery developer to check the licenses of the packages that he uses.\nSome licenses might be not-compatible with the project needs and\ncause some legal issue if not properly handled.\nUsing open-source libraries speeds up development in a\nsignificant way but has a drawback, nested dependencies.\nEach open-source library could depend on further open-source libraries\nthat could have different licenses for usage and re-distribution.\n",Given the previous concerns I suggest to use an automated\nlicenses checker on the installed packages.\nIn python you can use `pip-licenses` (which is distributed with MIT license)\n,96,3196,96,38
py-ddd-cqrs-microservice-boilerplate/001-pre-commit-hooks.md,## Context\nAdding some tools to the chain might cause developers to forget to run\nthem at every commit.\n,To prevent this from happening we set-up pre-commit hooks using\npython `pre-commit`: https://pre-commit.com/.\nThis will give us a way to specify our pre-commit hooks as a `.yaml`\nfile and version them in the repo without the need of installing hooks in every machine.\n,23,3197,23,62
py-ddd-cqrs-microservice-boilerplate/002-code_formatter.md,"## Context\nWhen joining a new team, people coming from different environments might be\nused to different styles in writing code.\nThis might seem trivial and also irrelevant but it actually affects a lot\nthe productivity of a team in at least two ways:\n1. A developer might write the code in its own style\n2. A developer might discuss the code style of another developer\n","In order to avoid this confusion and to promote a consistent style I propose\nto use an automatic code formatter, in this case `black`: https://black.readthedocs.io/\n",78,3198,78,38
py-ddd-cqrs-microservice-boilerplate/000-Readiness probe.md,## Context\nCloud providers often rely on the usage of Orchestration systems\nover a containerization technology.\nThe orchestration system needs to know at least 2 things about any service:\n* If the service started correctly after a deployment\n* If the service is still healthy at any moment in time\n,"When implementing an API that has to be deployed on the cloud,\nimplement the following endpoints:\n* `/ready` an endpoint that returns a 200 - OK if the service is ready to\naccept requests after a new deployment\n* `/health` an endpoint that returns a 200 - OK if the service is still alive\nand functioning correctly\n",62,3200,62,71
heptaconnect-docs/2020-10-30-job-messages-and-payload.md,## Context\nIn case of a structural change in a dataset you might need to migrate serialized data in a way to make in work with the latest code.\nThe data that is affected of the structural change can still be within a message queue provider and is often out of access until message handling.\nYou could unintentionally send duplicated messages to drain performance and increase I/O operations overall.\n,* Extract job actions from the messages into the storage.\n* Separate job payloads from their actions.\n* Prevent sending of duplicate messages.\n* Normalize the structure of a job regarding the current message structure.\n,77,3202,77,43
dlp-lux/0002-Blacklight-Version.md,"## Context\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\n","We will use Blacklight 7.\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\nsupported by Hyrax.\nThis decision accepts that minor styling differences may occur between Lux and Curate.\n",31,3215,31,55
bookit-api/0008-database.md,"## Context\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\n",* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\n* Use AWS RDS Aurora (MySQL) for integration/staging/production - better HA & continuous snapshots (enabled for production)\n* Use MariaDB JDBC driver - has native Aurora support for failover\n,29,3218,29,86
bookit-api/0005-use-id-token-from-microsoft-as-bearer-token.md,"## Context\nIn the interest of time and getting something to work, we are going to break up the steps further\n","* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\n* Proper validation of the id_token will still occur\n",24,3219,24,36
bookit-api/0010-jpa-manages-schema.md,"## Context\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\n","* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging/production databases (we will continue to drop/recreate all other databases....local, integration).\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\n",71,3221,71,66
bookit-api/0003-use-junit-for-tests-instead-of-spek.md,## Context\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\n,Use JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\n,61,3225,61,29
bookit-api/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3226,16,39
snippets-service/0004-stop-exporting-analytics-to-csv.md,## Context\nAs part of `2. Export ASRSnippet Metadata for Snippet Metrics\nProcessing` we started exporting Snippet Metadata to CSVs stored in\nS3.\nThe Dashboards consuming the CSVs have been long decomissioned and\nthere's no other consumer of the data.\n,We decide to stop exporting metadata to CSVs for Metrics Dashboards\nand remove related code and exports.\n,62,3228,62,22
snippets-service/0007-move-channel-targeting-to-browser.md,"## Context\nWe want to be able to create more complex Targets, specifically targets that\nwill evaluate to true for Profiles older than X weeks with X being different for\neach channel.\nThe requirement comes from an initiative to reduce the number of active Jobs per week and thus reduce the programming and analyzing time required. With this change we will be able to schedule one Job for multiple channels while maintaining different targeting for each channel.\n","We decide to move Channel targeting from the server to the browser. To accomplish this we will take advantage of `browser.update.channel` JEXL attribute to target snippets based on channel and remove any server side code that does channel targeting.\nWe will generate one bundle for each locale, instead of one bundle for each locale, channel combination.\nThis ADR replaces 0006 since all Jobs for locale will be included in all channels.\n",86,3233,86,89
snippets-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3235,16,39
cafebabel.com/0005-flask-architecture.md,"## Context\nFlask allows you to do whatever you want on your app/folders/templates/etc\nand it can quickly become messy.\nPlus, to deal with configurations within extensions is harder if you do not\ndeclare the Flask app once and for all then load the configuration.\n",An architecture based on https://github.com/stephane/flask-skeleton\n,58,3237,58,17
cafebabel.com/0003-deal-with-authentication.md,"## Context\nWe need user to be able to register, login, retrieve their lost password and so on.\n","After testing Flask-Login which was too limited, we went for Flask-Security\nwhich relies on it but is more complete, as a glue across other Flask extensions.\n",23,3239,23,35
cafebabel.com/0002-choose-an-orm.md,"## Context\nWe need an ORM, especially to use Flask-Security.\nOnly 4 are available at the moment:\n* Flask-SQLAlchemy: too bloated\n* Flask-MongoEngine: relies on Mongo (harder migrations)\n* Flask-Peewee: initial choice, not maintained anymore\n* PonyORM: looks great but the code is hard to contribute to\n",We will use Flask-MongoEngine and do migrations manually as of now.\nWe might want to find/code a tool for that in the future.\n,80,3240,80,31
cafebabel.com/0004-deal-with-translations.md,"## Context\nArticles can be translated in English, French, Spanish, Italian and\nGerman.\nOriginal articles are not necessarily in English and initial text of a\ngiven translation can be itself an already translated article.\nEach translation must keep a link to the original article.\nThe review process remains the same with a draft status.\nThe translated article must both have a redactor and a translator.\n",A new MongoDB document will be created for each translation.\nThat document will inherit from the Article model.\n,81,3241,81,22
cafebabel.com/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3242,16,39
registers-frontend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3245,16,39
lightweight-architecture-decision-records/0001-use-elasticsearch-for-search-api.md,"## Context\nThere is a need of having an API exposed which can be used to search enterprise wide common data model.\nThe data currently resides in a RDBMS database, it is difficult to expose micro-services directly querying out of RDBMS databases since the application runs out the same environment.\nThere are options like ElasticSearch or Solr where data can be replicated.\n",Use ElasticSearch for data indexing\n,76,3246,76,7
html-diff-service/0002-implement-using-spring-boot.md,"## Context\nThe HTML Diff Service is born out of need to increase performance of HTML\ndiffing by not requiring a command line execution in Contently's web\napplication. Additionally, we wanted to remove the Java dependency from our\nRails application container to simplify and reduce the threat surface.\n",Spring Boot is used to allow for the service to be standalone with support for\nembedded servers. The decision allows portability to deploy the service without\na J2EE server.\n,60,3249,60,37
html-diff-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3250,16,39
HES_pipeline/data_location.md,"## Context\nIncoming data from NHS is not handled by users of the HES pipeline. This makes\nit harder to control the directory format of the raw data, as well as the\nconsistency of that data and directory structure.\nAdditionally the SQLite database needs to be located in a writable environment.\n",The raw data will be copied manually from the Data directory to the Library\ndirectory. Subsequent updates to the raw data will also require this. This is\nalso where the database will be located and written to.\n,62,3252,62,45
HES_pipeline/storing_dates_in_database.md,"## Context\nSQLite does not feature a date format data type. As such writing a date format\ndata object from R, results in conversion to an integer with no relevance to the\noriginal date.\n","Incoming raw data will not be converted to date format in R, and instead\nmaintained as a string for full dates (Y-m-d) or part dates (Y-m) and as an\ninteger for years.\n",41,3254,41,45
HES_pipeline/imputing_admission_date.md,"## Context\nAdmission date (ADMIDATE in the APC dataset) can occasionally be missing,\nhowever it can be imputed from other variables.\n","If ADMIDATE is missing but EPISTART is present (start of hosptial episode),\nis the first episode (EPIORDER = 1) and is not a transfer (ADMIMETH not 67 or\n81 and ADMISORC not 51, 52 or 53), we accept that the date in EPISTART is also\nthe admission date.\n",32,3255,32,80
HES_pipeline/hardcoding_variables.md,## Context\nSeveral parts of the pipeline execute operations on specific columns.\n,"Where there was an easy way to do so, we gave users the options to supply variable names:\n* checking whether all expected columns are present\n* coercing data types.\nIn other sections of the pipeline, variable names had to be hardcoded:\n* cleaning variables (replacing missing values with NA)\n* deriving variables\n* deriving row quality and flagging duplicates (optional)\n",15,3256,15,80
Geotrek-rando-v3/deployment_solution.md,## Context and Problem Statement\nWe need to have an easy way to deploy this server for it to be runnable on the most various environments.\n## Decision Drivers sorted by priority\n- Easily installable and runnable on various environments\n- Easily deployed\n- Maintainable\n,"- Easily installable and runnable on various environments\n- Easily deployed\n- Maintainable\nChosen option: **""NextJS server side rendering solution for React""**.\n",54,3261,54,36
simple_note/0002-use-mongodb.md,## Context\nThe app needs a database to store user data including authentication and notes.\n,"The app will use mongoDB for its database system, as its JSON-like structure is well-suited to the storage of simple text.\n",18,3263,18,28
simple_note/0003-use-docker.md,"## Context\nThe app should be able to build in any basic environment, without the need to worry about environmental differences\n",The app will use Docker for production deployment.\n,24,3264,24,10
simple_note/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3265,16,39
voroni-postals/0002-use-rapidoid.md,"## Context\nThis project is being developed as a technology demonstrator. During my day job, we use Spring Boot for almost all of our REST services. I have been exploring several micro frameworks, and for this project I chose to look at [Rapidoid](https://www.rapidoid.org/).\n",Use Rapidoid for this experiment.\n,63,3267,63,8
voroni-postals/0003-switch-to-spring-boot.md,"## Context\nRapidoid was an interesting framework to try back in 2016, but since then I have wanted to switch this project back to Spring Boot for demonstration purposes.\n",This project will be switched to Spring Boot.\n,37,3268,37,10
voroni-postals/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3269,16,39
gti-genesearch/adr-006.md,"## Context\nAdding field info and carrying out array flattening requires access to DataTypeInfo for primary and linked searches.\nThe current implementation uses instances of JsonDataTypeProvider which read multiple types from a single file. This would mean lots of duplication between different searches.\nEach search needs to expose the correct data type, as it will provide a single data type now, albeit with multiple targets.\n","The situation can be greatly simplified, with the following changes:\n* Search provides a single DataTypeInfo which is passed on construction\n* DataTypeInfo has methods for building from JSON or a JSON resource\n* DataTypeInfoRegistry and implementing classes become unnecessary\n* SearchRegistry provides access to list of Searches, and hence DataTypes, for use in result decoration/array flattening\n",78,3272,78,73
gti-genesearch/adr-012.md,"## Context\nThe `analytics` Solr index contains condition data, but as the unstored field `conditionSearch`. We can search on this field, but not display it. To display, we need to include `conditions` from the `baselineCondition` index, using the experiment expression to join the data.\n","The best approach is to add two new searches, for `analytics` and `experiments`. `ExpressionSearch` passes through to an instance of `SolrSearch`\nfor searching `analytics` but can join `experiments` to another instance of `SolrSearch`.\nThis has meant the order of construction and registering of searches in `EndpointSearchProvider` has had to be modified a little.\n",64,3280,64,84
TDD-hexagonal-project/2020_12_02_9_20_PROJECT.md,### Context\nThis is the beginning of the project\n### Decision\nI decide to create a _Docs_ folder to store all the documentation.\nIn this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project.\n### Consequences\nThere will be a single point to store all the documentation\n,I decide to create a _Docs_ folder to store all the documentation.\nIn this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project.\n### Consequences\nThere will be a single point to store all the documentation\n,72,3287,72,58
TDD-hexagonal-project/2020_12_02_9_35_TECHNICAL.md,"### Context\nThis is the beginning on the project and for the moment all the commit are done on the master branch\n### Decision\nUse Gitflow to handle code source and project management.\n### Consequences\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\n","Use Gitflow to handle code source and project management.\n### Consequences\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\n",68,3288,68,41
paas-csls-splunk-broker/ADR007-paas-taking-ownership.md,"## Context\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n","The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\n",94,3291,94,42
paas-csls-splunk-broker/ADR004-deploy-broker-as-lambda.md,"## Context\nWe need to deploy the Broker somewhere.\nThe Broker implements the service broker API to generate per-application syslog\ndrain URLs (Adapter URLs).\nThe Adapter is written in Go.\nThe Broker is written in Go.\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\n",We will deploy the Broker as an AWS Lambda\n,77,3295,77,10
linshare-mobile-flutter-app/0006-download-file-manager.md,"## Context\nIn LinShare Flutter application, we can download files one by one from ""My Space"", ""Received Shares"" or ""Shared Spaces"" with the library `flutter_downloader` by adding it to the queue :\n```\nTaskId enqueue(String url, String saveDir, String header...)\n```\nWe can also clear or cancel file queue with dedicated functions.\n",We decided that LinShare could currently download a file one by one.\n,80,3297,80,15
linshare-mobile-flutter-app/0004-upgrade-file-picker-library-version.md,"## Context\nIn LinShare Flutter application, we can select file from system file manager by library `file_picker`\nThe current version is `2.0.7` is not supporting well for Android and iOs,\nin Android when get fileName we will have fileName along with fileExtension\nin iOS when get fileName we will have only fileName\n","We decided to use newer version `2.0.11`, with this, both Android and iOS now have fileName along with fileExtension\n",69,3298,69,28
linshare-mobile-flutter-app/0010-migrate-to-null-safety-support.md,## Context\n- Upload the latest stable Flutter version (2.0.5) and Dart (2.12.2)\n- Be familiar and apply new version change from dependency libraries\n,- Upgrade Flutter SDK and Dart environment\n- Update dependency libraries version to support null-safety\n,39,3299,39,19
linshare-mobile-flutter-app/0007-path-provider-library.md,## Context\nAndroid and iOS have different folder architectures.\nTherefore we need to provide *path folder* according to user's plateform.\n`path-provider` library only provide *Download Folder* for Android Version.\n,"We decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\n",45,3301,45,32
linshare-mobile-flutter-app/0005-i18n-with-dynamic-string.md,## Context\nWe need to provide i18n string with some requirements:\n- dynamic string\n- support plurals\n,Need to implement it with `intl`\n,25,3305,25,9
linshare-mobile-flutter-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3308,16,39
my_budgeting/0003_rails_event_store.md,"## Context\nAs I'm learning DDD with Arkency course, I'm going to start with solution propsed by them.\n","[Rails Event Storm](https://railseventstore.org/) is a library for publishing, consuming, storing and retrieving events. According to its creators from Arkency – it's your best companion for going with an Event-Driven Architecture for your Rails application.\n",27,3309,27,53
my_budgeting/0002_sql_databse.md,## Context\nI start a public project for which architecture decisions can be seen as weird.\n,"I'm going to use as sql database – PostgreSQL database. I used to work with no-sql database (MongoDB) last 3 years, it's time to get used to sql database. PostgreSQL is free, mature and very popular (not only in side projects like this one).\n",19,3310,19,59
orcid-client/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described at https://adr.github.io/\n",16,3311,16,17
orcid-client/0002-generated-swagger-models.md,## Context\n- Using the OrcidAPI we get back complex and deeply nested JSON objects.\n- Modeling each API endpoint by hand is time consuming.\n,"We will use models for these objects generated by the swagger-codegen code\ngeneration tool: https://github.com/swagger-api/swagger-codegen.\nWe will run the generator once, and add the resulting code to the lib directory. We don't intend to\nmaintain any changes to the generator that were required to produce these\nmodels.\n",32,3312,32,70
cukeness/0004-use-docker-and-docker-compose-for-development-environment.md,## Context\nIt needs to be very easy to reliably construct a development environment with all of the tooling required to build and run the application.\n,`docker` and `docker-compose` will be used for all development activities. starting a development version of the application should be a simple a running `docker-compose up`.\n,30,3315,30,36
cukeness/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3316,16,39
topo/0003-graphql-as-api-query-language-via-appolo-server.md,## Context\nWe need a way to populate our Neo4J database via an API layer.\n,"Use GraphQL libraries for the server API.\nGraphQL has been increasing in popularity and maturity lately. It provides a more flexible API query layer compared to REST based interactions. Specifically being able to perform arbitrarily structured queries, with optional sub-elements and with query parameters.\n[Apollo Server](https://www.apollographql.com/) will be used as it's a popular well documented implementation.\n",20,3317,20,79
topo/0002-use-neo4j-as-persistence.md,## Context\nWe need to save information into a persistence layer in order to query and explore the data. The data we will be working with heavily relies on describing relationships which is well suited for a graph like structure.\n,"We will use Neo4J as the persistence layer. It is a fairly popular graph database, well documented and mature product.\n",44,3318,44,26
topo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3319,16,39
react-template/0005-use-jest.md,"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n",We will use Jest as our testing framework.\n,40,3323,40,10
react-template/0002-use-next-js.md,"## Context\nWe want to be able to support full-featured React apps, without reinventing the\nwheel when it comes to routing and server-side rendering.\n[Next.js](https://nextjs.org/) is a framework that supports these features out\nof the box.\n",We will use Next.js as the basis for the project.\n,57,3324,57,13
react-template/0011-use-axe-for-automated-accessibility-testing.md,## Context\nWe want to be able to ensure our pages are accessible.\n[Axe](https://github.com/dequelabs/axe-core) is an actively supported\naccessibility testing engine for HTML-based user interfaces that supports all\nmodern browsers (including IE 9+).\n,"We will use axe for accessibility testing. We will integrate it with Jest via\n[`jest-axe`](https://github.com/nickcolley/jest-axe), enabling us to integrate\nwith all of our tests.\n",58,3325,58,46
react-template/0006-use-snapshot-testing.md,## Context\nWe want to be confident that any changes to how our React components display are\nintentional. Jest has built in support for snapshot testing.\n,We will use snapshot testing as part of testing components.\n,33,3326,33,12
react-template/0014-use-dependabot-to-keep-dependencies-up-to-date.md,## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n,We will use Dependabot to monitor dependency updates.\n,38,3328,38,12
react-template/0007-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript and React though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,71,3330,71,57
react-template/0009-use-selenium-for-feature-and-end-to-end-testing.md,"## Context\nWe want to be able to run feature and end to end tests in browsers and as part\nof continuous integration. We only want to write tests once, and have them run\nin all environments.\n[Selenium WebDriver](https://docs.seleniumhq.org/projects/webdriver//) is a\nbrowser automation framework. It supports all major browsers and is supported by\ncloud based browser testing services like\n[BrowserStack](https://www.browserstack.com/).\n",Use Selenium WebDriver to write feature and end to end tests via Jest.\n,96,3331,96,15
react-template/0010-support-cucumber-feature-tests.md,## Context\nWe want to be able to express the acceptance tests for features in a human\nreadable format. This would make it easier for non-technical people to\nunderstand what tests we have and maybe even write new ones.\n[Cucumber](https://cucumber.io/) is a tool for running automated tests written\nin plain language.\n,We will support writing feature tests in Cucumber. We will still support feature\ntests not written in Cucumber.\n,72,3333,72,25
react-template/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3334,16,40
dev/00001-dot-prefix-for-configuration.md,"## Context\n1. The current implementation goes against a convention where global configuration files are prefixed with a period\n2. I want to be able to do writebacks to the configuration, an exact example would be `dev add repo ${REPO_URL}`, and the current implementation makes it messy because the configuration files can be anywhere.\n",I have made the decision for configuration files to look like: `\.dev(\.[a-zA-Z\-\_\.]+)?\.yaml` and for the application to only search in the user home directory and current working directory. The `includes` property will be removed.\n,67,3335,67,56
arauto1160/2017-05-22-create-new-bdd-framework-for-AR.md,"## Context\n- `arauto`, the current GUI testing framework for AR is slow, hard to work and unmaintainable.\n- it was developed in Shanghai\n- taylor for Shanghai's environment/setup\n- the owner has left the company\n- it uses TestNG instead of JUnit\n","- write a new framework using a simpler, cleaner design\n- use `Selenide` library to write easier Selenium-based tests\n- use `Cucumber` library to write BDD tests\n- testers and developers are responsible for maintaining the framework\n- changes should go through the same PR process\n",62,3336,62,61
human-essentials/0003-multitenancy-instead-of-multiple-instances.md,## Context\nDiscussion about whether to make this application a multi-tenant (single instance with many Essentials Banks) or single-tenant (each Essentials Bank gets their own instance).\n,"We've decided to go with a multi-tenancy. Rails has good support for this, with some initial configuration. This will help keep costs down and allow us to provide it as a cheap/free service. This will require ongoing support by us, to maintain the production instance, but since this is intended to be an ongoing project that was implied anyways. This will also ensure that all Essentials Banks have access to the same version of the software, universally.\n",36,3345,36,92
human-essentials/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3346,16,39
plant-shop/0002-use-the-javadoc-to-document-classes-and-methods.md,## Context\nContext here...\n,Decision here...\n,7,3347,7,4
plant-shop/0005-use-mkdocs-for-maintaining-doc-pages.md,## Context\njavadocs are not very pleasant according to project marketing.\nWe like an easy way to publish some techical articles related to our project\n,1. We are using mkdocs with mkdocs gradle plugin to generate docs pages.\n2. We are going to host javadocs\n3. Our architectural decisions also would be shared on pages\n4. Pages would be hosted as github pages [plant-shop docs](http://wojciech.zarski.net/plant-shop)\n,32,3348,32,71
plant-shop/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3349,16,39
plant-shop/0004-use-the-plantuml-as-an-default-way-to-create-uml-diagrams.md,## Context\nIt's occuring very often that drawn documentations is no longer up to date.\n,We are going to use [PlantUml](http://plantuml.com/) to store our diagrams in repository.\n,21,3350,21,24
prm-gp2gp-transfer-classifier/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard][1].\n",16,3353,16,20
adr-example/0003-use-ember-as-the-main-frontend-js-framework.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3355,21,13
adr-example/0002-use-c-sharp-as-the-only-backend-language.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3356,21,13
adr-example/0004-asp-net-as-the-web-application-framework.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3357,21,13
adr-example/0005-use-react-as-the-main-frontend-js-framework.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3358,21,13
adr-example/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3359,16,39
dependency-track-maven-plugin/0002-use-maven-to-build-plugin.md,"## Context\nThis Maven Plugin needed a build tool to compile, test and package it.  Maven and Gradle were the only viable options.\n","I decided to use Maven as it felt a better fit to use Maven to build a Maven plugin, rather than using Gradle.\n",30,3364,30,27
dependency-track-maven-plugin/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3366,16,39
talk/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3368,16,39
dos-server/adr-11-exception-monitoring.md,## Context\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\nused by DLS for various applications.\n,We will use Sentry for application exception monitoring.\n,30,3370,30,10
dos-server/adr-1-metadata.md,"## Context\nDigital objects need to have associated metadata for various use cases (refer to\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\nTo avoid ""duplication"" of descriptive metadata, it is desired that DOS not store descriptive metadata.\n",Descriptive metadata will not be stored by DOS.\n,59,3372,59,11
dos-server/adr-12-openjdk.md,## Context\nOracle JDK requires a basic license. The main alternative is OpenJDK.\n,We will rely on OpenJDK in our production environment.\n,19,3373,19,13
dos-server/adr-5-open-api-specification.md,"## Context\nIn accordance with our architecture principles (API first design), an API specification\nis required to specify and share information about DOS API end points. OpenAPI is supported by most API design tools and it is already being used to specify other DLS APIs.\n",OpenAPI (Swagger) will be used.\n,54,3375,54,10
dos-server/adr-10-aws-fargate.md,"## Context\nInfrastructure is needed to run Docker containers (the current choice of deployment packaging). As we are apparently using Amazon Fargate for running containers of other applications, the Dockerized application\ncan be deployed to Amazon Fargate (until a replacement choice for running Docker containers is made by\nthe infrastructure team.\n",Fargate platform will be used.\n,66,3376,66,9
dos-server/adr-3-rds-for-db-services.md,"## Context\nA self-service cloud database can make it easy to set up and manage a database in the cloud. Amazon RDS\nmeets several desired characteristics such as performance, robustness, database-independence, scaling, and\ncost-effectiveness.\n",Amazon RDS will be used to persist digital objects data.\n,53,3377,53,13
dos-server/adr-13-postgres.md,## Context\nA database is needed to store technical and structural metadata of digital objects. PostgreSQL\nis a robust relational and transactional database already being supported for many projects.\n,PostgreSQL will be used for storing relevant digital object metadata.\n,35,3378,35,13
dos-server/adr-4-s3.md,"## Context\nDOS needs storage for digital object bitstreams (currently only PDFs and JPEGs). Amazon Simple Storage Service (S3) is a robust, scalable, and cost-effective object storage solution. S3 is already being used for various DLS projects and supported by the infrastructure team.\n",AWS S3 will be used for storing bitstreams of digital objects.\n,61,3379,61,15
dos-server/adr-9-airflow.md,## Context\nA tool is needed that can automatically run ingest scripts on a pre-defined schedule and provide status updates on ingest operations.\n,The project will use Apache Airflow to run ingest scripts.\n,27,3381,27,13
dos-server/adr-6-swaggerhub.md,"## Context\nWe need a tool to design and document the DOS API. Options include SwaggerHub, etc.\n",SwaggerHub will be used to document the API specification.\n,23,3382,23,12
dos-server/adr-2b-python.md,"## Context\nTo ingest content into DOS, or for integration use cases, Python is well suited as a powerful and highly\nflexible but user-friendly gluing language. Python may also be used for quick prototyping of staff admin\ninterface.\n",Python will be used for ingests and for integration with other systems.\n,50,3383,50,15
support-rota/0003-use-standard-rb.md,"## Context\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\n,36,3386,36,35
support-rota/0002-use-bullet-to-catch-nplus1-queries.md,## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n,Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n,37,3387,37,23
support-rota/0001-use-pull-request-templates.md,"## Context\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\n",Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n,89,3390,89,22
support-rota/0005-use-brakeman-for-security-analysis.md,## Context\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n,Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n,20,3391,20,29
support-rota/0002-use-a-changelog-for-tracking-changes-in-a-release.md,"## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",45,3392,45,57
support-rota/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3393,16,39
superwerker/living-documentation.md,"## Context\nA dashboard with more information and deep-links to resources, e.g. setting up SSO with existing identity providers, GuardDuty/Security Hub dashboards.\n",- Create a CloudWatch Dashboard called `superwerker` in the AWS management account. The CW dashboard a) ensures a deep link which can be used to link from the README.md and b) ensures the user is authorized to access the information.\n- Display DNS delegation state and setup instructions\n- Refresh dashboard with scheduler every minute since this removes the compexity to deal with event-based dashboard generation. Lambda invocations are completely covered by free-tier.\n,36,3399,36,93
mario/0013-use-sqs-and-airflow-for-task-execution.md,"## Context\nThe execution model described by ADR 12 was designed before we had Airflow. It works, but we'd like to simplify things by moving it to Airflow to avoid having similar processes handled in different ways.\n",We will change the S3 notification from triggering a Lambda to sending a message to an SQS queue. We will configure a single workflow in Airflow that begins with an SQS sensor.\n,47,3406,47,39
mario/0002-use-elasticsearch.md,## Context\nWe need to choose between using Solr and Elasticsearch for indexing.\n,We will use Elasticsearch. See https://docs.google.com/document/d/1LX3svZ59f2Ni5TNCPG6jIYb8CnSYOjR0ae0ujPOUN-k/edit for a more detailed description of how this decision was arrived at.\n,17,3410,17,62
mario/0005-use-aws-lambda.md,"## Context\nThe bulk of this application will consist of a data processing pipeline that takes metadata from incoming systems and indexes it in Elasticsearch. The processing will only need to be run for relatively short periods of time, usually, when new data arrives. We expect integrations with external systems to be minimal, likely limited only to S3 and Elasticsearch. Given the periodic nature of the application, it seems wasteful and needlessly complex to provision and maintain a VM for providing compute resources.\n",We will use AWS Lambdas as the compute model for the processing pipeline.\n,97,3411,97,16
mario/0009-elasticsearch-indexing-strategy.md,## Context\nThere are a number of different ways we could approach indexing in Elasticsearch. We would like to choose a path that allows us some flexibility to adjust as future needs arise. We also need to think about how to maintain index uptime while modifying the contents of the index.\n,Use an index alias for searching that points to a separate index for each source.\n,56,3412,56,17
mario/0012-use-lambda-and-fargate-for-task-execution.md,"## Context\nGiven the limitations of Lambdas we decided to rely on containers to handle the bulk of the processing. Fargate provides a cheap, accessible container runtime.\n",We will use AWS Lambda to trigger a Fargate task for the processing pipeline.\n,35,3415,35,18
mario/0003-follow-twelve-factor-methodology.md,## Context\nDesigning modern scalable cloud based applications requires intentionally\ndesigning the architecture to take advantage of the cloud.\nOne leading way to do that is\n[The Twelve Factor](https://12factor.net) methodology.\n,We will follow Twelve Factor methodology.\n,48,3419,48,8
mario/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3420,16,39
saas-plaform-tenant-identity-provider/0003-use-aws-cognito-as-idp-over-auth0-and-firbase-auth.md,"## Context\nWe need to decide for a hosted identity provider. An the different option are AWS Cogntio, Auth0 and Firebase Auth.\n",We select AWS Cogntio as the hosted IDP.\n,31,3422,31,14
saas-plaform-tenant-identity-provider/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3423,16,39
mediawiki-extensions-Popups/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[this article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3433,16,41
govuk-terraform-provisioning/0003-statefile-management.md,## Context\nCurrently a terraform run contains every resource we manage. As the scope\nof this repo expands that will eventually encompass everything on GOV.UK.\nWe would like to reduce this scope to make both reasoning about changes and\nlessen possible impact.\n,"In order to isolate changes, for safety and scope, we will separate terraform\nstate files in to multiple files, based on a combination of project and environment.\nOnly one instance of Terraform should be used at a time. This means you are limited to\nhaving a single local statefile. The tooling has been written to assume this is true.\n",55,3434,55,74
govuk-terraform-provisioning/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3436,16,39
k8s-vagrant-centos-cluster/0002-why-not-use-kubeadm.md,"## Context\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\n","Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\n",26,3440,26,22
k8s-vagrant-centos-cluster/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Lightweight Architecture Decision Records to maintain important decisions made during project evolution, https://www.thoughtworks.com/radar/techniques/lightweight-architecture-decision-records\n",16,3441,16,37
community/dr-001-Technologies.md,## Context\nThe Kyma developers need to select specific technologies. Their purpose is to:\n* Enable the most lightweight solution and cost-effective solution for the cluster installation.\n* Facilitate the cross-teams collaboration development.\n* Enable easy context-switching between teams.\n,"The decision is to use the **Go** language for all new implementations in Kyma. Go allows to create very efficient applications with low memory usage and a vast set of system libraries. Many projects which Kyma depends on are written in Go, including Kubernetes.\nUse the following frontend technologies within Kyma:\n* Open UI5\n* Angular (version 4 and later)\n* React\nThe recommended technologies fulfill the Kyma principles.\n",58,3442,58,95
community/dr-014-Certificate_against_remote_environment.md,## Context\nEvery remote environment in the cluster should be accessible only for clients authorized to this particular remote environment.\nThis requirement raises a need for a mechanism that will differentiate between clients' permissions.\n### Multiple server certificates\nHaving separate server certificate per remote environment would make it hard to manage all those certificates.\n### Single certificate\nHaving only one certificate creates the problem that the client who obtained signed certificate for accessing one of the existing remote environments can access all of them using this particular certificate.\n,The decision is to check the Distinguished Name of the client's certificate in Ingress-Nginx configuration using `nginx.ingress.kubernetes.io/configuration-snippet` in individual ingresses and grant permissions only in case the client certificate's Common Name matches the required one.\n,100,3446,100,56
community/dr-006-Kubeless_as_the_Faas_solution.md,## Context\nFunction as a Service (FaaS) is the main capability inside Kyma. This DR presents a market research and a detailed analysis of both Kubeless and fission.\n,The decision is to use Kubeless as the FaaS solution.\n,40,3447,40,15
community/dr-017-Application-Integration-without-Wormhole-Connector.md,## Context\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\n,"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\n",42,3448,42,52
community/0002-continuously-delivery-travis-ci.md,## [Context](https://github.com/libero/community/issues/13)\nLibero needs automated and human feedback over pull requests and release candidates.\n,"We will provide Travis CI builds for all repositories, covering both testing and deployment to a demo environment.\n",31,3459,31,21
community/0008-libero-infrastructure.md,"## Context\nLibero infrastructure (servers, Kubernetes, buckets) supports Libero development by providing demo environments.\nService providers need documentation to learn to run Libero products.\nService providers cater for the disparate, very specific needs of publishers.\nService providers may consolidate their infrastructure with the rest of the platforms.\n",Libero infrastructure should serve two purposes:\n- provide *demo* environments to showcase Libero products in certain configurations\n- provide realistic *reference* environments that do not serve real users but can be forked and adapted by service providers to kick start their Libero offering\n,64,3461,64,55
community/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3466,16,39
gp-finder/0006-use-prometheus-for-exposing-metrics.md,## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n,We will use Prometheus to monitor and alert on the state of the application.\n,86,3469,86,16
gp-finder/0005-add-cache-control-headers.md,"## Context\nCache control headers can be used to prevent a client's browser from\nre-requesting a page that has not changed, and may be leveraged by proxies to\nreturn the same cached pages for multiple clients.\n",Cache-control headers will be added to all valid requests. 500 and 404 errors\nwill not be cached.\n,46,3471,46,24
gp-finder/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3475,16,39
devops-challenge/0002-feat-create-staging-branch.md,"## Context\nSometimes while creating something new, code is written to research a\npossible solution, but later you could find a better way or a different\napproach to the solution.\n","To avoid messing the master branch with imprudent commits, I'm going to\ncreate a _new default branch_, called **staging**.  This way, all the pull\nrequests will be sent to the new branch after merged.  Then, if after\nthe pull request is found to have issues, I will be able to fix it\nwithout problems.\n",38,3477,38,75
git-en-boite/0005-use-redis-and-bull-for-background-tasks.md,"## Context\nWhen we do heavy lifting with git, like cloning repos, we can't do it during the user's HTTP request cycle. We need a way to put this work into the background.\n",We're using the [bull](https://github.com/OptimalBits/bull/) library which uses Redis.\n,41,3490,41,24
git-en-boite/0006-use-bare-git-repos.md,"## Context\nWhen we create git repos we can either use `git init --bare` or we can clone them. A cloned repo has one main working tree with the git directory inside it. You can add additional worktrees. A bare repo does not have a default worktree, but we can still check out branches using the `git worktree` command.\n","We should create a folder structure for each repo, something like\n```\n- git-repos\n- <hash of repo ID>\n- git # main bare git repo\n- branches # worktrees go under here\n- master # a worktree for the master branch\n```\nThis folder structure will also allow us to store other data about the repo in files\n",74,3492,74,77
git-en-boite/0015-remove-postgres-for-now.md,"## Context\nWe're migrating to a new production environment and realised that although we configure postgres in our environments, we don't use it.\n",Remove all dependencies on postgres from the code for now.\n,29,3495,29,12
git-en-boite/0013-separate-worker-process-and-container.md,## Context\nAs described in [this ADR](./0012-background-git-operations.md) we are implementing a way to run some git\noperations outside the main web server process.\n,The background worker will be started as a separate process. You can start it using `yarn app start:worker`.\nWe've also configured a separate docker container in docker-compose to demonstrate how we can spin up multiple workers to support a single web server.\n,39,3496,39,54
git-en-boite/0016-handle-concurrent-git-operations,"## Context\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\n",Reduce worker concurrency to 1 by using a single container (for now).\n,47,3497,47,16
git-en-boite/0004-use-npm-instead-of-yarn.md,## Context\nWe're getting security vulnerabilty warnings from GitHub due to transitive dependencies. Npm offers a `--depth` setting for updating dependencies that yarn doesn't seem to have. Which raises the question: why use yarn?\n,Switch to npm.\n,49,3499,49,5
git-en-boite/0007-use-yarn-workspaces-instead-of-lerna.md,"## Context\nLerna was being a pain; not working as described in the docs. It seems a bit old and crufty, and was built before yarn workspaces were a thing.\n",Switch to [yarn workspaces](https://classic.yarnpkg.com/en/docs/workspaces/). We may also use [mono](https://github.com/enzsft/mono) or [rush](https://rushjs.io) to add extra features for managing a monorepo.\n,39,3500,39,60
git-en-boite/0002-copy-source-code-and-tests-into-docker-container.md,"## Context\nPreviously the docker-compose.yaml file mapped a volume on the host to the `/app` directory on the container. This is great for a local development workflow, but it won't work in production.\n","The decision I've made is to configure the Dockerfile to copy the source code (and tests) to the container. This means the container will have everything it needs to run the web server, and also to run tests.\n",43,3501,43,46
git-en-boite/0003-use-typeorm-with-postgres.md,"## Context\nWe need somewhere to store state, and we need a way to get data in / out of that store.\n","Typeorm implements DataMapper, which is a low-coupled way to work with databases. It seems pretty mature and well-used. It's good enough for now.\n",26,3503,26,35
git-en-boite/0009-prefer-dugite-over-raw-git-commands-in-tests.md,## Context\nThe build failed because the version of git used in CircleCI was not what we expected.\n,Use `GitProcess.exec` even in tests that need to lean on the git process.\n,22,3504,22,19
git-en-boite/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3505,16,39
caia/0001-record-architecture-decisions.md,"## Context\nMaintaining a record of key decisions made in the project seems worthwhile.\nThe general idea is to record important decisions that affect the application\nas a whole, for the use of future developers and maintainers.\n",See [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions](1)\nfor general motivation and format for architecture decision records (ADRs).\n,45,3512,45,41
io-backend/0003-use-openapi-to-defined-the-api-specs.md,## Context\nWe need to define the API specifications of the services we're going to implement.\n,We use the [OpenAPI 2.0](https://swagger.io/specification/) specification (aka Swagger spec) as standard for our REST\nAPI definitions.\n,20,3515,20,34
io-backend/0004-use-a-dependency-injection-container.md,## Context\nWe need a simple way to decouple the code. We need a simple way to swap components with mocked one for testing purpose.\n,We use [Awilix](https://github.com/jeffijoe/awilix) to provide an IoC container where all services and controllers are\nregistered.\n,30,3516,30,37
io-backend/0002-backend-runs-on-docker-on-local-environments.md,## Context\nWe need a reliable infrastructure definition that will allow developers to replicate the environment on their local\nmachine.\n,We use Docker (1.13.0+) to encapsulate all architecture components. We use Docker Compose (1.13.0+) to orchestrate\nthe containers in a local environment.\n,24,3518,24,41
io-backend/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3520,16,39
clean-architecture-example/0004-documented-api.md,## Context\nWe need to build an API that other systems can use.\nThe API needs to be documented so other developers are not spending time asking how to use it.\n,We will use Spring HATEOAS to produce a navigable REST service.\nAlong with this we will use Swagger to generate documentation about our REST services.\n,35,3530,35,33
clean-architecture-example/0007-use-kotlin.md,"## Context\nThe code base is too verbose.\nCould do with writing less code, but avoid too large a learning curve away from Java.\n",We will assess the benefits of Kotlin by rewriting this application.\n,30,3531,30,13
clean-architecture-example/0002-use-clean-architecture.md,## Context\nWe need to distinguish what our application does versus how it does it.\n,We will use clean architecture to ensure a clear separation is maintained between what and how.\nSee [README.md](../../README.md) for details.\n,18,3532,18,31
clean-architecture-example/0003-use-spring-framework.md,## Context\nWe need to build the how part of the application\n,"We will use Springs ecosystem to implement how our application works.\nMore explicitly we will use Spring Boot, Spring Data and Spring WebMvc.\n",14,3533,14,29
clean-architecture-example/0006-avoid-api-versioning.md,## Context\nCode base is becomes too complicated.\nMaintaining and supporting multiple API versions is painful and expensive.\n,We will avoid API versioning.\nWe will not introduce breaking changes.\n,24,3534,24,16
clean-architecture-example/0005-api-versioning.md,## Context\nDifferent consumers are making request for information in different formats.\n,"Rather than duplicate the data in the response, we will version our api and allow consumers to choose which version best suits their needs.\nWe will use a custom Media Type (application/vnd.example.clean.v1+json) to version at the resource level.\n",15,3535,15,52
clean-architecture-example/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3536,16,39
share/2020-11-01_adrs.md,## Context\nI realise I'm making decisions and have not been recording them well.\n,Add a simple ADR folder to the repo. Try to log each significnat decision\ninto a new ADR.\nDate the filenames like `2020-11-01_adrs.md`.\n,18,3538,18,42
rotc/0006-use-standalone-jaeger.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3541,21,13
rotc/0005-use-standalone-prometheus.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3542,21,13
rotc/0007-use-standalone-kiali.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3544,21,13
rotc/0003-use-gcp-as-example-cloud-platform.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3545,21,13
rotc/0002-use-aws-as-example-cloud-platform.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3546,21,13
rotc/0004-use-azure-as-example-cloud-platform.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3547,21,13
rotc/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3548,16,39
adr-tools/0007-invoke-adr-config-executable-to-get-configuration.md,"## Context\nPackagers (e.g. Homebrew developers) want to configure adr-tools to match the conventions of their installation.\nCurrently, this is done by sourcing a file `config.sh`, which should sit beside the `adr` executable.\nThis name is too common.\nThe `config.sh` file is not executable, and so doesn't belong in a bin directory.\n","Replace `config.sh` with an executable, named `adr-config` that outputs configuration.\nEach script in ADR Tools will eval the output of `adr-config` to configure itself.\n",78,3564,78,39
adr-tools/0005-help-comments.md,"## Context\nThe tool will have a `help` subcommand to provide documentation\nfor users.\nIt's nice to have usage documentation in the script files\nthemselves, in comments.  When reading the code, that's the first\nplace to look for information about how to run a script.\n",Write usage documentation in comments in the source file.\nDistinguish between documentation comments and normal comments.\nDocumentation comments have two hash characters at the start of\nthe line.\nThe `adr help` command can parse comments out from the script\nusing the standard Unix tools `grep` and `cut`.\n,64,3566,64,63
adr-tools/0002-implement-as-shell-scripts.md,## Context\nADRs are plain text files stored in a subdirectory of the project.\nThe tool needs to create new files and apply small edits to\nthe Status section of existing files.\n,"The tool is implemented as shell scripts that use standard Unix\ntools -- grep, sed, awk, etc.\n",39,3569,39,24
adr-tools/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3570,16,39
adr-tools/0009-help-scripts.md,"## Context\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\nBut, it means that help text cannot include calculated values, such as the location of files.\n","Where necessary, help text can be generated by a script.\nThe script will be called _adr_help_<command>_<subcommand>\n",63,3571,63,28
konfetti/0004-use-lazy-loading.md,## Context\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n,"We will use a lazy evaluation approach, similar to [implemented in Django](https://github.com/django/django/blob/master/django/conf/__init__.py#L42)\n",37,3572,37,37
konfetti/0002-use-src-layout.md,## Context\nWe need to have reliable distribution publishing process and always run tests against installed package version.\n,We will use `src` code layout as [described by Ionel Christian Mărieș](https://blog.ionelmc.ro/2014/05/25/python-packaging/) and [Hynek Schlawack](https://hynek.me/articles/testing-packaging/).\n,21,3573,21,60
konfetti/0003-support-python-2-7.md,## Context\nWe need to help application developers to migrate their projects from Python 2.7 to 3.5+.\n,We will support Python 2.7 on the best effort level until 2020-01-01.\n,27,3574,27,23
konfetti/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3575,16,39
court-case-service/0006-fetch-noms-number-on-demand.md,"## Context\nThe call to the prison-api to get custody data requires a nomsNumber as the identifier. This is a new identifier for the court-case-service and two options were identified as ways we could get this:\n1. Incorporate nomsNumber into the case model, retrieve it as part of the matching process and have prepare-a-case pass it in the request to the custody endpoint\n2. Use CRN to retrieve this data on demand from the community-api\n",Option 2 chosen - Use CRN to retrieve this data on demand from the community-api.\n,96,3576,96,20
court-case-service/0007-adopt-kotlin.md,## Context\nKotlin has become the language of choice for back end development within HMPPS Digital.\n,"Adopt Kotlin as preferred language for new code within court-case-service. Where practical, migrate classes as we change them.\n",22,3583,22,25
court-case-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3585,16,39
api-catalogue/0002-reduce-internet-explorer-6-8.md,## Context\nThe API catalogue has been using an obsolete version of jQuery (1.x). This\nversion has reported security vulnerabilities which will not be patched:\n- https://npmjs.com/advisories/328\n- https://npmjs.com/advisories/796\n- https://npmjs.com/advisories/1518\n,"Upgrade to jQuery 3.x (the current major version at the time of writing). This\nremoves explicit support for IE 6, 7, 8.\nThis decision is in line with the broader GDS stance to change [testing\nrequirements for older version of Internet Explorer][gds-ie-testing]\n",69,3587,69,65
libmemory/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3590,16,39
adr-manager/0002-use-antlr-for-parsing-adrs.md,## Context and Problem Statement\nADR Manager needs to parse existing ADRs. Each ADR is stored in a markdown file. Example: https://github.com/adr/madr/blob/master/docs/adr/0001-use-CC0-as-license.md.\n,"Chosen option: ""Use ANTLR to create an ADR specific parser"", because comes out best (see below).\n### Positive Consequences\n* Some existing ADRs are accepted now without changes.\n* Better control over output\n### Negative Consequences\n* The new parser is worse at parsing ""invalid"" ADRs than the generic one.\n* ANTLR Parser seems to be quite slow.\n",53,3591,53,87
adr-manager/0003-use-pizzly-as-backend.md,"## Context and Problem Statement\nShould the ADR Manager have a backend? If so, how should it look?\n","Chosen option: ""Use Pizzly as Backend"", because comes out best (see below).\n",24,3592,24,20
adr-manager/0001-use-vue.js.md,## Context and Problem Statement\nA framework makes the creation of a web app significantly easier. Which framework should be used?\n,"Chosen option: ""Vue.js"", because the team has more experience with Vue.js.\n",25,3594,25,19
lumbergh/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3597,16,39
alfresco-anaxes-shipyard/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3607,16,39
govuk-infrastructure/0002-use-aws-eks-terraform-module.md,## Context\nA fully configured EKS cluster requires many AWS resources and a lot of configuration. Defining each resource in our own Terraform module maximises flexibility but also requires a significant level of effort. We could instead make use of the [existing Terraform registry EKS module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest) to optimise for speed of delivery.\n,Adopt the [existing Terraform registry EKS module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest).\n,83,3610,83,30
govuk-infrastructure/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3616,16,39
adrflow/2-Create_ADR_Object.md,"## Context\nDifferent commands in the ADR utility require manipulating the content of an ADR.\nFor example, changing its status.\nInstead of spreading the structure and management of ADR *content* into different commands, it would be better to centralize this related piece of knowledge into one file.\nAlso, we may want to change how manipulate content, or export it. Centralizing it in one place will encapsulate that aspect of the tool into one place.\n","All ADR content manipulation is to be written and centralized in a single ADR object, under the `core` directory.\n",96,3633,96,26
mychain/adr-template.md,"## Context\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\n","> This section explains all of the details of the proposed solution, including implementation details.\nIt should also describe affects / corollary items that may need to be changed as a part of this.\nIf the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\n(e.g. the optimal split of things to do between separate PR's)\n",43,3639,43,81
cli/0001-record-architecture-decisions.md,"## Context\nAs we are building out v7 of the CLI, we need to record the architectural decisions made on this project.\n","We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",27,3650,27,39
corona-hackathon/0010-branching-strategy.md,## Context\nWe have to make a decision on the branching strategy for development.\n,[Git Flow](https://danielkummer.github.io/git-flow-cheatsheet/) it will be.\n,17,3653,17,23
corona-hackathon/0005-lombok.md,## Context\nWhat libraries can we utilize to avoid boilerplate code?\n,Utilize [Lombok](https://projectlombok.org/) whenever possible.\n,15,3654,15,17
corona-hackathon/0009-frontend-framework.md,## Context\nWe have to decide for a frontend application and a styling/component framework.\n,We stick to [Angular](https://angular.io/) together with [NG ZORRO Ant Design](https://ng.ant.design/) and [NG Alain](https://ng-alain.com/).\n,18,3656,18,42
corona-hackathon/0011-postgres.md,## Context\nWe have to make a decision on how/where to host the DB.\n,We use a freely managed [Postgres on Heroku](https://www.heroku.com/postgres)\n,19,3657,19,21
corona-hackathon/0006-web-controller.md,## Context\nWe have to decide for a framework for web controller / adapter.\n,Utilize [Spring Reactive Web](https://docs.spring.io/spring-framework/docs/5.0.0.M1/spring-framework-reference/html/web-reactive.html).\n,17,3658,17,35
corona-hackathon/0007-knowledge-crunching.md,"## Context\nWe have to make a decision, how we describe the core use cases of the application.\n",We use [Domain Storytelling](https://domainstorytelling.org/) to describe core use cases & interactions.\nAll user stories can be found in the [`./doc/user-stories`](./doc/user-stories) directory.\n,22,3659,22,49
corona-hackathon/0002-hexagonal-architecture.md,## Context\nWe have to structure our backend into a certain kind of architecture.\n,We stick to the [Hexagonal / Ports & Adapter](https://softwarecampament.wordpress.com/portsadapters/) architecture.\n,17,3660,17,27
corona-hackathon/0008-e-signature.md,"## Context\nWe have to make a decision, how we support e-signatures for work contracts.\n",We use [Signature Pad](https://github.com/szimek/signature_pad) to implement HTML5 canvas-based e-signatures.\n,21,3661,21,29
corona-hackathon/0003-application-framework.md,## Context\nWe have to decide for a programming language / application framework.\n,"We stick to [Spring Boot](https://spring.io/projects/spring-boot), with [Java 11](https://docs.aws.amazon.com/corretto/latest/corretto-11-ug/downloads-list.html) and [Gradle](https://gradle.org/).\n",16,3662,16,58
corona-hackathon/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3663,16,39
xyz-spaces-python/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3665,16,39
elife-spectrum/0002-polling.md,"## Context\nMost of the operations triggered on the system under test are asynchronous and we do not have a clear way of understanding when they are completed, if they are at all. [Bare sleeps lead to test instability](https://martinfowler.com/articles/nonDeterminism.html#AsynchronousBehavior).\n","Polling is the preferred approach to checks: it is minimally invasive for the system under test and promotes the creation of APIs to monitor the activities inside the different services.\nPolling is not necessary where we are guaranteed consistent state by design. For example, after an article has been published on the elLife 2.0 API it should be immediately available on the public-facing website.\n",62,3666,62,79
elife-spectrum/0003-fail-fast.md,"## Context\nA full test suite run may take > 10 minutes. Given a particular failure, at the end2end level we may encounter many different failures as consequences: for example, a missing file in the publication process may lead to errors in downstream services that rely on it.\nMany tests also use polling, waiting for a success condition like a 200 response.\n","Fail a test that detects something is wrong as soon as possible, without triggering additional checks or commands on the system under test.\n",76,3667,76,26
elife-spectrum/0001-parallel-testing.md,"## Context\nEnd2end tests are generally prone to have a long execution time, due to the number of different components involved, natural latency between different nodes and the amount of computation involved.\n","We will execute all tests in parallel, with a fixed batch of workers.\n",39,3668,39,16
elife-spectrum/0004-subsets.md,"## Context\nA full test suite run may take > 10 minutes, so we should only run tests that are necessary.\nWhile testing project A, running tests that only check projects B, C, ... does not contribute to the testing of A. Especially if the code from A is not exercised.\nWhile testing project A, tests that fail due to instability on other services B and C do not contribute to the testing of A in the same way.\n","For each project, run a subset of the tests marked with `@pytest.mark.projectname`.\n",93,3669,93,21
eq-questionnaire-runner/0004-use-kid-to-identify-key-pair.md,## Context\nWe need the ability to identify which keys are used to sign and encrypt messages so that we can support multiple keys\n,We will use the kid value in the header of the JWE and JWT tokens to identify the key that was used to sign or encrypt the message payload.\nThe kid value will be a SHA1 hash of the digest of the Public Key\n,26,3677,26,48
eq-questionnaire-runner/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3678,16,39
nhsuk-prototype-kit-version-one/0001-use-node-and-express.md,"## Context\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\n",We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\n,83,3681,83,73
nhsuk-prototype-kit-version-one/0003-use-npm-scripts-and-gulp-for-running-tasks.md,"## Context\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\n","We will use a mixture on NPM scripts and [Gulp](https://gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\n",49,3682,49,76
nhsuk-prototype-kit-version-one/0004-use-basic-auth-to-secure-published-prototypes.md,"## Context\nGiven that prototypes built using the NHSUK prototype kit will use the same styling and branding as live NHSUK services, if a member of the public came across the prototype they may confuse it with a live service. It would be beneficial to secure these prototypes behind some level of authentication.\n",We will use the [basic-auth](https://www.npmjs.com/package/basic-auth) package to add a layer of authentication when the prototype application is run on a production environment. This will allow the creator of the prototype to set a username and password to access the prototype when published.\n,60,3683,60,59
nhsuk-prototype-kit-version-one/0005-use-handlebars-as-default-templating-solution.md,"## Context\nPrototypes built using the NHSUK prototype kit are a combination of markup and logic that\nexhibit a standardised visual language. In order to provide developers easy access to the\nvisual language and to build dynamic, data-driven prototypes, a templating system that\nsupports partials and conditional rendering is required. As the intention is to encode\nprototype logic in Javascript, the templating system is not required to provide logic\nfunctions of its own.\n",We will provide the [Handlebars](http://handlebarsjs.com) templating system as the default\nfor the NHSUK prototype kit. A library of Handlebars partials will be provided that encode\nthe markup necessary for prototypes to use the standard visual language.\n,94,3684,94,55
nhsuk-prototype-kit-version-one/0002-use-npm-for-package-management.md,"## Context\nThe prototype kit will need a way to consume the dependencies it requires to run. These will include think like getting the correct version of `Express`, the framework that prototype kit app is built on, and pulling in the correct version of the NHSUK UI kit assets.\n","We will use NPM (Node Package Manager). Yarn, an alternative Node based package manager, when first released, had some benefits over NPM. These differences have now been implemented in the newer version NPM, Yarn and NPM are now interchangeable. The main benefit of NPM is that it comes packaged with Node.js and does not require an additional install like Yarn, removing a step from the setup process.\n",57,3685,57,87
smarthub/0004-use-openzeppelin.md,"## Context\nSmarthub SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\n","Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\n",42,3686,42,27
smarthub/0007-rename-utils-demo-package-to-migrations.md,"## Context\nWe use the `utils-demo` package to deploy the whole Smarthub system. After receiving some feedback, we noticed that the name `utils-demo` didn't reflect the true intention of that package.\n",We've decided to rename the `utils-demo` package to `migrations` and publish it on NPM.\n,45,3687,45,24
smarthub/0006-rename-assets-to-devices.md,## Context\nThere has been confusion on why we call generating and consuming devices Assets in Smarthub. We have noticed that most Smarthub users would refer to Assets as Devices and not Assets.\n,"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\n",41,3688,41,26
smarthub/0005-simplify-off-chain-storage.md,## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n,"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\n",52,3690,52,39
smarthub/0002-use-lerna-for-package-versioning.md,## Context\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n,Migrate code base to monorepo structure and use `lerna` for versioning management.\n,39,3691,39,21
smarthub/0003-create-backend-client-lib.md,## Context\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\n,Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\n,29,3692,29,39
smarthub/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3693,16,39
reviewer/0003-server-side-node.md,"## Context\nAs part of on-going development, it has been noticed (thanks Norris) that the\ntranpilation is not what was expected and we should use a later version of\nECMA script to target.\n","From [here](https://kangax.github.io/compat-table/es2016plus/#node12_11)\nThe suggestion is to use node v12 and target ""ES2019"" in your `tsconfig.json`.\nThe minor node version will be updated to match the minor version of the Alpine docker image for node (currently 12.15)\n### Discussion\nDone.\n",45,3705,45,80
reviewer/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3707,16,39
manage-frontend/04-node.md,"## Context\nIt's not intended for this app to have a significant server-side component, instead it will directly send calls from the front end to a service layer. This service layer will be discussed in its own project, but presently comprises solely of [members data API](https://github.com/guardian/members-data-api), the same back end that currently provides account management functionality.\n","The back end for this app will have two responsibilities. Firstly, it will act as a proxy for calls to the service layer. Secondly, it will provide server side rendering capabilities to improve user experience.\n",78,3708,78,41
manage-frontend/03-typescript.md,"## Context\nWe've found the use of strongly typed programming languages to be advantageous in reducing software defects. We have previously used Flow, however its error messages are regularly impenetrable. Without the burden of a legacy codebase to support, the benefits of Flow over Typescript are significantly less.\n",We will use Typescript as the programming language for this project.\n,61,3709,61,14
manage-frontend/02-react.md,"## Context\nThis application will need to present a user interface based on the result of multiple API calls.\nThe majority of client side developers at the Guardian have some React experience, and it is succesfully used in several projects.\n",We will build the front end in React.\n,45,3710,45,10
manage-frontend/00-lightweight-architecture-decisions.md,"## Context\nThis project is growing to the size and maturity where documenting the decisions made in its creation will be strongly beneficial.\nThe convention of lightweight architectural decisions [is set out by think relevance](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). It is set out in the form of an architectural decision document, and should be read as forming the basis of this document.\n","For all architectural decisions in this project, we will create an architectural decision record with a filename of `XX-decision-title.md` where XX is the monotonically increasing number described in the linked document.\n",87,3711,87,42
saas-platform-frontend/0008-use-launchaco-com-to-generate-the-logo-for-free.md,## Context\nThe landing page need a logo for the name. This could be design by hand but this requires skill and time or an online service could be used.\n,Generate the logo with [launchaco.com](https://www.launchaco.com/logo/editor).\n,34,3714,34,19
saas-platform-frontend/0003-use-javascript-over-typescript.md,## Context\nI need to decide in which language I implement the frontend.\n,I use Javascript.\n,16,3715,16,5
saas-platform-frontend/0004-use-client-side-rendering-create-react-app.md,## Context\nI need to decided if I use a SPA or a Server Side rendered app. This include performance and maintenance considerations.\n,I use Client Side Rendering (Create React App).\n,27,3716,27,11
saas-platform-frontend/0001-use-azure-pipeline-for-cicd.md,"## Context\nTo automated your development you need a CICD platform that automated the build, test and deploy steps.\n","I use Azure Pipeline to build, test and deploy.\n",25,3717,25,12
saas-platform-frontend/0006-use-antdesign-as-the-ui-framework.md,## Context\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\n,We use Ant Design as the UI Framework.\n,28,3718,28,10
saas-platform-frontend/0007-use-ant-design-landing-to-create-the-landing-page.md,"## Context\nBuilding a landing page has different requirements then building a normal applications. I have to decide I you the normal UI Framework (AntDesign), build most of the landing page without a framework or use the landing page provided by ant design.\n",Use [Ant Design Landing](https://landing.ant.design) to create the landing page.\n,50,3719,50,19
saas-platform-frontend/0009-use-cypress-for-e2e-testing.md,## Context\nWe need to test the frontend interface from the user perspective.\n,We use [cypress](https://docs.cypress.io/guides/overview/why-cypress.html#In-a-nutshell) as our frontend e2e testing tool.\n,16,3720,16,39
saas-platform-frontend/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md,## Context\nI need to decided with which tools I build my react app.\n,"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https://www.udemy.com/react-the-complete-guide-incl-redux/).\n",17,3721,17,38
saas-platform-frontend/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md,## Context\nWe need a workflow to build our appliation and components.\n,We use Storybook only for building new self contained components.\n,16,3722,16,13
saas-platform-frontend/0011-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3723,16,39
saas-platform-frontend/0010-use-aws-amplify-coginito-for-login-and-the-identity-provider.md,"## Context\nA user expect that he can create an account and login into our application. This should be standard compliant, economical (cheap) and easy to implement.\n",We use AWS Amplify (as a frontend lib) and AWS Cognito as the Identity Provider.\n,34,3724,34,21
opg-modernising-lpa-docs/0003-github-source-control.md,"## Context\nWe want to store our source code in a open source, cloud based git provider.\n",We should continue to use our Ministry of Justice Github Enterprise account for our source code.\n### Consequences\nWe will be able to use our existing management infrastructure for user management and deployments without the need for additional cost or resources.\n,21,3725,21,47
opg-modernising-lpa-docs/0005-terraform.md,"## Context\nWe need a way to manage our infrastructure as code (IaC) to support CI/CD and manage and provision our computing, storage and networking resources in the cloud.\nWe use Terraform extensively on all our existing services, as well as a central Terraform repository for managing all our services resources, permissions and security needs.\n",We should continue to use Terraform taking advantage of our existing lessons learnt and best practices.\n### Consequences\nWe will be able to take advantage of our existing best practices and knowledge to quickly setup any new infrastructure.\n,71,3726,71,45
opg-modernising-lpa-docs/0004-govuk-design-system.md,## Context\nWe need to follow strict government guidelines on design and accessibility. The design should be consistent across all pages and work for as many browsers as possible.\n,We should and have to use the [GOV.UK Design System](https://design-system.service.gov.uk/).\n### Consequences\nWe will be able to work at a high cadence and understanding as the [GOV.UK Design System](https://design-system.service.gov.uk/) is used throughout all our services on a day to day basis.\n,33,3728,33,74
riptide/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3745,16,39
optimint/adr-005-serialization.md,## Context\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\n,"`protobuf` is used for data serialization both for storing and network communication.\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\n",27,3746,27,37
optimint/adr-004-core-types.md,## Context\nThis document describes the core data structures of any Rollkit-powered blockchain.\n,"We design the core data types as minimalistic as possible, i.e. they only contain the absolute necessary\ndata for an optimistic rollup to function properly.\nIf there are any additional fields that conflict with above's claimed minimalism, then they are necessarily inherited\nby the ABCI imposed separation between application state machine and consensus/networking (often also referred to as ABCI-server and -client).\nWhere such tradeoffs are made, we explicitly comment on them.\n",18,3747,18,96
optimint/adr-006-da-interface.md,## Context\nRollkit requires data availability layer. Different implementations are expected.\n,"Defined interface should be very generic.\nInterface should consist of 5 methods: `Init`, `Start`, `Stop`, `SubmitBlock`, `CheckBlockAvailability`.\nThere is also optional interface `BlockRetriever` for data availability layer clients that are also able to get block data.\nAll the details are implementation-specific.\n",16,3749,16,70
tech-events-calendar/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3753,16,39
go-octopusdeploy/0001-validate-octopus-objects.md,"## Context\nWhen constructing Octopus Deploy objects, such a project, some settings are mandatory, and others accept some specific properties, for example:\n* A Projects **name** is mandatory `string` property.\n* A Projects **Default failure mode** supports a string of values `EnvironmentDefault`, `Off` or `On`.\nThe API will reject Adds or Updates with incorrect or missing properties.\n","* As this client will be used mainly for a Terraform provider, doing validation first using `terraform plan` would catch these errors before a `terraform apply` if they are validated via the client.\n* There will be a single go file that contains all of the valid values for the different objects, which can then be used in the Terraform provider for validation. This saves storing the valid properties in both the provider and the client.\n",83,3754,83,89
mat-process-utils/0006-use-jest.md,"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n",We will use Jest as our testing framework.\n,40,3757,40,10
mat-process-utils/0005-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,69,3758,69,57
mat-process-utils/0007-use-dependabot-to-keep-dependencies-up-to-date.md,## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n,We will use Dependabot to monitor dependency updates.\n,38,3759,38,12
mat-process-utils/0003-use-rollup-to-build-distributables.md,## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n,We will build distributables using Rollup.js.\n,62,3761,62,12
mat-process-utils/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3763,16,40
unit-e-project/ADR-0007.md,"## Context\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\neven if the block is added to the index, its transactions are not added to the txdb.\n",We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\n,50,3764,50,73
unit-e-project/ADR-0004.md,## Context\nPutting the *kernel hash* into the Block Header of Unit-E was\n(proposed)[https://github.com/dtr-org/unit-e/issues/14]. Thinking\nabout it revealed that it does not help us.\n,We will not make the kernel hash or stake modifier part of the block header.\n,47,3767,47,17
elife-xpub/0001-pull-request-workflow.md,"## Context\nSoftware is built incrementally as an accumulation of changes.\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\nShort-lived pull requests allow:\n- visibility of who is changing what.\n- discussion and review from other people in the team.\n- automated testing and other kind of checks to run, offloading work from humans to machines.\n",We will open short-lived pull requests as the primary means for deploying a change.\n,85,3778,85,17
wikimediafoundation-org/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3781,16,39
TANF-app/001-ADR-template.md,## Context\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",37,3784,37,39
TANF-app/004-configuration-by-environment-variable.md,"## Context\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\n",We will use environment variables to configure applications.\n,35,3785,35,10
TANF-app/006-continuous-integration.md,"## Context\nFor improvement of our software engineering processes, we should implement continuous integration.\n",We will use CircleCI as our continuous integration platform.\n,18,3788,18,12
TANF-app/010-assign-superuser.md,"## Context\nUsually in Django we would assign the first superuser through the CLI, but because this will not\nbe available in production, we will need another method.\n","The Django Admin provides the easiest way to assign superuser status, so for most users that will\nbe the method employed. However, we still need a method for creating the first superuser, so that\nuser will be able to assign others as needed. We will assign this with a data migration using a\nusername defined in environment variables.\n",35,3792,35,70
TANF-app/011-buildpacks.md,"## Context\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https://dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\n","Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https://cloud.gov/docs/technology/responsibilities/) and ensure tightened security.\n",99,3793,99,46
TANF-app/002-Application-architecture.md,"## Context\nWhen designing the solution, we needed to establish our technology stack best suited for the solution.\n","For TANF Data Portal, the team decided that the backend shall be  Django Rest Framework (DRF) app and the frontend is ReactJS.\n",22,3796,22,32
register-trainee-teachers/0006-academic-cycles.md,"## Context\nWe have introduced the concept of academic cycles. All trainees, courses and funding rules will need to be linked to an academic cycle.\n","We need a way to ""associate"" all of the given entities above to reliably know a trainee's academic cycle and the associated funding rules applied for that cycle.\nOur `funding_methods` table will have a foreign key linking to the `academic cyles` table. Since funding rules are cycle specific, it made sense to have a hard association between these two tables as their start and end dates correlate.\n",31,3799,31,84
register-trainee-teachers/0004-non-integer-trainee-ids.md,## Context\nWe have been using trainee IDs in the URL as per standard Rails convention/operation.\nIt was felt that we should use non-integer IDs for a number of reasons:\n* remove predictability\n* interoperability with other systems without depending on DB IDs\n,We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\noption 1 would have caused.\n,57,3802,57,35
register-trainee-teachers/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3803,16,39
mercury-platform/0003-data-will-be-stored-and-treated-independent-from-triggers.md,## Context\nThere are a number of independent microservices as part of the Mercury Platform and each one of them needs to operate with our data in one way or another.\n,"In order to maximize reusability of the data and to make triggers as generic and lightweight as possible, data will be stored completely separately from other services and will not act as a data flow trigger in any way.\n",35,3808,35,44
mercury-platform/0002-use-controller-to-manage-all-input-from-users.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nWhile the initial idea was to have input be in the form of a file creation in a trigger bucket, in order to appropriately deliver results and progress reports for users, we will need a smarter solution.\nWe would also like to theoretically be able to provide a generic interface so that our tool could be used programmatically or via a UI as is desired by the end user.\n","We will create a JSON API controller for all end user interactions with the platform. This would handle the managment of the jobs concept (however it is to be implemented) and provide users a consistent way to create, view the status of, and retrieve results for their data processing jobs\n",97,3809,97,57
mercury-platform/0004-mercury-platform-is-deliberately-a-polyglot-system-with-language-suggestions.md,"## Context\nMercury platform could be accomplished by a handful of python scripts run locally on a consumer system.\nHowever, the project is intended to also be a learning platform that uses the strengths of individual languages and cloud native services\n",Mercury platform will use multiple languages to achieve it's goal.\nThe proposed list:\n- Data retrieval and manipulator (Wrangler): Elixir\n- Data processor and analyzer: Python\n- Controller for end user input: Go\n- Infrastructure: Pulumi via TypeScript\nLanguages will be changed out as we discover more about each language's limitation with regard to our needs.\n,47,3810,47,77
mercury-platform/0005-use-cloud-native-services-when-appropriate-and-prioritize-minimal-permanent-infrastructure.md,"## Context\nMercury Platform intends to be a teaching platform as well as a financial data processing platform.\nOne core principle is to use the tools that are best suited for the needs and prioritizing modern DevOps best practices (immutable deployments, microservice architecture, etc).\nThere is also an additional need of keeping costs low while income is minimal or non-existant.\n","The Mercury Platform will have minimal permanent infrastructure, prioritizing serverless tools when possible.\nContainers will be used whenever possible to promote reliable deployments, cloud-agnostic services, and ease of local testing.\nCloud native services will be used when they meet needs and are cost-effective.\n",76,3811,76,57
mercury-platform/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3812,16,39
adr-viewer/0005-distinguish-amendments-to-records-with-colour.md,## Context\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\n,"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\n",33,3813,33,33
adr-viewer/0002-expose-command-line-interface.md,"## Context\nWe want to maximise the usability of adr-viewer whilst maintaining flexibility in future for other output formats, e.g. a live webserver.\n",The entry point for this project will be a command-line utility called `adr-viewer`. We will use the python [click](http://click.pocoo.org/5/) library to provide command-line options and documentation.\n,33,3814,33,46
adr-viewer/0006-accessibility-as-a-first-class-concern.md,"## Context\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\n",Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\n,40,3815,40,23
adr-viewer/0003-use-same-colour-for-all-headers.md,"## Context\n`adr-viewer` presents all records with the same `lightgreen` header, even though records may be in different states.\n",We will keep the `lightgreen` colour for everything\n,30,3816,30,12
adr-viewer/0004-distinguish-superseded-records-with-colour.md,"## Context\n`adr-viewer` presents all records with the same `lightgreen` header, even though records may be in different states.\n",Records marked as 'Superseded' will distinguish themselves from 'Accepted'\n,30,3817,30,16
adr-viewer/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3818,16,39
log430-dashview-architecture/0003-systemvehicule-this-is-a-test.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3819,21,13
log430-dashview-architecture/0002-use-java-to-implement-complete-solution.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,3820,21,13
log430-dashview-architecture/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3821,16,39
meadow/0008-api-documentation.md,## Context\nWe want our [API](./0004-api.md) to be self-documenting and testable as we go.\n,Use an [OpenAPI hex package](https://hexdocs.pm/open_api_spex) to automate OpenAPI tasks.\n,28,3825,28,25
meadow/0023-uuids.md,"## Context\nMeadow's predecessor, DONUT, uses straight UUIDs (not ULIDs) as primary identifiers.\nSince DONUT's data will be migrated to Meadow, maintaining backward compatibility is\nmore important than the improved aesthetics or lexical sorting of ULIDs.\n",Remove ULID identifiers in favor of UUIDs.\n,56,3826,56,11
meadow/0011-yarn.md,## Context\nWe discussed the relative merits of different JS package managers.\n,"Use `yarn` instead of `npm` in all dev, test, and build environments.\n",15,3827,15,21
meadow/0009-tailwind-css-framework.md,"## Context\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\n",Use the Tailwind CSS framework for design and layout.\n,70,3829,70,12
meadow/0006-honeybadger.md,## Context\nDev team needs good error reporting and tracking to help resolve\nruntime issues efficiently\n,Use [Honeybadger](https://honeybadger.io/)\n,19,3830,19,17
meadow/0016-ingest-pipeline-spec.md,"## Context\nPer [issue #1104](https://github.com/nulib/next-generation-repository/issues/1104): Developers need a (basic/nothing fancy) general, conceptual/overall understanding/plan of what form the ingest pipeline will take so that different pieces may effectively be worked on by different people.\n","We developed a specification for a flexible, message-driven [Ingest Pipeline](../specs/ingest_pipeline.md).\n",66,3832,66,24
meadow/0019-directory-layout-revisions.md,"Amends [15. Phoenix Context Organization](0015-phoenix-context-organization.md)\n## Context\nThe code organization we agreed upon in [15. Phoenix Context Organization](0015-phoenix-context-organization.md), has become a bit unweildy in practice - specifically the nested schema module names and confusion around function placement within the hierarchy.\n",Put schemas inside a `schemas` directory at the top level of the context and place all schemas inside. Flatten other files at the top level alongside the schemas.\n,72,3833,72,33
meadow/0004-api.md,"## Context\nWe discussed options for the architecutre/tooling of our API including a JSON/REST API, GraphQL API.\n","We decided to implement a JSON API documented with [OpenAPI](https://swagger.io/docs/specification/about/) (formerly Swagger), using the OpenAPI 3.0 specification. We think that since GraphQL would be an additional learning curve, for now, it is better to stick with REST and enhance with GraphQL in the future if desired.\n",28,3834,28,69
meadow/0017-preservation-strategy.md,"## Context\nHaving a ""preservation first"" mindset has been decided upoan as a stated goal for Meadow. Generally, this means that digital preservation\npolicies, processes and deliverables should be planned and implemented in tandem with development of the applications features, processeses and infrastructure.\n","The digital preservation lifecycle for a Work and its FileSets begin as soon as an Ingest Sheet is ""approved"". Actions in the ingest pipeline are used to\nadd digital preservation ""artifacts"" to Work and FileSet metadata (such as checksum and timestamps) and move objects to preservation storage buckets in S3. Additionally, the\nsuccess and failure outcomes of these actions are added as AuditEntries that can be used verification, future audits and problem resolution.\n",59,3835,59,92
meadow/0005-multistage-docker.md,"## Context\nWe need an efficient, automated build process that creates an Elixir\nrelease within a compact Docker container.\n","We use a [multi-stage Dockerfile](https://docs.docker.com/develop/develop-images/multistage-build/) to install Elixir dependencies,\nbuild JavaScript assets, and create the Elixir release in separate\ncontainers, then copy all of the artifacts into a bare-bones Alpine\nruntime image.\n",25,3836,25,64
meadow/0020-test-coverage-strategy.md,"## Context\nLooking at our waning test coverage, we decided we needed a review of our test coverage\nstrategy, especially a consideration of which parts of the project should be included\nin test coverage reports.\n","* If low-level code is consistently coming up as uncovered, first check to\nsee if anything in the project is actually using that code. If not, remove\nit.\n* Continue to write/run front end tests as needed, but remove them from the\ncoverage report.\n* Remove mocks, views, and GraphQL types from the coverage report.\n",43,3837,43,72
meadow/0024-iiif-manifests.md,"## Context\nSince January, when we developed the strategy for IIIF Manifests in ADR 22, we have decided that Donut/Glaze will be decomissioned at or before the time that Meadow/Fen are live in production.\nIn light of this, we decided to go with a simpler approach to routing requests for Meadow vs. Donut manifests in the staging environment.\n","We will not build the lambda that checks for the host application, instead we will use the API Gateway to route all requests for public IIIF manifests to the pyramids bucket's public directory on S3.\n",80,3839,80,42
meadow/0014-active-directory-groups.md,## Context\nMeadow needs a mechanism to track user privileges.\n,Use existing Library Active Directory group membership to map users to\nsets of privileges and access controls.\n,15,3840,15,20
meadow/0029-npm.md,## Context\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\nSupersedes [11. Yarn](0011-yarn.md)\n,"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\n",37,3841,37,23
meadow/0012-websockets.md,"## Context\nWe need a way to provide live updates to the front-end for ingest sheet validation,\ningest status, etc.\n",We will use the [WebSocket API](https://www.w3.org/TR/websockets/) via [Phoenix Channels](https://hexdocs.pm/phoenix/channels.html) to enable real-time communication between the client and server.\n,27,3845,27,46
meadow/0003-terraform.md,## Context\nPreviously we've kept Terraform code in a separate Github repository (nulterra) and this has had the effect of making changes more cumbersome.\n,We've decided to keep Meadow Terraform code inside the Meadow repository in a root directory called `terraform`.\n,33,3847,33,23
meadow/0010-dependencies.md,"## Context\nWe want to guard against out-of-date dependencies, especially those with security issues.\n",Use github's [dependabot](https://dependabot.com/) to track dependencies and generate\npull requests to stay up to date.\n,20,3848,20,30
meadow/0013-use-graphql-for-api.md,"## Context\nGraphQL is a different way to think about APIs and can overcome some of the shortcomings of REST such as overfetching/underfetching, inflexibility, and also provides greater in depth analysis.\n",We have agreed to change our API from REST documented with OpenApi to GraphQL.\n,44,3849,44,17
meadow/0002-ulids.md,## Context\nPostgres can autoincrement identifiers for database tables. By default this is an integer.\n,"We decided to use [ULID](https://github.com/ulid/spec)'s for all database id's.  Serial integers present potential problems with scaling and concurrency. Unlike UUID's ULID's are lexicographically (i.e., alphabetically) sortable as the first 48 bits of the identifier contain a UNIX timestamp.\n",21,3850,21,68
meadow/0007-code-analysis.md,## Context\nWe need to make sure we adhere to our own designated code quality best practices.\n,"Use a code analysis tool (specifically, [credo](http://credo-ci.org/) for Elixir\nand [prettier](https://prettier.io/) for JavaScript).\n",20,3853,20,41
meadow/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3854,16,39
android-guidelines/0001-create-item-view-states-in-adapters.md,## Context and Problem Statement\nCreation of list items' ViewState in ViewModel/Usecase causes lots of model wrapping/operation during the data flow from ViewModel/Usecase to UI layer.\n,"In the context of item view state creation facing concern of tight coupling we decided to create item view states in adapters and neglected creating item view states in viewmodels/usecases, to achieve consistency and seperation of concerns.\n",38,3858,38,44
android-guidelines/0002-custom-views-should-have-their-own-models.md,## Context and Problem Statement\nUsing of common models in custom views causes tight coupling and to block moving as a library.\n,"In the context of custom view creation facing concern of tight coupling we decided to create models just for custom view's domain and neglected using common UI models, to achieve loose coupling between app components.\n",25,3859,25,39
android-guidelines/0009-gradle-rules.md,## Context and Problem Statement\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\n,* Disable generating BuildConfig file if its not needed in module.\n* Only enable *databinding* if you're going to use DataBinding in that module.\n* Do not apply *kapt* plugin if you're not going to use.\n* Do not create new variants other than *debug* and *release*.\n* Use groovy scripts on *build.gradle* files.\n,37,3860,37,83
android-guidelines/0004-use-template-for-mapper-class.md,## Context and Problem Statement\nLack of a certain method in mapper use causes implementation confusion.\n,"In the context of implementation mapper classes facing concern of methods of application we decided to use template and neglected using interface, to achieve project integrity and consistency.\n",20,3861,20,31
android-guidelines/0010-dependency-rules.md,## Context and Problem Statement\nNumber of module count has been increased because of new features and new channels. Dependencies between modules were getting hard to comprehend.\n,Channels should not depend on other channel's features. Only common modules can be used between channels.\nCheck our [module guideline](../../module_guideline/module_guideline.md) to understand more about build dependency management.\n,31,3862,31,44
android-guidelines/0007-use-deprecated-annotation-with-description.md,## Context and Problem Statement\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\n,"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\n",24,3863,24,22
android-guidelines/0006-no-repository-implemantation-to-view-model.md,## Context and Problem Statement\nimplementation repository layer directly in viewModel causes all methods in repositooru to be accessed from view model\n,In the context of implementation repository directly in viewModel facing concern of give the view model access on repository we decided to create useCase class for view model and neglected using repository class in view model to achieve view model is granted access to only the required methods.\n,27,3864,27,51
android-guidelines/0000-create-multiple-ui-network-models.md,"## Context and Problem Statement\nUsing response model directly in UI layer causes to create lots of god object, prevent flexibility and increase dependency with each layer.\n","In the context of creating multiple model classes for network and ui layers facing concern of creating god objects and limiting the ability to modularize the application we decided to use multiple model classes and neglected using a single model class, to achieve flexibility and fewer dependencies.\n",31,3866,31,51
android-guidelines/0005-use-same-mapper-for-library-and-ui-model.md,"## Context and Problem Statement\nConverting the response model required for the library to ui model and then to library model, causes to unnecessary mapper classes and ui models.\n","In the context of creating library model facing concern of convert process we decided to convert response model to library model in the same mapper class and neglected convert response model to ui model, then to library model in a new mapper class, to achieve use fewer model and class.\n",34,3867,34,54
cygnus-infra/0003-install-kubernetes-directly-on-metal-as-a-single-node.md,"## Context\n- I wasn't getting anything built with all of the libvirt shenanigans\n- I have a NAS now, so things I'm worried about running on Kubernetes can go there\n",Install Kubernetes straight onto a Debian install on `swan`. Use MetalLB or similar later to manage the routing.\n,40,3869,40,24
cygnus-infra/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3871,16,39
bosh-bootloader/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3874,16,39
gsp/ADR000-template.md,## Context\n,We will ...\n,3,3875,3,4
gsp/ADR010-placement-of-ci-cd-tools.md,"## Context\nThe placement of the CI and CD toolset, either within or external to the control cluster and / or tenant cluster, determines most aspects of the build and deployment toolset and influences architectural decisions.\n",- The CI and CD tools will run separate from the control cluster\n- The CI and CD tools will run within their own kubernetes cluster\n,43,3878,43,29
gsp/ADR009-multitenant-ci-cd.md,"## Context\nTwo models have been proposed concerning CI and CD tool sets:\n1. Multi-tenant: all tenants, including Reliability Engineering, share that same CI and CD instance\n2. Per-tenant: each tenant has their own CI and CD cluster\n",- There will be a single CI and CD toolset used by all tenants of the new service\n,54,3882,54,20
gsp/ADR007-identity-provider.md,## Context\nWe need to provide a way to authenticate users who will interact with our Kubernetes clusters.\nWe do not have a organisation-wide identity provider. Virtually everyone will have a Google account. Many people will have a GitHub account.\nPeople working on GitHub repositories are likely the same people who are deploying to a cluster. Access to repositories likely indicates which users should have access to a cluster. We can reuse this user:team mapping in order to control access to clusters.\n,We will use GitHub as our identify provider.\n,97,3885,97,10
gsp/ADR002-containers.md,"## Context\nAt the time of writing the infrastructure/deployment landscape is:\n* Many service teams are deploying applications to Virtual Machines (AWS EC2, VMWare, etc)\n* Some service teams are deploying applications as containers (AWS ECS, GOV.UK PaaS, Docker)\n* Few service teams are deploying applications as functions (AWS Lambda)\nThere is a mix of target infrastructure/providers in use, but there is a gradual migration towards hosting on AWS.\n",We will focus on providing the primitives to run stateless containerised workloads.\n,97,3892,97,17
gsp/ADR021-alerting.md,## Context\nThe teams need timely notifications based on key indicators in order that they can ensure reliability and respond to issues.\nThe prometheus operator included in the GSP cluster can provide Alertmanager however we would like to manage alert routing across GDS and not duplicate routing rules or manage multiple sets of alert targets.\n,We will route alerts to a separately hosted shared [Alertmanager](https://prometheus.io/docs/alerting/alertmanager/) to handle platform alert routing\n,63,3894,63,30
gsp/ADR014-sealed-secrets.md,## Context\nWe want to provide a simple way to pass sensitive values into environments via git.\nCurrently the only way to do this is by directly interacting with the cluster to inject secrets.\n,"We will deploy a [SealedSecrets](https://github.com/bitnami-labs/sealed-secrets) controller that allows sealing (encrypting) Kubernetes Secrets with a public key unique to each environment, making them safe to store as part of their deployment.\n",39,3904,39,55
gsp/ADR025-ingress.md,## Context\nWe currently have two [ingress][Ingress] systems:\n* Istio (see [ADR019])\n* nginx-ingress (see the old Ingress [ADR005])\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\nDo we need both?\n,No. We will use an [Istio Ingress Gateway](https://istio.io/docs/tasks/traffic-management/ingress/ingress-control/)\n,69,3905,69,34
gsp/ADR018-local-development.md,"## Context\nTeams using the GDS Supported platform require the ability to develop, test applications and prove conformance with the GDS Supported platform on local hardware. Teams need to learn how to use the GSP and to understand how applications are containerised, packaged and deployed to a cluster using the standard CICD tools provided by GSP.\n",We will [provide a way to run a full GSP compatible stack locally on a developer machine](/docs/gds-supported-platform/getting-started-gsp-local.md) without the cloud provider specific configuration.\n,70,3912,70,43
gsp/ADR038-sre-permissions-istio.md,"## Context\nDuring [ADR032](ADR032-sre-permissions.md), only core Kubernetes resources were considered. (The context in that ADR is relevant here and should be read first.)\nIt was realised after this that other things, such as Istio networking rules, also needed to be deleted sometimes, but could not be.\nThese resources should all be ultimately sourced from Git and, if removed in error, should be replaced automatically the next time the deployment pipeline is run.\n",We will add to the SRE permissions map the ability to delete the following Istio networking resources so an escalation to cluster admin is no longer necessary:\n* VirtualService\n* Gateway\n* ServiceEntry\n* DestinationRule\n* EnvoyFilter\n,100,3916,100,51
helix-authentication-extension/0003-use-libcurl-library.md,"## Context\nThe authentication integration in Helix Core server is implemented using extensions. The library of functions available to extensions is limited to HTTP/S calls using `libcurl`, and not much else. For instance, parsing XML is not available without writing your own parser. As such, the heavy-lifting of the integration is handled by an external authentication service. To connect to this service, the extension would use HTTP/S.\n","The only real option is to use `libcurl` to connect to the service from the extension. Other possibilities that were explored by the architect at the time included web sockets and public key cryptography. However, web sockets are complicated to build in Lua, and unnecessary when we can just use HTTP calls with client certificates. The use of SSL certificates also removes any benefit of public key cryptography, which is not available in Lua without building OpenSSL into the server.\n",85,3922,85,91
tech-radar/adr-3_structure_of_pattern.md,"## Context\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\n",We propose the following sections for pattern artefacts:\n* Context\n* Problem (optional)\n* Solution\n* Examples\n* Exceptions\n* Customer Facing\n* Integration Framework (Messaging)\n,26,3924,26,40
dos-capacity-status-api/009-proxy_limiting.md,## Context\nIn order to protect DoS services from being overloaded by too many capacity service api requests the proxy will throttle the numbers of requests per second that can be sent\n,The proxy will throttle the api requests to 3 requests/sec with an allowed burst of 8 requests in a 10 minute period\n,35,3931,35,27
dos-capacity-status-api/007-fhir_interface.md,## Context\nTODO: [What is the issue that we're seeing that is motivating this decision or change]\n,TODO: [What is the change that we're actually proposing or doing]\n,23,3934,23,16
dos-capacity-status-api/002-shared_database.md,"## Context\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\n","The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\n",54,3937,54,96
ensemblejs/0002-replace-zepto-with-jquery.md,## Context\nThe current approach for templates uses pug (jade) templates that are browserified into the client during the build step. These are appendend to the document. The issue with this approach is that `script` tags are not executed.\njQuery has an method called `getScript` that will get a script and in the success callback we can run more javascript. The zepto port of jQuery does not support this method.\n,Replace all instances of zepto with jQuery.\n,90,3938,90,10
ensemblejs/0003-use-immutablejs-for-immutability.md,"## Context\nThe cost of cloning JSON, while fast degrades with object size and as we're doing it serveral times per frame the cost is too much.\n","Implement ImmutableJS to avoid the need to clone data. I'm hoping the internal behaviour of ImmutableJS is smart enough to avoid the cost of cloning by cleverly moving references around. The data has to be created once, but I am hopeful that that is the only time.\n",34,3939,34,56
ensemblejs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3940,16,39
bus/0001-use-sdks.md,"## Context\nMultiple projects need to publish or subscribe to messages from the `bus`.\nProjects are written in a fixed set of supported programming languages.\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\n","Access the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\n- [bus-sdk-php](https://github.com/elifesciences/bus-sdk-php)\n- [bus-sdk-python](https://github.com/elifesciences/bus-sdk-python)\n",64,3941,64,58
bus/0003-raw-message-delivery.md,"## Context\nSNS is a service originally designed for notifications (e.g. smartphone push notifications) rather as a message queue. Its configuration needs to be tuned for our use case.\nIt's important to keep the channel as easy to use as possible, without additional complications.\n",Use [RawMessageDelivery](https://docs.aws.amazon.com/sns/latest/dg/large-payload-raw-message.html) on all SQS-to-SNS subscriptions.\n,57,3942,57,36
bus/0002-sns-sqs.md,"## Context\neLife's internal software is deployed on AWS.\nLow latency, short links between queues, publisher and consumers improve performance and security.\nConsistent technologies are easier to manage at the infrastructure level.\nThere is a varying (sometimes small) load over the `bus`, which pushes for a service-based solution rather than running more servers.\n",Use SNS topics and SQS queues subscribed to those topics for message delivery.\n,72,3943,72,17
bus/0004-ownership.md,"## Context\nThe `bus` is designed to decoupled projects from each other, rather than adding a many-to-many web of API calls.\nThe set of SNS topics should be very much stable as it contains published content types from the API.\nQueue listeners are deployed with each interested project, and are the responsibility of each project to empty.\n","Each project should own queues for the content types it consumes, while the topics should be owned by a global `bus` project.\n",72,3944,72,27
my-cv/0003-single-source-of-truth.md,"## Context\nWe want the data to be in one place, be it internal (in the static assets) or external (served by an API or an S3 bucket...)\nWe want the project to be data-driven, so that the document is re-rendered if any data changes.\nWe want the data being spread across the Components using the latest technologies.\n","The source of data is kept internally. It will stay in the webapge, as a static asset from now on.\nBut the project must be kept easily switchable to an external data source.\nWe spread the data across Components using React Context API.\n",76,3945,76,54
my-cv/0002-design-principles.md,## Context\nWhat do we want to achieve here and how we will achieve it ?\n,This project is:\n1. For recruiters. We provide the printable A4 resume they need for the recruitment process;\n2. Easy to develop and maintain. Based on the latest web technologies;\n3. Easy to share. Automaticallly deployed and hosted using the best of AWS;\n4. A demo of my current skills;\n,19,3946,19,70
my-cv/0001-record-architecture-decisions copy.md,"## Context\nWe need to record the architectural decisions made on this project. Not all decisions will be made at once, nor will all of them be done when the project begins. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite-sized pieces are easier for all stakeholders to consume.\n","We will use Architecture Decision Records, as [captured on ThoughtWorks technology radar](https://www.thoughtworks.com/radar/techniques/lightweight-architecture-decision-records).\n",87,3947,87,39
verify-self-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3951,16,39
PerformanceTestDocs/0006-vellum-node-fails.md,## Context\nVellum node fails\n,Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\n,9,3952,9,29
PerformanceTestDocs/0003-sprout-fails-after-dialog-established.md,## Context\nA sprout node fails after a dialog is established.\n,"the P-CSCF or UE retries – the Route header specifies the sprout cluster, so the P-CSCF applies round-robin DNS processing to them\n",15,3953,15,33
PerformanceTestDocs/0005-homestead-dime-fails.md,## Context\nHomestead instance fails before receiving request from sprout.\n,"Homestead’s HTTP interface is simple. If one homestead instance does not respond to sprout, sprout tries a different one\n",15,3954,15,27
PerformanceTestDocs/0004-sprout-fails-when-transaction-in-progress.md,"## Context\nA sprout node fails while a transaction is in progress, the transaction fails.\n","Either the UE will retry automatically or it will display an error to the user, who should retry. Transaction fail can't be routed to sprout2 (As from that point on the P-CSCF will not retry the transaction).\n",20,3955,20,48
PerformanceTestDocs/0002-sprout-fails-before-receiving-request.md,"## Context\nA sprout node fails before receiving a request from the P-CSCF, and the request fails.\n",P-CSCF retries to sprout2\n,25,3956,25,10
PerformanceTestDocs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,3957,16,39
PerformanceTestDocs/0007-vellum-fails-whilist-a-request-is-outstanding.md,## Context\nA Vellum node fails whilst a request from Sprout or Dime is outstanding. this will typically result in failure of the request that the Sprout / Dime node was processing (with a return code indicating that it should be retried)\n,"Sprout / Dime node sends request which is retried an alternative, Vellum node will be used instead\n",54,3958,54,24
PIMS/monorepo.md,## Context\nOur project involves developing a number of tools and layers to support PIMS.\nThis includes at present the following;\n- Frontend GUI Web Application\n- Backend RESTful API\n- Frontend GIS components\n- Backend database\n- Keycloak integration\n- OpenShift integration\n- Docker integration\n- ETL tools\n,The Exchange Lab's practice is to use the monorepo solution.\n,69,3963,69,15
PIMS/model-mapping.md,## Context\nThe current model mapping library [AutoMapper](https://automapper.org/) ([GitHub](https://github.com/AutoMapper/AutoMapper)) requires a lot of effort to configure.\nIt is near impossible to debug.\nIt is not intuitive to develop with.\nThe benefits and features it offers are far outweighed by the time invested in implementation and maintenance.\nA new library is required to speed up development and improve the debugging experience.\n,"[Mapster](https://github.com/MapsterMapper/Mapster/wiki) ([source](https://github.com/MapsterMapper/Mapster)) provides a more intuitive solution, along with performance benefits.\n",93,3964,93,44
PIMS/map-source-boundaries.md,## Context\nTo improve usability of the map it is required that we include the parcel boundaries as a default layer.\nData BC currently provides mapping layers (which includes parcel boundaries).\n- [openmaps.gov.bc.ca](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/web-based-mapping/map-services)\n,Add the **Data BC** openmaps parcel boundaries layer to the default map.\n,68,3965,68,17
PIMS/database.md,"## Context\nPIMS requires a database to store all property information.\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\nAdditionally it must be supported by Entity Framework Core 3.1.\n","Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\nThis will give us better performance and tighter integration with Entity Framework Core.\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\n",49,3966,49,68
PIMS/programming-languages.md,"## Context\nWe need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.\nThe languages selected should be part of the skillset and expierence of the team members.\nThey should have broad industry support.\nThey should be supported by GIS related plugins, libraries and tools.\n",We are choosing TypeScript and React for the front-end.\nWe are choosing .NET Core 3.1 for the back-end API.\n,82,3967,82,29
PIMS/leaflet.md,"## Context\nThe project requires a GIS component to display a map, properties on the map and property boundaries.\nThe following options were reviewed;\n- [ArcGIS](https://www.arcgis.com/index.html)\n- [Leaflet](https://leafletjs.com/)\n- [Data BC](https://data.gov.bc.ca/)\n- [LTSA](https://ltsa.ca/)\n",The PIMS project will use Leaflet for the basic GIS solution.\n,85,3969,85,15
dns-resolution/0001-migrate-to-pihole.md,"## Context\nBlocking ads at the DNS level would save bandwidth and provide a better user exprience on the home network. To provide that functionality, I would like to use PiHole.\n",Replace stock dnsmasq with PiHole docker image.\n,40,3970,40,14
dns-resolution/0000-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3971,16,39
documents-api/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3988,16,39
blog.prskavec.net-hugo/0002-choose-deployment-platform.md,"## Context\nDeployment can be done into CDN and static pages (Github Pages, S3, Netlify)\n",I choose Netlify for custom domain with TLS and very good support for Hugo.\n,23,3989,23,17
blog.prskavec.net-hugo/0001-choose-static-generator.md,## Context\nAll my website are updated only a few times per year and I need that will works without backend system.\n,"I decide using plain html over Wordpress, Ghost or similar solution for simplify process publishing new articles using only git and my current editor (VSCode). No costs for hosting and better reliability than my own hosting.\n",25,3990,25,42
blog.prskavec.net-hugo/0000-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3991,16,39
bedrock/0005-use-a-single-docker-image-for-all-deployments.md,"## Context\nWe currently build an individual docker image for each deployment (dev, stage, and prod) that contains the\nproper data for that environment. It would save time and testing if we only built a single image that could\nbe promoted to each environment and loaded with the proper data at startup.\n",We will use a Kubernetes DaemonSet to ensure that a data updater pod is running on each node in a cluster. This\npod will keep the database and l10n files updated in a volume that will be used by the other bedrock pods to\naccess the data.\n[GitHub issue](https://github.com/mozmeao/infra/issues/1306)\n,62,3995,62,76
bedrock/0004-use-fluent-for-localization.md,"## Context\nThe current localization (l10n) system uses the outdated and unsupported .lang format, which our l10n team would prefer\nto no longer support. Mozilla's current l10n standard for products and websites is [Fluent][].\n",In order to update our l10n practices and technology and support from Mozilla's existing l10n infrastructure and teams\nwe will decomission the .lang system in bedrock and implement one based on [Fluent][]. We will support both during a\ntransition period.\n,55,3996,55,56
bedrock/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,3998,16,39
decode-demo/0003-remove-spectre-css-and-replace-with-bootstrap.md,"## Context\nThe deployed demo site must be designed to render nicely on mobile devices.\nThis is so that participants in the pilot can pretend the demo site provides\nthe same basic user experience as the wallet once this exists.\nSpectre CSS at least in the configuration implemented in the original version\nof this app doesn't render a UI that is usable from a mobile device,\nparticularly when it comes to form inputs (buttons and fields were tiny and\nhard to read).\n",We will replace [Spectre.CSS](https://picturepan2.github.io/spectre/) with\n[Bootstrap](https://getbootstrap.com/).\n,98,3999,98,33
decode-demo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4001,16,39
karma-sabarivka-reporter/0001-documenting-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4002,16,39
karma-sabarivka-reporter/0003-high-test-coverage-thresholds.md,"## Context\n* This is Open Source app which may be used by big variety of projects\n* We want to have this app stable and safe\n* @kopach, as main contributor and author of this lib treats this project as place, where best practices should met\n",We'll use 100% coverage threshold for all source code written in this project\n,55,4004,55,17
apply-for-teacher-training/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4024,16,39
govuk-aws/0025-use-elasticache-for-redis.md,"## Context\nTraditionally we provisioned our own redis machines in a non-clustered state.\nAWS provide Elasticache which has a Redis engine, and can be configured to be clustered.\nWe should consider provisioning Elasticache to replace our provisioned Redis instances.\n",We are using Elasticache instead of provisioning our own Redis instances.\n,54,4029,54,14
govuk-aws/0012-security-groups-in-terraform.md,## Context\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\n,"Security groups will be defined separate to their rules.\nAdditionally each security group will be defined, in its entirety, in a single file.\n",89,4032,89,29
govuk-aws/0026-remove-load-balancer-tier.md,"## Context\nWe currently have a dedicated tier of load balancers for the backend, frontend and api\nVDCs. In the Amazon world these are less useful as we also have ELBs in place to handle\nthe autoscaling of instances. We want to reduce the networking complexity and remove these machines.\n","We will not change the current vcloud based platforms.\nIn the AWS environment we will remove the load balancer hosts / ELBs and instead add external\nELBs to a number of other autoscaling groups (such as the frontend / backend hosts themselves)\nto make them directly available, but security group restricted.\nWe have also decided to split the domains we use to an internal and external one. That\nis discussed in ADR TODO: TBC.\n",63,4038,63,95
govuk-aws/0002-hosting-platforms.md,"## Context\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\n","We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https://github.com/alphagov/gds-tech/pull/7).\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\n",86,4041,86,83
govuk-aws/0005-terraform-module-location.md,"## Context\nDocument structure and deployment of Terraform [modules](https://www.terraform.io/docs/modules/index.html).\nTerraform can fetch modules from multiple [sources](https://www.terraform.io/docs/modules/sources.html) (e.g. Github, local files, S3).\n","Whilst developing a module we will source it locally (within this repository).\nOnce we feel our modules ""ready"" we will consider moving them to separate repositories.\n",60,4046,60,33
govuk-aws/0016-internal-dns-zones.md,## Context\nWe have a number of internal services that need to have\ninternal only service records. This allows clients to connect\nwithin their own environment or stack without needing a fully\nqualified domain name.\nThese records should not be visible outside the local environment or stack.\n,We've bought a domain name that will not be connected to the rest of the public\nDNS system. This will be used by our internal services.\nThis domain will *not* have DNS records added to it.\n,56,4051,56,45
govuk-aws/0035-bouncer-load-balancer-on-port-80-and-443.md,"## Context\nCurrently, Bouncer public load balancer accepts HTTPS request on port 443 and\nredirects the traffic to port 80 on the Bouncer servers.\nThere are a primary as well as a secondary SSL/TLS certificate attached to this HTTPS\nlistener of the load balancer.\n",The configuration of the Bouncer public load balancer was modified to:\n1. Add a second listener to the load balancer so that HTTP request on port 80 can be\nserved by the Bouncer servers. This functionality exists in the Carrenza environment\nand we aim to replicate this feature to maintain feature parity.\n,61,4052,61,68
govuk-aws/0004-dns-definitions-for-hosts-and-services.md,## Context\nFor our current monitoring to work we need Icinga to be able to communicate with hosts and services.\n,"Each stack will have an internal, private, zone for internal services such as the puppetmaster. These\nwill be in the following format:\n$servicename.$stackname.internal\npuppet.perftesting.internal\nmonitoring.mystack.internal\n",25,4055,25,51
govuk-aws/0023-use-separate-data-repository.md,"## Context\nWe have a large amount of data which is needed to run the Terraform projects.\nSome of the data we need to use is sensitive (e.g. RDS passwords). It is considered best practice to keep sensitive information encrypted in private repositories. This means that access can be controlled and the that the data will remain secure, even if it is accidentally published.\n",The `terraform/data` directory will be split out into its own repository that will be kept private and all sensitive data within it will be encrypted using [sops](https://github.com/mozilla/sops).\n,77,4058,77,43
govuk-aws/0020-merge-api-postgresql-instance-into-main-postgresql-instance.md,"## Context\nThe only database that runs on API PostgreSQL instances is Stagecraft, which is\nfor the Performance Platform.\nThis is able to run on the main PostgreSQL instance as we do not have the split\nof ""vDCs"" that we have in our current hosting provider.\nWork is ongoing to migrate the Performance Platform away from our infrastructure\nand to The Government PaaS, so the long term plan is not have this database in\nour infrastructure at all.\n",We should merge the API PostgreSQL instance into the main PostgreSQL instance.\n,97,4059,97,14
govuk-aws/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[documenting architecture decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,4060,16,44
CardsApp/cardops-module-architecture.md,##Context\nNeed to choose an applicable architectural approach to cardops module so that it can be consistently followed and the module can be developed and extended easily.\n,"This module is best realized with Hexagonal Architecture, aka ports and adapters. Since its the core of\nour domain. This architecture should enable easy prototyping and easy testing.\n",32,4063,32,36
CardsApp/testing-framework.md,##Context\nWe need to choose a testing framework for our system. It's best if it's used across all solution so that it's a standard approach.\n,"We decided to go with Spock, due to its readability and the fact that it's a well known framework. Some groovy langauge tricks enable quick test preparation.\n",33,4064,33,35
CardsApp/card-operations-database-choice.md,"##Context\nWe need to choose the database for our CardOperations. The database needs to store data in persistent form but it's not required that data is stored across sessions.\nApplication is a prototype, so it should be good for a quick setup.\n","We decided to go with H2 database for its ease of use, easy setup and familiarity.\n",52,4065,52,20
planet4-docs/adr-0014-choose-a-ticketing-system.md,"## Context and Problem Statement\nThere is an ongoing project to upgrade Jira to version 8, that potentially may require some migration of workflows and other data. That triggers this discussion, as a good point in time to evaluate all available options.\n","Chosen option: **Stay on Jira, revisit idea of moving to github if maintenance of two backlogs feels unmanageable.**\n",50,4088,50,30
ArchStudy/0002-implement-as-unix-shell-scripts.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4093,21,13
ArchStudy/0003-stop-recording-architecture-decisions.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4094,21,13
ArchStudy/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4095,16,39
new-orbit/0001-use-openshift-as-cloud-provider.md,## Context and Problem Statement\nWe want to deploy our application in docker containers that can be easily updated\n,"Chosen option: ""OpenShift"", because\n* Built on Kubernetes.\nThe bank has experience on it.\nProvides a lot of added value tools for CI/CD, automated builds.\nIs supported by RedHat and we have a great support contract for it.\n",21,4096,21,55
celestia-core/adr-062-p2p-architecture.md,"## Context\nIn [ADR 061](adr-061-p2p-refactor-scope.md) we decided to refactor the peer-to-peer (P2P) networking stack. The first phase is to redesign and refactor the internal P2P architecture, while retaining protocol compatibility as far as possible.\n","The P2P stack will be redesigned as a message-oriented architecture, primarily relying on Go channels for communication and scheduling. It will use a message-oriented transport to binary messages with individual peers, bidirectional peer-addressable channels to send and receive Protobuf messages, a router to route messages between reactors and peers, and a peer manager to manage peer lifecycle information. Message passing is asynchronous with at-most-once delivery.\n",62,4100,62,84
celestia-core/adr-036-empty-blocks-abci.md,"## Context\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\n","> This section explains all of the details of the proposed solution, including implementation details.\n> It should also describe affects / corollary items that may need to be changed as a part of this.\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\n> (e.g. the optimal split of things to do between separate PR's)\n",43,4108,43,85
celestia-core/adr-038-non-zero-start-height.md,"## Context\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\n","> This section explains all of the details of the proposed solution, including implementation details.\n> It should also describe affects / corollary items that may need to be changed as a part of this.\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\n> (e.g. the optimal split of things to do between separate PR's)\n",43,4135,43,85
adr-j/0002-implement-as-Java.md,## Context\nADRS need to be created on a wide variety of operating systems.\n,The tool is written in Java 8 and uses the operating system independence of the Java platform.\n,18,4156,18,20
adr-j/0007-use-specific-environment-variables-for-editors.md,"## Context\nUsers of the ADT tool might want to edit ADRs in the editor that's not the default system editor.\nADT should introduce an additional `ADR_EDITOR` and `ADR_VISUAL` variables, so users may choose editors for ADRs.\nThe enhancement proposal is in [project issues](https://github.com/adoble/adr-j/issues/4)\n","We will read editor command from additional `ADR_EDITOR` and `ADR_VISUAL` variables.\nIf custom variables are not set, we will fall back to reading `EDITOR` and `VISUAL` variables.\nWe will extract editor command resolving code from ADR class and move it to dedicated class.\nThat will improve testability and make further modifications easier.\nWe will reflect adding new variables in the docs.\n",79,4157,79,86
adr-j/0005-help-comments.md,"## Context\nThe tool will have a help subcommand to provide documentation for users.\nIt is usful to have the usage documentation in the code. When reading the code, that's the first place to look for information about how to run the command.\n","The command classes are annotated with usage documentation. This is actively read when usage documentation needs to be displayed, thus avoiding the use of separate help files etc..\n",53,4159,53,32
adr-j/0006-use-command-line-processing-package.md,"## Context\nAs the ADT tool becomes more complex, the processing of the arguments associated with the subcommands also becomes more complex and requires a programming effort out of proportion to the actual implementation of the functionality.\n","We will use a external package for command line processing.\nThe ""traditional"" Apache CLI library is too limited for the processing required (e.g. subcommands).\nThe decison is to use the [Picocli package](https://picocli.info/) as this fits well with the current architecture of the ADR tool (e.g. subcommands are implemented as seperate classes).\n",43,4160,43,78
adr-j/0001-record-architecture-decisions.md,## Context\nRecord the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",13,4162,13,39
dagpenger-old/2018-09-07 Be transparent.md,## Context\nNAV is paid for by the public for the purpose of providing public services. What\nwe produce should be open and available for the public.\n,"We should strive to be as transparent as possible, and work by the principle\n""Open by default"".\n",32,4163,32,23
dagpenger-old/2018-09-07 Use norwegian for domain concepts.md,"## Context\nWe work with a domain that naturally contains a lot of very domain specific\nwords and phrases in Norwegian.\nTranslating them back and forth between english and norwegian requires a lot of\nmental effort, reduces understanding and prohibits us from having an ubiquitous\nlanguage in the team.\nThe same in reverse goes for technical aspects and terminology. Translating them\nto Norwegian removes a lot of cognitive understanding.\n",Use English for technical terms and use Norwegian for business domain specific\nterms.\n,87,4164,87,18
rails-template/0010-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript and React though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,71,4173,71,57
rails-template/0004-use-a-changelog-for-tracking-changes-in-a-release.md,"## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",45,4174,45,57
rails-template/0006-use-bullet-to-catch-nplus1-queries.md,## Context\nIt can be easy to miss an inefficient database query during code review. These\ncan build up and have detremental performance on the application and effect the\nuser experience.\n,Add an automatic check to the test suite to ensure (through CI) that these are\nfixed before being deployed.\n,40,4175,40,24
rails-template/0003-use-standard-rb.md,"## Context\nWe need to make sure our code is written in a standard style for clarity,\nconsistency across a project and to avoid back and forth between developers\nabout code style.\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the\nstandard.rb rake task to lint the code as part of the test suite.\n,39,4176,39,36
rails-template/0009-use-scripts-to-rule-them-all.md,## Context\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\nAll[1].\nThis repository should include reference and document this decision.\n[1]\nhttps://github.com/dxw/tech-team-rfcs/blob/main/rfc-023-use-scripts-to-rule-them-all.md\n,By default we will follow the Scripts To Rule Them All pattern for common tasks\nin this template.\n,67,4178,67,21
rails-template/0006-use-simplecov-to-monitor-code-test-coverage.md,## Context\nWe want to keep our test coverage as high as possible without having to run\nmanual checks as these take time and are easy to forget.\n,Use Simplecov with RSpec to monitor coverage changes on every test run\n,32,4179,32,15
rails-template/0002-use-pull-request-templates.md,"## Context\nThe quality of information included in our pull requests varies greatly which\ncan lead to code reviews which take longer and are harder for the person to\nunderstand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have\nbeen a positive change. Prompting what pull request descriptions should include\nhas lead to better documented changes that have been easier to review on the\nwhole.\n",Include a basic pull request template for GitHub so that every pull request\nprompts every author to fill it out.\n,96,4180,96,24
rails-template/0008-use-brakeman-for-security-analysis.md,## Context\nWe need a mechanism for highlighting security vulnerabilities in our code before\nit reaches production environments\n,Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool\nto find vulnerabilities in development and test\n,21,4182,21,31
rails-template/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4184,16,40
app-performance-summary/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4192,16,39
app-performance-summary/0003-integrate-with-drive.md,## Context\n- We're using [Google Data Studio](https://datastudio.google.com) to present KPIs to stakeholders.\n- We're calculating application metrics on a periodic basis\n- We need a way to get the data into a dashboard\n,Integrate with google drive so we can load KPI reports into a spreadsheet within the GOV.UK team drive.\nUse the [Pygsheets](http://pygsheets.readthedocs.io/) library for this.\nThe data in the sheet can then be used as a data source within data studio.\n,52,4193,52,64
push-sdk-ios/0003-test-network-stack-via-injected-protocols.md,"## Context\nThere are [two main ways to go about testing your networking stack][2]:\n1. introduce protocols that wrap Apple foundation types (like `URLSession`,\n`URLSessionDataTask`, etc.)\n2. introduce a stubbing library like [OHHTTPStubs][1] that leverage\n`URLProtocol` and a small bit of swizzling to allow using JSON fixture data\n","While there are cases where both approaches are appropriate (even in the same\nproject), we've decided to opt for option 1 (protocol-based testing) of our\nnetworking layer. This allows a more unit-based approach to testing.\n",84,4194,84,49
push-sdk-ios/0002-avoid-runtime-dependencies.md,## Context\nWe don't want to create an SDK that requires customers to add a slew of\nexternal dependencies.\n,"We will avoid external runtime dependencies. For instance, we will use the core\nApple networking libraries (e.g. URLSession) instead of introducing a\ndependency on something like [Alamofire][1].\nExternal development dependencies (those necessary for contributing, but which\nare not bundled with the SDK) will be added as necessary.\n",24,4195,24,70
push-sdk-ios/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[this article.][1]\n",16,4196,16,22
holdings-backend/0002-mongodb.md,"## Context\nThe existing print holdings system keeps all data in MySQL. While this allows\nflexible querying, it is also computationally expensive and is difficult to\nscale.\nKnown use cases largely involve computing things specific to a clusters'\nholdings and HathiTrust items, so one way to parallelize queries is in a\nmap-reduce fashion - compute something about each cluster, then process the\ndata from each cluster into a final result.\n","We will use MongoDB for a persistent data store. Each print holdings cluster\nwill be a document, and will contain holdings, HathiTrust items, and shared\nprint commitments for that cluster as sub-documents.\n",93,4197,93,44
holdings-backend/0006-batch-loading.md,"## Context\nLike many database systems, MongoDB supports batch updates to data that are\nsignificantly faster than individual updates. In some cases when loading data,\nwe must do many updates to a particular document. If we need to search for and\ndeserialize the document each time, the load operation may suffer from a\npolynomial slowdown -- that is, each update takes progressively longer as the\ndocument grows in size.\n","When loading data, we should perform all updates to a given document at the\nsame time. In particular, we should minimize the number of times we deserialize\nany given cluster.\n",88,4198,88,37
holdings-backend/0001-incremental-building.md,"## Context\nIn the existing print holdings system, the build process starts from scratch\neach month and reloads all the data. This is computationally expensive, results\nin periods of time where particular kinds of data are unavailable in the\nsystem, and results in long lags between submitted data and the visibility of\nthat data for downstream uses.\n","There will have a persistent data store. We will apply new data as it comes in\nto the data store.\nWe should have robust backups for the data store and separately retain the data\nloaded into the data store, so that we can revert to a particular point in time\nin case of problems or rebuild from scratch if needed.\n",72,4199,72,70
verify-hub/0002-extend-policy-session-length.md,## Context\nAt present approximately 20% of users take >1 hour to verify at an IDP. In order to improve completion rate in the hub we are proposing to increase this time to 1.5 hours.\nThis will mean that almost all users should verify before the session times out in Policy.\n,The policy session timeout value is set via application config (authn_session_validity_period) managed by ida-webops.\n,64,4205,64,26
verify-hub/0001-record-architechture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4206,16,39
verify-hub/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4207,16,39
react-native-app/0003-set-environment-variables.md,## Context\nWe need to set some APIs keys without publishing them on GitHub.\n,We will add `config.js.example` and then explain in the README under a setup section that you need to copy that and call it `config.js` and add the secrets to it. `config.js` added to `.gitignore`.\n,17,4208,17,50
react-native-app/0007-detach-from-expo.md,"## Context\nThe size for our Android app is 28mb even if it only has 2 screens with simple tabs and lists.\nThis is because the size for an Expo app on iOS is approximately 33mb (download), and Android is about 20mb because Expo includes a bunch of APIs regardless of whether or not you are using them. Expo will make this customizable in the future but for now, there is no option to customize it.\n",Detach from Expo.\n,92,4209,92,5
react-native-app/0002-use-redux-persist-to-store-a-user-data.md,"## Context\nWe don't store any user data on back-end yet, so we can do it in AsyncStorage.\n",We will use [redux-persist](https://github.com/rt2zz/redux-persist)\n,24,4210,24,22
react-native-app/0006-make-tabs-swipeable.md,"## Context\nTo navigate between tabs, users can swipe left or right or click a tab.\n","We will use NativeBase's component ""Tabs"".\nPreviously used [react-native-tab-view](https://github.com/react-native-community/react-native-tab-view) since it's a cross-platform component that works perfectly on iOS and Android.\n",20,4211,20,48
react-native-app/0004-use-amplitude-for-analytics.md,## Context\nTo build the product we need to understand users\n,"We will use [Amplitude's free plan](https://amplitude.com/pricing?ref=nav), it has everything that we need at the beginning.\n",13,4212,13,33
react-native-app/0008-use-nativebase.md,"## Context\nWe want to quickly build and test ideas, not spend time on writing styles\n",Use NativeBase since it has everything we need and it's customisable\n,19,4213,19,15
react-native-app/0005-tracking-js-errors.md,## Context\nWe need to monitor and fix JS errors.\n,Use [Sentry](https://sentry.io) since Expo provides easy setup https://docs.expo.io/versions/v29.0.0/guides/errors\n,13,4214,13,35
react-native-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4215,16,39
vercel/specify_api_version_for_every_operation.md,"## Context\nSince the API version for a given resource is unpredictable, we need a way to specify the api version on a request by request basis. We also need to make sure this remains backwards and forward compatible in case versions are updated before the library is.\n","An operation should default it's API version to the latest version, but allow it to be overridden by passing in a `api_version` argument.\n",53,4217,53,30
vercel/normalize_ids.md,## Context\nIdentifiers in API responses are inconsistent.\nSee [here]() vs [here]() and [here]().\n,"In all resource classes, both `id` and `uid` fields will be mapped to the `id` property.\n",27,4218,27,25
adr-cli/0003-use-docs-adr-as-folder.md,## Context\n`adr-cli` needs to store the markdown files somewhere.\n[Michael Nygard's article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) proposes `doc/arch/adr-NNN.md` as directory and filename pattern.\n[adr-tools](https://github.com/npryce/adr-tools) uses `docs/adr/NNNN-title-with-dashes.md` as pattern.\n,"Use `docs/adr/NNNN-title-with-dashes.md` as pattern to be\n1) consistent with adr-tools and\n2) enable rendering in GitHub pages, because GitHub pages can be rendered from the `docs/` sub folder, but not from `doc` subfolder.\n",97,4219,97,60
acs-deployment-aws/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4222,16,39
react-library-template/0006-use-jest.md,"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n",We will use Jest as our testing framework.\n,40,4223,40,10
react-library-template/0005-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,69,4224,69,57
react-library-template/0007-use-dependabot-to-keep-dependencies-up-to-date.md,## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n,We will use Dependabot to monitor dependency updates.\n,38,4225,38,12
react-library-template/0003-use-rollup-to-build-distributables.md,## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n,We will build distributables using Rollup.js.\n,62,4227,62,12
react-library-template/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4229,16,40
Horace/0014-use-jenkins-for-network-path-storage.md,## Context\nThe [large data storage area](./0012-use-network-storage-for-large-datafiles.md) is shared by all the PACE projects and accessed via as network path.\nThis path should not be hard-coded in any scripts in order that it can be easily updated if the network storage location is moved.\nFor security reasons it is undesirable for this path to be stored in configuration files in the GitHub repositories.\n,The data will be stored in ANVIL through the available Jenkins [Credentials](https://plugins.jenkins.io/credentials/) plugin.\nThe path will be stored the `PACE-neutrons`  store with the ID `SAN_path`.\n,86,4231,86,50
Horace/0012-use-network-storage-for-large-datafiles.md,"## Context\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and ""expected"" results for unit and system testing.\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\nSimilar data files are also required for Euphonic testing.\n","The data will be stored in STFC hosted SAN (storage area network).\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\n",78,4234,78,40
Horace/0007-use-herbert-as-library-dependency.md,## Context\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\n,To make the depdencency explicit Herbert will be regarded as a library.\nAs a consequence:\n- Herbert builds will NOT trigger Horace builds\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\n,48,4245,48,88
Horace/0010-package-dependencies-in-repo.md,"## Context\nThere are various C++ dependencies present in Horace. For example, the version\nof MPICH used should match the version used by Matlab - this avoids incompatible\nshared libraries being used when executing mex functions. These libraries may\nbe old versions and may not be easily available for users to download.\n","Dependencies will be packaged within the repo to be built against. Matlab's own\nshared libraries will be built against where possible, but the static libraries\nand headers required will be within the repository. Libraries will be statically\nlinked where possible so that the libraries do not need to be shipped to users.\n",64,4246,64,61
Horace/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4248,16,39
Horace/0003-use-cmake-for-build.md,"## Context\nThe project will need to be built for multiple platforms Windows (VStudio), MacOS, Linux (gcc/Make) and include compilation of Matlab and C++ (with Matlab API wrapper).\n",We will use [CMake](https://cmake.org/) to provide a platform agnostic build definition that can be configured for each target platform.\n,41,4249,41,31
laundromat/0002-scoring-mechanism.md,"## Context\nA confidence score is very useful when evaluating the output of any model. It enables the user to put varying degrees of faith in the model predictions and allows for certain performance metrics. Sadly the SpaCy NER model’s architecture is such that it does not output confidence scores. There are ways around this, but they are unsatisfying and produce very low quality confidence scores. Therefore another method of gaining these confidence scores is required.\n","We have chosen to create a model that will take as input the SpaCy NER model’s predicted entities, and output a confidence score. The architecture of this model has not been decided yet, but we will explore simple GLMs first before trying neural models.\n",89,4262,89,53
continuouspipe/0003-merge-the-builder-micro-service-within-river.md,"## Context\nWe are running 12 micro-services, a lot of them are Symfony applications and we operate them differently.\n",We are going to merge some into this river application.\n,25,4267,25,12
continuouspipe/0002-store-the-new-tide-projections-into-firebase.md,"## Context\nWe want to be able to store a projection of the tides and especially their tasks statuses. The aim is to display\na per-task detailed pipeline-like UI, as much real time as possible.\n","Firebase helped us a lot with the logs and appear to be successful, it makes sense to extend its usage\nto this projection.\nIn order to ensure a permission control, the tides will be stored with the following hierarchy:\n- `/flows/{flowUuid}/tides/{tideUuid}`\n",44,4268,44,64
continuouspipe/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4269,16,39
cautious-waffle/help_handlers.md,## Context\nHow is the responsibility for providing help context defined.\n,All handlers defined for slash commands are responsible for exporting\na 'handlerHelp' function that will be callable with the string[] supplied\nby the help handler. This delegates the responsibility for help to the\nauthor of the slash command handler and co-locates the help definition\nwith the handler it self.\n,14,4270,14,63
sexual-health-service-finder/0001-record-architecture-decision-records.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,4272,16,43
sexual-health-service-finder/0004-use-azure-search.md,## Context\nAzure Search is the organisation/programme's strategic vision for serving both\norganisation and service data. This removes the need to maintain an instance of\nElasticsearch and related infrastructure.\n,The application will use Azure Search API to provide sexual health service\ndata.\n,41,4273,41,16
sexual-health-service-finder/0005-calculate-distance-between-origin-and-result-items-within-the-application.md,"## Context\nThe move to Azure search has introduced the need to calculate the distance\nbetween the search point and each result item. Previously, when using\nElasticsearch, the distance was returned within the query response. Azure\nsearch does not have this capability, it is currently a\n[feature request](https://feedback.azure.com/forums/263029-azure-search/suggestions/17760211-support-geo-distance-in-select-result).\n",The decision is to calculate the distance between the search point and each\nresult item within the consuming application i.e. the web app. The calculation\nfor\n[great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance)\nis well known and available in numerous languages.\n,90,4274,90,58
sexual-health-service-finder/0003-use-elasticsearch.md,"## Context\nElasticsearch is configured as a cluster for reliability and failover, and provides a single point for data updates.\nElasticsearch is used for filtering for the different service data requirements of the application.\n",The application will use Elasticsearch for service data retrieval purposes.\n,45,4275,45,12
early-careers-framework/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4279,16,39
verify-stub-idp/0003-deploying-metadata-to-paas.md,"## Context\nCurrently we are migrating stub IDP from UKCloud to PaaS. Stub IDP uses hub metadata to validate signatures\nand to achieve this, we need to whitelist the IPs that needs access to hub metadata. Inorder to whitelist we need\nstatic IP address for stub IDP in PaaS for certain environments (`joint, integration, staging, perf`).\nThis feature is currently not offered by PaaS.\n",We have decided to deploy hub metadata to PaaS as part of the current metadata release process.\nWhenever metadata is being deployed to UKCloud there is a copy of it being pushed to\nPaaS so that both (UKCloud and PaaS) use the same version of metadata.\n,87,4282,87,58
verify-stub-idp/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4284,16,39
bob/0002-replace-compojure-with-contract-first-solution.md,"## Context\nCompojure is a routing library that we are using right now. The downside is that one has to specify the routes\nas code. We want an openapi-spec that defines our API and a router that creates routes from it. The specs should\nbe openapi 3 conform.\nBesides that we need to model complex flows, an easy way to model auth per resource, easy swagger docs\nand we want good async performance.\n",Since there is no obvious Clojure implementation of a spec-first routing library and [juxt/apex](https://github.com/juxt/apex)\nis in its infancy we are stuck with Java options. [openapi.tools](https://openapi.tools/#server) tells us there\nis Spring and Vert.x so we chose to use [Vert.x](https://vertx.io/) for our new API-server.\n,92,4286,92,85
bob/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4288,16,39
molgenis-js-auth/0003-this-will-be-a-general-ui.md,## Context\nThis project started out as a permission UI specifically for the Armadillo suite. Along the way it became clear that it could\nbe used for other Fusion Auth applications as well.\n,The server and UI in this project are generalized in such a way that they can be used for any Fusion Auth application.\n,40,4290,40,25
molgenis-js-auth/0001-use-adr-to-describe-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n",16,4291,16,40
disc-golf-statistics/0003-use-node-js-and-express-js.md,## Context\nNode.js acts as the server/backend for this project and Express.js is a simpler way to work with Node.js.\n,We will use Node.js and Express.js as the server and backend framework for this project.\n,27,4293,27,19
disc-golf-statistics/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4294,16,39
disc-golf-statistics/0002-use-es2016-modules.md,## Context\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\n,We will use ES2016 modules to organize and share code. More information can be found here: http://exploringjs.com/es6/ch_modules.html#sec_modules-in-javascript\n,36,4295,36,38
winery/0026-store-license-and-readme-in-entity-root-folder-in-csar.md,## Context and Problem Statement\n`LICENSE` and `README.md` have to be stored in a standardized location when CSARs are exported.\n## Decision Drivers\n- Standardized CSAR structure\n,"- Standardized CSAR structure\nChosen option: ""Store these files in a root folder of a respective entity"", because\n- Less visual clutter\n- In repository, these files are not separated and stored together with the definition file\n### Positive Consequences\n- Standardized access to `LICENSE` and `README.md` files in any Winery-exported CSAR\n",41,4298,41,76
winery/0028-semantics-in-the-model.md,"## Context and Problem Statement\nIn the problem detection and solving approach by Saatkamp et al., detected problems in a topology are solved by specific algorithms.\nThese algorithms must know some semantics in order to perform correctly.\nTherefore, collection of predefined and known elements which the algorithms can work on is required.\n","Chosen option: ""Predefined elements in constants"".\nHowever, in near future, we could make this configurable by using the new configuration which is currently implemented by some students.\n",63,4300,63,38
winery/0028-use-hardcoded-namespaces-for-threat-modeling.md,"## Context and Problem Statement\nThe threat modeling approach relies on pairs of threats and mitigations.\nEach ""threat"" should be referenced by one particular ""mitigation"".\n","Chosen option: hardcoded namespaces, due to ease of implementation and static nature of the problem\n### Positive Consequences\nIn the context of threat modeling multiple different types of threats/mitigations are not necesaary so a minimal base type that carries the required properties (reference) can be used and extended\n",36,4301,36,62
winery/0030-deployable-docker-components.md,"## Context and Problem Statement\nThe execution time of an online crawler of the DeployableComponents project can not be foreseen, because it depends on the accessed online service.\nAn architecture, which considers this problem, is needed.\n","Chosen option: ""Asynchronous round-based background execution"", because best performance with best fitting execution procedure to the expected use case.\n### Positive Consequences <!-- optional -->\n* good performance\n* use case of hugh source data set is fulfilled\n### Negative consequences <!-- optional -->\n* more complex architecture\n",47,4302,47,65
winery/0023-use-maven-as-build-tool.md,## Context and Problem Statement\nWhich build tool should be used?\n,"Chosen option: ""Maven"", because\n* None of Gradle's customizability and the overhead in setup that comes with that is required.\n* The structure the comes with Maven makes the build files easier to understand compared to ANT.\n",14,4306,14,51
winery/0027-use-dasherization-for-filenames.md,## Context and Problem Statement\nGraphics files have to take a consistent file name\n## Decision Drivers <!-- optional -->\n* Easy to process by Jekyll\n* No WTFs at the creators\n,"* Easy to process by Jekyll\n* No WTFs at the creators\nChosen option: ""Dasherization"", because\n* clear separation of parts of the name\n* consistent to other URLs (which are typically lowercase)\n",40,4308,40,48
winery/0020-TOSCA-definitions-contain-extactly-one-element.md,## Context and Problem Statement\nHow should TOSCA data be stored?\n,"Chosen option: ""Allow exactly one TOSCA Definitions child in a definition"", because\n- Definitions are not modeled as explicit element. Only the nested elements are handled by Winery.\n- That means, it is not possible to specify custom definitions bundling a customized subset of available elements.\n",16,4310,16,61
news/0010-polymer-and-web-components.md,"## Context\nOur `polymer.js` implementation is read-only.\nhttp://localhost:8080/home.html?use-polymer\nThe `lit-html` tepmplating we're using does not support data binding, and I can't understand `lit-element`, which wants to create [custom web components](https://developer.mozilla.org/en-US/docs/Web/Web_Components).\nI tried it briefly but could not get a data feed in.\n",Leave this one read only -- it does show the bookmarks list at least.\n,92,4322,92,16
news/0002-displaying-lobste-rs-news.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4327,21,13
news/0013-visualising-layout.md,"## Context\nTrying out `https://github.com/githubocto/repo-visualizer` command line version.\n<img src=""./assets/0013/news.svg"" />\n",The change that we're proposing or have agreed to implement.\n,36,4328,36,13
news/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4331,16,39
katas/000-use-adrs.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\narticle: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4333,16,40
katas/002-no-eslint.md,"## Context\nSee [ADR1](./001-formatting-katas.md), which describes the formatting of a kata.\nThis would normally lead to using a linter.\n","But I am not a big fan of a huge set of rules\nand currently configuring a linter just for those couple of rules is overhead.\nSomething like prettier which automatically applies a rule set is fine, but a linter\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\ndoes clean up instead of just talking about it, my 2 cents.\n",36,4336,36,87
imageclass-uplift/0002-use-the-design-system.md,"## Context\nThe uplift front-end work should be based on the [U-M Library Design System](https://design-system.lib.umich.edu/):\n* The Digital Collections Platform long-term goal is that ""DLXS collections"" should blend with the Library (unless contrained by stakeholder requirements, e.g. the Bentley Library).\n* Building templates against the Design System today should mean more of these templates can be re-purposed for however the DCP evolves.\n",Use the Design System components and CSS to build out the uplift front-end.\nSet a milestone to evaluate which (if any) uplift patterns can be extracted up to the Design System.\n,96,4339,96,38
imageclass-uplift/0005-configuring-dlxs.md,"## Context\nDLXS has machinery to assemble an XSLT template via a path fallback mechanism.\nBecause the uplift will not be immediately applied across all collections, DLXS will need to be extended to support an alternative assembly process.\n","Update DLXS:\n* Configure a `collmgr` setting to flag a collection as being ""uplifted""\n* If detected, the fallback machinery will look in an uplift-specific path fallback to avoid any existing XSLT templates\n",48,4340,48,49
imageclass-uplift/0004-transformation-pipeline.md,## Context\nThe timeline for how long the uplift will be in operation is vague and undetermined.\n* DLXS has a concept of collection-specific templates to extend/modify the default transformations\n* It is likely that DCC staff will need to continue creating local templates in the future\n* Some of the proposed Design System technologies can be verbose; updating component definitions across collections would be tedious\n,"We will adopt a **transformation pipeline**.\n* DLXS XML will be transformed into **UI XML**, which will write to pre-defined component ""blocks""\n* The UI XML will then be transformed into **Design System HTML**, which is served to the client\nDCC staff will _ideally_ work against the UI XML.\nUpdates to the Design System transformation can be centrally managed.\n",79,4341,79,82
imageclass-uplift/0003-develop-locally.md,"## Context\nDeveloping in DLXS can be tedious, especially experiments:\n* DLXS uses CVS for version control (no easy branches!)\n* the CVS repository is *everything* (no easy branches!)\n* Mostly developers work against production data\n* Configuring modern front-end tooling will be its own project\nBut:\n* DLXS can trivially generate the XML that is processed as source by the page templates\n",The uplift front-end will be developed locally using modern front-end tooling. Experiments can be deployed to Netlify (etc) for sharing/comments.\n,90,4342,90,31
imageclass-uplift/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4343,16,39
hello/007-secrets.md,## Context\nWe need a way to store secrets that are used by our application.\n,We are going to use AWS SSM. We will store our secrets as Secure Strings.\n,18,4356,18,19
hello/003-runtime.md,"## Context\nWe want to build a serverless application with a function that\nreturns a `""Hello World!""` message. We need to pick a programming\nlanguage.\n",We will use [Python3.8]\n,35,4357,35,10
hello/005-test-framework.md,## Context\nWe want our code to be well tested. What tools or frameworks can we\nleverage?\n,"We will use the [pytest] framework to test our [Python] code. In\naddition, we will use the [mock] library to prevent our tests\nfrom interacting with external services.\n",23,4359,23,41
hello/004-serverless-framework.md,"## Context\nWe are building a `""Hello World""` application with AWS Lambda. What\ntools or frameworks can we leverage?\n",We will use the [Chalice] framework to buiild our serverless\napplication.\n,27,4360,27,20
hello/001-record-decisions.md,## Context\nWe want to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael\nNygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4361,16,41
hello/006-access-control.md,## Context\nWe need a way to protect our app. Only a small number of people\nshould be able to access the application. This includes the\ndevelopers and the intended end users (i.e. you).\n,"To protect our application, we will require all requests to include\nan `Authorization` header containing a JWT. Any request that is missing\nthis header will be rejected. Futhermore, the JWT will include an\nexpiry so we can control the time period in which users can access\nthe application.\nThe authentication process will be implemented as an additional AWS\nLambda function. In [Chalice], this is referred to as a Custom Authorizer\n",46,4362,46,91
hello/002-compute.md,"## Context\nWe want to build a `""Hello World""` application using one of the\nfollowing approaches:\n* **Containers** (i.e. K8S, ECS, EC2, etc.)\n* **Serverless** (i.e. AWS Lambda)\n",We will use AWS Lambda for this particular assignment.\n,56,4363,56,11
commcare-hq/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4369,16,39
teaching-vacancies/0001_get_postcode_from_coordinates.md,## Context\nWe need to get a postcode from the coordinates we get from the browser.\n,"To use postcodes.io instead of geocoder gem and just make a simple AJAX call from the browser.\n# Consequences\nWe avoid creating an endpoint on the server, therefore reducing the load we have to manage. On the other side we rely on a service that is less trusted than Google Maps, but open source and based on Open Data.\n",19,4372,19,71
teaching-vacancies/0009_build_job_applications_rather_than_buy_cots.md,"## Context\nTeaching Vacancies is evolving the set of features available to its users.\nStarting from next year, jobseekers will be able to apply for jobs directly through the service. Delivering this new functionality requires a significant amount of change and opens up the question of whether we should opt for a COTS (commercial off the shelf) Application Tracking solution, or take on the development effort and build it ourselves.\n",Teaching Vacancies will build Job Application functionality rather than purchasing an off the shelf solution.\n,86,4376,86,19
teaching-vacancies/0007_use_devise_for_authentication.md,"## Context\nTeaching Vacancies will soon start to develop a new set of features which ultimately will allow job seekers to apply for jobs directly on the TVS website.\nThis feature constitutes an important milestone for the service and will require job seekers users to create accounts to manage their job applications. Account functionality will permit teachers to initiate web sessions on TVS website as well as manage their account (password resets, email verification and account closing).\n",TVS will adopt Devise as authentication library.\n,89,4377,89,11
molgenis-ops-helm/0001-use-adr-to-describe-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n",16,4379,16,40
occurrent/0002-mongodb-cloudevent-serialization.md,"## Context\nCurrently, Occurrent is doing ""unnecessary"" work when converting from a `CloudEvent` to `Document` and vice versa\nSee [issue 196](https://github.com/cloudevents/sdk-java/issues/196) in the cloud event java sdk project.\n",None yet\n,59,4382,59,3
occurrent/0003-add-streamid-to-cloudevent-when-writing-to-eventstore.md,"## Context\nPreviously my thoughts regarding a ""streamid"" in the `CloudEvent` has been that since it's not visible to the user.\nThe ""streamid"" extension property was added on write and removed on read to avoid surprises to the user.\nHowever this leads to problems implementing snapshotting and sagas since then it's highly likely that we want to use the streamId\nin these cases (and the user probably wants to know the stream id in a saga).\n","For this reason the implementors of the `WriteEventStream` api will _add_ a ""streamid"" to each `CloudEvent`.\n",98,4384,98,31
openfido-workflow-service/0002-pipelines.md,"## Context\nA couple of client projects need infrastructure to process GridLabD jobs.\nThe goal would be that other projects could either include this project as its\nown service in their infrastructure, or incorporate it directly into their\nFlask-based project.\nThese kinds of jobs are long running, and produce artifacts that each project\nmay store in different ways (but primarily S3).\n","Create a Flask Rest service, coupled with Celery for job processing.\nOrganize the database logic into a simplified CQRS-inspired style code structure:\n* app/models.py contain all sql models.\n* app/services.py contain all db commands that modify database state.\n* app/queries.py contain all db queries to the database.\nUse the structure of the [presence-account-service](https://github.com/PresencePG/presence-account-service) project as a reference.\n",78,4388,78,98
openfido-workflow-service/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4390,16,39
gp-redirect/0003-use-prometheus-for-exposing-metrics.md,## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n,We will use Prometheus to monitor and alert on the state of the application.\n,86,4400,86,16
gp-redirect/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4401,16,39
rtk-companion/0002-define-goal-for-an-mvp.md,## Context and Problem Statement\nWhat will an MVP of this plugin look like and do.\n,Mvp will have:\n- Data for 10 kanji\n- User to click to view next and previous\n- Has an input text field for the user story\n- Has a button that once clicked will add a note to an existing deck\nA Tool menu button must open a window to display the kanji data.\n,19,4404,19,67
Conduit/0002-solve-the-xunit-msbuild-problem-with-assembly-merging.md,## Context\nWe have encountered probelms with the `xunit` task when run with `msbuild` (`xbuild works`) in that it is unable to locate required xunit assemblies.\n,This can be solved by copying the missing files to the same directory as `msbuild.exe` -- but this way may be easier to consume.\nMerge the required assemblies into `Conduit.Adapters.Build.dll` or  `Conduit.Adapters.Targets.dll`\n,41,4405,41,55
Conduit/0003-try-the-xunit-target.md,"## Context\nWe have created our own target, but it appears there is already one that might be worth trying. If this works then we don't need to reimplment unless we want fancy formatting.o\n",Give it a go.\n,42,4406,42,6
Conduit/0004-postpone-xunit-msbuild.md,"## Context\nThe xunit msbuild target does not work under xbuild and also we would like to be able to select files by glob pattern. The xunit target requires you know the name up front. In a CI environment these may differ, for example you may be unpacking an artifact archive.\n",Try implementing our own one that allows globbing but uses the internal from the xunit version\n,62,4407,62,19
Conduit/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4408,16,39
opg-digideps/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4412,16,39
faculty-planner-django/0001-get-current-month-course-date.md,"## Context\nWe need a way of getting all courses for the current month, and year.\n","For May(2018) E.g:\n```\n[{\ndate:""2018-05-01"", course_dates:{...}\n},{\ndate:""2018-05-02"", course_dates:{...}\n},{\ndate:""2018-05-03"", course_dates:{...}\n}, ...]\n```\n",20,4413,20,74
buildit-all/0008-database.md,"## Context\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\n",* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\n* Use AWS RDS Aurora (MySQL) for integration/staging/production - better HA & continuous snapshots (enabled for production)\n* Use MariaDB JDBC driver - has native Aurora support for failover\n,29,4415,29,86
buildit-all/0005-use-id-token-from-microsoft-as-bearer-token.md,"## Context\nIn the interest of time and getting something to work, we are going to break up the steps further\n","* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\n* Proper validation of the id_token will still occur\n",24,4416,24,36
buildit-all/0002-use-aws-bare-metal-rig-approach.md,## Context\nWe need to create a riglet for our new twig project so that we practice what we preach.\n,"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: CodePipeline, with Jenkins as an eventual target\n",24,4417,24,90
buildit-all/0010-jpa-manages-schema.md,"## Context\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\n","* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging/production databases (we will continue to drop/recreate all other databases....local, integration).\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\n",71,4419,71,66
buildit-all/0003-start-with-aws-codepipeline-and-codebuild.md,## Context\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer/simpler riglet flavor\nand put newer approaches to the test.\n,"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n",40,4420,40,57
buildit-all/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md,## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,36,4425,36,50
buildit-all/0003-use-junit-for-tests-instead-of-spek.md,## Context\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\n,Use JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\n,61,4426,61,29
buildit-all/0004-use-cloudwatch-logs-for-log-aggregation.md,## Context\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\n,We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\n,27,4427,27,29
buildit-all/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4428,16,39
Shelter/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4430,16,39
Shelter/0003-using-styled-components.md,"## Context\nUsing either plain CSS or CSS Modules instead of using Styled Components.\nOne main driver for not using styled components was that it could be more portable in the future, but there was no real use case for this.\nA limitation of CSS modules is the inability to pass values to CSS. CSS modules would potentially have to use a mix of inline styles and classes to achieve this.\n",We will use Styled Components\n,80,4431,80,6
verify-service-provider/0020-we-will-put-verified-status-inside-json-object.md,"## Context\nWhen receiving a User Account Creation response from the MSA, the response contains the attributes required for account creation.\nA User Account Creation SAML object contains *_VERIFIED attributes which are siblings of the attribute they reference.\n",The Verify Service Provider will require the service to have requested both an attribute and its\ncorresponding verified attribute or it will error. This is already enforced by the integration environment form.\nThe JSON the VSP returns will group the attribute value and verified status together.\n,48,4432,48,55
verify-service-provider/0010-we-will-keep-the-config-as-simple-as-possible.md,## Context\nExisting verify configuration for things like metadata can be quite\nintimidatingly complex. We'd like the application to be as simple as\npossible to configure.\n,We'll try to make the config have sane defaults wherever possible\nso that the user doesn't have to specify things they shouldn't care\nabout.\nThe user shouldn't have to specify things we don't use (e.g. truststorePath).\n,36,4433,36,51
verify-service-provider/0009-we-will-not-use-a-dependency-injection-framework.md,## Context\nWe're writing a really small project and we don't think that a\nframework like Guice will provide enough benefit to outweigh the cost.\n,"We will use constructor injection, but we won't use a DI framework. We'll use a\nfactory object instead.  We'll avoid calling new inside constructors where\npractical.\n",32,4434,32,38
verify-service-provider/0014-html-will-not-be-generated-by-the-verify-service-provider.md,"## Context\nAs well as the SAML AuthnRequest something needs to render an HTML form to handle\nthe ""SAML Post Binding"". This form contains the SAML request, the relay state, and\noptionally any ""hints"" the relying party needs to send the hub.\nThis form could either be generated by the verify-service-provider, the client library\n(e.g. passport-verify), or the service itself.\n",The HTML form will be generated by the client library.\n,88,4437,88,12
verify-service-provider/0007-we-will-document-a-strawman-api.md,## Context\nThe client and the service provider will have to communicate using some API.\nWe need to decide how the requests and responses will look like.\n,We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\n,32,4439,32,35
verify-service-provider/0012-we-will-use-the-full-profile.md,"## Context\nVerify's SAML profile specifies that Responses and Assertions should be signed.\nResponses should be signed by the Verify Hub and Assertions should be signed by\nthe Matching Service Adapter.\nThis profile causes problems with some off-the-shelf SAML service providers,\nwhich can't handle multiple signatures from different keys in the same message.\nAs a workaround, Verify introduced a ""simple"" profile where we do not sign Responses.\n","We will use the ""full"" profile, not the ""simple"" profile. The hub will sign responses\nand the service provider will validate them against the hub's metadata.\n",89,4441,89,36
verify-service-provider/0006-we-will-build-a-js-client.md,## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n,We will initially build only a node / passport client. We will want to build\nanother client in another language as soon as possible to make sure the API\nis well designed.\nUsers should also be able to interact with the API directly if we haven't built\nan appropriate client for their use case.\n,43,4443,43,63
verify-service-provider/0002-how-do-we-secure-the-api.md,"## Context\nWe need to secure the interaction between the ""client"" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n",If possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\n,59,4446,59,73
verify-service-provider/0004-users-will-be-able-to-provide-relay-state.md,"## Context\nIn SAML RPs can provide some extra data along with the request. This is\ncalled RelayState. Some existing RPs use this, but we're not sure what\nthey use it for.\nWe're not aware of any need for the service-provider to use relay state itself.\n",Users will be able to specify whatever relay state they want to and it will be\nprovided in the response.\n,64,4447,64,23
verify-service-provider/0021-we-will-use-http-200-for-valid-saml.md,"## Context\nWhen communicating with the Verify Service Provider API, we need to decide what status code to respond with\nfor correctly formatted SAML that represents some kind of authentication failure (eg. NO_MATCH).\n","Any valid SAML will return a 200 OK response and can be deserialized as a <code>TranslatedRepsonseBody</code>.\nWe will have to define an enum of possible SAML outcomes (<code>Scenario</code>) as we can't use HTTP codes\nInvalid JSON/SAML or internal errors will use a relevant, different HTTP status code.\n",42,4450,42,76
verify-service-provider/0005-sp-will-generate-request-id.md,"## Context\nAuthnRequests contain an ID attribute the value of which will be sent back in\nthe Response as an ""InResponseTo"" attribute.\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\n",The service provider will generate a random GUID to use as the AuthnRequest ID.\n,62,4453,62,18
verify-service-provider/0016-we-will-have-a-healthcheck-endpoint.md,## Context\nIn various user research sessions we've observed users start the MSA or the verify service provider\nand then want to check whether it's working correctly. There's also a need for users to be able to\nmonitor the health of the system once it's deployed to their environment.\nDropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary\nactions that check the health of the system.\n,We will have a healthcheck endpoint that will check the verify-service-provider can read metadata from\nthe hub and the MSA.\n,90,4455,90,27
verify-service-provider/0003-use-files-to-store-private-keys.md,"## Context\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\ndecrypt Response Assertions.\nThey will need to provide these to the verify-service-provider in some, reasonably\nsecure way. Different users may have different opinions on how best to do this.\n",Initially we'll use files for this.\nWe chose not to use environment variables because they're visible to other processes.\nWe chose not to use a more complicated solution because it would be more complicated.\n,62,4456,62,42
verify-service-provider/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4457,16,39
wikibase-release-pipeline/0010-queryservice-tarball.md,## Context\nFor the building of the queryservice docker image we are currently using the versioned release tarballs [published by the WMF](https://archiva.wikimedia.org/repository/releases/org/wikidata/query/rdf/service).\nThis package already contains all the required components to build the docker image and we don't see any reason to alter or publish these releases further.\n,Do not produce a queryservice tarball to be published.\n,77,4458,77,13
wikibase-release-pipeline/0002-tarball-hosting.md,## Context\nWe need to determine a place to host our new release artifacts (tarballs). Currently releases are being served by the Extension Distributor and the release branches of the git repositories.\n,- Wikibase release artifacts will be hosted on the WMF-controlled domain https://releases.wikimedia.org/.\n,39,4459,39,24
mbed-tools/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4476,16,39
gatsby-template-adr/1-Choosing-a-frontend-framework.md,## Context\nProvide reasons and context of this ADR.\n,Provide the decision taken by the team.\n,13,4485,13,9
gatsby-template-adr/2-Switching-frontend-technology.md,## Context\nProvide reasons and context of this ADR.\n,Provide the decision taken by the team.\n,13,4486,13,9
hoard/0004-use-airflow-and-fargate.md,## Context\nThe ingest process will be a scheduled task and we have an Airflow instance designed for just this sort of thing.\n,We will use Airflow to handle scheduling the ingest. The ingest process itself will be run inside a Fargate container.\n,27,4487,27,26
hoard/0002-use-python.md,## Context\nThe data ingest process will be custom software. We need to choose a language.\n,We will use Python.\n,20,4488,20,6
hoard/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4489,16,39
hoard/0003-use-type-hinting.md,"## Context\nPython 3 added support for static type checking (see: https://docs.python.org/3/library/typing.html). Type hinting is not an all or nothing thing, and can be applied in a progressive manner. It's also worth noting that type checks are not applied at runtime.\n","We will use type hinting. Our focus should be on type hints for function arguments and return values, and not aim for full coverage.\n",62,4490,62,29
platform/2020-09-08-CustomField-label-loading-in-storefront.md,"## Context\nWe want to provide the labels of custom fields in the storefront to third party developers.\nOn one hand we could add the labels to every loaded entity, but this will cause a heavy leak of performance and the labels\nare often not used in the template.\n","We implemented a subscriber, which listen on the `custom_field.written` event to add also snippets to all snippet sets with\nthe given label translations of the custom field. The `translationKey` of the snippets are prefixed with `customFields.`,\nfollowed by the technical name of the custom field. Thus the snippets can be used in the storefront.\n",56,4491,56,75
platform/2020-07-02-Implement-sales-channel-context-token-requirement.md,"## Context\nSome routes for the sales-channel-api and the store-api depend on a sales-channel-context-token to identify the correct context.\nTo ensure these routes cannot be called accidentally or intentionally without a token, a route parameter is in need to distinguish open routes and those that need a token.\n",Every route that depends on a sales-channel-token will only be callable with such a token provided.\nTo decide whether a route depends on a token or not the following questions should help:\n* Will the automatic generation of the token be a security Issue?\n* Will the automatic generation of the token lead to an abandoned entity? (e.g. the cart)\n* Can every possible caller create or know the needed token beforehand? (e.g. the asynchronous payment provider cannot)\n,60,4505,60,98
platform/2021-06-14-introduce-jest-fail-on-console.md,"## Context\nA jest pipeline run produced previously hundreds of errors and warnings, which made it hard to see why a test failed and if a passing test isn’t just a false positive.\n","To combat this, we decided to introduce the npm package [jest-fail-on-console](https://github.com/ricardo-ch/jest-fail-on-console#readme), which causes individual unit tests to fail if they log an error or a warning to the console.\n",38,4514,38,56
platform/2020-11-06-creating-events.md,## Context\nEvents throughout Shopware are quite inconsistent.\nIt is not defined which data it must or can contain.\nThis mainly depends on the domain where the events are thrown.\n,"Developers should always have access to the right context of the current request,\nat least the `Shopware\Core\Framework\Context` should be present as property in events.\nIf the event is thrown in a SalesChannel context,\nthe `Shopware\Core\System\SalesChannel\SalesChannelContext` should also be present as property.\n",38,4517,38,68
platform/2021-11-23-add-possibility-for-plugin-to-add-a-html-file.md,## Context\nThe new ExtensionAPI is based on a iFrame communication architecture. The old App system for the admin relies on the XML\nfile. And the normal plugin architecture in the admin is based on component overriding. The ideal way for developing\nadmin extensions will be the ExtensionAPI.\n,To provide a smooth transition for plugin developer to the new ExtensionAPI which will be introduced soon we need to make sure that plugin can also\nbehave like Apps in the administration. To fulfill this we need to provide a solution to show their own iFrame views.\nThis is now directly possible when the plugin developer adds a `index.html` file to the plugin in the administration folder.\nThis file will automatically be used by webpack and can be used like a normal web application.\n,59,4519,59,98
platform/2021-10-01-payment-flow.md,## Context\nWe have to provide a standardized way for Shopware extensions to implement custom payments.\n,"We implement two possible handlers **Synchronous Payment** and **Asynchronous Payment**. Both handlers can optionally implement [Accepting-pre-created-payments](#accepting-pre-created-payments). If a [payment transaction fails](#after-order-payment-error-case), the user can choose an alternative payment method and trigger the flow again.\n",20,4520,20,67
platform/2021-10-13-refund-handling.md,## Context\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\n,We want to implement the following structure to offer a unified refund handling for all extension types.\n,29,4527,29,19
platform/2020-12-02-removing-api-version.md,"## Context\nDue to the new deprecation strategy and the 6-8 months major cycle, API versioning is no longer  reasonable or even possible.\nDeprecated fields and routes are currently tagged in a minor version and will be removed with the next major version.\nThe API version is currently not increased in a minor version, which would not make sense, because with every second minor version deprecations would have to be removed.\n",By removing the API versioning within the URL we want to simplify usage and the deprecation strategy of our API.\nThe deprecated fields and routes are shown in the OpenAPI scheme as well as the API changelog and will be removed with the next major version (`6.x`).\n,88,4529,88,57
ockam/0002-profile-event-signing.md,"## Context\nProfile events can include multiple keys so the profile can prove the keys are valid. The question centers on\nwhen adding new keys, should a proof of possession be required? This involves the new key signing the current\nevent data, then the current profile root key signing the event data with the proof into the event.\n",It has been decided that a proof of possession will be required for now when adding a new key to the profile.\n,68,4534,68,24
ockam/0007-rust-error-handling.md,"## Context\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\nThere are multiple requirements to error handling:\n- agile enough to be used in different cases\n- portable to work in different environments with different constraints\n- convenient for both library developer and library user\n","In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\n",91,4536,91,53
ockam/0005-routing-data-formats.md,## Context\nWe need a standard format for messages to be exchanged by the routing protocol.\nThis format would be used by routers on different implementations.\n,"We use the following formats:\nFor a message:\n```\n{\nonward_route: Route,\nreturn_route: Route,\npayload: Any\n}\n```\nWhere\n`Route` - an ordered list of addresses.\nFor an address:\n```\n{\ntype: Integer,\ndata: Any\n}\n```\n",31,4538,31,74
ockam/0001-record-architectural-decisions.md,## Context\nWe need to record the architectural decisions that we make as we develop Ockam.\n,"We will keep a collection of records for ""architecturally significant"" decisions: those that\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\ntechniques.\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\n",21,4539,21,59
send-letter-service/0001-increase-pod-memory-for-oom-error.md,## Context\nWe have received a couple of Out of Memory errors in relation to large pdfs which cause a problem themselves or when they are combined through the `count` parameter.\n,Given the low number of occurances it has been decided to simply increase the memory available to the JVM in the pod.\nShould this problem continue to occur we will need to look at options such as:\n* Streaming the zip file data to the SFTP server directly\n* Storing binary data in Blob Storage instead of Postgresql and potentially shifting processing to a tmpfs volume\n,37,4540,37,77
forkhandles/0002-monorepo-and-bom.md,"## Context\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\nThe former decouples release cadences.\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\n",We will have a monorepo.\n,86,4542,86,9
forkhandles/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4543,16,39
unfinished-design-system/001-components-documentation.md,* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n,"* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\nWe've decided to use [Storybook](https://storybook.js.org/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\nSome major companies with mature design systems use this tool, which could be a precious asset.\n",53,4544,53,95
unfinished-design-system/007-package-repository.md,* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n,"* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\nWe've decided to use [Github Packages](https://github.com/features/packages). It is straightforward to set up and use, and we're already using other Github tools.\nAlso, our team has a good experience with it, and that can boost our development speed.\n",53,4545,53,92
unfinished-design-system/006-yarn-workspaces.md,* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [Experience Report](#experience-report)\n* [More reading](#more-reading)\n* [Updates](#updates)\n,* [Status](#status)\n* [Consequences](#consequences)\n* [Experience Report](#experience-report)\n* [More reading](#more-reading)\n* [Updates](#updates)\nWe've decided to use [Yarn Workspaces](https://classic.yarnpkg.com/en/docs/workspaces/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\n,63,4549,63,97
scholarsphere/0006-public-discovery-access.md,"## Context\nMetadata for works and collections should be publicly viewable. Only the binary content, i.e. the files, within the\nwork needs to be restricted based on visibility or other permissions.\n","Grant discovery access to the public on all works and collections by creating the appropriate ACL for each work and\ncollection. Discovery access stipulates that all metadata is viewable, but that binary content is not downloadable.\n",41,4551,41,44
scholarsphere/0004-blacklight-for-search-only.md,"## Context\nThere are two ways we can display works and work versions in Scholarsphere: 1) using the record that is in the\nPostgres database; or, 2) using the record that is in Solr.\n","We going to use the Postgres record for displaying individual records, leaving Blacklight's Solr record for displaying\nsearch results only. The Solr record, or SolrDocument, will not be used when displaying the detailed record for a\nwork or work version. It will only be used within the context of a list of search results.\n",48,4552,48,70
scholarsphere/0009-acl-actor-permissions.md,"## Context\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\nWhen determining who has access to a given resource, both these sources may need to be consulted.\n","Access controls and depositor or proxy depositor rights are independent from one another.\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\n",79,4554,79,84
scholarsphere/0007-published-date-field.md,"## Context\nThe field for _Published Date_ is a free-text field, but also needs to be expressed as a date. Scholarsphere 3 has\nentries that cannot be mapped to actual dates, so we need a way to store non-parseable dates but also have some kind of\nvalidation as well.\n","Published date will only be validate at the UI and API level. The database will not validate any of the entries. This\nallows us to store anything when migrating, or creating records through means other that the API or UI. When values are\nentered through the API or UI, an EDTF date must be used.\n",64,4555,64,65
scholarsphere/0002-define-use-of-decorators-and-presenters.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\nsituation.\n","Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\nPresenters take the form of ""plain ol' Ruby objects"" (POROs) and would generally not delegate methods to an object.\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\n",46,4556,46,69
scholarsphere/0008-asset-pipeline.md,## Context\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\nfact that it was present as both a gem and an npm package.\n,We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\nvia yarn.\n,41,4557,41,27
scholarsphere/0005-order-files-alphanumerically.md,## Context\nFiles within a work should be displayed in a certain order. The ordering can be automatic or arbitrary.\n,"Files will be ordered alphanumerically, according to their names. The application can now render them in the same order\neverytime, without additional metadata.\n",24,4558,24,34
scholarsphere/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4559,16,39
terraform-aws-ecs/0003-ecs-iam-and-security-groups.md,## Context\nThe ECS IAM roles are currently in the global IAM module and the security groups\nare in the VPC modules. These made sense when first developed and based on the\nexisting multi environment in one account set up. They don't make sense when\ndoing a new single environment account - which is the way we are transitioning\nto.\n,Refactor the IAM roles and security groups into the ECS Cluster in a way that\nmaintains backward compatability and supports the future approach of single\nenvironment accounts.\nToggles can be used to support the backwards compatability and future\napproaches.\n,73,4566,73,54
terraform-aws-ecs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4567,16,39
arduino-printf/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4569,16,39
mental-health-service-finder/0010-use-nhsuk-frontend-library.md,## Context\nThe service uses an old version of the nhsuk-frontend library. When the service moves from beta to\nlive we want it to look consistent with other nhs.uk services.\n,"Use the Nunjucks macros, SCSS and JS available in the nhsuk-frontend NPM package. Consuming these\ndirectly from the package allow us to take advantage of any updates to the library.\n",41,4570,41,46
mental-health-service-finder/0007-use-service-specific-contact-details.md,"## Context\nThe search index contains two sets of contact details, one for the\norganisations and one for the psychological therapies services provided by the\norganisations. The service contact details are usually more specific, e.g. a\ndirect telephone number rather than the hospital switchboard, so likely to be\nmore useful to someone searching for a psychological therapies service.\n",Use the service specific contact details stored as metrics instead of the\norganisation contact details in the Contacts field.\n,75,4572,75,22
mental-health-service-finder/0001-record-architecture-decision-records.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",16,4573,16,43
mental-health-service-finder/0006-use-local-data-for-west-hampshire-ccg.md,## Context\nThe data synchronisation between front end and back end is broken and is taking\ntime to fix. In the interim the data can be stored locally within the\napplication.\n,"The data for West Hampshire will be stored locally within the application until\nsuch a time as the data synchronisation has been fixed. Once fixed, the local\ndata store will be removed so the centrally held data can be requested and used\nwithin the application.\n",39,4574,39,53
mental-health-service-finder/0008-do-not-use-local-data-for-ccgs.md,## Context\nAll current IAPT services should have an ODS code. Any future services must\nhave an ODS code assigned. There for it is no longer necessary to use local\ndata to relate CCGs to IAPT services.\n,Remove local data for all CCGs. The source for CCG to IAPT service relationship\ndata will only be the central data store.\n,50,4575,50,30
mental-health-service-finder/0005-use-local-data-for-tower-hamlets-ccg.md,## Context\nTower Hamlets CCG's IAPT service does not have an ODS code. As a consequence it\nis not possible to relate the CCG to the IAPT service. When a GP is providing\nservices for Tower Hamlets CCG this will allow data to be displayed.\n,Rather than display no result we know the contact information for the IAPT\nservice. We have decided to store this information locally and display it for\nthe user.\nThis is only a temporary measure and the change will be reverted once the data\nhas been loaded into the central system.\n,61,4576,61,59
mental-health-service-finder/0009-add-service-search-to-site-root.md,## Context\nSitting directly on the `/find-a-psychological-therapies-service` path is not in keeping with the organisations plans for\ninformation architecture going forward. When the service moves from beta.nhs.uk to nhs.uk we want\nit to sit on path consistent with other finder services.\n,The site root will be changed to be `/service-search/find-a-psychological-therapies-service`\n,64,4577,64,23
mental-health-service-finder/0004-use-local-data-for-redbridge-and-north-cumbria-ccgs.md,## Context\nNorth Cumbria CCG and Redbridge CCG do not have an ODS code. As a consequence it is not possible to relate them to the IAPT services. When a GP is serving under either of these CCGs we can display the information for the CCG.\n,Rather than display no result we know the contact information for each of these CCGs. We have decided to store this information locally and display it for the user.\nThis is only a temporary measure and the change will be reverted once the data has been loaded into the central system.\n,61,4579,61,58
mental-health-service-finder/0011-use-nhsuk-header-and-footer-api.md,## Context\nA API has been created which can be used to build header and footer links for NHS.UK.\nWe want to use this across all NHS.UK applications.\n,Build a middleware function which can get the header and footer links from the API and\nmake them available within the Nunjucks templates. We will also cache these to prevent\na massive amount of calls to the API\n,37,4580,37,44
TechChallengeApp/0004-config-only-in-config-file.md,## Context\nApplication configuration can be overridden by command line flags and environment variables. Is this something we want to take advantage of?\n,"No, configuration will be limited to the configuration file for sake of simplicity and having a single way to configure the application.\n",27,4581,27,25
TechChallengeApp/0005-skip-create-database.md,## Context\nDatabase upgrade script contains everything required to deploy and seed a test database. Some PaaS services like azure db have requirements that means it is difficult to have a generic way to create the database.\n,"To make it easier, we've decided to add an option to skip the database creation and allow for an external process to create the database while still creating tables and seeding it with data.\n",42,4582,42,38
TechChallengeApp/0003-removed-scaffolding.md,"## Context\nShould we provide scaffolding for the test takers, or should we expect them to be able to set that up themselves?\n",We decided to remove the scaffolding and rather suggest that the test taker uses the default VPC to deploy their application.\n,29,4583,29,26
TechChallengeApp/0006-environment-variables-overrides-config-file.md,"## Context\nIn some environments the port and other variables are only available on startup, and will have to be overriden. See https://github.com/servian/TechChallengeApp/issues/21\n",Add environment variables overrides back\n,42,4584,42,6
TechChallengeApp/0002-use-viper-for-config.md,"## Context\nThe solution was built using a custom toml configuration solution, should we standardise on a library for less maintnance overhead?\n","Decided to use viper as the configuration library, as it tightly integrates with cobra which we already use for helping with command line integration.\n",30,4585,30,29
TechChallengeApp/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4586,16,39
grafana-simple-grpc-datasource/0002-add-provide-listmetrics-with-targetquerytype.md,"## Context\nSome metrics do not support all query types. The list of metrics, however, does not have any context about the query for which query type the metrics are selected.\nProviding the `ListMetrics` API with more context information would make it possible to filter metric lists\n",Reject this change because it degrades the user experience of the plugin.\n,58,4595,58,15
octagon/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4597,16,39
adr/0002-typescript-for-programming-language.md,"## Context\nRuntime errors happens pretty often when using JavaScript. As the application grows, we face errors that would not be thrown if we were using a statically-typed language.\n",Use TypeScript when developing frontends and node.js backends\n,36,4603,36,12
adr/0001-prettier-for-code-formatting.md,"## Context\nCode formatting is a problem when scaling a team. Each developer has his own code styling preferences, causing unconsented code.\nWe need a tool for normalize our javascript code formatting.\n",Use prettier as the tool for javascript code formatting\n,41,4605,41,11
adr/0001-SYS-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4607,16,39
adr/0002-SYS-rdbms.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4616,21,13
adr/ADR-9-DecentralandInterface-evolution-plan.md,## Context and Problem Statement\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\nTaking into consideration:\n* All the scenes currently deployed must work as they work today.\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\n* Maintainance costs should be taken into account\n,"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\n* We should not be limited by the decisions of the past SDK\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\n* Avoid sunk cost fallacy with the old SDK\n",72,4618,72,79
adr/0003-sys-use-other-theme-for-adrs.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4621,21,13
adr/ADR-22-quests-progress-ui.md,"## Context and Problem Statement\nHow to render the Quests progression?\nBased on the context of a previous meeting with Pravus, Alex, Pablo, Marcosnc and Mendez on Jan 7, 2021:\n","We choose B because it enables us to render the quests UI without coupling the quests controller and therefore, the server. Also positions Decentraland in a more decentralized path by making centralized servers not required to access the features.\nAnyone could now implement their own custom Quests servers without asking permission or having the platform as a limitation.\n",47,4622,47,67
adr/ADR-10-profile-deployment-debouncing.md,"## Context and Problem Statement\nHow do we debouncing profile/scene deployments to avoid DDoS and wasted bandwidth?\n- As of today we only deploy profiles when changing the avatar\n- Soon, a new deployment will happen after mute/unmute\n",We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\n,51,4626,51,31
adr/ADR-37-explorer-desktop-launcher-technology.md,"## Context and Problem Statement\nThe Desktop Client Project needs an Application that can easily install and update the Desktop Client.\nIt must be compatible cross-platform with Windows, macOS and Linux.\nThe launcher will be the main application that the user will download from the website. So it must be a small size app.\nTo make this application, the team considered the following options.\n","The selected option it's option **C**, `Electron App with React JS`. There are many advantages: for example, that the project can be split up into multiple teams. It has the self-update feature integrated so the team can deliver the product faster.\nMore details of how the application will work: https://www.notion.so/decentraland/Desktop-Launcher-ed6aadd11d7b4fd48e5a88400d761ed9\n",76,4631,76,96
monocle/0011-search-query-language.md,## Context and Problem Statement\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\nWe would like to use flexible search expressions based on a proper query language.\n,"Chosen option: ""Monocle Query Language"", because it comes out best (see below).\n### Positive Consequences\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\n- We create a re-usable component.\n### Negative Consequences\n- We need to maintain a language toolchain.\n",44,4637,44,72
monocle/0003-choice-of-react.md,## Context and Problem Statement\nWe need to provide a client software to consume the Monocle web API. This client software must be easily consumable via a WEB browser.\n,"Chosen option: ""React"".\nBecause it is a today standard. It provides a wide variety of libraries to ease building s complex UI. The React community is pretty large and lot of doc resources are available. Also as that library is popular, it might benefit the project in terms of external contributions.\n",36,4639,36,63
monocle/0004-high-level-components.md,"## Context and Problem Statement\nWe want to start with a set of components each dedicated to a task to ensure Monocle being able to scale and also to fit well in a cloud like deployment. No monolithic app, but microservices. As we start we also don't want to overengineer, so just keep the basic components and architecture.\n","Choosen Option: ""A database, a crawler, a, CLI, a WEB API, a WEB UI"".\nBecause, it fits the described context. The database (Elasticsearch) is scalable, and the other components are stateless, each of them use the database or the API as backend so this will ease in term of scalability. This architecture makes it easy to deploy using docker/podman or even k8s.\n",72,4641,72,90
monocle/0002-choice-of-elasticsearch.md,## Context and Problem Statement\nWe need to store changes data (Pull Requests and Reviewes) in a scalable way. The stored data must be easily accessible in an intelligible manner.\n,"Chosen option: ""ElasticSearch"".\nBecause it fits better our need regarding the style of data we expect to store and how we expect to query the data.\n",38,4643,38,36
verify-matching-service-adapter/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4647,16,39
polarbears/ADR-1.md,## Context\nThere are several databases within the system. One per module.\n,Under no circumstance should a module read/write from more than one database.\n,16,4649,16,15
cache-flush-slack-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4664,16,40
OpenTermsArchive/0002-service-history.md,"## Context and Problem Statement\nWe need to be able to regenerate versions from snapshots. As documents is aim to change over time (location or filters) we can't rely on the last version of the declaration to regenerate the version from an old snapshot. So we need a system to keep track of declaration changes, that's what we called declarations and filters versioning.\n","[After consulting the community](https://github.com/OpenTermsArchive/engine/issues/156), the options 2A is retained as it hide complexity (compared to Option 1) of the history while increasing its discoverability (compared to Option 3) for contributors who might become more “adventurous”.\n",74,4667,74,66
docs/0015-building-oci-images.md,## Context and Problem Statement\nWe want to build OCI images based on Dockerfiles inside our Kubernetes cluster.\n## Decision Drivers\n* MUST run completely in userspace (no privileged rights required)\n* MUST be runable on Kubernetes\n* MUST be controlled in an automatic manner (CI/CD)\n* SHOULD be compatible with Knative Build\n,"* MUST run completely in userspace (no privileged rights required)\n* MUST be runable on Kubernetes\n* MUST be controlled in an automatic manner (CI/CD)\n* SHOULD be compatible with Knative Build\nChosen option: Kaniko, because it is designed for the use case we need. It works on Kubernetes and build OCI images without any daemon and without privileged rights.\n",71,4668,71,80
docs/0001-java-framework.md,## Context and Problem Statement\nWe want to use a well established Java framework in order to bootstrap and boost our development process.\n,"We chose option 1, Spring Boot, since existing knowledge and experience with this framework is available.\nTo reduce training costs (time) we chose this option.\n",26,4670,26,34
docs/0002-vrs-type.md,"## Context\nThe Voice Assistance Platform's job is to identify if the call is implicit or an explicit utterance. If explicit, then it will engage and call VRS.\nOur goal is to identify who is responsible for determining the VRS in the utterance.\n","Based on the meeting's (03.04.2021) discussion, the quorum for the best part forward is option 1. The biggest reason is keeping the role of the VRS as straightforward as possible. The team acknowledged that the lower-level details solutions need to be done in partnership with Voice Assistant Platform.\n<br>\n",55,4672,55,68
docs/0006-ui-framework.md,## Context and Problem Statement\nWe want to use a well established UI framework in order to bootstrap and boost our development process for state-of-the-art web applications.\n,"We chosen option 1, Angular, since existing knowledge and experience with this framework is available.\n",33,4673,33,20
docs/0010-rest-api-design.md,"## Context and Problem Statement\nRESTful APIs can be designed differently, e.g., there are discussions about how to name resources correctly or what kind of HTTP methods should be used to trigger actions.\n",We follow the guidelines proposed by the [Learn REST: A RESTful Tutorial](https://www.restapitutorial.com).\n,40,4675,40,26
docs/0024-language-for-a-generic-composition-pattern-implementation.md,## Context and Problem Statement\nWe need to decide which language to use for implementing a MICO composition service.\nThe framework will be [Apache Kafka](https://kafka.apache.org) [See ADR0021](0021-kafka-as-messaging-middleware.md)\n## Decision Drivers\n* Should be known by everyone (on the developement team)\n* Must support Apache Kafka\n,* Should be known by everyone (on the developement team)\n* Must support Apache Kafka\nWe want to use Java. Since existing knowledge and experience is given for everyone.\n,80,4678,80,37
docs/0003-quality-attribute-security.md,## Context and Problem Statement\nShould we consider and address security requirements?\n,For the initial start of the project we do not consider security as a must have.\n,15,4681,15,18
docs/0008-browser-compatibility.md,## Context and Problem Statement\nWhich browser should we actively support with the MICO web interface?\n,We will only actively support Chrome (above version 70). We may test the web interface occasionally with Firefox (above version 63) to fix issues rendering the web interface unusable with firefox.\n,20,4682,20,40
docs/0005-mono-repo.md,## Context and Problem Statement\nShould we use different repositories for Frontend and Backend or use a single repository?\n,"We chose option 1, Mono Repo, because as a starting point it reduces unnecessary complexity when working with frontend and backend logic.\nFurthermore, we can have a single CI build in order to verify the whole project source code.\n",23,4685,23,47
docs/0009-features-first.md,## Context and Problem Statement\nHow should we prioritise certain non-/extra- functional requirements?\n,For the initial project we won't implement the requirements from above to increase our efficiency in implementing features.\n```eval_rst\n.. seealso::\n* :doc:`0002-quality-attribute-performance`\n* :doc:`0003-quality-attribute-security`\n```\n,20,4687,20,59
docs/0020-configurable-service-dependencies.md,## Context and Problem Statement\nServices currently have a static network of dependencies. This makes it impossible to have a service that depends on a SQL based database use a different database that is compatible with the service.\n## Decision Drivers\n* Should fit in the current domain model\n* Easy to add afterwards\n* Should support existing services with their static dependencies\n,* Should fit in the current domain model\n* Easy to add afterwards\n* Should support existing services with their static dependencies\nTo be decided.\nThis adr purely documents possible decisions for this problem.\n,71,4690,71,41
docs/0023-faas.md,"## Context and Problem Statement\nTo execute a variety of different functions that are based on different messaging patterns we want to use the Function as a Service (FaaS) approach.\n## Decision Drivers\n* MUST support Kubernetes\n* MUST support Kafka to trigger events\n* SHOULD be a well known and proven solution\n* SHOULD support the most common programming languages (at least Java, Python, JavaScript)\n* SHOULD support the Serverless framework\n","* MUST support Kubernetes\n* MUST support Kafka to trigger events\n* SHOULD be a well known and proven solution\n* SHOULD support the most common programming languages (at least Java, Python, JavaScript)\n* SHOULD support the Serverless framework\nWe want to use OpenFaaS.\n",89,4691,89,58
docs/0004-monolith-first.md,## Context and Problem Statement\nWhich architectural style should we use to build MICO?\n,"For the initial project we apply the ""Monolith first"" approach in order to avoid the additional complexity of a microservice architecture.\n",18,4692,18,27
docs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4693,16,39
docs/0018-requirements-application-composition.md,## Context and Problem Statement\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\n## Decision Drivers\n* MUST be compatible with Lombok\n,"* MUST be compatible with Lombok\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\n### Positive Consequences\n* Lombok can be used.\n* Better code quality.\n### Negative consequences\n* Applications cannot be created using other existing applications.\n",42,4695,42,70
docs/0002-quality-attribute-performance.md,## Context and Problem Statement\nShould we consider and address performance requirements?\n,For the initial start of the project we do not consider performance as a must have.\n,15,4697,15,18
openjdk-api-v3/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4699,16,39
latis3/0004-Custom-adapters-defined-outside-core-FDML-schema.md,## Context and Problem Statement\nThere are an unlimited number of adapters that can be defined by the FDML schema syntax.  A way to reduce complexity suggests moving adapter schemas out of the core FDML schema.\n## Decision Drivers\n* keep the main schema simple\n* increased complexity of multiple schema files\n*\n,* keep the main schema simple\n* increased complexity of multiple schema files\n*\nChosen option: Keep a core FDML.xsd separate from specialized adapter schemas\n### Positive Consequences <!-- optional -->\n* the core schema will be stable for periods of months\n* many adapter schemas can be created without changing the core schema\n### Negative Consequences\n* some datasets will require two linked schemas\n,64,4714,64,82
latis3/0003-Sample-objects-implemented-as-domain-range-tuple.md,## Context and Problem Statement\nSample objects are a fundamental concept in the functional data model.  Several different approaches to represent Sample objects are available.\n## Decision Drivers\n* performance\n* conceptual integrity\n* mathematical correctness\n,* performance\n* conceptual integrity\n* mathematical correctness\nDomain variables and range variables will be separated in two vectors\n### Positive Consequences\n* removing the integer describing the size of the domain reduces complexity\n* concept of functions as a domain mapping to a domain is emphasized\n### Negative Consequences\n* samples are now broken down into 2 parts\n,45,4715,45,71
latis3/0002-Decouple-data-and-datatype-objects.md,"## Context and Problem Statement\nPreviously LaTiS V2 combined Data and Metadata in the Variable trait.  What appeared to be a good object-oriented design decision turns out to create more problems than are solved by combining these two concepts.\n## Decision Drivers\n* performance, computing  with metadata is simply inefficient\n* separation of concerns\n* inconsistency between data and metadata\n","* performance, computing  with metadata is simply inefficient\n* separation of concerns\n* inconsistency between data and metadata\nChosen option: Data and metadata will be separated by removing metadata from the Sample class.\n### Positive Consequences\n* Loading data into Spark will be greatly simplified by removing the metadata\n* Applying transformations to the data and the model separately will reduce code complexity\n### Negative Consequences\n* data and metadata may become out of sync\n",75,4717,75,91
polaris/developmentLanguage.md,## Context\nReactNative supports both JavaScript and TypeScript.\n,"Javascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\n",12,4720,12,40
jskatas.org/000-use-adrs.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\narticle: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4722,16,40
jskatas.org/002-use-lit-html.md,"## Context\nIn [this PR][1] I started to give the site some dynamic feature, and using (p)react for it\ndoes always mangle state handling with the DOM lib (which (p)react are).\nThis is an anti-pattern imho, I want the business logic separated and independent of the\n(DOM) rendering part.\n","Therefore I switched to lit-html, since it seems to provide this.\n[SSR][2] might be a bit more work here, but it feels right.\n",75,4723,75,35
jskatas.org/001-server-side-rendering.md,"## Context\nThe web was born as HTML, lately we fallback too often to JS as our default.\nThis makes pages slow, adds unnecessary load to the web and excludes users.\nSSR is the least we can do to make sites easier to use, have less load on the\nclients and deliver speed by default.\n",Use SSR.\n,67,4724,67,4
biosamples-v4/0006-submitting-the-same-sample-twice.md,## Context\nWhat should we do if a user submitted same sample twice ?\n1. Update the first one by checking name and domain fields\n2. Create another sample with an accession without considering the first one\n3. Send an error message explaining that a sample exist with same name and domain\n,For now we decided to create a new sample with a new accession as this updating a sample could cause accidental data loss.\n,61,4725,61,25
biosamples-v4/0004-inverse-relationships-sample-update.md,"## Context\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\nThis is associated with BSD-1088 - https://www.ebi.ac.uk/panda/jira/browse/BSD-1088\n",We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\nand no real information is added to the sample\n,74,4726,74,60
biosamples-v4/0005-release-and-update-date.md,## Context\nSample should have a release and update date when we store it in database.\nWhen user submit a sample we could give flexibility to the user by filling release and update date to today if they are missing.\nBut filling release date can accidentally make a sample public.\n,"We decided that it is best to send an error message if sample release date is not provided.\nOn the other hand, we will fill update date to today's date if it is missing.\n",57,4728,57,40
biosamples-v4/0002-have-taxid-attribute-at-top-level.md,"## Context\nENA presentation requires a top-level, numerical only taxID.\n","We have added a taxID field at the top-level of all biosamples.\n- This is a single, top-level, numeric field named 'taxId'.\n- If no taxon available the value of taxId will be 0.\n- There is no support for multiple taxId as there is no data that meets this requirement.\n",16,4729,16,72
jabref/0001-use-crowdin-for-translations.md,## Context and Problem Statement\nThe JabRef UI is offered in multiple languages. It should be easy for translators to translate the strings.\n,"Chosen option: ""Use Crowdin"", because Crowdin is easy to use, integrates in our GitHub workflow, and is free for OSS projects.\n",28,4732,28,31
jabref/0017-allow-model-access-logic.md,"## Context and Problem Statement\n- How to create a maintainable architecture?\n- How to split model, logic, and UI\n## Decision Drivers\n- Newcomers should find the architecture ""split"" natural\n- The architecture should be a help (and not a burden)\n","- Newcomers should find the architecture ""split"" natural\n- The architecture should be a help (and not a burden)\nChosen option: ""`org.jabref.model` may use `org.jabref.logic` in defined cases"", because comes out best \(see below\).\n",58,4735,58,61
jabref/0008-use-public-final-instead-of-getters.md,"## Context and Problem Statement\nWhen making immutable data accessible in a java class, should it be using getters or by non-modifiable fields?\n","Chosen option: ""Offer public static field"", because getters used to be a convention which was even more manifested due to libraries depending on the existence on getters/setters. In the case of immutable variables, adding public getters is just useless since one is not hiding anything.\n### Positive Consequences\n* Shorter code\n### Negative Consequences\n* newcomers could get confused, because getters/setters are still taught\n",29,4736,29,84
jabref/0003-use-gradle-as-build-tool.md,## Context and Problem Statement\nWhich build tool should be used?\n,"Chosen option: ""Gradle"", because it is lean and fits our development style.\n",14,4737,14,19
jabref/0019-implement-special-fields-as-separate-fields.md,## Context and Problem Statement\nHow to implement special fields in bibtex databases?\n,"Chosen option: ""Special fields as separate fields"", because comes out best (see below)\n",18,4739,18,20
jabref/0021-keep-study-as-a-dto.md,## Context and Problem Statement\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\n,"Chosen option: ""Keep study as DTO and use transformators"", because comes out best (see below).\n",49,4741,49,23
jabref/0023-localized-preferences.md,"## Context and Problem Statement\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\nThe problem is how to store the default values.\n","Chosen option: ""_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it."", because Achieves goals without requiring too much refactoring and without (known) downsides.\n",50,4744,50,56
jabref/0004-use-mariadb-connector.md,## Context and Problem Statement\nJabRef needs to connect to a MySQL database. See [Shared SQL Database](https://docs.jabref.org/collaborative-work/sqldatabase) for more information.\n,"Chosen option: ""Use MariaDB Connector"", because comes out best \(see below\).\n",45,4745,45,20
jabref/0013-add-native-support-biblatex-software.md,"## Context and Problem Statement\nRight now, JabRef does not have support for Biblatex-Software out of the box, users have to add custom entry types.\nWith citing software becoming fairly common, native support is helpful.\n## Decision Drivers\n* None of the existing flows should be impacted\n","* None of the existing flows should be impacted\nChosen option: ""Add a new divider"", because comes out best (see below).\n### Positive Consequences\n* Inbuilt coverage for a entry type that is getting more and more importance\n### Negative Consequences\n* Adds a little bit more clutter to the Add Entry pane\n",61,4746,61,68
jabref/0010-use-h2-as-internal-database.md,## Context and Problem Statement\nWe need to store data internally in a structured way to gain performance.\n## Decision Drivers\n* Easy to integrate\n* Easy to use\n* Common technology\n,"* Easy to integrate\n* Easy to use\n* Common technology\nChosen option: ""H2 Database Engine"", because it was straight-forward to use.\n",39,4747,39,33
jabref/0005-fully-support-utf8-only-for-latex-files.md,## Context and Problem Statement\nThe feature [search for citations](https://github.com/JabRef/user-documentation/issues/210) displays the content of LaTeX files. The LaTeX files are text files and might be encoded arbitrarily.\n,"Chosen option: ""Support UTF-8 encoding only"", because comes out best \(see below\).\n### Positive Consequences\n* All content of LaTeX files are displayed in JabRef\n### Negative Consequences\n* When a LaTeX files is encoded in another encoding, the user might see strange characters in JabRef\n",47,4749,47,65
jabref/0020-use-Jackson-to-parse-study-yml.md,## Context and Problem Statement\nThe study definition file is formulated as a YAML document.\nTo accessed the definition within JabRef this document has to be parsed.\nWhat parser should be used to parse YAML files?\n,"Chosen option: Jackson, because as it is a dedicated library for parsing YAML. yamlbeans also seem to be viable. They all offer similar functionality\n",44,4750,44,31
jabref/0009-use-plain-junit5-for-testing.md,## Context and Problem Statement\nHow to write readable test assertions?\nHow to write readable test assertions for advanced tests?\n,"Chosen option: ""Plain JUnit5"", because comes out best \(see below\).\n### Positive Consequences\n* Tests are more readable\n* More easy to write tests\n* More readable assertions\n### Negative Consequences\n* More complicated testing leads to more complicated assertions\n",25,4751,25,58
jabref/0016-mutable-preferences-objects.md,## Context and Problem Statement\nTo create an immutable preferences object every time seems to be a waste of time and computer memory.\n,"Chosen option: ""Alter the exiting object"", because the preferences objects are just wrappers around the basic preferences framework of JDK. They\nshould be mutable on-the-fly similar to objects with a Builder inside and to be stored immediately again in the\npreferences.\n",26,4753,26,52
subscribe-with-google/001-google-play-developer-account.md,"## Context\nOur SKUs exist within a Google Play Developer account which is not linked to the ""The Guardian"" project (instead it is linked to the ""Google Play Android Developer"" project).\nThis means we are unable to programmatically obtain Play related data (i.e. SKUs) via Google APIs.\n","Create a new Google Play Developer account and link it to ""The Guardian"" project. This allows us to query Google APIS for Play related data.\n",64,4756,64,31
insight/0002-tweepy.md,## Context\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\n,We choose [Tweepy](https://github.com/tweepy/tweepy) as our Twitter API Pyhon library.\n,36,4760,36,27
insight/0001-python.md,## Context\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n,We choose Python as our base programming language.\n,33,4761,33,10
island.is-glosur/0005-error-tracking-and-monitoring.md,"## Context and Problem Statement\nKnow it before they do!\nWe need a tool to discover, triage, and prioritize errors in real-time.\n","Chosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\n",31,4772,31,42
sfpowerscripts/002-parallel-development-process.md,"## Context and Problem Statement\nWhen creating a parallel development streams (e.g. release), there are a list of manual steps that must be performed:\n- package versions may need to be updated so that packages between streams do not share the same version-space\n- a new artifact feed or npm tags need to be created\n- a set of artifacts needs to be created for the new development stream\n","All this issues arised from the fact that with the assumption of using multiple feeds. As we are moving to ask users to utilize a single feed/artifact repository and how it is in most platforms like GitHub or GitLab, there is no specific need for a helper tool. Users should be versioning their artifacts using semantic version when dealing with multiple development streams\n",80,4776,80,72
sfpowerscripts/001-artifact-version-duplication.md,"## Context and Problem Statement\nOn some CICD platforms, where build numbers are not unique across the organisation, artifacts versions might be duplicated when creating release candidates for the first time and the build number starts from zero on a release pipeline. The outcome would be a mutation error on the artifact registry that prevents the artifact from being published.\nThis is a problem for source & data packages only. Unlocked package versions are handled by Salesforce.\n","Due to various complexities, it is better to utilize #2, user's has to follow semantic version rather than sfpowerscripts doing any form of automation\n",89,4777,89,32
sfpowerscripts/005-cutting-release-branch.md,"## Context and Problem Statement\nWhen cutting release branches, a set of rc artifacts are not automatically created. The user must disable `--diffcheck` to generate a set of rc artifacts, or manually publish the last dev artifacts to the rc feed, or update the rc tags in the case of single registry.\nThis process might be made simpler for the end-user, so that intervention is not required when cutting a release branch.\n","We move away from recommending multiple feeds, rather to use a single feed originating from the trunk, and release definitions will utilize the LATEST_TAG or LATEST_GIT_TAG to do a release from the from the release branch. Users are expected to follow semantic versioning to prevent conflicts. In the case of parallel development, it is still possible to use semantic versioning in a single artifact repository.\n",87,4779,87,80
sfpowerscripts/001-release.md,"## Context and Problem Statement\nsfpowerscripts currently does not have a notion of 'release', which some CICD platforms like Azure pipelines support, allowing users to create release definitions, each with their own list of artifacts and task configurations. To achieve this across CICD platforms, the fetch, deploy and changelog generator commands all need to be combined to form a notion of release. Doing so will ensure that the notion of a release is available on all CICD platforms.\n","Chosen option: 1. `orchestrator:release` command\nAn orchestrator command is more intuitive to use than a shell script, and it's independent from OS. Though this is bit inflexible and tied to the options being provided by the sfpowerscripts, it fastens adoption of the tooling. Users who have requirements that are not satisfied by the release commands can switch to a shell script and orchestrate it.\n",99,4780,99,90
sfpowerscripts/002-release-installing-packages.md,"## Context and Problem Statement\nAs part of a release, we need to install package dependencies. There are two different ways we could represent the package dependencies to be installed, each with their own merits.\n","Chosen option: 2. Duplicate dependencies in the release definition\nAlthough it requires more input from the user, re-defining dependencies within the release definition is more reliable, as you have precise control over what dependencies are being installed. It also allows package dependencies to be tracked within the release definition file.\n<!-- markdownlint-disable-file MD013 -->\n",41,4783,41,72
mnt-teet/ADR-1-Code-and-documentation-language.md,## Context\nTEET is a multi-site project that will have development work done both in Oulu and Tallinn\noffices. The developers and customer do not have the same native language.\n,"Code, databases, API definitions and documentation will be in English.\nExisting code that is reused from other projects that have different source\nlanguage may be used as is if necessary.\nAll domain concepts must be well defined and updated to a glossary of terms.\n",40,4793,40,54
coloseo/0003-use-mern-stack.md,## Context\n,The project is build with the MERN Stack.\n,3,4797,3,11
coloseo/0002-use-docker-compose.md,## Context\nThe solution has to be portable and lightweight and work without special infrastructure.\n,Docker-compose is used as multi-container solution.\n,18,4798,18,11
coloseo/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4799,16,39
nr-arch/2020-03-12-ARCH-record-architecture-decisions-simple-template.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4804,21,13
java-template/0002-hexagonal-architecture.md,"## Context and Problem Statement\n- How can we maintain a clean design?\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\n## Decision Drivers\n* We employ TDD, which favors small iterations.\n* We aim to implement full Continuous Delivery.\n","* We employ TDD, which favors small iterations.\n* We aim to implement full Continuous Delivery.\n",63,4805,63,22
project-blueprint/0002-use-google-closure-compiler.md,## Context\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\n,"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\n",38,4808,38,68
project-blueprint/0003-test-distributed-files-only.md,## Context\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\n,Testing will be made against the production bundle to catch compilation errors before they reach our users.\n,67,4809,67,19
project-blueprint/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4810,16,39
semplate/0007-representation-of-semantic-metadata-in-markdown-documents.md,## Context and Problem Statement\nSemantic information has to be inserted into the markdown document. How is this represented?\n,"The chosen option is **2**, as this\n* preserves the markdown for future updates of the document\n* is typically not visible to readers of the markdown.\n",23,4811,23,33
semplate/0004-supports-a-limited-number-of-java-data-types.md,"## Context and Problem Statement\nAs the library will a) generate markdown text from Java objects and b) read in the markdown and generate Java objects, the data types need to be  converted to text and back again.\nAs not all data types can be implemented, what data types should be supported by the library?\n","Data types that a native to Java are supported, i.e.:\n* Primitive Java data types\n* Java wrappers for the primitive data types\n* Java Strings\n* Java classes representing dates.\n* Java classes representing URLs (for use in document links).\nUsing a language agnostic set of data type would provide extra complexity in translating from the Java data types and was rejected\n",65,4812,65,78
semplate/0009-fluid-interface.md,## Context and Problem Statement\nHow does a client use the interface?\n,"Chosen option is to implement as a **fluent API**, as early use of the library showed that this was easier to understand and provided a means\nto expand the evolve the functionality.\n",15,4813,15,40
semplate/0005-type-representation-in-markdown-documents.md,## Context and Problem Statement\nThe values of fields need to be represented in the markdown documents. How are different types represented?\n,"Default Java representations are used.\nRepresentations chosen by the user in the template have not been rejected, but are maybe a feature for a future release.\n",26,4814,26,32
semplate/0006-agnostic-as-to-markdown-type.md,## Context and Problem Statement\nMore then one type of markdown exists. Which one should be supported?\n,The library can handle all forms of markdown.\n### Positive Consequences <!-- optional -->\n* The form of markdown used is determined by the user supplied template.\n### Negative Consequences <!-- optional -->\n* The library can only use a subset of the features that are common to all markdown languages (e.g. single new lines are ignored).\n,21,4815,21,72
semplate/0003-components.md,## Context and Problem Statement\nWhat are the main components of the library?\n,"Chosen option is to implement as a **single class**, as the foreseen complexity of the library does not justify the extra complexity of developing a facade pattern implementation.\n",16,4816,16,34
semplate/0002-easily-integrated-with-java-projects.md,"## Context and Problem Statement\n`semplate` is intended to provide functionality to java developers so that they can build tools that create, use and/or manage documents that are written in markdown. As such, it should be easily to integrate it with programs written in Java.\n","Chosen option is to create as a standard Java JAR library\nThe option to create a microservice was rejected as these costs of hosting it could not be met. However, this option has not been excluded as additional to a standard Java JAR.\n",55,4817,55,52
semplate/0008-not-to-use-semantic-triples.md,## Context and Problem Statement\nSemantic information can be represented using sematic triples  as defined in the [W3C Resource Description Framework - RDF](https://www.w3.org/RDF/).\nIs this an option for `semplate`?\n,"As the library is intended to be directly call from Java programs, the **notation is based on the fieldname of Java object being processed**.\n",52,4818,52,30
protraffic/itd-001.md,## Context and Problem Statement\nProcessing center should be hosted. Where to host it?\n,"Even though using an on-premise data center might result in cost savings, a data center is not a product differentiator. Meanwhile, it requires additional operational and implementation efforts. It’s easier to focus on product value and rely on a cloud service provider.\n",18,4819,18,52
figgy/0005-data-migrations.md,"## Context\nThere are several ways of migrating data, including:\n* Using an ActiveRecord Migration\n* Writing a Rake task and running it after deploying a code update\n","1. When migrations do not change the database structure or otherwise break the application, we should\nwrite a Rake task to migrate data.\n",35,4822,35,29
figgy/0001-document-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4823,16,39
figgy/0009-unlinked-files.md,"## Context\nWhen an ingest fails in the middle of a transaction which is adding files, the\nFileSets will not get persisted. However, the files will have already been\ncopied to the repository via `FileAppender`. This results in files in the\nrepository which have no corresponding database record.\nFixing this will require development of a transactional disk StorageAdapter\nwhich moves files at the end of a metadata transaction.\n","1. We don't have time to implement a transactional disk StorageAdapter at this\ntime.\n2. Accept this situation, document it here, and know we can free up space in the\nfuture by looking for unlinked files and deleting them.\n",90,4824,90,54
paths/0002-use-a-single-machine.md,## Context\nMultiple machines add a bunch of complexity - we don't want to deal with that while we're experimenting with how this\nactually works. The advantages of microservices are pretty minimal for a 1 person team.\n,1. Put server logic on 1 machine\n,46,4829,46,10
paths/0003-separate-reads-and-writes.md,## Context\nModelling reads and writes with the same model does not match reality - AddResult is very different from getResults. We should make sure to keep those models seperate.\n,Use seperate models for reads and writes (or actions).\n,37,4830,37,12
opg-use-an-lpa/0003-session-storage-using-jwt.md,## Context\nVery small amount of data stored in session\n,JSON Web Tokens (JWT) for session storage (rather than DynamoDB or ElastiCache)\n,12,4835,12,20
opg-use-an-lpa/0013-application-layer-naming.md,## Context\nWe need clearing naming for each layer of the Use an LPA service.\nThere are 3 layers:\n- A front service layer through which Actors will access the service.\n- A front service layer through which Viewers will access the service.\n- A backend service shared by the two front services which will provide data access and some domain logic.\n,The 3 services will be called:\n- Actor Front\n- Viewer Front\n- Api\n,76,4836,76,20
opg-use-an-lpa/0014-library-for-application-views.md,## Context\nWe need to use a common library for managing HTML views within the service.\nTwo libraries were investigated; Plates and Twig.\n,To use Twig.\n,29,4838,29,5
opg-use-an-lpa/0012-zend-dependency-injection.md,## Context\nZend Expressive allows us to pick from a number of Dependency Injection libraries.\nThe developers would specifically like to use a library that supports autowiring.\n,To use [PHP-DI](http://php-di.org/). This was the only library that supported autowiring with no additional setup.\n,34,4839,34,30
opg-use-an-lpa/0005-automated-testing.md,## Context\nMoJ guidance is that tests should be automated where possible\n,"Include and implement automated code coverage tests, accessibility tests, and security and penetration testing\n",15,4840,15,17
opg-use-an-lpa/0002-persistent-storage-uses-dynamodb.md,"## Context\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\n* Current MoJ strategy is to use managed services where possible\n",Use DynamoDB for persistent storage\n,37,4841,37,7
opg-use-an-lpa/0004-no-cdn-required.md,"## Context\n* No expected high demand\n* There will be a natural ramp up in usage, as the service is only available to applications registered after a certain date\n* Not complex to add a CDN after\n",No Content Delivery Network (CDN) for delivery of static assets\n,43,4842,43,14
opg-use-an-lpa/0008-session-storage-using-an-encrypted-cookie.md,"## Context\n* We will be storing a very small amount of data in the session.\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\n","To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\nNot to use JWT, because:\n* To ensure message secrecy, additional libraries are needed.\n* The resulting cookie value is significantly larger.\n* Concerns over the general suitability around using JWT for client side sessions.\n",86,4843,86,67
opg-use-an-lpa/0019-will-use-symfony-translation-components.md,## Context\nWe need to provide translation capabilities in a way that slots into our twig based rendering pipeline\n,We're going to use symfony/translation to provide the translation capabilities\n,21,4845,21,15
opg-use-an-lpa/0009-use-aws-kms-to-manage-session-encryption-keys.md,## Context\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\n* The encrypted cookie will need to be encrypted using a key.\n* Keys should be able to be rotated easily and often.\n* Key rotations should have no effect on active users.\n,* We will use AWS' KMS to manage our encryption keys.\n* Keys will be cached at the contained level in volatile memory.\n,67,4848,67,29
opg-use-an-lpa/0006-continuous-delivery.md,## Context\nFrequent small releases are preferred strategy to prevent accumulating risk and deliver benefits to users more quickly\n,Aim for continuous integration and continuous delivery. [Proposed Flow](../diagrams/CI%20CD%20Pipelines.png)\n,22,4849,22,29
opg-use-an-lpa/0011-the-same-zend-application-will-be-used-for-both-viewer-and-actor-components.md,"## Context\nUse an LPA will be made up of two components - those for use by LPA _actors_, and those used by third\nparty groups who are the _viewers_ of the LPA.\nAt present it is expected that these two components will be hosted on two different domains.\n",That both `Viewer` and `Actor` will both be separate modules of the same Zend application.\nNote: it is still expected that they will be deployed separately into two containers.\n,63,4850,63,38
opg-use-an-lpa/0015-library-for-application-forms.md,## Context\nWe want to use a common library for managing HTML forms used within the service.\nTwo libraries were investigated; Zend Form and Symfony Forms.\n,We will use Zend Form.\n,32,4851,32,7
opg-use-an-lpa/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4853,16,39
cape-cod-coastal-planner/0004-use-postgresql-as-db.md,"## Context\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\n","A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\n",73,4859,73,54
cape-cod-coastal-planner/0003-use-elm-as-frontend-language.md,"## Context\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a reliable and fast UI/UX that can be easily maintained.\n",Elm is a pure functional language for building reliable web apps with great performance and no runtime exceptions.\n,83,4862,83,21
mysql-monitoring-release/0002-common-go-linter.md,"## Context\nThe `mysql-monitoring-release` comprises several self-contained `src` modules.\nThe expectation is that each contains a `bin` directory, with a `test` bash\nscript representing the set of unit tests for a given module.\nThis approach makes sense, but is leading to some copied linting code.\n","We considered:\n1. Making a separate module that each of the other modules' tests would source\n1. Pulling the linting code out of the modules entirely, and putting it in CI\nBoth options eliminate the redundant, difficult-to-maintain copypasta.\nWe opted for option 1, as we felt that it enabled the quickest feedback loop on\ncode formatting, because it still existed inside of the `bin/test` scripts.\n",67,4863,67,94
mysql-monitoring-release/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4864,16,39
GDD-app/0002-use-preferences-for-data-storage.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4867,21,13
GDD-app/0003-use-bitrise-for-ci.md,## Context\nWe need an easy way to integrate and test out code that is fast and reliable.\n,We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\nIt also allows us to notify users in a easy way and with different roles.\n,21,4868,21,50
GDD-app/0004-decouple-domain-models-from-representation.md,"## Context\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\nto define a proper and strong domain model.\n","Separate DTOs should be defined for persistence and over-the-wire communication, which can have a\nmore adequate representation for their use-case, without affecting our domain model.\n",33,4869,33,36
GDD-app/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4870,16,39
content-publisher/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,4886,16,39
mercury-rust/0002-choosing-capnproto.md,## Context\nWe need something better than ProtoBuf\n,"We checked the Cap'n'Proto documentation and examples on RPC.\nSeems to be a perfect match for our needs, so we started integrating it to our SDK for communication between client/server, replacing the previous ProtoBuf.\n",11,4902,11,46
mercury-rust/0001-choosing-rust.md,"## Context\nIoP had a Mercury/Connect SDK before this, but it was implemented in Java/C# and it was not maintainable anymore. We had to decide if we want to maintain somehow or\nrewrite it from scratch.\n","The Rust language was chosen to be used based on it’s almost C level speed and rusts memory safety.\nThe language also possesses really good bindings. Basically you can bind any code written in C into Rust.\nWhile Rust is still in its early years, it’s growing steadily, and it also has a good, stable, and growing community.\n",48,4903,48,71
mercury-rust/0004-error-handling.md,"## Context\nSince some of our applications are expected to operate in an error resistant way, it's important to lay out how we detect and manage unexpected situations. The prinicples described in this article are just partially Rust-specific, some of them applies to the design or testing activity space.\n",Principles described below should be followed by every contributor. Existing code should be refactored ASAP.\n,60,4905,60,22
architecture-decision-log/0009-open-source.md,* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n,"* [Status](#status)\n* [Consequences](#consequences)\nBased on the stated context, we've decided to go open-source for our entire codebase. Every new application our developers think must consider it's community, collaborations, and possible improvements. By doing so, we increase our ecosystem coverage, security, and allow modifications and customizations by our customers.\n",34,4913,34,78
architecture-decision-log/0002-github-template-for-issues-and-pull-requests.md,* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n,"* [Status](#status)\n* [Consequences](#consequences)\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\nYou can check the suggested templates [in the following folder](../assets/0002-github-template-for-issues-and-pull-requests).\n",34,4919,34,91
road-registry/002-track-municipalities.md,"## Context\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\n",We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\n,96,4930,96,68
road-registry/001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n### Changes to Nygard's approach\nWe will keep ADRs in the project repository under `docs/adr/NNN-explanation-of-adr.md`.\n",16,4932,16,74
godspeed-you-blocked-developer/005. Serverless-offline.md,"## Context\nI can't be doing a deployment to CloudFormation every time I want to test a new line of code. Serverless-offline is a plugin that allows local emulation of AWS Lambda and API Gateway. By running it via node --debug, you can [use it as a debugger from VS Code](https://medium.com/@OneMuppet_/debugging-lambada-functions-locally-in-vscode-with-actual-break-points-deee6235f590).\n",I will use the serverless-offline plugin to run a local copy of my Serverless stack.\n,97,4933,97,21
Playground/adr-002.md,"## Context\nThe Identity, Metadata, and Storage currently have no way to be scaled. As it stands, they are only able to be ran as a single instance in a cloud specific way.\nAn engineering goal for this product is to be cloud agnostic. This means that we need to be able to run on abstracted hardware provided by some Cloud or Host.\n",In order to achieve One of the engineering goals for this project is to be cloud agnostic.\n,75,4960,75,20
adr-poc/0004-generate-help-file.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4962,21,13
adr-poc/0003-write-help-file.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4963,21,13
adr-poc/0006-use-dependency-injection.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4964,21,13
adr-poc/0002-use-swagger-to-document-apis.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4965,21,13
adr-poc/0005-hardcode-dependencies.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4966,21,13
adr-poc/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4967,16,39
adr-poc/0008-use-autofac-for-dependency-injection.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4968,21,13
adr-poc/0007-use-unity-for-dependency-injection.md,"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",The change that we're proposing or have agreed to implement.\n,21,4969,21,13
ios-architecture-decision-logs/0008-layering-modular-ios-application.md,## Context\nOur problem was monolith single view app. Our project will grow out of control.\n,We decided splitting it into individual feature modules. This is similar to the micro-service architecture.\n,21,4970,21,19
ios-architecture-decision-logs/0014-use-CoreTracker-for-trackingEvents.md,## Context\nOur team created new Core Tracker Interface for tracking events. For consistency we should replace and use CoreTrackable instead of Legacy Trackable Interface.\n,Every new events must use CoreTrackable. Also every new event should use new Tracker approach.\n,32,4971,32,20
ios-architecture-decision-logs/0017-theme-manager.md,## Context\nWe are creating a design guideline. All team members should follow defined rules.\n,* Do not assign\n- corner radius\n- shadow\n- border width/color\n- text color\n- background color\n- text size\n- tint color\non storyboard.\n* Do not use any color that isn't in our color palette. If that is the case contact with designer team.\n* Add icons as SVGs. Choose single scale while adding.\n,19,4972,19,76
ios-architecture-decision-logs/0005-inject-ab-config-global-values-toPresenter.md,"## Context\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\n",We decided to inject this variables to related presenters from their constructors.\n,45,4973,45,15
ios-architecture-decision-logs/0012-use-networkManager-for-network-requests.md,## Context\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\n,Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\n,27,4974,27,20
ios-architecture-decision-logs/0002-use-view-model-on-necessary-place.md,## Context\nOur problem was that if we need to manipulate the response we was doing it on DTO object. And that object getting larger. Also if some field changes on response object we need to update the ui's fields. But nothing change on user interface. That's why we need to seperate DTO and UI's Model's from each other.\n,"~~If it is necessary to manipulate the DTO models that the UI will use, the ViewModel class needs to be created.~~\n",70,4975,70,27
ios-architecture-decision-logs/0019-naming-branch-name.md,## Context\nWe run too many separate sprints and our naming gets mixed up. Also it is difficult to manage when too many branches are gathered under one folder.\n,Branch names should be named in such way:  `Channel Name/Sprint Name/Task Code + Name`\nFor Example: `MPDP/Sprint1/MPDP-1050-ProductDetailPage`  or  `DISCO/Sprint50/IOSDISCO-1250-FixWidgetHeight`\n,34,4976,34,65
ios-architecture-decision-logs/0010-use-localizable-kit-for-localization.md,"## Context\nLocalizable files have been defined in different ways to date. Therefore, we may need to duplicate text for other modules, or if we add new language support, it will be difficult to manage localizable files for common kits & modules from many different places within the channels.\n","* Common localization files across the channels modules' should be defined in the associated channel LocalizableKit.\n* Localizable files of common kits, modules and components used throughout the application should be defined within itself. It should not be defined in any LocalizableKit.\n* Localizable files of channels that only concern their own modules should be defined within their own modules.\n",58,4978,58,75
ios-architecture-decision-logs/0003-dont-use-chain-delegate.md,## Context\nOur problem was using chain delegate without doing any changes on every scope. It generates too many repeated codes.\n,Don't use chain delegate if you are not make any changes on every scope.\n,25,4979,25,17
ios-architecture-decision-logs/0007-presenter-casting-as-delegate.md,## Context\nWe faced a problem that different implementations of presenter's conforming delegates\n,We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\n,17,4980,17,64
ios-architecture-decision-logs/0015-use-UITestablePageProtocol-for-AccessibilityIdentifiers.md,## Context\nOur team created new UITestablePage Interface for setting accessibility Identifiers. For consistency we should replace and use UITestablePage.\n,Every View Controller should conform UITestablePage.\n,31,4981,31,11
ios-architecture-decision-logs/0000-use-xctest-for-unit-test.md,"## Context\nOur problem was that using third party library for unittest instead of XCTest. And our unittests were most likely behavioural, we also wanted to change this approach and write unittest directly\n",Every new unittest will be written with XCTest instead of Quick/Nimble\n,38,4982,38,15
ios-architecture-decision-logs/0013-unit-tests-required.md,## Context\nUnit testing finds problems early in the development cycle.\n,Adding unit tests is required when a change is made to related Presenter.\n,14,4984,14,15
ios-architecture-decision-logs/0016-selector-function-unit-test.md,## Context\nThe aim is to cover presenter's private selector's functions and increase unit tests coverage results.\n,A new variable should be created with AnyObject type\nThe presenter should be set to the new variable\nSelector function should be trigger with perform method which inside the new variable.\n,22,4985,22,35
ios-architecture-decision-logs/0009-layering-modular-ios-application-approach.md,## Context\nOur problem was defining the limit of modules.\n,We decided splitting frameworks with their user flows.\n,13,4986,13,10
ios-architecture-decision-logs/0018-use-preferredNavigationBarType.md,## Context\nThe aim is to cover doubled or none navigation bar issues with deeplink routings.\n,"Should use `BaseViewController` or `TYRootViewController` and manage it there, and override if need your preferred `NavigationBarType`\n",22,4988,22,28
ios-architecture-decision-logs/0020-inject-theme-manager-for-common-places.md,## Context\nEach channel should be able to give its own theme to the common views. We need to follow common rules about theme in common places.\n,"For common views, theme manager should injected into the associated presenter.\n",31,4990,31,14
halfpipe/0002-task-cache-volumes.md,"## Context\nConcourse has [task caches](https://concourse-ci.org/tasks.html#task-caches) to save state between runs of the same task on the same worker. This can greatly speed up tasks - making users happy, and reduce load - making operators happy. win win.\n","Change halfpipe to provide one directory `/var/halfpipe/cache` instead of a list of directories specific to common build tools.\nThis will allow users to configure any build tool to use the cache instead of the onus being on halfpipe to add support.\nAlso there is a small overhead to mounting each cache volume, so one volume is better than n.\n",60,4993,60,75
court-case-matcher/0002-retain-and-process-sns-metadata.md,"## Context\nThe service receives case messages from the SQS queue `court-case-matcher-queue`. These messages, being produced from an SNS subscription, are embedded in JSON containing metadata about the message. It is possible to remove this metadata through configuration of AWS, thereby allowing for easier processing of the case itself in court-case-matcher or to retain it.\nThe metadata includes the following fields\n* `messageId`\n* `topicArn`\n* `timestamp`\n",We have decided to retain the message metadata. There is no immediate use for the fields at but the cost of processing is low and there's a possibility that `messageId` will be useful for tracing.\n### Links\nDescription of the terraform field for enabling delivery of the message without metadata.\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sns_topic_subscription#raw_message_delivery\n,97,4994,97,84
court-case-matcher/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,4996,16,39
webwritertechandhumanity.com/0002-how-to-implement-special-pages.md,"## Context\nI need to implement two special pages, the privacy policy page and the about me page.\nThey can't be in the map, I don't want them there because the user would\nbe forced to read them when scanning the spiral.\n",I'll implement solution 1 because solution 2 is complicated and I want to prioritise\nthe release of a first working website.\n,52,5001,52,28
webwritertechandhumanity.com/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5002,16,39
hmpps-book-secure-move/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5008,16,39
trade-access-program/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5015,16,39
RaPPMap/0000-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5017,16,39
link_platform/0011-use-rails-for-backend.md,"## Context\nAt the onset of the Link Platform project we needed to decide what framework to use for our backend. After some\ndiscussion two choices presented themselves:\n1. Serverless architecture using AWS (lamda, API Gateway, etc.)\n1. Ruby on Rails\n",The decision was made to use Ruby on Rails for our backend.\n,59,5019,59,14
link_platform/0014-use-redux.md,## Context\nRedux is a state container for JavaScript applications. It helps to manage state across an application.\n[This article](https://hackernoon.com/the-react-state-museum-a278c726315) provides a nice non-comprehensive listing of various alternative state management libraries.\n,We will use Redux in `link_platform` for state management.\n,58,5021,58,14
link_platform/0016-use-devise-for-admin-authentication.md,## Context\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\n,[Devise](https://github.com/plataformatec/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https://github.com/omniauth/omniauth/wiki/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\n,37,5024,37,79
link_platform/0003-use-postgresql.md,"## Context\nWe want the ability to run in many environments, including Heroku, and PostgreSQL is the most well supported in our desired environments.\n",We will use PostgreSQL as our data persistence layer.\n,30,5028,30,11
link_platform/0007-use-rspec-and-factorybot-for-unit-testing.md,## Context\nWe want a unit test framework that is more feature rich than MiniTest.\nFactoryBot adds more features to test data than provided with fixtures.\n,To add RSpec and FactoryBot to the project.\n,33,5031,33,11
link_platform/0002-use-the-adr-tools-kit-to-manage-adr-docs.md,## Context\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\n`https://github.com/npryce/adr-tools`\nBut I installed it using brew:\n`brew install adr-tools`\n,We will use the system adr-tools to manage the ADR documents\n,79,5033,79,14
link_platform/0005-create-react-app-npm-no-yarn.md,## Context\nWe needed to decide how best to create and enforce a uniform file structure for our separate directories containing\nreact applications. This also led to a discussion about whether we should use Yarn or NPM.\n,"After discussion, it was determined that we would use [Create React App](https://github.com/facebook/create-react-app)\nto create a uniform project structure. With regard to Yarn vs. NPM, we concluded that using Yarn would introduce yet\nanother dependency and level of abstraction, and that it would not bring enough value to the project to justify those\ndrawbacks.\n",44,5034,44,79
link_platform/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5035,16,39
link_platform/0017-use-strings-for-all-model-id-fields.md,## Context\nTo enhance the ability to import data from any source that is compliant with the OpenReferral\nmodle use strings for fields that are keys.\n,Convert existing models to use strings instead of integers as the id fields and references.\n,33,5036,33,17
link_platform/0008-use-docker-images-to-deploy-the-application.md,## Context\nUse docker images to deploy the application.\n,"Run the application as a docker image even without container orchestrator.\nFor exmaple, we may run the new build as a container with a different port for testing on the same host as the\ncurrent production version.\nThis doesn't require all execution contexts to be docker images.\nFor example,  we can run the platform in a development container format but run it in Heroku as a non docker instance.\n",12,5037,12,87
verify-local-matching-service-example/0002-http-status-code-for-validation-errors.md,## Context\nThere can be validation errors when the request does not match the contract of the endpoint. These errors are mapped to\nhttp status code 422(Unprocessable Entity) with one or many error message(s).\n,"Matching Service Adapter(MSA) ignores if the response has any other status code other than http status code 200 (OK).\nSo for now we are letting the Dropwizard handle exception and return status code to MSA.\nWhen there is validation error, it returns http status code 422 with the error message.\n",44,5038,44,65
verify-local-matching-service-example/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5040,16,39
push-sdk-android/0004-divorce-the-sdk-from-firebase.md,"## Context\nThere was much discussion over whether the SDK should implement the\nFirebase-related services for token refresh and notification handling, or leave\nthat up to the consuming app. If the SDK handles it, it's less setup for the\nconsumer; however, this comes at the cost of much flexiblity.\n","In the end, we decided that the lack of flexibility (e.g. for supporting both\n""register on first launch"" and ""register/unregister on log in and log out""\nmodels) warranted removing the services from the SDK. They will be implemented\nin the demo application instead.\n",65,5042,65,60
push-sdk-android/0002-avoid-runtime-dependencies.md,## Context\nWe don't want to create an SDK that requires customers to add a slew of\nexternal dependencies.\n,"We will avoid external runtime dependencies. For instance, we will use the core\nJava networking libraries (e.g. HTTPUrlConnection) instead of introducing a\ndependency on something like [Volley][1].\nExternal development dependencies (those necessary for contributing, but which\nare not bundled with the SDK) will be added as necessary.\n",24,5043,24,71
push-sdk-android/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5044,16,39
sesopenko_diamond_square/ADR-1.md,"Context goes here. Describe the forces at play, including technological political, social, and project local. These forces are likely in tension and should be called out as such. The language in this section is value-neutral. It is simply describing facts. Rationale should be self-evident from the context\n","This section describes our response to these forces. It is stated in full sentences, with active voice. ""We will ...“\n",61,5045,61,27
testtrack-cli/adr-002.md,"## Context\nWe are transitioning from Rails migrations and a legacy TestTrack schema\nformat to testtrack CLI. In order to have confidence that migrations\nwill apply cleanly in production, we need to validate as much as we can,\nand we have a path to full information on extant splits.\n",We will not allow retirements of splits missing from the schema. We will\ninstead import all legacy splits and validate against the schema.\n,61,5048,61,28
FlowKit/0011-redaction-strategy-for-labelled-aggregates.md,"## Context\nAt present, any rows in a spatial query are dropped if they return an aggregate of 15 subscribers or less. With new\nlabelling disaggregation queries being added to Flowkit, there is an increased risk of deanonymization attacks\nusing the increased resolution of information - we need to consider further redaction strategies to mitigate this.\n","For any labelled spatial aggregate queries, we drop any aggregation zone that contains any disaggregation less than 15\n(for consistency with the rest of the dissagregation strategy).\n",71,5055,71,35
phpadr/0001-documenting-architecture-decisions.md,## Context\nRecord certain design decisions for the benefit of future team members as well as for external oversight.\n,"Use Architecture Decision Records (ADR), that is a technique for capturing important architectural decisions, along with their context and consequences as described by [Michael Nygard](https://twitter.com/mtnygard) in his article: [Documenting Architecture Decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",22,5067,22,74
phpadr/0002-develop-tool-to-manage-adrs.md,## Context\nEncourage and facilitate the use of documentation in agile projects in world of evolutionary architecture using the technique Architecture Decision Records (ADR).\n,Develop a command-line tool to manage the Architecture Decision Records (ADR) that will be stored in version control along with the project source code.\n,29,5068,29,29
phpadr/0006-yaml-as-configuration-file.md,"## Context\nIn order to use a custom ADR template, it must be possible to configure the path to it. The same template must be used so that all ADR`s are structured in the same way.\n",The template path can be defined via a [YAML](http://yaml.org/) configuration file.\n,44,5069,44,21
phpadr/0005-phpunit-as-testing-framework.md,"## Context\nEnsure good code quality with ease for change, integration and error correction.\n",It will be used the [PHPUnit](https://phpunit.de/) as testing framework.\n,18,5070,18,19
phpadr/0003-php-as-scripting-language.md,## Context\nThe tool must be cross-platform and developed using a open source programming language in command-line interface to be used in any project independent of the tech stack.\n,Develop using [PHP](http://php.net/) (PHP Hypertext Preprocessor) to work command line scripting.\n,34,5071,34,24
phpadr/0004-composer-as-dependency-management.md,"## Context\nManaging dependencies manually in any programming language is hard work, then we will need to use a dependency management tool for that project.\n",It will be used the [Composer](https://getcomposer.org/) as tool for dependency management.\nThis project can also be installed with Composer using the following command:\n```\ncomposer require globtec/phpadr --dev dev-master\n```\n,29,5072,29,52
docker-texlive/0002-provide-all-packages.md,## Context and Problem Statement\nShould the Docker image include all packages or a subset of the packages?\n,"Chosen option: ""Provide all packages"", because\n* texliveonfly does not work on all packages\n* speeds-up compilation time (because no additional download)\nWe accept that the final image is ~2GB of size.\n",21,5073,21,49
my-notes/stylelint.md,"### Context\n- linting for CSS\n- After running it, it didn't pick up a single error\n### Decision\nDon't start using stylelint\n",Don't start using stylelint\n,33,5084,33,7
my-notes/sentry.md,### Context\n- error tracking\n### Decision\nI don't think it will be easy to justify to a client because it is not yet mainstream in enterprise development and I'm not super passionate about it. So I'm not going to invest the time learning it now.\n,I don't think it will be easy to justify to a client because it is not yet mainstream in enterprise development and I'm not super passionate about it. So I'm not going to invest the time learning it now.\n,55,5085,55,45
my-notes/react-hooks.md,"### Context\n- new way of writing React code\n### Decision\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\n","Since it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\n",39,5086,39,25
my-notes/prettier.md,### Context\nPrettier integrates with well with vscode but conflicts with Standard\n### Decision\nReplace Standard with Prettier\n,Replace Standard with Prettier\n,26,5087,26,7
my-notes/classnames.md,### Context\nThe `classnames` library makes it easier to combine class names.\n### Decision\nStart using\n,Start using\n,23,5088,23,3
my-notes/typescript.md,### Context\nTypescript offers better IDE integration and type checking.\n### Decision\nStart using Typescript.\n,Start using Typescript.\n,23,5089,23,6
my-notes/styled-components.md,### Context\n- easier to maintain than traditional CSS\n- I was not able to get my tests working with it properly\n### Decision\nDon't start using Styled Components\n,Don't start using Styled Components\n,35,5090,35,7
my-notes/cypress.md,### Context\n- e2e/UI testing framework\n- Was not able to get it to run on Netlify\n### Decision\nI won't be able to start using it until I can get it working with a CI tool.\n,I won't be able to start using it until I can get it working with a CI tool.\n,48,5091,48,21
my-notes/react-testing-library.md,### Context\nIt's annoying when tests break when refactoring.\n### Decision\nStart using React Testing Library\n,Start using React Testing Library\n,23,5092,23,6
apim-blue-green-deploy/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5094,16,39
postfacto/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5096,16,39
form-builder/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nWe will keep ADRs in the `decisions` directory of this repository.\nDecisions will be written using Markdown and named `adr-NNN-AAAA.md`.\n",16,5105,16,77
afterwriting-labs/003-release-process.md,"## Context\nTravisCI is currently used for building feature branches, pull requests and releases.\nThe way releases, however, are performed is a bit convoluted and not flexible. By default minor version is updated\nand it's not clear how to do a major or patch release. Due to the way TravisCI builds feature branches vs pull requests\nit requires hacks in travis config.\n",Solution implemented in https://github.com/ifrost/starterkit have worked quite well so far. It's based on adding\nlabels to commit messages when merging a pull request to master.\nIt's way easier to do major/minor/patch or hotfix release and publish to npm separately.\n,80,5106,80,60
dp/0002.md,"## Context\nWhile developing APIs which store metadata about datasets, versions, filter jobs\nand import jobs using Postgres, it has been found that some updates required\ncomplex transactional updates across multiple tables.\nWe need to assess our data model and decide if a relational database\nbest fits this use case, or what alternative would be best.\n","MongoDB will be used to store metadata for datasets, versions, filter and import\njobs. This allows us to prioritise the read heavy interactions, storing the\ndata in the same JSON format it will be presented to users.\n",70,5109,70,47
dp/0005.md,"## Context\nAdditional screens for the publishing workflow were designed given all the\npossible fields regardless of how they are retrieved or edited from the API.\nThese fields were broken down into suggested logical groupings for users to edit.\nThis separation means multiple API calls could be needed to save updates on a\nsingle screen, and that datasets will have to be added with versions depending\non which fields are updated (going against [decision 0004](0004.md)).\n",Screens should be separated to update either the dataset or version metadata\nbut not both in one.\n,96,5110,96,20
dp/0001.md,"## Context\nWe need to decide whether the Customise My Data beta will be hosted\nas part of the existing ONS website and infrastructure, or whether\nit will be hosted independently.\n","The beta will be hosted on beta.ons.gov.uk, on its own dedicated infrastructure.\nThis reduces the risk of CMD-specific changes impacting on our existing service.\n",39,5112,39,34
dp/0006.md,"## Context\nIn order to create a filter, a user must provide a unique way of referencing\na specific version/instance document. Instance IDs uniquely identify the document\nbut it may not be clear to users what this ID represents.\nIt is likely that users understand which version, of which edition, of which\ndataset they are interested in - and this also uniquely identifies the document.\n",Filters should be created using a combination of dataset/edition/version IDs\n,80,5113,80,14
dp/0004.md,"## Context\nWhen designing APIs to handle metadata about datasets, editions and versions\nno particular limitations were placed on which order these documents should be\nedited in.\nIn the existing publishing workflow, a single item can be added to only one\ncollection at a time. As the APIs have separated formerly joined concepts, it\nis necessary to understand whether datasets can be edited or published in\nisolation from versions.\n",Adding a version to a collection does not prevent the dataset being edited in a different collection\n,85,5114,85,18
dp/0003.md,"## Context\nWhen building new micro-services defaults were given to some configuration values so\napplications would work locally 'out of the box'. This was not done for all values\nor in all services.\nThere was discussion around whether it would be better for the service to\nerror/exit if any configuration value was not provided on startup, to be clear\non what needs to be set.\n",All services will be built with default configuration values set - with defaults\nthat will most likely work when running services locally.\n,80,5115,80,25
ikforth/0002-32-bits-code.md,## Context\n32-bits systems were all the rage when this project was concieved back in 1999.\n64-bits systems were not available yet.\n16-bits and 8-bit systems were not interesting.\n,IKForth is implemented as 32-bits code with 32-bits CELL size.\n,47,5119,47,20
ikforth/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5120,16,39
ontrack/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5122,16,39
SearchServices/0005-merge-tests-and-production-code.md,"## Context\nIn [ADR 3: ""Combined Codebase""](0003-combined-codebase.md) we decided to merge the production and end-to-end test\nrepositories. In [ADR 4: ""Community Mirror""](0004-community-mirror.md) we discuss setting up a mirror for the community\ncode.\n","We will separate the end-to-end test code in half so that any code solely related to Insight Engine won't be mirrored.\nWe will remove the existing test groups for the different versions of Search Services and Insight Engine, and instead\ndelete any tests from branches where they should not be run.\n",68,5129,68,61
SearchServices/0001-record-architecture-decisions.md,## Context\nCapture and record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",15,5132,15,39
content-data-admin/adr-000-document-architectural-decisions.md,## Context\nWe aim to:\n- Make it easier to understand the codebase and its status\n- Reduce the number of meetings to handover information across teams\n- Facilitate team rotations across GOV.UK\n,"Track architectural decision that impact the status of the Content Data Admin, [following a lightweight format: ADR][1]\n",46,5133,46,25
university-manager-mock-frontend/adr-001.md,"## Context\nSince both developers will be working in paralel, and time is very limited,\nan architecture that allows paralel work is highly beneficial.\n","We will implement the backend using REST API principles, allowing both modules to work independently.\n",33,5134,33,18
university-manager-mock-frontend/adr-002.md,"## Context\nSince both developers will be working in paralel, and time is very limited,\nan architecture that allows paralel work is highly beneficial.\n","We will implement the backend using REST API principles, allowing both modules to work independently.\n",33,5135,33,18
profiles-db-elastic/0004-bulk-load-gp-data.md,## Context\nOther work within the same domain has created a JSON file containing all GP practices that need to be searched and exposed via ES. This datafile was originally created to be loaded into MongoDB. Whilst Elastic Search (ES) can import the JSON document in the existing format it takes considerable time. Re-shaping the data to that required by the bulk import API of ES means the import can happen in a matter of seconds rather than many minutes otherwise.\n,We will use the existing data file but reshape the data specifically to suit the needs of ES.\n,92,5137,92,20
profiles-db-elastic/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5139,16,39
adr-generation-tool/rate_limit_adr_0001.md,## Context\nContext for rate limiting pattern\n,Prosa about the decision in the ADD\n,9,5142,9,9
adr-generation-tool/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5143,16,39
commcare-cloud/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5146,16,39
api-docs/0004-use-markdown-as-documentation-format.md,"## Context\nProject documentation is formatted using HTML. Documentation is accessible only online.\nA browser is needed to read the pages. Too much noise around the important text.\nDocumentation can differ from the version bundled with the project,\nusually not updated online. Update needs HTML editing/formatting/reformatting.\nNot portable.\n",Use the [markdown syntax](https://daringfireball.net/projects/markdown/syntax)\nto format documentation.\n,67,5152,67,25
api-docs/0002-use-jekyll-to-serve-documentation.md,## Context\nProject documentation must be published on the internet.\n,Use [Jekyll](https://jekyllrb.com/) on our servers to serve these documents just like GitHub Pages do.\n,13,5153,13,26
api-docs/0003-blog-post-as-a-documentation-page.md,## Context\nA document must be inserted into Jekyll.\n,"For each project document, create a new blog post in Jekyll.\n",13,5154,13,15
api-docs/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5155,16,39
sepa-customer-platform/0001-user-ms-dynamics-for-contact-case-document-management.md,"## Context\nHandling pre-applications for licenses requires opening cases, attaching relevant supporting information (documents) and recording the decisions & communications made by operators and SEPA staff.\nSEPA also needs to be able store information about the operator contacts.\n","MS Dynamics offers the best value in terms of ‘out the box’ functionality, interoperability with other cloud services & existing SEPA systems (CLAS) and has an existing knowledge and resource base in house.\n",50,5158,50,43
sepa-customer-platform/0002-use-azure-blob-storage-store-large-supporting-data-files.md,## Context\nA cloud storage solution is required to allow operators and SEPA staff to upload the large supporting data files required to process an application.\nThe storage solution would need to integrate with the case management tool (Microsoft Dynamics 365)\n,"MS Dynamics offers close integration with Azure Cloud services, therefore Azure Blob Storage has been selected.\n",48,5166,48,19
identity-site/0003-remove-basscss.md,"## Context\nIn order to seamlessly incorporate the Login.gov and US Web Design System (USWDS), we need to remove BassCSS\n",We will remove BassCSS FROM THE `identity-site` repo.\n,28,5168,28,14
identity-site/0002-add-netlifycms.md,## Context\nThe team would like more control in editing Login.gov content instead of having an engineer edit a large YML file. The goal is to not have to know code in order to make edits to Login.gov.\n,We will add Netlify CMS (Content Management System) to the architecture of the Login.gov.\n,45,5169,45,20
identity-site/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5170,16,39
pace-developers/0003-pace-projects-will-have-independent-documentation.md,"## Context\nPACE contains a number of independently developed and released projects with separate source and build pipelines.\nCoordination of documentation from projects using three different primary languages (MATLAB, Python, C) with varying release cycles will create tight coupling between the projects and their builds.\n",Each project will maintain its own GitHub pages documentation that will be updated with the project's build-release-deploy pipeline.\n,56,5173,56,24
pace-developers/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5182,16,39
umbrella/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",16,5186,16,39
js-sdk/0017-migrate-testnet-devnet-to-use-std-stellar-wallet.md,## Context\nStellar testnet network is overloaded recently and causes a lot of delay and errors\n,"Using Stellar Mainnet network (STD TFT). But prices will vary according to the explorer type, example: (devnet would cost 1% from mainnet, testnet would cost 10% from mainnet price)\n",20,5187,20,46
js-sdk/0008-add-kwargs-to-3bot-start.md,## Context\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\n,"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\n",28,5188,28,30
js-sdk/0012-use-poetry-lock-file-to-install-instead-of-poetry-update.md,## Context\nDependancies versions update can lead to failure. They needed to be tested well before using them. Poetry update bumps version to latest each time to execute and takes a long time\n,- Use poetry lock file to install dependancies using specific versions instead of executing poetry update each time\n- Updating versions will be by the repository maintainers after making sure no compitablity issues\n,39,5189,39,40
js-sdk/0018-change-domain-names-to-shorter-ones.md,## Context\nDomain names in deployed 3bots and solutions are too long\n,"Change domain names to shorter ones, and in case of duplication it will add a random number appended to the domain\n",16,5190,16,23
js-sdk/0010-update-3bot-deployer-image-to-use-ubuntu-20-04.md,## Context\nUbuntu 19.10 has become deprecated and poetry version is old which causes poetry failures in 3Bot deployer after removing poetry update and sticking to peotry install using lockfile\n,Update base image (phusion) to use ubuntu 20.04 instead of ubuntu 19.10 and use it in 3Bot deployer\n,41,5191,41,31
js-sdk/0009-add-logs-in-trc-and-nginx-container.md,## Context\nAdd feature to stream logs from trc and nginx container for better debugging\n,Update the nginx and trc flists to use zinit and redirect logs to stdout to be streamed from redis\n,18,5192,18,23
js-sdk/0011-nginx-expose-option.md,"## Context\nMinio solution listens for HTTP only. To safely expose it and use it for backups with restic for example, it needs to use HTTPS and use a valid certifcate.\n",Add option to solution expose chatflow to expose solutions using nginx reverse proxy to terminate ssl connections.\n,40,5193,40,20
js-sdk/0006-add-preinstall-to-packages.md,## Context\nFailed to install weblibs package on fresh installation\n,Add pre-install method to packages. This method will have the code that will be executed once before installation and package can't go without and that will seperate using install with kwargs for configuring the package and pre-install to dependancies like git clone.\n,14,5194,14,49
js-sdk/0004-marketplace-to-automate-payment.md,## Context\nMarketplace demos website to work on testnet only and automate the payment\n,- use a defined wallet `demos_wallet` for payments\n- the wallet will have lots of TFTs to use\n- solutions to live max 3 hours\n,18,5195,18,34
js-sdk/0020-dynamically-get-branch-for-threebot-deployer.md,"## Context\nIn threebot deployer we hardcode the branch we deploy from in the code, this will be annoying if we need to deploy from another branch.\n",Getting the branch dynamically from the active branch of js-sdk repository\n,34,5196,34,13
js-sdk/0007-payment-for-3bot-deployer.md,## Context\nThe deployment of 3Bot is not guaranteed to succeed due to network failures or misbehaving of nodes on the grid. Users who pay for a 3Bot reserve capacity and in case of failing to initialize the solution they lose their money (and that capacity won't be usable after payment)\n,"- 3Bot deployer to start with a funded wallet\n- when the user wants to create an instance, we create a pool for this instance using the funded wallet for 15 mins (that should be enough for the initialization step).\n- When we manage to deploy and initialize the 3Bot, we ask the user to extend the lifetime of the 3Bot in the same chatflow\n- In case of pool extension failure they will be refunded from the explorer\n",63,5197,63,96
js-sdk/0003-allow-only-tft-token.md,"## Context\nDrop FreeTFT, TFTA tokens from SDK\n","Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\n",14,5198,14,29
js-sdk/0014-managed-domain-verification-and-blocking-in-apps.md,## Context\nThere is no validation for the managed domains specified in the gateways information on the explorer side. Some of these domains are not delegated properly to the gateway's name server so all subdomains we create using these domains are not populated and not resolvable.\n,Create a test subdomain of each managed domain and verify that the subdomain is resolvable and block managed domains that fail this check for a certain amount of time.\n,54,5199,54,34
js-sdk/0015-parameterize-zos-sal-in-identity.md,## Context\nParameterize zos sal in identity so if we want to switch identity at certain point we can for example to deploy workloads with that identity\n,- Make zos sal paramertized with specific identity. If not passed it will use the default identity\n,32,5200,32,22
js-sdk/0016-update-threebot-deployer-flist.md,## Context\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\n,Update the flist with the up to date one\n,48,5201,48,11
js-sdk/0013-update-cryptpad-flist.md,## Context\nCryptpad solution image used in docker was deprecated and flist doesn't make use of volumes\n,- Update flist to make use of volumes\n- Update base image in docker file to be able to maintain\n,22,5202,22,23
js-sdk/0019-use-custom-activation-wallet-for-threebot-deployers.md,## Context\nActivation service fails a lot and that lead to starting online threebot without a wallet\n,Add option to activate the 3bot wallet via custom activation wallet on the deployer in case threefold service activation fails and pass the secret in the secret env\n,20,5203,20,33
js-sdk/0021-wait-for-pool-payments.md,## Context\nPools payment can fail due to stellar transaction submission timeout.\n,"Since the endpont `https://<explorer-url>/api/v1/reservations/pools/payment/<pool-id>` is deployed now, A wait method is added to wait for the payment info until it's done successfully\n",16,5204,16,46
js-sdk/0005-3bot-to-start-with-a-testnet-funded-wallet.md,## Context\nTo reduce overhead of users interaction with the Grid and the hassle with the money flow\n,Every 3bot starts with testnet wallet funded from faucet with 1K TFT\n,20,5205,20,18
js-sdk/0002-block-misbehaving-nodes.md,## Context\nBecause of failures on zos side e.g [zdb address in use](https://github.com/threefoldtech/zos/issues/916) and [failed to retrieve owner of vollume](https://github.com/threefoldtech/zos/issues/919) We need a feature to block specific nodes or specific farms that acting weird or with low performance\n,"When deploying every threebot is aware of failing nodes, and maintains a disallow list typically blocking failing nodes for 4 hours.\n",76,5206,76,27
js-sdk/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5207,16,39
report-a-defect/0002-use-sentry-for-application-monitoring.md,## Context\nHackney have a preference for using this tool across their digital services: https://github.com/LBHackney-IT/API-Playbook#centralised-exception-logging\n,Sentry will be the third party service we use for collecting and managing the exceptions for all environments.\n,39,5208,39,21
report-a-defect/0005-use-docker.md,## Context\nHackney have a preference for using containers across their digital services: https://github.com/LBHackney-IT/API-Playbook#containers\n,We will build and run this service using Docker and Docker Compose. Heroku container hosting will be used.\n,33,5209,33,23
report-a-defect/0014-use-heroku-scheduler.md,## Context\nThere is a user need to receive daily emails around a certain time.\nHeroku Scheduler is a free service created by Heroku that lets you schedule\ntasks to run at specific times.\nHackney have already used Heroku scheduler in the [Hackney Repairs project](https://dashboard.heroku.com/apps/hackney-repairs-production/scheduler).\n,Since there is an established pattern of using Heroku Scheduler in other projects\nwe should continue this pattern by adopting it in this project.\n,76,5210,76,28
report-a-defect/0003-use-new-relic-for-performance-monitoring.md,## Context\nHackney have a preference for using this tool across their digital services: https://github.com/LBHackney-IT/API-Playbook#centralised-exception-logging\n,New Relic will be the third party service we use for collecting and managing performance data for all environments.\n,39,5214,39,22
report-a-defect/0007-use-postgres-search.md,## Context\nThe New Build Team need to be able to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\n,To implement search we will use the built in Postgres Search rather than adding a new dependency on another service like ElasticSearch.\n,32,5217,32,26
report-a-defect/0004-use-pingdom-for-uptime-monitoring.md,## Context\nHackney have a preference for using this tool across their digital services: https://github.com/LBHackney-IT/API-Playbook#centralised-exception-logging\n,Pingdom will be the third party service we use for collecting and managing performance data for all environments.\n,39,5218,39,21
report-a-defect/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5220,16,39
reactive-interaction-gateway/0005-maintain-changelog.md,## Context\nWe need to track what changes have been made to RIG and make it clear to everyone. It should also help with identifying of issues that may be caused by changes in the past.\n,We decided to use `CHANGELOG.md` to have it as a single source of truth to what happened in RIG. Every submitted Pull Request will contain update to this file as well. By this everyone can clearly see which version has which features or fixes.\n,41,5228,41,53
php-docker-template/0004-default-php-settings-memory-limit.md,"## Context\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\n","Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\n",92,5235,92,38
php-docker-template/0002-nginx-configuration-is-shaped-for-php-needs.md,"## Context\nNginx is a webserver which can work on many ways, from proxy, to reverse proxy, load balancer and traditional web server.\nWe need a configuration which suits the need for PHP fpm under sockets\n","The default configuration and custom variables will be shaped for PHP fpm needs, allowing an easy plug-and-play for these kind of projects.\nThis includes using a shared PHP fpm socket under `/var/run`.\n",48,5236,48,44
php-docker-template/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https://github.com/usabilla/architecture-chapter/blob/master/docs/adr/0001-record-architecture-decisions.md).\n",16,5237,16,46
remultiform/0006-use-jest.md,"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n",We will use Jest as our testing framework.\n,40,5252,40,10
remultiform/0005-use-eslint.md,"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,69,5253,69,57
remultiform/0007-use-dependabot-to-keep-dependencies-up-to-date.md,## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n,We will use Dependabot to monitor dependency updates.\n,38,5254,38,12
remultiform/0007-make-remultiform-configuration-driven.md,"## Context\nWe want to build a reusable React library, that generates multipage forms\nquickly. We need logic that extends beyond simple ""show this element on this\npage"", such as conditional logic for displaying sections dependant on values in\nother sections, and to skip pages based on values elsewhere in the journey.\n",We will build remultiform to export a series of small orchestration components.\nWe will pass the entire multipage form configuration in via props to those\norchestrators. The configuration should be agnostic of the set of components it\nmight receive.\n,66,5256,66,53
remultiform/0003-use-rollup-to-build-distributables.md,## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n,We will build distributables using Rollup.js.\n,62,5259,62,12
remultiform/0001-record-architecture-decisions.md,## Context\nWe need to record the architectural decisions made on this project.\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",16,5261,16,40
