prompt,completion
"The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfo’s validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a “Closure” flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Let’s show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","We will create the following datatype:

    -- | ValidityInterval is a half open interval. Closed (inclusive) on the bottom, open
    -- (exclusive) on the top. A 'Nothing' on the bottom is negative infinity, and a 'Nothing'
    -- on the top is positive infinity.
    data ValidityInterval a = ValidityInterval
      { invalidBefore :: !(Maybe a) -- ^ Inclusive lower bound or negative infinity
      , invalidHereafter :: !(Maybe a) -- ^ Exclusive upper bound or positive infinity
      }

    We will add the following constraint and smart constructor:

    data TxConstraint =
      ...
      MustValidateInTimeRange !(ValidityInterval POSIXTime)

    mustValidateInTimeRange :: !(ValidityInterval POSIXTime) -> TxConstraints

    We will remove the MustValidateIn constraint and deprecate the the mustValidateIn smart constructor which will be replaced by mustValidateInTimeRange.

    We will create the smart constructor

    mustValidateInSlotRange :: !(ValidityInterval Slot) -> TxConstraints

    which will translate the provide validity slot range into a POSIXTimeRange using Ledger.TimeSlot.posixTimeRangeToContainedSlotRange."
"XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","* *Chosen Alternative: D*
* This is in line with other editors: They allow to save, but warn if the file has compile errors, validation errors, ...

"
"IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream.","Standardization of the repositories has been deemed a worthwhile endeavour, though of very low priority.

    Phase 1 of the standardization process will be carried out in parallel with Move Marconi to a separate repository. A separate repository will be created for Marconi, and from the very beginning it will use std. This way the benefits, limitations and integration costs of std can be experienced and measured, and an informed, definitive decision on standardizing plutus-core and plutus-apps themselves can be made."
"Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","We will put Marconi in a separate Github repository

    Until we resolve the issues with creating a separate Github repository (see Context), we will keep Marconi as a separate project in plutus-apps"
"Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed.","We will document every architecture-level decision for Arachne and its
core modules with an
[Architecture Decision Record](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). These
are a well structured, relatively lightweight way to capture
architectural proposals. They can serve as an artifact for discussion,
and remain as an enduring record of the context and motivation of past
decisions.

The workflow will be:

1. A developer creates an ADR document outlining an approach for a
   particular question or problem. The ADR has an initial status of ""proposed.""
2. The developers and steering group discuss the ADR. During this
   period, the ADR should be updated to reflect additional context,
   concerns raised, and proposed changes.
3. Once consensus is reached, ADR can be transitioned to either an
   ""accepted"" or ""rejected"" state.
4. Only after an ADR is accepted should implementing code be committed
   to the master branch of the relevant project/module.
5. If a decision is revisited and a different conclusion is reached, a
   new ADR should be created documenting the context and rationale for
   the change. The new ADR should reference the old one, and once the
   new one is accepted, the old one should (in its ""status"" section)
   be updated to point to the new one. The old ADR should not be
   removed or otherwise modified except for the annotation pointing to
   the new ADR.

"
"Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)","* Chosen Alternative: *Manual serialization*
* SnakeYAML does not support annotations for serialization
* jackson-dataformat-yaml seems not to support annotations, such as [jackson-annotations](https://github.com/FasterXML/jackson-annotations)

"
"We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  
",Decided on Bulma.
"We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.
",Decided on .env files with related default file and schema file.
"In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","Arachne will use Boot to provide an abstract asset pipeline. Boot has built-in support for immutable Filesets, temp directory management, and file watchers.

As with everything in Arachne, the pipeline will be specified as pure data in the configuration, specifying inputs, outputs, and transformations explicitly.

Modules that participate in the asset pipeline will develop against a well-defined API built around Boot Filesets.

"
"We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","We have decided to use the Java programming language for our project, considering its security features, concurrency support, ability to handle large codebases, and versatility. This decision aligns with our project requirements, and we believe that choosing Java will ensure its success. "
"How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","Chosen option: ""OneFlow"" because it provides a single eternal branch with well structured releases.
We'll implement OneFlow with these details:
Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.Release and quality managers from each team are responsible for reviewing and approving releases.Releases apply to all apps in the monorepo.Releases are versioned like this: {cycle}.{sprint}.{hotfix}. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.Feature branches are merged using ""Squash and merge"", so they can be easily reverted.There are two ways to build larger features.If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.
If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.
Future strategy
With time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.
Hosting environments
We'll set up continuous delivery to different hosting environments:
EnvironmentGit sourceDatabases/servicesFeaturessandboxfeature branchTestAlldevmainTestAllstagingmainProdAllpre-prodrelease/hotfix branchProdFinishedprodlatest release tagProdFinished
We'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage."
"We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.",We have decided to use Python as our primary programming language for our project.
"We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible.","Chosen option: ""Nx"", because:
It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).It's relatively easy to learn with focused documentation.It has schematics to generate apps, libraries and components that includes all of our tools.It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended."
"The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.",Adopt Microsoft Azure as the cloud infrastructure for our organization.
"Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use.","We will separate the problems of creating rich exceptions, and catching them and displaying them to the user.

### Creating Errors

Whenever a well-behaved Arachne module needs to report an error, it should throw an info-bearing exception. This exception should be formed such that it is handled gracefully by any JVM tooling; the message should be terse but communicative, containing key information with no newlines.

However, in the `ex-data`, the exception will also contain much more detailed information, that can be used (in the correct context) to provide much more detailed or verbose errors. Specifically, it may contain the following keys:

- `:arachne.error/message` - The short-form error message (the same as the Exception message.)
- `:arachne.error/explanation` - a long-form error message, complete with newlines and formatting.
- `:arachne.error/suggestions` - Zero or more suggestions on how the error might be fixed.
- `:arachne.error/type` - a namespaced keyword that uniquely identifies the type of error.
- `:arachne.error/spec` - The spec that failed (if applicable)
- `:arachne.error/failed-data` - The data that failed to match the spec (if applicable)
- `:arachne.error/explain-data` - An explain-data for the spec that failed (if applicable).
- `:arachne.error/env` - A map of the locals in the env at the time the error was thrown.

Exceptions may, of course, contain additional data; these are the common keys that tools can use to more effectively render errors.

There will be a suite of tools, provided with Arachne's core, for conveniently generating errors that match this pattern.

### Displaying Errors

We will use a pluggable ""error handling system"", where users can explicitly install an exception handler other than the default.

If the user does not install any exception handlers, errors will be handled the same way as they are by default (usually, dumped with the message and stack trace to  `System/err`.) This will not change.

However, Arachne will also provide a function that a user can invoke in their main process, prior to doing anything else. Invoking this function will install a set of default exception handlers that will handle errors in a richer, more Arachne-specific way. This includes printing out the long-form error, or even (eventually) popping open a graphical data browser/debugger (if applicable.)

"
"Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply.","Developers will have the option of writing configuration using either
native Datomic-style, data, or code-based *configuration
scripts*. Configuration scripts are Clojure files which, when
evaluated, update a configuration stored in an atom currently in
context (using a dynamically bound var.)

Configuration scripts are Clojure source files in a distinct directory
that by convention is *outside* the application's classpath:
configuration code is conceptually and physically separate from
application code. Conceptually, loading the configuration scripts
could take place in an entirely different process from the primary
application, serializing the resulting config before handing it to the
runtime application.

To further emphasize the difference between configuration scripts and
runtime code, and because they are not on the classpath, configuration
scripts will not have namespaces and will instead include each other
via Clojure's `load` function.

Arachne will provide code supporting the ability of module authors to
write ""configuration DSLs"" for users to invoke from their
configuration scripts. These DSLs will emphasize making it easy to
create appropriate entities in the configuration. In general, DSL
forms will have an imperative style: they will convert their arguments
to configuration data and immediately transact it to the context
configuration.

As a trivial example, instead of writing the verbose configuration data:

```clojure
{:arachne/id :my.app/server
 :arachne.http.server/port 8080
 :arachne.http.server/debug true}
 ```

You could write the corresponding DSL:

```clojure
(server :id :my.app/server, :port 8080, :debug true)
```

Note that this is an illustrative example and does not represent the
actual DSL or config for the HTTP module.

DSLs should make heavy use of Spec to make errors as comprehensible as possible.

"
"Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database","* *Chosen Alternative: Filesystem*

"
"Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories","* Chosen Alternative: *Builders*
* Flexible
* Simple for complex objects
* Extensions cause problems (solved with generic builders) 

### Generic Builders

Generic Builders are used to enable safe method chaining for Builders with extend other Builders.
Another discussion is made at [stackoverflow].

The method `self()` is necessary because all setter methods should return the Builder used for instantiation and not the builder that is extended. `self()` can not be replace by `this` because the expected type is `<T>` and casting to `<T>` results in warnings.

Builders which are not abstract and are extended by other builders are generic and implement the `self()` method by casting `this` to `<T>`. To reduce warnings this casting is only used in this case.

Example:
```java
// part of ExtensibleElements.Builder
public abstract static class Builder<T extends Builder<T>> {
    private List<TDocumentation> documentation;
    
    // setter returns generic <T> 
    public T setDocumentation(List<TDocumentation> documentation) {
        this.documentation = documentation;
        // return this; => IncompatibleType exception either cast with warnings or use self() method
        return self();
    }
    
    // overwritten method
    public abstract T self();
}

// part of TEntityType.Builder
public abstract static class Builder<T extends Builder<T>> extends TExtensibleElements.Builder<T> {
	
}

// part of TNodeType.Builder
public static class Builder extends TEntityType.Builder<Builder> {
    @Override
    public Builder self() {
        return this;
    }
}
```



[Builders]:(https://en.wikipedia.org/wiki/Builder_pattern)
[stackoverflow]: https://stackoverflow.com/a/5818701/8235252

"
"The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.",To implement continuous integration as a solution to the challenges experienced in the development process.
