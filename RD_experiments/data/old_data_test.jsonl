{"prompt":"island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\nWhich CMS system would best suit the needs of island.is?\nDecision Drivers\nContent needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute","completion":"Devs narrowed the choice down to two options Contentful and Contentstack.\nBoth systems meet the required featureset.\nA decision from management was made to use Contentful. Contentful is deemed to have a larger presence in the Icelandic dev community. Contentful is also believed to have a stronger funding base. Contentful is already implemented in some of our projects."}
{"prompt":"The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype \n\nThis is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.\n\nHowever, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:\n\n- There is a desire for some types to be \"abstract\", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.\n- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.\n- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.\n- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to \"denormalization\", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.\n\nAll of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)","completion":"The concept of type extension and attribute inheritance does not provide benefits proportional to the cost.\n\nWe will remove all concept of supertypes, subtypes and attribute inheritance from Chimera's data model.\n\nChimera's data model will remain \"flat\". In order to achieve attribute reuse for data stores for which that is idiomatic (such as Datomic), multiple Chimera attributes can be mapped to a single DB-level attribute in the adapter mapping metadata.\n   \n"}
{"prompt":"Arachne has several very explicit goals that make the practice and\ndiscipline of architecture very important:\n\n- We want to think deeply about all our architectural decisions,\n  exploring all alternatives and making a careful, considered,\n  well-researched choice.\n- We want to be as transparent as possible in our decision-making\n  process.\n- We don't want decisions to be made unilaterally in a\n  vacuum. Specifically, we want to give our steering group the\n  opportunity to review every major decision.\n- Despite being a geographically and temporally distributed team, we\n  want our contributors to have a strong shared understanding of the\n  technical rationale behind decisions.\n- We want to be able to revisit prior decisions to determine fairly if\n  they still make sense, and if the motivating circumstances or\n  conditions have changed.","completion":"We will document every architecture-level decision for Arachne and its\ncore modules with an\n[Architecture Decision Record](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). These\nare a well structured, relatively lightweight way to capture\narchitectural proposals. They can serve as an artifact for discussion,\nand remain as an enduring record of the context and motivation of past\ndecisions.\n\nThe workflow will be:\n\n1. A developer creates an ADR document outlining an approach for a\n   particular question or problem. The ADR has an initial status of \"proposed.\"\n2. The developers and steering group discuss the ADR. During this\n   period, the ADR should be updated to reflect additional context,\n   concerns raised, and proposed changes.\n3. Once consensus is reached, ADR can be transitioned to either an\n   \"accepted\" or \"rejected\" state.\n4. Only after an ADR is accepted should implementing code be committed\n   to the master branch of the relevant project\/module.\n5. If a decision is revisited and a different conclusion is reached, a\n   new ADR should be created documenting the context and rationale for\n   the change. The new ADR should reference the old one, and once the\n   new one is accepted, the old one should (in its \"status\" section)\n   be updated to point to the new one. The old ADR should not be\n   removed or otherwise modified except for the annotation pointing to\n   the new ADR.\n\n"}
{"prompt":"The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.","completion":"Adopt Microsoft Azure as the cloud infrastructure for our organization."}
{"prompt":"While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.\n\nThere are a number of possible benefits to this approach:\n\n- Deployments from the same configuration are highly reproducible\n- Organizations can maintain an immutable persistent log of configuration changes over time.\n- External tooling can be used to persistently build and define configurations, up to and including full \"drag and drop\" architecture or application design.\n\nDoing this introduces a number of additional challenges:\n\n- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)\n  \n  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.\n- **Scope & Naming**: It is extremely convenient to use `:db.unique\/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is \"the same\" as an older entity in the system.\n\n  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and \"collide\", causing inadvertent linkages between what ought to be logically distinct configurations.\n \n  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.\n\n- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support \"forking\" or maintaining multiple concurrent versions of the same logical data set.\n\n  This does introduce complexities when thinking about \"modifying\" a configuration, while still keeping the old one. This kind of \"fork\" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique\/identity` attrs.\n  \n  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.\n\n- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the \"same\" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.\n\n  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.\n  \n- **Relationship with module load process**: Would the stored configuration represent only the \"initial\" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?\n\n  Both alternatives present issues.\n  \n  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.\n  \n  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would \"old\" or stale module-generated values be removed?\n\n#### Goals\n\nWe need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:\n\n- Define a new configuration from an init script.\n- Run an init script on an existing configuration, updating it.\n- Edit an existing configuration using the REPL.\n- Edit an existing configuration using a UI.\n- Clone a configuration\n- Deploy based on a specific configuration\n\nAt the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.","completion":"We will not attempt to implement a concrete strategy for config persistence at this time; it runs the risk of becoming a quagmire that will halt forward momentum.\n\nInstead, we will make a minimal set of choices and observations that will enable forward progress while preserving the ability to revisit the issue of persistent configuration at some point in the future.\n\n1. The configuration schema itself should be compatible with having several configurations present in the same persistent database. Specifically:\n  - Each logical configuration should have its own namespace, which will be used as the namespace of all `:db.unique\/identity` values, ensuring their global uniqueness.\n  - There is a 'configuration' entity that reifies a config, its possible root components, how it was constructed, etc.\n  - The entities in a configuration must form a connected graph. That is, every entity in a configuration must be reachable from the base 'config' entity. This is required to have any ability to identify the config as a whole within for any purpose.\n\n2. The current initial _tooling_ for building configurations (including the init scripts) will focus on building configurations from scratch. Tooling capable of \"editing\" an existing configuration is sufficiently different, with a different set of requirements and constraints, that it needs its own design process.\n\n3. Any future tooling for storing, viewing and editing configurations will need to explicitly determine whether it wants to work with the configuration before or after processing by the modules, since there is a distinct set of tradeoffs.\n\n"}
{"prompt":"Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.","completion":"Our organization has decided to use Google Cloud Platform as the cloud infrastructure for our application. The primary considerations for this decision are:\n\n   - Cost-effectiveness\n\n   - Scalability\n\n   - Reliability\n\n   - Flexibility"}
{"prompt":"We want to use a CSS framework to create our web applications:\n\n  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.\n\n  * We want rapid iteration on design, layout, UI\/UX, etc.\n\n  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  \n","completion":"Decided on Bulma."}
{"prompt":"Our project involves developing three major categories of software:\n\n  * Front-end GUIs\n  * Middleware services\n  * Back-end servers\n\nWhen we develop, our source code management (SCM) version control system (VCS) is git.\n\nWe need to choose how we use git to organize our code.\n\nThe top-level choice is to organize as a \"monorepo\" or \"polyrepo\" or \"hybrid\":\n\n  * Monorepo means we put all pieces into one big repo\n  * Polyrepo means we put each piece in its own repo\n  * Hybrid means some mix of monorepo and polyrepo\n\nFor more please see https:\/\/github.com\/joelparkerhenderson\/monorepo-vs-polyrepo\n","completion":"Monorepo when an organization\/team\/project is relatively small, and rapid iteration is higher priority than sustaining stability.\n\nPolyrepo when an organization\/team\/project is relatively large, and sustaining stability is higher priority than rapid iteration."}
{"prompt":"Reflection test for TOSCA YAML builder\n\nThe TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. \n\n```\n...\nmetadata:\n  assert: |\n    repositories.rp1.url = http:\/\/github.com\/kleinech\n    node_types.ntp1.requirements.0.rqr1.capability = cbt1\n...\n```\nEach assert line contains a keyname and a value.\n*[context and problem statement]*\n*[decision drivers | forces]* <!-- optional -->\n\n## Considered Alternatives\n\n* *reflection tests*\n* *manual test*","completion":"* Chosen Alternative: *reflection tests*\n* Only alternative, which meets simplifies the effort to make complete tests \n"}
{"prompt":"Per [ADR-003](adr-003-config-implementation.md), Arachne uses\nDatomic-shaped data for configuration. Although this is a flexible,\nextensible data structure which is a great fit for programmatic\nmanipulation, in its literal form it is quite verbose.\n\nIt is quite difficult to understand the structure of Datomic data by\nreading its native textual representation, and it is similarly hard to\nwrite, containing enough repeated elements that copying and pasting\nquickly becomes the default.\n\nOne of Arachne's core values is ease of use and a fluent experience\nfor developers. Since much of a developer's interaction with Arachne\nwill be writing to the config, it is of paramount importance that\nthere be some easy way to create configuration data.\n\nThe question is, what is the best way for developers of Arachne\napplications to interact with their application's configuration?\n\n#### Option: Raw Datomic Txdata\n\nThis would require end users to write Datomic transaction data by hand\nin order to configure their application.\n\nThis is the \"simplest\" option, and has the fewest moving\nparts. However, as mentioned above, it is very far from ideal for\nhuman interactions.\n\n#### Option: Custom EDN data formats\n\nIn this scenario, users would write EDN data in some some nested\nstructure of maps, sets, seqs and primitives. This is currently the\nmost common way to configure Clojure applications.\n\nEach module would then need to provide a mapping from the EDN config\nformat to the underlying Datomic-style config data.\n\nBecause Arachne's configuration is so much broader, and defines so\nmuch more of an application than a typical application config file, \nit is questionable if standard nested EDN data would be a good fit \nfor representing it.\n\n#### Option: Code-based configuration\n\nAnother option would be to go in the direction of some other\nframeworks, such as Ruby on Rails, and have the user-facing\nconfiguration be *code* rather than data.\n\nIt should be noted that the primary motivation for having a\ndata-oriented configuration language, that it makes it easier to\ninteract with programmatically, doesn't really apply in Arachne's\ncase. Since applications are always free to interact richly with\nArachne's full configuration database, the ability to programmatically\nmanipulate the precursor data is moot. As such, one major argument\nagainst a code-based configuration strategy does not apply.","completion":"Developers will have the option of writing configuration using either\nnative Datomic-style, data, or code-based *configuration\nscripts*. Configuration scripts are Clojure files which, when\nevaluated, update a configuration stored in an atom currently in\ncontext (using a dynamically bound var.)\n\nConfiguration scripts are Clojure source files in a distinct directory\nthat by convention is *outside* the application's classpath:\nconfiguration code is conceptually and physically separate from\napplication code. Conceptually, loading the configuration scripts\ncould take place in an entirely different process from the primary\napplication, serializing the resulting config before handing it to the\nruntime application.\n\nTo further emphasize the difference between configuration scripts and\nruntime code, and because they are not on the classpath, configuration\nscripts will not have namespaces and will instead include each other\nvia Clojure's `load` function.\n\nArachne will provide code supporting the ability of module authors to\nwrite \"configuration DSLs\" for users to invoke from their\nconfiguration scripts. These DSLs will emphasize making it easy to\ncreate appropriate entities in the configuration. In general, DSL\nforms will have an imperative style: they will convert their arguments\nto configuration data and immediately transact it to the context\nconfiguration.\n\nAs a trivial example, instead of writing the verbose configuration data:\n\n```clojure\n{:arachne\/id :my.app\/server\n :arachne.http.server\/port 8080\n :arachne.http.server\/debug true}\n ```\n\nYou could write the corresponding DSL:\n\n```clojure\n(server :id :my.app\/server, :port 8080, :debug true)\n```\n\nNote that this is an illustrative example and does not represent the\nactual DSL or config for the HTTP module.\n\nDSLs should make heavy use of Spec to make errors as comprehensible as possible.\n\n"}
{"prompt":"Use Builder Pattern for Model Classes\n\nModel classes should be instantiable simple without using large constructors.\n\n## Considered Alternatives\n\n* [Builders]\n* Setters, getters and default constructor \n* Large constructors\n* Factories","completion":"* Chosen Alternative: *Builders*\n* Flexible\n* Simple for complex objects\n* Extensions cause problems (solved with generic builders) \n\n### Generic Builders\n\nGeneric Builders are used to enable safe method chaining for Builders with extend other Builders.\nAnother discussion is made at [stackoverflow].\n\nThe method `self()` is necessary because all setter methods should return the Builder used for instantiation and not the builder that is extended. `self()` can not be replace by `this` because the expected type is `<T>` and casting to `<T>` results in warnings.\n\nBuilders which are not abstract and are extended by other builders are generic and implement the `self()` method by casting `this` to `<T>`. To reduce warnings this casting is only used in this case.\n\nExample:\n```java\n\/\/ part of ExtensibleElements.Builder\npublic abstract static class Builder<T extends Builder<T>> {\n    private List<TDocumentation> documentation;\n    \n    \/\/ setter returns generic <T> \n    public T setDocumentation(List<TDocumentation> documentation) {\n        this.documentation = documentation;\n        \/\/ return this; => IncompatibleType exception either cast with warnings or use self() method\n        return self();\n    }\n    \n    \/\/ overwritten method\n    public abstract T self();\n}\n\n\/\/ part of TEntityType.Builder\npublic abstract static class Builder<T extends Builder<T>> extends TExtensibleElements.Builder<T> {\n\t\n}\n\n\/\/ part of TNodeType.Builder\npublic static class Builder extends TEntityType.Builder<Builder> {\n    @Override\n    public Builder self() {\n        return this;\n    }\n}\n```\n\n\n\n[Builders]:(https:\/\/en.wikipedia.org\/wiki\/Builder_pattern)\n[stackoverflow]: https:\/\/stackoverflow.com\/a\/5818701\/8235252\n\n"}
{"prompt":"In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.\n\nThese \"static assets\" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:\n\n- SCSS\/LESS to CSS\n- CoffeeScript to JavaScript\n- ClojureScript to JavaScript\n- Full-size images to thumbnails\n- Compress files using gzip\n\nAdditionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.\n\nIn this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.\n\nArachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.\n\n### Development vs Production\n\nRegardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.\n\nProduction usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.\n\n### Deployment & Distribution\n\nFor development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.\n\n### Entirely Static Sites\n\nThere is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of \"static site generation\" include Jekyll, Middleman, Brunch, and many more.\n\nBy including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","completion":"Arachne will use Boot to provide an abstract asset pipeline. Boot has built-in support for immutable Filesets, temp directory management, and file watchers.\n\nAs with everything in Arachne, the pipeline will be specified as pure data in the configuration, specifying inputs, outputs, and transformations explicitly.\n\nModules that participate in the asset pipeline will develop against a well-defined API built around Boot Filesets.\n\n"}
{"prompt":"Use filesystem as backend\n\nWinery needs to store its contents.\nThese contents need to be shared.\n\n## Considered Alternatives\n\n* Filesystem\n* Database","completion":"* *Chosen Alternative: Filesystem*\n\n"}
{"prompt":"We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\nDecision Drivers\nLow complexity and overhead in development.Fit for our stack.Optimize CI\/CD with dependency graphs and\/or caching.Flexible.","completion":"Chosen option: \"Nx\", because:\nIt's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).It's relatively easy to learn with focused documentation.It has schematics to generate apps, libraries and components that includes all of our tools.It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended."}
{"prompt":"We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (\u201cADR\u201d), that we can integrate into our workflow.\n\nThis does not replace actual architecture documentation, but provides people who are contributing:\n\n    the means to understand architectural and design decisions that were made\n\n    a framework for proposing changes to the current architecture\n\nFor each decision, it is important to consider the following factors:\n\n    what we have decided to do\n\n    why we have made this decision\n\n    what we expect the impact of this decision to be\n\n    what we have learned in the process\n\nAs we\u2019re already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.","completion":"We will use ADRs to document, propose and discuss any important or significant architectural and design decisions.\n\n    The ADR format will follow the format described in Implications section.\n\n    We will follow the convention of storing those ADRs as rST or Markdown formatted documents stored under the docs\/adr directory, as exemplified in Nat Pryce\u2019s adr-tools. This does not imply that we will be using adr-tools itself, as we might diverge from the proposed structure.\n\n    We will keep rejected ADRs\n\n    We will strive, if possible, to create an ADR as early as possible in relation to the actual implementation."}
{"prompt":"We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.\n\n### Considerations\n\n1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.\n\n2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.\n\n3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.\n\n4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","completion":"We have decided to use the Java programming language for our project, considering its security features, concurrency support, ability to handle large codebases, and versatility. This decision aligns with our project requirements, and we believe that choosing Java will ensure its success. "}
{"prompt":"At some point, every Arachne application needs to start; to bootstrap\nitself from a static project or deployment artifact, initialize what\nneeds initializing, and begin servicing requests, connecting to\ndatabases, processing data, etc.\n\nThere are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.\n\n- Starting the JVM\n    - Assembling the project's dependencies\n    - Building a JVM classpath\n    - Starting a JVM\n- Arachne Specific\n    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))\n    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))\n- Application Specific\n    - Instantiate user and module-defined objects that needs to exist at runtime.\n    - Start and stop user and module-defined services\n\nAs discussed in [ADR-004](adr-004-module-loading.md), tasks in the \"starting the JVM\" category are not in-scope for Arachne; rather, they are offloaded to whatever build\/dependency tool the project is using (usually either [boot](http:\/\/boot-clj.com) or [leiningen](http:\/\/leiningen.org).)\n\nThis leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.\n\nAdditionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.","completion":"#### Components\n\nArachne uses the [Component](https:\/\/github.com\/stuartsierra\/component) library to manage system components. Instead of requiring users to define a component system map manually, however, Arachne itself builds one based upon the Arachne config via *Configuration Entities* that appear in the configuration.\n\nComponent entities may be added to the config directly by end users (via a initialization script as per [ADR-005](adr-005-user-facing-config.md)), or by modules in their `configure` function ([ADR-004](adr-004-module-loading.md).)\n\nComponent entities have attributes which indicates which other components they depend upon. Circular dependencies are not allowed; the component dependency structure must form a Directed Acyclic Graph (DAG.) The dependency attributes also specify the key that Component will use to `assoc` dependencies.\n\nComponent entities also have an attribute that specifies a *component constructor function* (via a fully qualified name.) Component constructor functions must take two arguments: the configuration, and the entity ID of the component that is to be constructed. When invoked, a component constructor must return a runtime component object, to be used by the Component library. This may be any object that implements `clojure.lang.Associative`, and may also optionally satisfy Component's `Lifecycle` protocol.\n\n#### Arachne Runtime\n\nThe top-level entity in an Arachne system is a reified *Arachne Runtime* object. This object contains both the Component system object, and the configuration value upon which the runtime is based. It satisfies the `Lifecycle` protocol itself; when it is started or stopped, all of the component objects it contains are started or stopped in the appropriate order.\n\nThe constructor function for a Runtime takes a configuration value and some number of \"roots\"; entity IDs or lookup refs of Component entities in the config. Only these root components and their transitive dependencies will be instantiated or added to the Component system. In other words, only component entities that are actually used will be instantiated; unused component entities defined in the config will be ignored.\n\nA `lookup` function will be provided to find the runtime object instance of a component, given its entity ID or lookup ref in the configuraiton.\n\n#### Startup Procedure\n\nArachne will rely upon an external build tool (such as boot or leiningen.) to handle downloading dependencies, assembling a classpath, and starting a JVM.\n\nOnce JVM with the correct classpath is running, the following steps are required to yield a running Arachne runtime:\n\n1. Determine a set of modules to use (the \"active modules\")\n2. Build a configuration schema by querying each active module using its `schema` function ([ADR-004](module-loading.md))\n3. Update the config with initial configuration data from user init scripts ([ADR-005](adr-005-user-facing-config.md))\n4. In module dependency order, give each module a chance to query and update the configuration using its `configure` function ([ADR-004](module-loading.md))\n5. Create a new Arachne runtime, given the configuration and a set of root components.\n6. Call the runtime's `start` method. \n\nThe Arachne codebase will provide entry points to automatically perform these steps for common development and production scenarios. Alternatively, they can always be be executed individually in a REPL, or composed in custom startup functions.\n \n"}
{"prompt":"The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.\n\nCurrently, the plutus-ledger-constraint library has the MustValidateIn constraint which\n\n    validates that a given POSIXTimeRange` contains the TxInfo\u2019s validity range\n\n    creates a transaction with the provided POSIXTimeRange\n\nThe implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.\n\nCurrently, provided a POSIXTimeRange, plutus-contract does the following:\n\n    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange\n\n    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))\n\nThe issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a \u201cClosure\u201d flag for each of the bounds.\n\nTherefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.\n\nLet\u2019s show some examples to showcase the issue.\n\n> let sc = SlotConfig 1000 0\n> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))\n> let r = posixTimeRangeToContainedSlotRange sc interval\n> r\nInterval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}\n> let txValidRange = toCardanoValidityRange r\n> txValidRange\nRight (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)\n\nWhen creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:\n\n(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))\n\nIn practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.\n\nWe can identify a similar behavior with the upper bound.\n\n> let sc = SlotConfig 1000 0\n> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))\n> let r = posixTimeRangeToContainedSlotRange sc interval\n> r\nInterval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}\n> let txValidRange = toCardanoValidityRange r\n> txValidRange\nRight (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))\n\nWhen creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:\n\n(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))\n\nAgain, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.\n\nAdditionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","completion":"We will create the following datatype:\n\n    -- | ValidityInterval is a half open interval. Closed (inclusive) on the bottom, open\n    -- (exclusive) on the top. A 'Nothing' on the bottom is negative infinity, and a 'Nothing'\n    -- on the top is positive infinity.\n    data ValidityInterval a = ValidityInterval\n      { invalidBefore :: !(Maybe a) -- ^ Inclusive lower bound or negative infinity\n      , invalidHereafter :: !(Maybe a) -- ^ Exclusive upper bound or positive infinity\n      }\n\n    We will add the following constraint and smart constructor:\n\n    data TxConstraint =\n      ...\n      MustValidateInTimeRange !(ValidityInterval POSIXTime)\n\n    mustValidateInTimeRange :: !(ValidityInterval POSIXTime) -> TxConstraints\n\n    We will remove the MustValidateIn constraint and deprecate the the mustValidateIn smart constructor which will be replaced by mustValidateInTimeRange.\n\n    We will create the smart constructor\n\n    mustValidateInSlotRange :: !(ValidityInterval Slot) -> TxConstraints\n\n    which will translate the provide validity slot range into a POSIXTimeRange using Ledger.TimeSlot.posixTimeRangeToContainedSlotRange."}
{"prompt":"As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.\n\nTherefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:\n\n- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.\n- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.\n\nThere are two \"kinds\" of config validation.\n\nThe first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.\n\nThe second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.","completion":"Arachne will perform both kinds of validation. To disambiguate them (since they are logically distinct), we will term the structural\/schema validation \"configuration validation\", while the validation of the runtime objects will be \"runtime validation.\"\n\nBoth styles of validation should be extensible by modules, so modules can specify additional validations, where necessary.\n\n#### Configuration Validation\n\nConfiguration validation is ensuring that an Arachne configuration object is consistent with itself and with its schema.\n\nBecause this is ultimately validating a set of Datomic style `eavt` tuples, the natural form for checking tuple data is Datalog queries and query rules, to search for and locate data that is \"incorrect.\" \n\nEach logical validation will have its own \"validator\", a function which takes a config, queries it, and either returns or throws an exception. To validate a config, it is passed through every validator as the final step of building a module.\n\nThe set of validators is open, and defined in the configuration itself. To add new validators, a module can transact entities for them during its configuration building phase.\n\n#### Runtime Validation\n\nRuntime validation occurs after a runtime is instantiated, but before it is started. Validation happens on the component level; each component may be subject to validation.\n\nUnlike Configuration validation, Runtime validation uses Spec. What specs should be applied to each component are defined in the configuration using a keyword-valued attribute. Specs may be defined on individual component entities, or to the *type* of a component entity. When a component is validated, it is validated using all the specs defined for it or any of its supertypes.\n\n"}
