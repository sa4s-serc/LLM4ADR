{"prompt":"The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.\n\nCurrently, the plutus-ledger-constraint library has the MustValidateIn constraint which\n\n    validates that a given POSIXTimeRange` contains the TxInfo\u2019s validity range\n\n    creates a transaction with the provided POSIXTimeRange\n\nThe implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.\n\nCurrently, provided a POSIXTimeRange, plutus-contract does the following:\n\n    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange\n\n    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))\n\nThe issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a \u201cClosure\u201d flag for each of the bounds.\n\nTherefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.\n\nLet\u2019s show some examples to showcase the issue.\n\n> let sc = SlotConfig 1000 0\n> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))\n> let r = posixTimeRangeToContainedSlotRange sc interval\n> r\nInterval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}\n> let txValidRange = toCardanoValidityRange r\n> txValidRange\nRight (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)\n\nWhen creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:\n\n(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))\n\nIn practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.\n\nWe can identify a similar behavior with the upper bound.\n\n> let sc = SlotConfig 1000 0\n> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))\n> let r = posixTimeRangeToContainedSlotRange sc interval\n> r\nInterval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}\n> let txValidRange = toCardanoValidityRange r\n> txValidRange\nRight (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))\n\nWhen creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:\n\n(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))\n\nAgain, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.\n\nAdditionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","completion":"We will create the following datatype:\n\n    -- | ValidityInterval is a half open interval. Closed (inclusive) on the bottom, open\n    -- (exclusive) on the top. A 'Nothing' on the bottom is negative infinity, and a 'Nothing'\n    -- on the top is positive infinity.\n    data ValidityInterval a = ValidityInterval\n      { invalidBefore :: !(Maybe a) -- ^ Inclusive lower bound or negative infinity\n      , invalidHereafter :: !(Maybe a) -- ^ Exclusive upper bound or positive infinity\n      }\n\n    We will add the following constraint and smart constructor:\n\n    data TxConstraint =\n      ...\n      MustValidateInTimeRange !(ValidityInterval POSIXTime)\n\n    mustValidateInTimeRange :: !(ValidityInterval POSIXTime) -> TxConstraints\n\n    We will remove the MustValidateIn constraint and deprecate the the mustValidateIn smart constructor which will be replaced by mustValidateInTimeRange.\n\n    We will create the smart constructor\n\n    mustValidateInSlotRange :: !(ValidityInterval Slot) -> TxConstraints\n\n    which will translate the provide validity slot range into a POSIXTimeRange using Ledger.TimeSlot.posixTimeRangeToContainedSlotRange."}
{"prompt":"XML editor does not enforce validation\n\n**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?\n\n## Considered Alternatives\n\n* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template\n* Winery generate random data to gain schema-conforming XML\n* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.\n* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","completion":"* *Chosen Alternative: D*\n* This is in line with other editors: They allow to save, but warn if the file has compile errors, validation errors, ...\n\n"}
{"prompt":"IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.\n\nExamples of this are:\n\n    input-output-hk\/cardano-world\n\n    input-output-hk\/ci-world\n\n    input-output-hk\/atala-world\n\nThis initiative appears to be championed by the SRE team who are the creators of divnix\/std. Indeed std is at the heart of the standardization dream.","completion":"Standardization of the repositories has been deemed a worthwhile endeavour, though of very low priority.\n\n    Phase 1 of the standardization process will be carried out in parallel with Move Marconi to a separate repository. A separate repository will be created for Marconi, and from the very beginning it will use std. This way the benefits, limitations and integration costs of std can be experienced and measured, and an informed, definitive decision on standardizing plutus-core and plutus-apps themselves can be made."}
{"prompt":"Marconi is a Haskell executable and library that lives in plutus-chain-index.\n\nIt is desirable to move it into a separate repository for the following reasons:\n\n    Better visibility and easier to discover\n\n    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps\n\n    It is a farily independent component, therefore it warrants its own repository\n\nHowever, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","completion":"We will put Marconi in a separate Github repository\n\n    Until we resolve the issues with creating a separate Github repository (see Context), we will keep Marconi as a separate project in plutus-apps"}
{"prompt":"Arachne has several very explicit goals that make the practice and\ndiscipline of architecture very important:\n\n- We want to think deeply about all our architectural decisions,\n  exploring all alternatives and making a careful, considered,\n  well-researched choice.\n- We want to be as transparent as possible in our decision-making\n  process.\n- We don't want decisions to be made unilaterally in a\n  vacuum. Specifically, we want to give our steering group the\n  opportunity to review every major decision.\n- Despite being a geographically and temporally distributed team, we\n  want our contributors to have a strong shared understanding of the\n  technical rationale behind decisions.\n- We want to be able to revisit prior decisions to determine fairly if\n  they still make sense, and if the motivating circumstances or\n  conditions have changed.","completion":"We will document every architecture-level decision for Arachne and its\ncore modules with an\n[Architecture Decision Record](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). These\nare a well structured, relatively lightweight way to capture\narchitectural proposals. They can serve as an artifact for discussion,\nand remain as an enduring record of the context and motivation of past\ndecisions.\n\nThe workflow will be:\n\n1. A developer creates an ADR document outlining an approach for a\n   particular question or problem. The ADR has an initial status of \"proposed.\"\n2. The developers and steering group discuss the ADR. During this\n   period, the ADR should be updated to reflect additional context,\n   concerns raised, and proposed changes.\n3. Once consensus is reached, ADR can be transitioned to either an\n   \"accepted\" or \"rejected\" state.\n4. Only after an ADR is accepted should implementing code be committed\n   to the master branch of the relevant project\/module.\n5. If a decision is revisited and a different conclusion is reached, a\n   new ADR should be created documenting the context and rationale for\n   the change. The new ADR should reference the old one, and once the\n   new one is accepted, the old one should (in its \"status\" section)\n   be updated to point to the new one. The old ADR should not be\n   removed or otherwise modified except for the annotation pointing to\n   the new ADR.\n\n"}
{"prompt":"Manual serialization of SnakeYAML\n\nThe TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).\n\n## Considered Alternatives\n\n* Manual serialization\n* [SnakeYAML](https:\/\/bitbucket.org\/asomov\/snakeyaml)\n* [jackson-dataformat-yaml](https:\/\/github.com\/FasterXML\/jackson-dataformat-yaml)","completion":"* Chosen Alternative: *Manual serialization*\n* SnakeYAML does not support annotations for serialization\n* jackson-dataformat-yaml seems not to support annotations, such as [jackson-annotations](https:\/\/github.com\/FasterXML\/jackson-annotations)\n\n"}
{"prompt":"We want to use a CSS framework to create our web applications:\n\n  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.\n\n  * We want rapid iteration on design, layout, UI\/UX, etc.\n\n  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  \n","completion":"Decided on Bulma."}
{"prompt":"We want our applications to be configurable beyond artifacts\/binaries\/source, such that one build can behave differently depending on its deployment environment.\n\n  * To accomplish this, we want to use environment variable configuration.\n\n  * We want to manage the configuration by using files that we can version control.\n\n  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.\n","completion":"Decided on .env files with related default file and schema file."}
{"prompt":"In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.\n\nThese \"static assets\" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:\n\n- SCSS\/LESS to CSS\n- CoffeeScript to JavaScript\n- ClojureScript to JavaScript\n- Full-size images to thumbnails\n- Compress files using gzip\n\nAdditionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.\n\nIn this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.\n\nArachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.\n\n### Development vs Production\n\nRegardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.\n\nProduction usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.\n\n### Deployment & Distribution\n\nFor development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.\n\n### Entirely Static Sites\n\nThere is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of \"static site generation\" include Jekyll, Middleman, Brunch, and many more.\n\nBy including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","completion":"Arachne will use Boot to provide an abstract asset pipeline. Boot has built-in support for immutable Filesets, temp directory management, and file watchers.\n\nAs with everything in Arachne, the pipeline will be specified as pure data in the configuration, specifying inputs, outputs, and transformations explicitly.\n\nModules that participate in the asset pipeline will develop against a well-defined API built around Boot Filesets.\n\n"}
{"prompt":"We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.\n\n### Considerations\n\n1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.\n\n2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.\n\n3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.\n\n4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","completion":"We have decided to use the Java programming language for our project, considering its security features, concurrency support, ability to handle large codebases, and versatility. This decision aligns with our project requirements, and we believe that choosing Java will ensure its success. "}
{"prompt":"How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\nDecision Drivers\nWe need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","completion":"Chosen option: \"OneFlow\" because it provides a single eternal branch with well structured releases.\nWe'll implement OneFlow with these details:\nRelease branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.Release and quality managers from each team are responsible for reviewing and approving releases.Releases apply to all apps in the monorepo.Releases are versioned like this: {cycle}.{sprint}.{hotfix}. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.Feature branches are merged using \"Squash and merge\", so they can be easily reverted.There are two ways to build larger features.If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\nIf the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\nFuture strategy\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\nHosting environments\nWe'll set up continuous delivery to different hosting environments:\nEnvironmentGit sourceDatabases\/servicesFeaturessandboxfeature branchTestAlldevmainTestAllstagingmainProdAllpre-prodrelease\/hotfix branchProdFinishedprodlatest release tagProdFinished\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage."}
{"prompt":"We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.","completion":"We have decided to use Python as our primary programming language for our project."}
{"prompt":"We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\nDecision Drivers\nLow complexity and overhead in development.Fit for our stack.Optimize CI\/CD with dependency graphs and\/or caching.Flexible.","completion":"Chosen option: \"Nx\", because:\nIt's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).It's relatively easy to learn with focused documentation.It has schematics to generate apps, libraries and components that includes all of our tools.It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended."}
{"prompt":"The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.","completion":"Adopt Microsoft Azure as the cloud infrastructure for our organization."}
{"prompt":"Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.\n\nAdditionally, prior to the advent of `clojure.spec`, Clojure errors were often \"deep\": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.\n\nThere are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes \"lossy\", dropping information that could be desirable to a developer.\n\nSpec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.\n\nIdeally, Arachne's error handling should exhibit the following qualities:\n\n- Never hide possibly relevant information.\n- Allow module developers to be as helpful as possible to people using their tools.\n- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)\n- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.\n- Not violate expectations of experienced Clojure programmers.\n- Be robust enough not to cause additional problems.\n- Not break existing logging tools for production use.","completion":"We will separate the problems of creating rich exceptions, and catching them and displaying them to the user.\n\n### Creating Errors\n\nWhenever a well-behaved Arachne module needs to report an error, it should throw an info-bearing exception. This exception should be formed such that it is handled gracefully by any JVM tooling; the message should be terse but communicative, containing key information with no newlines.\n\nHowever, in the `ex-data`, the exception will also contain much more detailed information, that can be used (in the correct context) to provide much more detailed or verbose errors. Specifically, it may contain the following keys:\n\n- `:arachne.error\/message` - The short-form error message (the same as the Exception message.)\n- `:arachne.error\/explanation` - a long-form error message, complete with newlines and formatting.\n- `:arachne.error\/suggestions` - Zero or more suggestions on how the error might be fixed.\n- `:arachne.error\/type` - a namespaced keyword that uniquely identifies the type of error.\n- `:arachne.error\/spec` - The spec that failed (if applicable)\n- `:arachne.error\/failed-data` - The data that failed to match the spec (if applicable)\n- `:arachne.error\/explain-data` - An explain-data for the spec that failed (if applicable).\n- `:arachne.error\/env` - A map of the locals in the env at the time the error was thrown.\n\nExceptions may, of course, contain additional data; these are the common keys that tools can use to more effectively render errors.\n\nThere will be a suite of tools, provided with Arachne's core, for conveniently generating errors that match this pattern.\n\n### Displaying Errors\n\nWe will use a pluggable \"error handling system\", where users can explicitly install an exception handler other than the default.\n\nIf the user does not install any exception handlers, errors will be handled the same way as they are by default (usually, dumped with the message and stack trace to  `System\/err`.) This will not change.\n\nHowever, Arachne will also provide a function that a user can invoke in their main process, prior to doing anything else. Invoking this function will install a set of default exception handlers that will handle errors in a richer, more Arachne-specific way. This includes printing out the long-form error, or even (eventually) popping open a graphical data browser\/debugger (if applicable.)\n\n"}
{"prompt":"Per [ADR-003](adr-003-config-implementation.md), Arachne uses\nDatomic-shaped data for configuration. Although this is a flexible,\nextensible data structure which is a great fit for programmatic\nmanipulation, in its literal form it is quite verbose.\n\nIt is quite difficult to understand the structure of Datomic data by\nreading its native textual representation, and it is similarly hard to\nwrite, containing enough repeated elements that copying and pasting\nquickly becomes the default.\n\nOne of Arachne's core values is ease of use and a fluent experience\nfor developers. Since much of a developer's interaction with Arachne\nwill be writing to the config, it is of paramount importance that\nthere be some easy way to create configuration data.\n\nThe question is, what is the best way for developers of Arachne\napplications to interact with their application's configuration?\n\n#### Option: Raw Datomic Txdata\n\nThis would require end users to write Datomic transaction data by hand\nin order to configure their application.\n\nThis is the \"simplest\" option, and has the fewest moving\nparts. However, as mentioned above, it is very far from ideal for\nhuman interactions.\n\n#### Option: Custom EDN data formats\n\nIn this scenario, users would write EDN data in some some nested\nstructure of maps, sets, seqs and primitives. This is currently the\nmost common way to configure Clojure applications.\n\nEach module would then need to provide a mapping from the EDN config\nformat to the underlying Datomic-style config data.\n\nBecause Arachne's configuration is so much broader, and defines so\nmuch more of an application than a typical application config file, \nit is questionable if standard nested EDN data would be a good fit \nfor representing it.\n\n#### Option: Code-based configuration\n\nAnother option would be to go in the direction of some other\nframeworks, such as Ruby on Rails, and have the user-facing\nconfiguration be *code* rather than data.\n\nIt should be noted that the primary motivation for having a\ndata-oriented configuration language, that it makes it easier to\ninteract with programmatically, doesn't really apply in Arachne's\ncase. Since applications are always free to interact richly with\nArachne's full configuration database, the ability to programmatically\nmanipulate the precursor data is moot. As such, one major argument\nagainst a code-based configuration strategy does not apply.","completion":"Developers will have the option of writing configuration using either\nnative Datomic-style, data, or code-based *configuration\nscripts*. Configuration scripts are Clojure files which, when\nevaluated, update a configuration stored in an atom currently in\ncontext (using a dynamically bound var.)\n\nConfiguration scripts are Clojure source files in a distinct directory\nthat by convention is *outside* the application's classpath:\nconfiguration code is conceptually and physically separate from\napplication code. Conceptually, loading the configuration scripts\ncould take place in an entirely different process from the primary\napplication, serializing the resulting config before handing it to the\nruntime application.\n\nTo further emphasize the difference between configuration scripts and\nruntime code, and because they are not on the classpath, configuration\nscripts will not have namespaces and will instead include each other\nvia Clojure's `load` function.\n\nArachne will provide code supporting the ability of module authors to\nwrite \"configuration DSLs\" for users to invoke from their\nconfiguration scripts. These DSLs will emphasize making it easy to\ncreate appropriate entities in the configuration. In general, DSL\nforms will have an imperative style: they will convert their arguments\nto configuration data and immediately transact it to the context\nconfiguration.\n\nAs a trivial example, instead of writing the verbose configuration data:\n\n```clojure\n{:arachne\/id :my.app\/server\n :arachne.http.server\/port 8080\n :arachne.http.server\/debug true}\n ```\n\nYou could write the corresponding DSL:\n\n```clojure\n(server :id :my.app\/server, :port 8080, :debug true)\n```\n\nNote that this is an illustrative example and does not represent the\nactual DSL or config for the HTTP module.\n\nDSLs should make heavy use of Spec to make errors as comprehensible as possible.\n\n"}
{"prompt":"Use filesystem as backend\n\nWinery needs to store its contents.\nThese contents need to be shared.\n\n## Considered Alternatives\n\n* Filesystem\n* Database","completion":"* *Chosen Alternative: Filesystem*\n\n"}
{"prompt":"Use Builder Pattern for Model Classes\n\nModel classes should be instantiable simple without using large constructors.\n\n## Considered Alternatives\n\n* [Builders]\n* Setters, getters and default constructor \n* Large constructors\n* Factories","completion":"* Chosen Alternative: *Builders*\n* Flexible\n* Simple for complex objects\n* Extensions cause problems (solved with generic builders) \n\n### Generic Builders\n\nGeneric Builders are used to enable safe method chaining for Builders with extend other Builders.\nAnother discussion is made at [stackoverflow].\n\nThe method `self()` is necessary because all setter methods should return the Builder used for instantiation and not the builder that is extended. `self()` can not be replace by `this` because the expected type is `<T>` and casting to `<T>` results in warnings.\n\nBuilders which are not abstract and are extended by other builders are generic and implement the `self()` method by casting `this` to `<T>`. To reduce warnings this casting is only used in this case.\n\nExample:\n```java\n\/\/ part of ExtensibleElements.Builder\npublic abstract static class Builder<T extends Builder<T>> {\n    private List<TDocumentation> documentation;\n    \n    \/\/ setter returns generic <T> \n    public T setDocumentation(List<TDocumentation> documentation) {\n        this.documentation = documentation;\n        \/\/ return this; => IncompatibleType exception either cast with warnings or use self() method\n        return self();\n    }\n    \n    \/\/ overwritten method\n    public abstract T self();\n}\n\n\/\/ part of TEntityType.Builder\npublic abstract static class Builder<T extends Builder<T>> extends TExtensibleElements.Builder<T> {\n\t\n}\n\n\/\/ part of TNodeType.Builder\npublic static class Builder extends TEntityType.Builder<Builder> {\n    @Override\n    public Builder self() {\n        return this;\n    }\n}\n```\n\n\n\n[Builders]:(https:\/\/en.wikipedia.org\/wiki\/Builder_pattern)\n[stackoverflow]: https:\/\/stackoverflow.com\/a\/5818701\/8235252\n\n"}
{"prompt":"The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.","completion":"To implement continuous integration as a solution to the challenges experienced in the development process."}
