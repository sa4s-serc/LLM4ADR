prompt,completion
"Arachne has several very explicit goals that make the practice and
discipline of architecture very important:

- We want to think deeply about all our architectural decisions,
  exploring all alternatives and making a careful, considered,
  well-researched choice.
- We want to be as transparent as possible in our decision-making
  process.
- We don't want decisions to be made unilaterally in a
  vacuum. Specifically, we want to give our steering group the
  opportunity to review every major decision.
- Despite being a geographically and temporally distributed team, we
  want our contributors to have a strong shared understanding of the
  technical rationale behind decisions.
- We want to be able to revisit prior decisions to determine fairly if
  they still make sense, and if the motivating circumstances or
  conditions have changed.","We will document every architecture-level decision for Arachne and its
core modules with an
[Architecture Decision Record](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). These
are a well structured, relatively lightweight way to capture
architectural proposals. They can serve as an artifact for discussion,
and remain as an enduring record of the context and motivation of past
decisions.

The workflow will be:

1. A developer creates an ADR document outlining an approach for a
   particular question or problem. The ADR has an initial status of ""proposed.""
2. The developers and steering group discuss the ADR. During this
   period, the ADR should be updated to reflect additional context,
   concerns raised, and proposed changes.
3. Once consensus is reached, ADR can be transitioned to either an
   ""accepted"" or ""rejected"" state.
4. Only after an ADR is accepted should implementing code be committed
   to the master branch of the relevant project/module.
5. If a decision is revisited and a different conclusion is reached, a
   new ADR should be created documenting the context and rationale for
   the change. The new ADR should reference the old one, and once the
   new one is accepted, the old one should (in its ""status"" section)
   be updated to point to the new one. The old ADR should not be
   removed or otherwise modified except for the annotation pointing to
   the new ADR.

"
"Arachne has a number of goals.

1. It needs to be *modular*. Different software packages, written by
   different developers, should be usable and swappable in the same
   application with a minimum of effort.

2. Arachne applications need to be *transparent* and
   *introspectable*. It should always be as clear as possible what is
   going on at any given moment, and why the application is behaving
   in the way it does.

3. As a general-purpose web framework, it needs to provide a strong
   set of default settings which are also highly overridable, and
   *configurable* to suit the unique needs of users.


Also, it is a good development practice (particularly in Clojure) to
code to a specific information model (that is, data) rather than to
particular functions or APIs. Along with other benefits, this helps
separate (avoids ""complecting"") the intended operation and its
implementation.

Documenting the full rationale for this ""data first"" philosophy is
beyond the scope of this document, but some resources that explain it (among other things) are:

- [Simple Made Easy](http://www.infoq.com/presentations/Simple-Made-Easy) - Rich Hickey
- [Narcissistic Design](https://vimeo.com/77199361) - Stuart Halloway
- [Data Beats Functions](https://malcolmsparks.com/posts/data-beats-functions.html) - Malcolm Sparks
- [Always Be Composing](https://www.youtube.com/watch?v=3oQTSP4FngY) - Zach Tellman
- [Data > Functions > Macros](http://www.lispcast.com/data-functions-macros-why) - Eric Normand

Finally, one weakness of many existing Clojure libraries, especially
web development libraries, is the way in which they overload the
Clojure runtime (particularly vars and reified namespaces) to store
information about the webapp. Because both the Clojure runtime and
many web application entities (e.g servers) are stateful, this causes
a variety of issues, particularly with reloading namespaces. Therefore,
as much as possible, we would like to avoid entangling information
about an Arachne application with the Clojure runtime itself.","Arachne will take the ""everything is data"" philosophy to its logical
extreme, and encode as much information about the application as
possible in a single, highly general data structure. This will include
not just data that is normally thought of as ""config"" data, but the
structure and definition of the application itself. Everything that
does not have to be arbitrary executable code will be
reflected in the application config value.

Some concrete examples include (but are not limited to):

- Dependency injection components
- Runtime entities (servers, caches, connections, pools, etc)
- HTTP routes and middleware
- Persistence schemas and migrations
- Locations of static and dynamic assets

This configuration value will have a *schema* that defines what types
of entities can exist in the configuration, and what their expected
properties are.

Each distinct module will have the ability to contribute to the schema
and define entity types specific to its own domain. Modules may
interact by referencing entity types and properties defined in other
modules.

Although it has much in common with a fully general in-memory
database, the configuration value will be a single immutable value,
not a stateful data store. This will avoid many of the complexities
of state and change, and will eliminate the temptation to use the
configuration itself as dynamic storage for runtime data.

"
"[ADR-002](adr-002-configuration.md) indicates that we will store the
entire application config in a single rich data structure with a schema.

### Config as Database

This implies that it should be possible to easily search, query and
update the configuration value. It also implies that the configuration
value is general enough to store arbitrary data; we don't know what
kinds of things users or module authors will need to include.

If what we need is a system that allows you to define, query, and
update arbitrary data with a schema, then we are looking for a
database.

Required data store characteristics:

1. It must be available under a permissive open source
   license. Anything else will impose unwanted restrictions on who can
   use Arachne.
2. It can operate embedded in a JVM process. We do not want to force
   users to install anything else or run multiple processes just to
   get Arachne to work.
3. The database must be serializable. It must be possible to write the
   entire configuration to disk, and then reconstitute it in the same
   exact state in a separate process.
4. Because modules build up the schema progressively, the schema must
   be inherently extensible. It should be possible for modules to
   progressively add both new entity types and new attributes to
   existing entity types.
5. It should be usable from Clojure without a painful impedance mismatch.

### Configuration as Ontology

As an extension of the rationale discussed in
[ADR-002](adr-002-configuration.md), it is useful to enumerate the
possible use cases of the configuration and configuration schema
together.

- The configuration is read by the application during bootstrap and
  controls the behavior of the application.
- The configuration schema defines what types of values the
  application can or will read to modify its structure and behavior at
  boot time and run time.
- The configuration is how an application author communicates their
  intent about how their application should fit together and run, at a
  higher, more conceptual level than code.
- The configuration schema is how module authors communicate to
  application authors what settings, entities and structures
  are available for them to use in their applications.
- The configuration schema is how module authors communicate to other
  potential module authors what their extension points are; module
  extenders can safely read or write any entities/attributes declared
  by the modules upon which they depend.
- The configuration schema can be used to validate a particular
  configuration, and explain where and how it deviates from what is
  actually supported.
- The configuration can be exposed (via user interfaces of various
  types) to end users for analytics and debugging, explaining the
  structure of their application and why things are the way they are.
- A serialization of the configuration, together with a particular
  codebase (identified by a git SHA) form a precise, complete, 100%
  reproducible definition of the behavior of an application.

To the extent that the configuration schema expresses and communicates
the ""categories of being"" or ""possibility space"" of an application, it
is a formal [Ontology](https://en.wikipedia.org/wiki/Ontology). This is
a desirable characteristic, and to the degree that it is practical to
do so, it will be useful to learn from or re-use existing work around
formal ontological systems.


### Implementation Options

There are instances of four broad categories of data stores that match
the first three of the data store characteristics defined above.

- Relational (Derby, HSQLDB, etc)
- Key/value (BerkelyDB, hashtables, etc)
- RDF/RDFs/OWL stores (Jena)
- Datomic-style (Datascript)

We can eliminate relational solutions fairly quickly; SQL schemas are
not generally extensible or flexible, failing condition #4. In
addition, they do not fare well on #5 -- using SQL for queries and updates
is not particularly fluent in Clojure.

Similarly, we can eliminate key/value style data stores. In general,
these do not have schemas at all (or at least, not the type of rich
schema that provides a meaningful data contract or ontology, which is the point
for Arachne.)

This leaves solutions based on the RDF stack, and Datomic-style data
stores. Both are viable options which would provide unique benefits
for Arachne, and both have different drawbacks.

Explaining the core technical characteristics of RDF/OWL and Datomic
is beyond the scope of this document; please see the
[Jena](https://jena.apache.org/documentation/index.html) and
 [Datomic](http://docs.datomic.com) documentation for more
details. More information on RDF, OWL and the Semantic web in general:

- [Wikipedia article on RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)
- [Wikipedia article on OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)
- [OWL Semantics](http://www.w3.org/TR/owl-semantics/) standards document.

### RDF

The clear choice for a JVM-based, permissively licensed,
standards-compliant RDF API is Apache Jena.

#### Benefits for Arachne

- OWL is a good fit insofar as Arachne's goal is to define an
  ontology of applications. The point of the configuration schema is
  first and foremost to serve as unambiguous communication regarding
  the types of entities that can exist in an application, and what the
  possible relationships between them are. By definition, this is
  defining an ontology, and is the exact use case which OWL is
  designed to address.
- Information model is a good fit for Clojure: tuples and declarative logic.
- Open and extensible by design.
- Well researched by very smart people, likely to avoid common
  mistakes that would result from building an ontology-like system
  ourselves.
- Existing technology, well known beyond the Clojure
  ecosystem. Existing tools could work with Arachne project
  configurations out of the box.
- The open-world assumption is a good fit for Arachne's per-module
  schema modeling, since modules cannot know what other modules might
  be present in the application.
- We're likely to want to introduce RDFs/OWL to the application
  anyway, at some point, as an abstract entity meta-schema (note: this
  has not been firmly decided yet.)

#### Tradeoffs for Arachne (with mitigations)

- OWL is complex. Learning to use it effectively is a skill in its own
  right and it might be asking a lot to require of module authors.
- OWLs representation of some common concepts can be verbose and/or
  convoluted in ways that would make schema more difficult to
  read/write. (e.g, Restriction classes)
- OWL is not a schema. Although the open world assumption is valid and
  good when writing ontologies, it means that OWL inferencing is
  incapable of performing many of the kind of validations we would
  want to apply once we do have a complete configuration and want to
  check it for correctness. For example, open-world reasoning can
  never validate a `owl:minCardinality` rule.
    - Mitigation: Although OWL inferencing cannot provide closed-world
    validation of a given RDF dataset, such tools do exist. Some
    mechanisms for validating a particular closed set of RDF triples
    include:
       1. Writing SPARQL queries that catch various types of validation errors.
       2. Deriving validation errors using Jena's rules engine.
       3. Using an existing RDF validator such as
      [Eyeball](https://jena.apache.org/documentation/tools/eyeball-getting-started.html)
      (although, unfortunately, Eyeball does not seem to be well
      maintained.)
    - For Clojure, it would be possible to validate a given OWL class
      by generating a specification using `clojure.spec` that could be
      applied to concrete instances of the class in their map form.
- Jena's API is aggressively object oriented and at odds with Clojure
  idioms.
    - Mitigation: Write a data-oriented wrapper (note: I have a
    working proof of concept already.)
- SPARQL is a string-based query language, as opposed to a composable data API.
    - Mitigation: It is possible to hook into Jena's ARQ query engine
      at the object layer, and expose a data-oriented API from there,
      with SPARQL semantics but an API similar to Datomic datalog.
- OWL inferencing is known to have performance issues with complex
  inferences. While Arachne configurations are tiny (as knowledge bases
  go), and we are unlikely to use the more esoteric derivations, it is
  unknown whether this will cause problems with the kinds of
  ontologies we do need.
    - Mitigation: We could restrict ourselves to the OWL DL or even
      OWL Lite sub-languages, which have more tractable inferencing
      rules.
- Jena's APIs are such that it is impossible to write an immutable
  version of a RDF model (at least without breaking most of Jena's
  API.) It's trivial to write a data-oriented wrapper, but intractable
  to write a persistent immutable one.

### Datomic

Note that Datomic itself does not satisfy the first requirement; it is
closed-source, proprietary software. There *is* an open source
project, Datascript, which emulates Datomic's APIs (without any of the
storage elements). Either one would work for Arachne, since Arachne
only needs the subset of features they both support. In, fact, if
Arachne goes the Datomic-inspired route, we would probably want to
support *both*: Datomic, for those who have an existing investment
there, and Datascript for those who desire open source all the way.

#### Benefits for Arachne

- Well known to most Clojurists
- Highly idiomatic to use from Clojure
- There is no question that it would be performant and technically
  suitable for Arachne-sized data.
- Datomic's schema is a real validating schema; data transacted to
  Datomic must always be valid.
- Datomic Schema is open and extensible.

#### Tradeoffs for Arachne (with mitigations)

- The expressivity of Datomic's schema is anemic compared to RDFs/OWL;
  for example, it has no built-in notion of types. It is focused
  towards data storage and integrity rather than defining a public
  ontology, which would be useful for Arachne.
    - Mitigation: If we did want something more ontologically focused,
      it is possible to build an ontology system on top of Datomic
      using meta-attributes and Datalog rules. Examples of such
      systems already exist.
- If we did build our own ontology system on top of Datomic (or use an
  existing one) we would still be responsible for ""getting it right"",
  ensuring that it meets any potential use case for Arachne while
  maintaining internal and logical consistency.
    - Mitigation: we could still use the work that has been done in
      the OWL world and re-implement a subset of axioms and
      derivations on top of Datomic.
- Any ontological system built on top of Datomic would be novel to
  module authors, and therefore would require careful, extensive
  documentation regarding its capabilities and usage.
- To satisfy users of Datomic as well as those who have a requirement
  for open source, it will be necessary to abstract across both
  Datomic and Datascript.
    - Mitigation: This work is already done (provided users stay
      within the subset of features that is supported by both
      products.)","The steering group decided the RDF/OWL approach is too high-risk to
wrap in Clojure and implement at this time, while the rewards are
mostly intangible ""openness"" and ""interoperability"" rather than
something that will help move Arachne forward in the short term.

Therefore, we will use a Datomic style schema for Arachne's configuration.

Users may use either Datomic Pro, Datomic Free or Datascript at
runtime in their applications. We will provide a ""multiplexer""
configuration implementation that utilizes both, and asserts that the
results are equal: this can be used by module authors to ensure they
stay within the subset of features supported by both platforms.

Before Arachne leaves ""alpha"" status (that is, before it is declared
ready for experimental production use or for the release of
third-party modules), we will revisit the question of whether OWL
would be more appropriate, and whether we have encountered issues that
OWL would have made easier. If so, and if time allows, we reserve the
option to either refactor the configuration layer to use Jena as a
primary store (porting existing modules), or provide an OWL
view/rendering of an ontology stored in Datomic.

"
"Arachne needs to be as modular as possible. Not only do we want the
community to be able to contribute new abilities and features that
integrate well with the core and with eachother, we want some of the
basic functionality of Arachne to be swappable for alternatives as
well.

[ADR-002](adr-002-configuration.md) specifies that one role of modules
is to contribute schema to the application config. Other roles of
modules would include providing code (as any library does), and
querying and updating the config during the startup
process. Additionally, since modules can depend upon each other, they
must specify which modules they depend upon.

Ideally there will be as little overhead as possible for creating and
consuming modules.

Some of the general problems associated with plugin/module systems include:

- Finding and downloading the implementation of the module.
- Discovering and activating the correct set of installed modules.
- Managing module versions and dependencies.

There are some existing systems for modularity in the Java
ecosystem. The most notable is OSGi, which provides not only a module
system addressing the concerns above, but also service runtime with
classpath isolation, dynamic loading and unloading and lazy
activation.

OSGi (and other systems of comparable scope) are overkill for
Arachne. Although they come with benefits, they are very heavyweight
and carry a high complexity burden, not just for Arachne development
but also for end users. Specifically, Arachne applications will be
drastically simpler if (at runtime) they exist as a straightforward
codebase in a single classloader space. Features like lazy loading and
dynamic start-stop are likewise out of scope; the goal is for an
Arachne runtime itself to be lightweight enough that starting and
stopping when modules change is not an issue.","Arachne will not be responsible for packaging, distribution or
downloading of modules. These jobs will be delegated to an external
dependency management & packaging tool. Initially, that tool will be
Maven/Leiningen/Boot, or some other tool that works with Maven
artifact repositories, since that is currently the standard for JVM
projects.

Modules that have a dependency on another module must specify a
dependency using Maven (or other dependency management tool.)

Arachne will provide no versioning system beyond what the packaging
tool provides.

Each module JAR will contain a special `arachne-modules.edn` file at
the root of its classpath. This data file (when read) contains a
sequence of *module definition maps*.

Each module definition map contains the following information:

- The formal name of the module (as a namespaced symbol.)
- A list of dependencies of the module (as a set of namespaced
  symbols.) Module dependencies must form a directed acyclic graph;
  circular dependencies are not allowed.
- A namespace qualified symbol that resolves to the module's *schema
  function.* A schema function is a function with no arguments that
  returns transactable data containing the schema of the module.
- A namespace qualified symbol that resolves to the module's
  *configure function*. A configure function is a function that takes a
  configuration value and returns an updated configuration.

When an application is defined, the user must specify a set of module
names to use (exact mechanism TBD.) Only the specified modules (and
their dependencies) will be considered by Arachne. In other words,
merely including a module as a dependency in the package manager is
not sufficient to activate it and cause it to be used in an
application.

"
"Per [ADR-003](adr-003-config-implementation.md), Arachne uses
Datomic-shaped data for configuration. Although this is a flexible,
extensible data structure which is a great fit for programmatic
manipulation, in its literal form it is quite verbose.

It is quite difficult to understand the structure of Datomic data by
reading its native textual representation, and it is similarly hard to
write, containing enough repeated elements that copying and pasting
quickly becomes the default.

One of Arachne's core values is ease of use and a fluent experience
for developers. Since much of a developer's interaction with Arachne
will be writing to the config, it is of paramount importance that
there be some easy way to create configuration data.

The question is, what is the best way for developers of Arachne
applications to interact with their application's configuration?

#### Option: Raw Datomic Txdata

This would require end users to write Datomic transaction data by hand
in order to configure their application.

This is the ""simplest"" option, and has the fewest moving
parts. However, as mentioned above, it is very far from ideal for
human interactions.

#### Option: Custom EDN data formats

In this scenario, users would write EDN data in some some nested
structure of maps, sets, seqs and primitives. This is currently the
most common way to configure Clojure applications.

Each module would then need to provide a mapping from the EDN config
format to the underlying Datomic-style config data.

Because Arachne's configuration is so much broader, and defines so
much more of an application than a typical application config file, 
it is questionable if standard nested EDN data would be a good fit 
for representing it.

#### Option: Code-based configuration

Another option would be to go in the direction of some other
frameworks, such as Ruby on Rails, and have the user-facing
configuration be *code* rather than data.

It should be noted that the primary motivation for having a
data-oriented configuration language, that it makes it easier to
interact with programmatically, doesn't really apply in Arachne's
case. Since applications are always free to interact richly with
Arachne's full configuration database, the ability to programmatically
manipulate the precursor data is moot. As such, one major argument
against a code-based configuration strategy does not apply.","Developers will have the option of writing configuration using either
native Datomic-style, data, or code-based *configuration
scripts*. Configuration scripts are Clojure files which, when
evaluated, update a configuration stored in an atom currently in
context (using a dynamically bound var.)

Configuration scripts are Clojure source files in a distinct directory
that by convention is *outside* the application's classpath:
configuration code is conceptually and physically separate from
application code. Conceptually, loading the configuration scripts
could take place in an entirely different process from the primary
application, serializing the resulting config before handing it to the
runtime application.

To further emphasize the difference between configuration scripts and
runtime code, and because they are not on the classpath, configuration
scripts will not have namespaces and will instead include each other
via Clojure's `load` function.

Arachne will provide code supporting the ability of module authors to
write ""configuration DSLs"" for users to invoke from their
configuration scripts. These DSLs will emphasize making it easy to
create appropriate entities in the configuration. In general, DSL
forms will have an imperative style: they will convert their arguments
to configuration data and immediately transact it to the context
configuration.

As a trivial example, instead of writing the verbose configuration data:

```clojure
{:arachne/id :my.app/server
 :arachne.http.server/port 8080
 :arachne.http.server/debug true}
 ```

You could write the corresponding DSL:

```clojure
(server :id :my.app/server, :port 8080, :debug true)
```

Note that this is an illustrative example and does not represent the
actual DSL or config for the HTTP module.

DSLs should make heavy use of Spec to make errors as comprehensible as possible.

"
"At some point, every Arachne application needs to start; to bootstrap
itself from a static project or deployment artifact, initialize what
needs initializing, and begin servicing requests, connecting to
databases, processing data, etc.

There are several logically inherent subtasks to this bootstrapping process, which can be broken down as follows.

- Starting the JVM
    - Assembling the project's dependencies
    - Building a JVM classpath
    - Starting a JVM
- Arachne Specific
    - Reading the initial user-supplied configuration (i.e, the configuration scripts from [ADR-005](adr-005-user-facing-config.md))
    - Initializing the Arachne configuration given a project's set of modules (described in [ADR-002](adr-002-configuration.md) and [ADR-004](adr-004-module-loading.md))
- Application Specific
    - Instantiate user and module-defined objects that needs to exist at runtime.
    - Start and stop user and module-defined services

As discussed in [ADR-004](adr-004-module-loading.md), tasks in the ""starting the JVM"" category are not in-scope for Arachne; rather, they are offloaded to whatever build/dependency tool the project is using (usually either [boot](http://boot-clj.com) or [leiningen](http://leiningen.org).)

This leaves the Arachne and application-specific startup tasks. Arachne should provide an orderly, structured startup (and shutdown) procedure, and make it possible for modules and application authors to hook into it to ensure that their own code initializes, starts and stops as desired.

Additionally, it must be possible for different system components to have dependencies on eachother, such that when starting, services start *after* the services upon which they depend. Stopping should occur in reverse-dependency order, such that a service is never in a state where it is running but one of its dependencies is stopped.","#### Components

Arachne uses the [Component](https://github.com/stuartsierra/component) library to manage system components. Instead of requiring users to define a component system map manually, however, Arachne itself builds one based upon the Arachne config via *Configuration Entities* that appear in the configuration.

Component entities may be added to the config directly by end users (via a initialization script as per [ADR-005](adr-005-user-facing-config.md)), or by modules in their `configure` function ([ADR-004](adr-004-module-loading.md).)

Component entities have attributes which indicates which other components they depend upon. Circular dependencies are not allowed; the component dependency structure must form a Directed Acyclic Graph (DAG.) The dependency attributes also specify the key that Component will use to `assoc` dependencies.

Component entities also have an attribute that specifies a *component constructor function* (via a fully qualified name.) Component constructor functions must take two arguments: the configuration, and the entity ID of the component that is to be constructed. When invoked, a component constructor must return a runtime component object, to be used by the Component library. This may be any object that implements `clojure.lang.Associative`, and may also optionally satisfy Component's `Lifecycle` protocol.

#### Arachne Runtime

The top-level entity in an Arachne system is a reified *Arachne Runtime* object. This object contains both the Component system object, and the configuration value upon which the runtime is based. It satisfies the `Lifecycle` protocol itself; when it is started or stopped, all of the component objects it contains are started or stopped in the appropriate order.

The constructor function for a Runtime takes a configuration value and some number of ""roots""; entity IDs or lookup refs of Component entities in the config. Only these root components and their transitive dependencies will be instantiated or added to the Component system. In other words, only component entities that are actually used will be instantiated; unused component entities defined in the config will be ignored.

A `lookup` function will be provided to find the runtime object instance of a component, given its entity ID or lookup ref in the configuraiton.

#### Startup Procedure

Arachne will rely upon an external build tool (such as boot or leiningen.) to handle downloading dependencies, assembling a classpath, and starting a JVM.

Once JVM with the correct classpath is running, the following steps are required to yield a running Arachne runtime:

1. Determine a set of modules to use (the ""active modules"")
2. Build a configuration schema by querying each active module using its `schema` function ([ADR-004](module-loading.md))
3. Update the config with initial configuration data from user init scripts ([ADR-005](adr-005-user-facing-config.md))
4. In module dependency order, give each module a chance to query and update the configuration using its `configure` function ([ADR-004](module-loading.md))
5. Create a new Arachne runtime, given the configuration and a set of root components.
6. Call the runtime's `start` method. 

The Arachne codebase will provide entry points to automatically perform these steps for common development and production scenarios. Alternatively, they can always be be executed individually in a REPL, or composed in custom startup functions.
 
"
"A core part of the process of developing an application is making changes to its configuration. With its emphasis on configuration, this is even more true of Arachne than with most other web frameworks.

In a development context, developers will want to see these changes reflected in their running application as quickly as possible. Keeping the test/modify cycle short is an important goal.

However, accommodating change is a source of complexity. Extra code would be required to handle  ""update"" scenarios. Components are initialized with a particular configuration in hand. While it would be possible to require that every component support an `update` operation to receive an arbitrary new config, implementing this is non-trivial and would likely need to involve conditional logic to determine the ways in which the new configuration is different from the old. If any mistakes where made in the implementation of `update`, *for any component*, such that the result was not identical to a clean restart, it would be possible to put the system in an inconsistent, unreproducible state.

The ""simplest"" approach is to avoid the issue and completely discard and rebuild the Arachne runtime ([ADR-006](adr-006-core-runtime)) every time the configuration is updated. Every modification to the config would be applied via a clean start, guaranteeing reproducibility and a single code path.

However, this simple baseline approach has two major drawbacks:

1. The shutdown, initialization, and startup times of the entire set of components will be incurred every time the configuration is updated.
2. The developer will lose any application state stored in the components whenever the config is modified.

The startup and shutdown time issues are potentially problematic because of the general increase to cycle time. However, it might not be too bad depending on exactly how long it takes sub-components to start. Most commonly-used components take only a few milliseconds to rebuild and restart. This is a cost that most Component workflows absorb without too much trouble.

The second issue is more problematic. Not only is losing state a drain on overall cycle speed, it is a direct source of frustration, causing developers to repeat the same tasks over and over. It will mean that touching the configuration has a real cost, and will cause developers to be hesitant to do so.


### Prior Art

There is a library designed to solve the startup/shutdown problem, in conjunction with Component: [Suspendable](https://github.com/weavejester/suspendable). It is not an ideal fit for Arachne, since it focuses on suspending and resuming the same Component instances rather than rebuilding, but its approach may be instructive.","Whenever the configuration changes, we will use the simple approach of stopping and discarding the entire old Arachne runtime (and all its components), and starting a new one.

To mitigate the issue of lost state, Arachne will provide a new protocol called `Preservable` (name subject to change, pending a better one.) Components may optionally implement `Preservable`; it is not required. `Preservable` defines a single method, `preserve`. 

Whenever the configuration changes, the following procedure will be used:

1. Call `stop` on the old runtime.
2. Instantiate the new runtime.
3. For all components in the new runtime which implement `Preservable`, invoke the `preserve` function, passing it the corresponding component from the old runtime (if there is one).
4. The `preserve` function will selectively copy state out of the old, stopped component into the new, not-yet-started component. It should be careful not to copy any state that would be invalidated by a configuration change.
5. Call `start` on the new runtime.

Arachne will not provide a mitigation for avoiding the cost of stopping and starting individual components. If this becomes a pain point, we can explore solutions such as that offered by Suspendable.
 
"
"One design goal of Arachne is to have modules be relatively easily swappable. Users should not be permanently committed to particular technical choices, but instead should have some flexibility in choosing their preferred tech, as long as it exists in the form of an Arachne module.

Some examples of the alternative implementations that people might wish to use for various parts of their application:

- HTTP Server: Pedestal or Ring
- Database: Datomic, an RDBMS or one of many NoSQL options.
- HTML Templating: Hiccup, Enlive, StringTemplate, etc.
- Client-side code: ClojureScript, CoffeeScript, Elm, etc.
- Authentication: Password-based, OpenID, Facebook, Google, etc.
- Emailing: SMTP, one of many third-party services.

This is only a representative sample; the actual list is unbounded.

The need for this kind of flexibility raises some design concerns: 

**Capability**. Users should always be able to leverage the full power of their chosen technology. That is, they should not have to code to the ""least common denominator"" of capability. If they use Datomic Pro, for example, they should be able to write Datalog and fully utilize the in-process Peer model, not be restricted to an anemic ""ORM"" that is also compatible with RDBMSs.

**Uniformity**. At tension with capability is the desire for uniformity; where the feature set of two alternatives is *not* particularly distinct, it is desirable to use a common API, so that implementations can be swapped out with little or no effort. For example, the user-facing API for sending a single email should (probably) not care whether it is ultimately sent via a local Sendmail server or a third-party service.

**Composition**. Modules should also *compose* as much as possible, and they should be as general as possible in their dependencies to maximize the number of compatible modules. In this situation, it is actually desirable to have a ""least common denominator"" that modules can have a dependency on, rather than depending on specific implementations. For example, many modules will need to persist data and ultimately will need to work in projects that use Datomic or SQL. Rather than providing multiple versions, one for Datomic users and another for SQL, it would be ideal if they could code against a common persistence abstraction, and therefore be usable in *any* project with a persistence layer.

### What does it mean to use a module?

The following list enumerates the ways in which it is possible to ""use"" a module, either from a user application or from another module. (See [ADR-004](ADR-004-module-loading.md)).

1. You can call code that the module provides (the same as any Clojure library.)
2. You can extend a protocol that the module provides (the same as any Clojure library.)
3. You can read the attributes defined in the module from the configuration.
4. You can write configuration data using the attributes defined in the module.

These tools allow the definition of modules with many different kinds of relationships to each other. Speaking loosely, these relationships can correspond to other well-known patterns in software development including composition, mixins, interface/implementation, inheritance, etc.","In order to simultaneously meet the needs for capability, uniformity and composition, Arachne's core modules will (as appropriate) use the pattern of *abstract modules*.

Abstract modules define certain attributes (and possibly also corresponding init script DSLs) that describe entities in a particular domain, *without* providing any runtime implementation which uses them. Then, other modules can ""implement"" the abstract module, reading the abstract entities and doing something concrete with them at runtime, as well as defining their own more specific attributes.

In this way, user applications and dependent modules can rely either on the common, abstract module or the specific, concrete module as appropriate. Coding against the abstract module will yield a more generic ""least common denominator"" experience, while coding against a specific implementor will give more access to the unique distinguishing features of that particular technology, at the cost of generality.

Similar relationships should hold in the library code which modules expose (if any.) An abstract module, for example, would be free to define a protocol, intended to be implemented concretely by code in an implementing module.

This pattern is fully extensible; it isn't limited to a single level of abstraction. An abstract module could itself be a narrowing or refinement of another, even more general abstract module.

### Concrete Example

As mentioned above, Arachne would like to support both Ring and Pedestal as HTTP servers. Both systems have a number of things in common:

- The concept of a ""server"" running on a port.
- The concept of a URL path/route
- The concept of a terminal ""handler"" function which receives a request and returns a response.

They also have some key differences:

- Ring composes ""middleware"" functions, whereas Pedestal uses ""interceptor"" objects
- Asynchronous responses are handled differently

Therefore, it makes sense to define an abstract HTTP module which defines the basic domain concepts; servers, routes, handlers, etc. Many dependent modules and applications will be able to make real use of this subset.

Then, there will be the two modules which provide concrete implementations; one for Pedestal, one for Ring. These will contain the code that actually reads the configuration, and at runtime builds appropriate routing tables, starts server instances, etc. Applications which wish to make direct use of a specific feature like Pedestal interceptors may freely do so, using attributes defined by the Pedestal module.

"
"In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. 

Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.

Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:

- Types of entities which can exist
- Relationships between those types
- Logical constraints on the values of attributes:
    - more fine grained cardinality; optional/required attributes
    - valid value ranges
    - target entity type (for ref attributes)

This additional data could serve three purposes:

- Documentation about the intended purpose and structure of the configuration defined by a module.
- Deeper, more specific validation of user-supplied configuration values
- Machine-readable integration point for tools which consume and produce Arachne configurations.","- We will add meta-attributes to the schema of every configuration, expressing basic ontological relationships.
- These attributes will be semantically compatible with OWL (such that we could conceivably in the future generate an OWL ontology from a config schema)
- The initial set of these attributes will be minimal, and targeted towards the information necessary to generate rich schema diagrams
  - classes and superclass
  - attribute domain
  - attribute range (for ref attributes)
  - min and max cardinality
- Arachne core will provide some (optional) utility functions for schema generation, to make writing module schemas less verbose.

"
"While many Arachne applications will use a transient config which is rebuilt from its initialization scripts every time an instance is started, some users might wish instead to store their config persistently in a full Datomic instance.

There are a number of possible benefits to this approach:

- Deployments from the same configuration are highly reproducible
- Organizations can maintain an immutable persistent log of configuration changes over time.
- External tooling can be used to persistently build and define configurations, up to and including full ""drag and drop"" architecture or application design.

Doing this introduces a number of additional challenges:

- **Initialization Scripts**: Having a persistent configuration introduces the question of what role initialization scripts play in the setup. Merely having a persistent config does not make it easier to modify by hand - quite the opposite. While an init script could be used to create the configuration, it's not clear how they would be updated from that point (absent a full config editor UI.)
  
  Re-running a modified configuration script on an existing configuration poses challenges as well; it would require that all scripts be idempotent, so as not to create spurious objects on subsequent runs. Also, scripts would then need to support some concept of retraction.
- **Scope & Naming**: It is extremely convenient to use `:db.unique/identity` attributes to identify particular entities in a configuration and configuration init scripts. This is not only convenient, but *required* if init scripts are to be idempotent, since this is the only mechanism by which Datomic can determine that a new entity is ""the same"" as an older entity in the system.

  However, if there are multiple different configurations in the same database, there is the risk that some of these unique values might be unintentionally the same and ""collide"", causing inadvertent linkages between what ought to be logically distinct configurations.
 
  While this can be mitigated in the simple case by ensuring that every config uses its own unique namespace, it is still something to keep in mind.

- **Configuration Copying & Versioning** Although Datomic supports a full history, that history is linear. Datomic does not currently support ""forking"" or maintaining multiple concurrent versions of the same logical data set.

  This does introduce complexities when thinking about ""modifying"" a configuration, while still keeping the old one. This kind of ""fork"" would require a deep clone of all the entities in the config, *as well as* renaming all of the `:db.unique/identity` attrs.
  
  Renaming identity attributes compounds the complexity, since it implies that either idents cannot be hardcoded in initialization scripts, or the same init script cannot be used to generate or update two different configurations.

- **Environment-specific Configuration**: Some applications need slightly different configurations for different instances of the ""same"" application. For instance, some software needs to be told what its own IP address is. While it makes sense to put this data in the configuration, this means that there would no longer be a single configuration, but N distinct (yet 99% identical) configurations.

  One solution would be to not store this data in the configuration (instead picking it up at runtime from an environment variable or secondary config file), but multiplying the sources of configuration runs counter to Arachne's overriding philosophy of putting everything in the configuration to start with.
  
- **Relationship with module load process**: Would the stored configuration represent only the ""initial"" configuration, before being updated by the active modules? Or would it represent the complete configuration, after all the modules have completed their updates?

  Both alternatives present issues.
  
  If only the user-supplied, initial config is stored, then the usefulness of the stored config is diminished, since it does not provide a comprehensive, complete view of the configuration.
  
  On the other hand, if the complete, post-module config is persisted, it raises more questions. What happens if the user edits the configuration in ways that would cause modules to do something different with the config? Is it possible to run the module update process multiple times on the same config? If so, how would ""old"" or stale module-generated values be removed?

#### Goals

We need a technical approach with good answers to the challenges described above, that enables a clean user workflow. As such, it is useful to enumerate the specific activities that it would be useful for a persistent config implementation to support:

- Define a new configuration from an init script.
- Run an init script on an existing configuration, updating it.
- Edit an existing configuration using the REPL.
- Edit an existing configuration using a UI.
- Clone a configuration
- Deploy based on a specific configuration

At the same time, we need to be careful not to overly complicate things for the common case; most applications will still use the pattern of generating a configuration from an init script immediately before running an application using it.","We will not attempt to implement a concrete strategy for config persistence at this time; it runs the risk of becoming a quagmire that will halt forward momentum.

Instead, we will make a minimal set of choices and observations that will enable forward progress while preserving the ability to revisit the issue of persistent configuration at some point in the future.

1. The configuration schema itself should be compatible with having several configurations present in the same persistent database. Specifically:
  - Each logical configuration should have its own namespace, which will be used as the namespace of all `:db.unique/identity` values, ensuring their global uniqueness.
  - There is a 'configuration' entity that reifies a config, its possible root components, how it was constructed, etc.
  - The entities in a configuration must form a connected graph. That is, every entity in a configuration must be reachable from the base 'config' entity. This is required to have any ability to identify the config as a whole within for any purpose.

2. The current initial _tooling_ for building configurations (including the init scripts) will focus on building configurations from scratch. Tooling capable of ""editing"" an existing configuration is sufficiently different, with a different set of requirements and constraints, that it needs its own design process.

3. Any future tooling for storing, viewing and editing configurations will need to explicitly determine whether it wants to work with the configuration before or after processing by the modules, since there is a distinct set of tradeoffs.

"
"In addition to handling arbitrary HTTP requests, we would like for Arachne to make it easy to serve up certain types of well-known resources, such as static HTML, images, CSS, and JavaScript.

These ""static assets"" can generally be served to users as files directly, without processing at the time they are served. However, it is extremely useful to provide *pre-processing*, to convert assets in one format to another format prior to serving them. Examples of such transformations include:

- SCSS/LESS to CSS
- CoffeeScript to JavaScript
- ClojureScript to JavaScript
- Full-size images to thumbnails
- Compress files using gzip

Additionally, in some cases, several such transformations might be required, on the same resource. For example, a file might need to be converted from CoffeeScript to JavaScript, then minified, then gzipped.

In this case, asset transformations form a logical pipeline, applying a set of transformations in a known order to resources that meet certain criteria.

Arachne needs a module that defines a way to specify what assets are, and what transformations ought to apply and in what order. Like everything else, this system needs to be open to extension by other modules, to provide custom processing steps.

### Development vs Production

Regardless of how the asset pipeline is implemented, it must provide a good development experience such that the developer can see their changes immediately. When the user modifies an asset file, it should be automatically reflected in the running application in near realtime. This keeps development cycle times low, and provides a fluid, low-friction development experience that allows developers to focus on their application.

Production usage, however, has a different set of priorities. Being able to reflect changes is less important; instead, minimizing processing cost and response time is paramount. In production, systems will generally want to do as much processing as they can ahead of time (during or before deployment), and then cache aggressively.

### Deployment & Distribution

For development and simple deployments, Arachne should be capable of serving assets itself. However, whatever technique it uses to implement the asset pipeline, it should also be capable of sending the final assets to a separate cache or CDN such that they can be served statically with optimal efficiency. This may be implemented as a separate module from the core asset pipeline, however.

### Entirely Static Sites

There is a large class of websites which actually do not require any dynamic behavior at all; they can be built entirely from static assets (and associated pre-processing.) Examples of frameworks that cater specifically to this type of ""static site generation"" include Jekyll, Middleman, Brunch, and many more.

By including the asset pipeline module, and *not* the HTTP or Pedestal modules, Arachne also ought to be able to function as a capable and extensible static site generator.","Arachne will use Boot to provide an abstract asset pipeline. Boot has built-in support for immutable Filesets, temp directory management, and file watchers.

As with everything in Arachne, the pipeline will be specified as pure data in the configuration, specifying inputs, outputs, and transformations explicitly.

Modules that participate in the asset pipeline will develop against a well-defined API built around Boot Filesets.

"
"As much as possible, an Arachne application should be defined by its configuration. If something is wrong with the configuration, there is no way that an application can be expected to work correctly.

Therefore, it is desirable to validate that a configuration is correct to the greatest extent possible, at the earliest possible moment. This is important for two distinct reasons:

- Ease of use and developer friendliness. Config validation can return helpful errors that point out exactly what's wrong instead of deep failures with lengthy debug sessions.
- Program correctness. Some types of errors in configs might not be discovered at all during testing or development, and aggressively failing on invalid configs will prevent those issues from affecting end users in production.

There are two ""kinds"" of config validation.

The first is ensuring that a configuration as data is structurally correct; that it adheres to its own schema. This includes validating types and cardinalities as expressed by the Arachne's core ontology system.

The second is ensuring that the Arachne Runtime constructed from a given configuration is correct; that the runtime component instances returned by component constructors are of the correct type and likely to work.","Arachne will perform both kinds of validation. To disambiguate them (since they are logically distinct), we will term the structural/schema validation ""configuration validation"", while the validation of the runtime objects will be ""runtime validation.""

Both styles of validation should be extensible by modules, so modules can specify additional validations, where necessary.

#### Configuration Validation

Configuration validation is ensuring that an Arachne configuration object is consistent with itself and with its schema.

Because this is ultimately validating a set of Datomic style `eavt` tuples, the natural form for checking tuple data is Datalog queries and query rules, to search for and locate data that is ""incorrect."" 

Each logical validation will have its own ""validator"", a function which takes a config, queries it, and either returns or throws an exception. To validate a config, it is passed through every validator as the final step of building a module.

The set of validators is open, and defined in the configuration itself. To add new validators, a module can transact entities for them during its configuration building phase.

#### Runtime Validation

Runtime validation occurs after a runtime is instantiated, but before it is started. Validation happens on the component level; each component may be subject to validation.

Unlike Configuration validation, Runtime validation uses Spec. What specs should be applied to each component are defined in the configuration using a keyword-valued attribute. Specs may be defined on individual component entities, or to the *type* of a component entity. When a component is validated, it is validated using all the specs defined for it or any of its supertypes.

"
"Historically, error handling has not been Clojure's strong suit. For the most part, errors take the form of a JVM exception, with a long stack trace that includes a lot of Clojure's implementation as well as stack frames that pertain directly to user code.

Additionally, prior to the advent of `clojure.spec`, Clojure errors were often ""deep"": a very generic error (like a NullPointerException) would be thrown from far within a branch, rather than eagerly validating inputs.

There are Clojure libraries which make an attempt to improve the situation, but they typically do it by overriding Clojure's default exception printing functions across the board, and are sometimes ""lossy"", dropping information that could be desirable to a developer.

Spec provides an opportunity to improve the situation across the board, and with Arachne we want to be on the leading edge of providing helpful error messages that point straight to the problem, minimize time spent trying to figure out what's going on, and let developers get straight back to working on what matters to them.

Ideally, Arachne's error handling should exhibit the following qualities:

- Never hide possibly relevant information.
- Allow module developers to be as helpful as possible to people using their tools.
- Provide rich, colorful, multi-line detailed explanations of what went wrong (when applicable.)
- Be compatible with existing Clojure error-handling practices for errors thrown from libraries that Arachne doesn't control.
- Not violate expectations of experienced Clojure programmers.
- Be robust enough not to cause additional problems.
- Not break existing logging tools for production use.","We will separate the problems of creating rich exceptions, and catching them and displaying them to the user.

### Creating Errors

Whenever a well-behaved Arachne module needs to report an error, it should throw an info-bearing exception. This exception should be formed such that it is handled gracefully by any JVM tooling; the message should be terse but communicative, containing key information with no newlines.

However, in the `ex-data`, the exception will also contain much more detailed information, that can be used (in the correct context) to provide much more detailed or verbose errors. Specifically, it may contain the following keys:

- `:arachne.error/message` - The short-form error message (the same as the Exception message.)
- `:arachne.error/explanation` - a long-form error message, complete with newlines and formatting.
- `:arachne.error/suggestions` - Zero or more suggestions on how the error might be fixed.
- `:arachne.error/type` - a namespaced keyword that uniquely identifies the type of error.
- `:arachne.error/spec` - The spec that failed (if applicable)
- `:arachne.error/failed-data` - The data that failed to match the spec (if applicable)
- `:arachne.error/explain-data` - An explain-data for the spec that failed (if applicable).
- `:arachne.error/env` - A map of the locals in the env at the time the error was thrown.

Exceptions may, of course, contain additional data; these are the common keys that tools can use to more effectively render errors.

There will be a suite of tools, provided with Arachne's core, for conveniently generating errors that match this pattern.

### Displaying Errors

We will use a pluggable ""error handling system"", where users can explicitly install an exception handler other than the default.

If the user does not install any exception handlers, errors will be handled the same way as they are by default (usually, dumped with the message and stack trace to  `System/err`.) This will not change.

However, Arachne will also provide a function that a user can invoke in their main process, prior to doing anything else. Invoking this function will install a set of default exception handlers that will handle errors in a richer, more Arachne-specific way. This includes printing out the long-form error, or even (eventually) popping open a graphical data browser/debugger (if applicable.)

"
"When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.

### Lein templates
In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.

However, they have two major drawbacks:

- They only work when using Leiningen as a build tool.
- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.

### Rails templates

Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. 

Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. ","To start with, Arachne templates will be standard git repositories containing an Arachne project. They will use no special syntax, and will be valid, runnable projects out of the box.

In order to allow users to create their own projects, these template projects will include a `rename` script. The `rename` script will recursively rename an entire project directory to something that the user chooses, and will delete `.git` and re-run `git init`, 

Therefore, the process to start a new Arachne project will be:

1. Choose an appropriate project template.
2. Clone its git repository from Github
3. Run the `rename` script to rename the project to whatever you wish
4. Start a repl, and begin editing.

### Maven Distribution

There are certain development environments where there is not full access to the open internet (particularly in certain governmental applications.) Therefore, accessing GitHub can prove difficult. However, in order to support developers, these organizations often run their own Maven mirrors.

As a convenience to users in these situations, when it is necessary, we can build a wrapper that can compress and install a project directory as a Maven artifact. Then, using standard Maven command line tooling, it will be possible to download and decompress the artifact into a local filesystem directory, and proceed as normal.

"
"The Chimera data model (as described in ADR-15 and ADR-16) includes the concepts of _entity types_ in the domain data model: a defined entity type may have supertypes, and inherits all the attributes of a given supertype 

This is quite expressive, and is a good fit for certain types of data stores (such as Datomic, graph databases, and some object stores.) It makes it possible to compose types, and re-use attributes effectively.

However, it leads to a number of conceptual problems, as well as implementation complexities. These issues include but are not limited to:

- There is a desire for some types to be ""abstract"", in that they exist purely to be extended and are not intented to be reified in the target database (e.g, as a table.) In the current model it is ambiguous whether this is the case or not.
- A singe `extend-type` migration operation may need to create multiple columns in multiple tables, which some databases do not support transactionally.
- When doing a lookup by attribute that exists in multiple types, it is ambiguous which type is intended.
- In a SQL database, how to best model an extended type becomes ambiguous: copying the column leads to ""denormalization"", which might not be desired. On the other hand, creating a separate table for the shared columns leads to more complex queries with more joins.

All of these issues can be resolved or worked around. But they add a variable amount of complexity cost to every Chimera adapter, and create a domain with large amounts of ambigous behavior that must be resolved (and which might not be discovered until writing a particular adapter.)","The concept of type extension and attribute inheritance does not provide benefits proportional to the cost.

We will remove all concept of supertypes, subtypes and attribute inheritance from Chimera's data model.

Chimera's data model will remain ""flat"". In order to achieve attribute reuse for data stores for which that is idiomatic (such as Datomic), multiple Chimera attributes can be mapped to a single DB-level attribute in the adapter mapping metadata.
   
"
"Use Markdown Architectural Decision Records

Should we record the architectural decisions made in this project?
And if we do, how to structure these recordings?

## Considered Alternatives

* [MADR](https://adr.github.io/madr/) - Markdown Architectural Decision Records
* [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) - The first incarnation of the term ""ADR"". Maintainable by [adr-tools](https://github.com/npryce/adr-tools).
* [Sustainable Architectural Decisions](https://www.infoq.com/articles/sustainable-architectural-design-decisions) - The Y-Statements
* [DecisionRecord](https://github.com/schubmat/DecisionCapture) - Agile records by [@schubmat](https://github.com/schubmat/)
* Other templates listed at <https://github.com/joelparkerhenderson/architecture_decision_record>
* No records","* Chosen Alternative: MADR
* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions later on.
  See also [A rational design process: How and why to fake it](https://doi.org/10.1109/TSE.1986.6312940).
* The MADR template is lean and fits our development style.

<!-- Pros and cons of alternatives straight-forward to elicit and therefore not captured. -->

"
"Use filesystem as backend

Winery needs to store its contents.
These contents need to be shared.

## Considered Alternatives

* Filesystem
* Database","* *Chosen Alternative: Filesystem*

"
"File system folder structure using type-namespace-id structure

Winery's data is stored in a a file system [ADR-0001](0001-use-filesystem-as-backend).
The contents of the repository should be

- human readable
- machine processable

## Considered Alternatives

* Folder structure using type-namespace-id
* Everything in one folder. Hash-based storing similar to git.","*Chosen Alternative: Folders subdivided in type-namespace-id*

The final file system layout itself is documented at [RepositoryLayout](../dev/RepositoryLayout).

### human readable

Everything in one directory causes many files listed and thus humans will have difficulties to find the right file.

The folders in the *top level* are the TOSCA ""components"", e.g., Node Type, Relationship Type, Service Template, ...

The *second structuring element* are namespaces.
[Namespaces are an established method to avoid naming conflicts](https://www.w3schools.com/xml/xml_namespaces.asp) and are a structuring element.
TOSCA is an open system and everyone can create Node Types.
One has no global control which names are given to Node Types.
Thus, there might be two different Node Types with the same name.
The namespaces provide a natural structuring and Winery reuses this idea.

The *third structuring element* are the ids of the respective definitions child (type, template, ...):
Each element has an id contained in the respective namespace.
This id can be directly used as folder name.

Within each folder, the component-specific information is stored.

### machine processable

Windows cannot create directories named `http://www.example.com`.
Therefore, the names have to be [encoded](https://en.wikipedia.org/wiki/Character_encoding) so that an appropriate folder can be generated.


"
"Double Encoded URLs

URLs should be human-readable, slashes are not allowed.
Encoded slahes are not enabled as default due to security reasons.

More information about encoding is available at [dev/Encoding.md](dev/Encoding).

## Considered Alternatives

* Using namespace prefixes in the URLs
* Single-encoded URLs and forcing the environment to be reconfigered
* Double-encoded URLs","* *Chosen Alternative: Double-encoded URLs*

"
"OAuth with GitHub

**UserStory:**

In order to use (private) repositories instead of uploading all files into a Artifact Template, it is necessary to log in into GitHub.
  Therefore, the OAuth flow is implented as follows:
  
1. By clicking the `Login with GitHub` button, the browser gets redirected to GitHub for authentication.
    1. Login and authorize the Application to access the private repositories.
    1. Or, if the application was already been authorized, GitHub automatically continues with step 2
1. GitHub answers with the `status` and `code` parameters which are parsed and send to our server to get the access token.
1. Our server responses with the access token.

![OAuth flow diagram](graphics/oauth-flow.png)  

## Considered Alternatives
* *[ALTERNATIVE 1]* Perform the whole login process in the browser
* *[ALTERNATIVE 2]* Perform the whole login process in the backend
* *[ALTERNATIVE 3]* Mix both, frontend and backend to get the access token"," *Chosen Alternative: [ALTERNATIVE 3]*
   because we can easily protect our client secret on the server, store the token safely in the clients local storage and 
   do not need to keep the state at the server. Further, it is possible to save additional user information in the local
   storage without the need for getting it every time from the server.

"
"XML editor does not enforce validation

**UserStory:** Winery offers editing the stored XML of the TOSCA definitions. What to do with validation?

## Considered Alternatives

* Winery never creates an non-schema-conforming XML. For instance, the user has to create a topology template first before he is allowed to save the service template
* Winery generate random data to gain schema-conforming XML
* Winery generates non-schema-conforming XML, but assumes that the user makes it eventually valid. In casea the user uses the XML tab, the user knows what he does. Winery forces the user to generate schema-conforming in the XML editor.
* Winery generates non-schema-conforming XML and warns the user when the user uses the XML editor. Winery does NOT force the user to generate schema-conforming XML in the XML editor.","* *Chosen Alternative: D*
* This is in line with other editors: They allow to save, but warn if the file has compile errors, validation errors, ...

"
"Wrap properties in TOSCA properties element

When GETting/PUTting the properties of an entitty template, the content has to be serialized somehow.

## Considered Alternatives

* Wrap properties in TOSCA properties element
* Use nested XML element (`getAny()`)","* Chosen Alternative: Wrap properties in TOSCA properties element
* Receiving an XML element is not possible with JAX-B/JAX-RS as that setting relies on strong typing.

"
"Custom URI for lifecycle interface

Winery can generate a lifecycle interface.
That interface has to take a URI for a name

## Considered Alternatives

* `http://opentosca.org/interfaces/lifecycle`
* `http://www.example.com/interfaces/lifecycle` (from http://docs.oasis-open.org/tosca/tosca-primer/v1.0/tosca-primer-v1.0.html)
* `tosca.interfaces.node.lifecycle.Standard` (from http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html)  ","* Chosen Alternative: `http://www.example.com/interfaces/lifecycle`
* Although the alternative is not standardized, it is the one consistent with the primer for TOSCA 1.0.


"
"No support for local git source clones

A user wants to edit source files locally in his favourite IDE.
Therefore, he wants to use the usual ways to retrieve source files.
Typically, this is a `git clone` from a git repository having the respective source files.

A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.
It would be beneficial to have the source of an artifact template available as git checkout.

The source files of an artifact implementation are currently directly editable in the winery once they are uploaded. 
The only way to edit sources locally is to download and upload them again.
The solution for the user should be:
- easy to use
- scalable in terms of storage required in Winery's repository


## Considered Alternatives

* No support for local clones
* Git repositories as submodules
* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)
* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)","* Chosen Alternative: no support for local edit

Since all alternatives require either too many additional git repositories or are very inconvenient to apply for the user,
we decided to not support any clone/push functionality.


"
"Manual serialization of SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* Manual serialization
* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml)
* [jackson-dataformat-yaml](https://github.com/FasterXML/jackson-dataformat-yaml)","* Chosen Alternative: *Manual serialization*
* SnakeYAML does not support annotations for serialization
* jackson-dataformat-yaml seems not to support annotations, such as [jackson-annotations](https://github.com/FasterXML/jackson-annotations)

"
"TOSCA YAML deserialization using SnakeYAML

The TOSCA YAML files have to be read into a Java model (deserialized) and written from the Java model into files (serialized).

## Considered Alternatives

* [SnakeYAML](https://bitbucket.org/asomov/snakeyaml) writing into intermediate model consisting of default Java types
* SnakeYAML writing into final Java model
* Manual reading","* Chosen Alternative: *SnakeYAML writing into intermediate model consisting of default Java types*

# Pros and Cons of the Alternatives
  
### SnakeYAML writing into intermediate model consisting of default Java types
  
* `+` Basic YAML parsing can be done using SnakeYAML
* `+` TOSCA Java classes can be filled directly
* `-` Programming effort for conversion

### SnakeYAML writing into final Java model

* `+` Established library
* `+` Less error prone 
* `-` SnakeYAML has to be adapted to be able to convert YAML into TOSCA models
* `-` SnakeYAML is not well-prepared for adaptions
* `-` SnakeYAML has issues to write into complex Java classes (which are not Java base types). E.g., List of Maps. - see <https://bitbucket.org/asomov/snakeyaml/issues/361/list-does-not-create-property-objects>
* `-` huge effort, first attempt did not result in a working converter

### Manual reading

* `+` Can write directly into Java model
* `-` Special cases of YAML have to be handled manually
* `-` Error prone

"
"Use Builder Pattern for Model Classes

Model classes should be instantiable simple without using large constructors.

## Considered Alternatives

* [Builders]
* Setters, getters and default constructor 
* Large constructors
* Factories","* Chosen Alternative: *Builders*
* Flexible
* Simple for complex objects
* Extensions cause problems (solved with generic builders) 

### Generic Builders

Generic Builders are used to enable safe method chaining for Builders with extend other Builders.
Another discussion is made at [stackoverflow].

The method `self()` is necessary because all setter methods should return the Builder used for instantiation and not the builder that is extended. `self()` can not be replace by `this` because the expected type is `<T>` and casting to `<T>` results in warnings.

Builders which are not abstract and are extended by other builders are generic and implement the `self()` method by casting `this` to `<T>`. To reduce warnings this casting is only used in this case.

Example:
```java
// part of ExtensibleElements.Builder
public abstract static class Builder<T extends Builder<T>> {
    private List<TDocumentation> documentation;
    
    // setter returns generic <T> 
    public T setDocumentation(List<TDocumentation> documentation) {
        this.documentation = documentation;
        // return this; => IncompatibleType exception either cast with warnings or use self() method
        return self();
    }
    
    // overwritten method
    public abstract T self();
}

// part of TEntityType.Builder
public abstract static class Builder<T extends Builder<T>> extends TExtensibleElements.Builder<T> {
	
}

// part of TNodeType.Builder
public static class Builder extends TEntityType.Builder<Builder> {
    @Override
    public Builder self() {
        return this;
    }
}
```



[Builders]:(https://en.wikipedia.org/wiki/Builder_pattern)
[stackoverflow]: https://stackoverflow.com/a/5818701/8235252

"
"Provide Support for Custom Key-Value Properties

Most properties are key/value

## Considered Alternatives

* Provide support for custom key-value properties
* Support XML-only","* Chosen Alternative: Provide support for custom key-value properties
* Nice UI experience

"
"Routes in the Repository Angular App

## Considered Alternatives
* Using Wildcards for the `ToscaTypes`
* Explicitly Define the Routes for Each `ToscaType`"," Chosen Alternative: *Explicitly Define the Routes for Each `ToscaType`*
* By choosing this alternative, the whole project gets more type save by the (lightweight) trade-off
 of maintaining a list of all available `MainRoutes` in the `ToscaTypes` enum. It is now harder to add
 new main routes, because you need to add extra `Modules` and `RoutingModules` for each type. However,
 because of this decision, it is easier to define invalid routes which lead to a `404 - Not Found` error page.

"
"Use Eclipse Orion as Editor

Editor should provide syntax highlighting and work in browser.

## Considered Alternatives

* [Eclipse Orion](https://wiki.eclipse.org/Orion)
* [Eclipse Che](https://www.eclipse.org/che/)
* [Codeanywhere](https://codeanywhere.com)
* [Cloud9](https://c9.io/)
* [Codiad](http://codiad.com/)
* [Theia](https://github.com/theia-ide/theia)","* Chosen Alternative: Eclipse Orion
* Reasons: 
  * Extension of other IDEs either too time consuming or not possible.
  * Communication between other IDEs and Winery unclear.
  * Eclipse Orion was already used in the project.
  
For more details see the [ultimate comparison](https://github.com/ultimate-comparisons/ultimate-webIDE-comparison).

"
"Offer copying files from the source to the files folder

Forces:

- Source code needs to be versioned in Winery
- Support for scripting languages do not need any processing and only need to be copied as they are
- Support for compiled languages need to be processed before copying

## Considered Alternatives

* Copying the sources to files as they are
* Require external tooling to go from source to files"," Chosen Alternative: Copying the sources as they are
* For supporting compiled languages, it is relied on an external IDE (see [ADR-0014]).
  This IDE stores the files in the ""source"" folder and manages the copying to the files folder.
  Thus, the only left support is for scripting languages.
  In that case, the source can be directly used as files (""binary"") in an artifact template.

"
"Reflection test for TOSCA YAML builder

The TOSCA YAML builder converts Java Objects to instances of TOSCA YAML classes. To get clean an good instances validation is needed. Reflection test are Junit5 test which take yaml service templates with metadata that describes what assertions should be made for the resulting TOSCA YAML class instances. 

```
...
metadata:
  assert: |
    repositories.rp1.url = http://github.com/kleinech
    node_types.ntp1.requirements.0.rqr1.capability = cbt1
...
```
Each assert line contains a keyname and a value.
*[context and problem statement]*
*[decision drivers | forces]* <!-- optional -->

## Considered Alternatives

* *reflection tests*
* *manual test*","* Chosen Alternative: *reflection tests*
* Only alternative, which meets simplifies the effort to make complete tests 
"
"Our organization has been considering a switch to a 4-day workweek. The current work schedule of 5 days per week, 8 hours per day, has resulted in low productivity, employee burnout, and high turnover rates. The proposed change aims to increase employee satisfaction, engagement, and retention, and ultimately improve organizational performance.",We have decided to implement a 4-day workweek starting from the next quarter for all employees across the organization.
"The Team has been considering the adoption of Agile software development methodology to increase productivity, improve efficiency, and better adapt to the rapidly changing software development industry.

Key Points:

- Agile emphasizes iterative development, frequent communication, and flexibility in requirements 

- Agile can lead to better collaboration among team members

- Agile can help anticipate and respond to changes in project scope and requirements

- Agile can lead to improved time-to-market and better overall project outcomes","After much deliberation and discussion, the Team has decided to adopt Agile software development methodology. "
"Our organization is growing rapidly and we need a reliable and scalable cloud infrastructure to meet the demands of our business. After considering a number of cloud infrastructure providers, it was concluded that AWS offers the most suitable range of services to meet our current and future requirements.
","After a thorough evaluation of various cloud infrastructure platforms, it has been decided that Amazon Web Services (AWS) will be adopted by our organization for our cloud infrastructure needs."
"We are designing an API for a new service that will be used by multiple clients. We have been considering two options for implementing the API: using JSON over HTTP or using gRPC.

JSON over HTTP is a widely-used approach for building APIs, and it is supported by many programming languages and frameworks. This approach is simple, lightweight, and easy to understand, making it a good choice for many projects. However, it can be less efficient than other options, especially when it comes to handling large amounts of data.

gRPC, on the other hand, is a newer technology that offers a more efficient way of building APIs. It uses binary serialization to transfer data, which can be faster and more compact than using JSON. gRPC also supports bidirectional streaming, making it a good choice for real-time applications.","After considering the pros and cons of both options, we have decided to use gRPC for our API. Although JSON over HTTP is a simpler option, we believe that gRPC will provide a more efficient and scalable solution for our service. We also anticipate that our API will handle a large amount of data, and gRPC's binary serialization will be more efficient for this use case.

In addition, we believe that gRPC's support for bidirectional streaming will be beneficial for real-time applications that we may develop in the future."
"We are designing a new application that requires storing and retrieving data in a scalable and performant manner. We have identified three types of database technologies that are commonly used: relational databases, document databases, and event databases.

Relational databases store data in tables with fixed schemas and enforce strict data integrity constraints. They are suitable for applications that require complex data relationships and transactions. Examples include MySQL, PostgreSQL, and Oracle.

Document databases store data in JSON-like documents and are schema-less. They are well-suited for applications that require flexible data models and scaling horizontally. Examples include MongoDB, Couchbase, and Amazon DynamoDB.

Event databases store data as a series of events, capturing every change to the data. They are suitable for applications that require auditing, event sourcing, and complex data processing. Examples include Apache Kafka, Apache Pulsar, and AWS Kinesis.
","After carefully evaluating the requirements and constraints of our application, we have decided to use a document database."
"The company has been experiencing challenges with the development process as a result of manual testing, build processes, and deployments. This has led to frequent delays, errors and inconsistencies in the quality and delivery of the software.",To implement continuous integration as a solution to the challenges experienced in the development process.
"We want to use a CSS framework to create our web applications:

  * We want user experience to be fast and reliable, on all popular browsers and screen sizes.

  * We want rapid iteration on design, layout, UI/UX, etc.

  * We want responsive applications, especially for smaller screens such as on mobile devices, larger screens such as on 4K widescreens, and dynamic screens such as rotatable displays.  
",Decided on Bulma.
"We are considering different container orchestration tools to manage our microservices-based architecture. We have evaluated different solutions like Kubernetes, Docker Swarm, and Mesosphere DC/OS. However, we have decided to focus on Docker Swarm due to its simplicity, integration with Docker, and built-in load balancing.","We have decided to use Docker Swarm as our container orchestration tool. Docker Swarm provides a simple and intuitive way to manage containerized applications across a cluster of nodes. It also allows us to leverage our existing Docker-based workflows and infrastructure. With Docker Swarm, we can easily deploy, scale, and manage our applications, all while taking advantage of built-in load balancing."
"We want our applications to be configurable beyond artifacts/binaries/source, such that one build can behave differently depending on its deployment environment.

  * To accomplish this, we want to use environment variable configuration.

  * We want to manage the configuration by using files that we can version control.

  * We want to provide some developer experience ergonomics, such as knowing what can be configured and any relevant defaults.
",Decided on .env files with related default file and schema file.
"Our organization has been primarily using Java programming language for building web applications. However, we have been experiencing some challenges due to Java's verbosity and the complexity of some frameworks. Also, our team has shown interest in exploring modern programming languages for more efficient and robust development. After conducting research, we have identified Go as a potential solution to the problems we face with Java.",We will adopt Go programming language for building web applications going forward.
"Google Cloud Platform (GCP) is a prominent cloud computing platform that offers various cloud services, including computing, storage, and networking solutions. This ADR aims to document the architectural decisions made for developing and implementing a GCP-based infrastructure for our organization.","Our organization has decided to use Google Cloud Platform as the cloud infrastructure for our application. The primary considerations for this decision are:

   - Cost-effectiveness

   - Scalability

   - Reliability

   - Flexibility"
"We intended to build a web-based application that could run on multiple operating systems. Our project required a language that could provide strong security features, handle concurrency, support large code bases, and be versatile enough to meet our future growth requirements. We evaluated several languages, considering their strengths and limitations, before deciding on Java.

### Considerations

1. **Security:**  Java provides excellent security features through its well-defined security policies and access control. It also incorporates features like bytecode verification, which helps prevent malicious software from running on a system.

2. **Concurrency:**  Java has built-in support for multithreading, which allows applications to perform several tasks simultaneously. This feature makes Java an excellent choice for developing large, complex applications with multiple features and functionalities.

3. **Large codebases:**  Java supports object-oriented programming, which makes it a suitable choice for developing large codebases. Its modular nature and use of encapsulation and abstraction patterns further add to the software development process's ease.

4. **Versatility:**  Java is versatile and provides the ability to use a wide range of libraries and frameworks, making it an excellent choice for both web-based and enterprise-level applications.","We have decided to use the Java programming language for our project, considering its security features, concurrency support, ability to handle large codebases, and versatility. This decision aligns with our project requirements, and we believe that choosing Java will ensure its success. "
"We need to select a container orchestration platform for our growing cloud-native application portfolio. Our current legacy platform deployment is too slow, and not agile enough to keep up with our growing needs. Were looking for a system that will allow us to scale our services in the most efficient way possible without compromising on agility or ease of use.
","After conducting a thorough analysis of each container orchestration platform, we have decided to adopt Kubernetes as the best option for our enterprise needs."
"The organization is planning to migrate from traditional on-premises infrastructure to cloud infrastructure. In order to achieve this, the organization has evaluated multiple cloud service providers such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and IBM Cloud. Each provider has its own set of features, benefits, and pricing structure. After a detailed analysis, it was concluded that Microsoft Azure is the most suitable option for our organization.",Adopt Microsoft Azure as the cloud infrastructure for our organization.
"We want to use devops to build, integrate, deploy, and host our projects. We are considering Microsoft Azure DevOps.

  * We want developer experience to be fast and reliable, for the setup of the devops e.g. configuring as well as ongoing use e.g. fast build times.
  
  * We want to consider using Microsoft Azure as whole, for hosting the project apps, databases, etc.
",Decided against Microsoft Azure DevOps.
"Our project involves developing three major categories of software:

  * Front-end GUIs
  * Middleware services
  * Back-end servers

When we develop, our source code management (SCM) version control system (VCS) is git.

We need to choose how we use git to organize our code.

The top-level choice is to organize as a ""monorepo"" or ""polyrepo"" or ""hybrid"":

  * Monorepo means we put all pieces into one big repo
  * Polyrepo means we put each piece in its own repo
  * Hybrid means some mix of monorepo and polyrepo

For more please see https://github.com/joelparkerhenderson/monorepo-vs-polyrepo
","Monorepo when an organization/team/project is relatively small, and rapid iteration is higher priority than sustaining stability.

Polyrepo when an organization/team/project is relatively large, and sustaining stability is higher priority than rapid iteration."
"We need to decide on which database management system (DBMS) to use for Project X. The database will be used to store and manage large amounts of data from multiple sources. We need a DBMS that can handle transactions, offer scalability, and provide high reliability and security. Among various options available, we are considering MySQL as a possible choice.
","onsiderations

- Ease of use and maintenance

- Community support and resources

- Performance and scalability

- Security and reliability

- Cost and licensing

- Compatibility with our technology stack

### Considered Options

- MySQL

- PostgreSQL

- Oracle

- Microsoft SQL Server

- MongoDB

## Decision

After evaluating the above options based on our decision considerations, we have decided to choose MySQL as our DBMS for Project X.

MySQL is a popular open-source system with a strong development community and a large pool of resources for problem-solving and knowledge sharing. It is well-known for its excellent performance and scalability capabilities, making it ideal for handling vast amounts of data with high levels of efficiency. The platform is secure, reliable, and has a wide range of features that are essential for our project, including ACID compliance for transactions, flexible data model, and support for various programming languages and frameworks.

MySQL is also compatible with the majority of our technology stack, including our web development framework, hosting solutions, and other essential tools. Plus, its cost and licensing terms are competitive compared to other proprietary systems like Oracle and Microsoft SQL Server."
"Our organization is currently evaluating options for the most suitable database management system to be used in our upcoming project. We have narrowed our search down to two options  PostgreSQL and MySQL. After careful consideration, we have decided to use PostgreSQL due to its advanced features and performance capabilities.",We will use PostgreSQL as the preferred database management system for our organization's upcoming project.
"Programming code editors are an essential tool for developers to write and edit code. There are numerous code editors available, each with its own set of features, advantages, and disadvantages. The purpose of this ADR is to document the architectural decisions made for programming code editors.

## Priorities

The architecture for programming code editors should prioritize the following:

* **Modularity**: The code editor should be designed in a modular way, allowing developers to customize and extend it as needed. This allows for a flexible architecture that can adapt to the needs of different developers and teams.

* **Performance**: The code editor should be performant and responsive, allowing developers to work efficiently without being slowed down by the tool they are using.

* **User Interface**: The user interface should be intuitive and easy to use, allowing developers to focus on their code rather than struggling with the editor.

* **Extensibility**: The code editor should be designed to allow for easy extension with third-party plugins and integrations.

* **Compatibility**: The code editor should be compatible with a wide range of programming languages and technologies, making it a useful tool for a broad range of developers.","Based on these priorities, the architecture for programming code editors should be designed with the following components:

* **Core**: This component provides the basic functionality of the code editor, such as syntax highlighting, text editing, and file management.

* **UI**: This component provides the user interface for the code editor, including menus, toolbars, and keyboard shortcuts.

* **Plugins**: This component allows developers to extend the functionality of the code editor by installing third-party plugins. Plugins can provide additional features, such as code completion, linting, or debugging.

* **Integrations**: This component allows the code editor to integrate with other tools and technologies, such as version control systems, build systems, or debugging tools."
"We need to choose programming languages for our software. We have two major needs: a front-end programming language suitable for web applications, and a back-end programming language suitable for server applications.
","We are choosing TypeScript for the front-end.

We are choosing Rust for the back-end."
Our organization is planning to develop a web application that manages customer data. We have chosen Python as the programming language and are considering Django as the web framework for the development of the application.,We have decided to use the Django web framework for the development of the web application. Django provides a robust set of tools and features for building web applications quickly and efficiently.
"We need to decide on whether to use Python as a programming language for our project. Our project involves data analysis, machine learning, and web development.",We have decided to use Python as our primary programming language for our project.
"We need to choose a front-end JavaScript library for our application to develop a responsive and interactive user interface.
  
## Option Considered

1. React
  
2. Vue

3. Svelte
  ",We will use React as our front-end JavaScript library.
"- The organization is developing a web application with complex business logic and multiple integrations with third-party services.

- The team has experience in building web applications with Ruby on Rails.

- There is a need to choose a web framework that promotes rapid development, scalability, and maintainability.",- Use Ruby on Rails as the framework for developing the web application.
"We need to store secrets, such as passwords, private keys, authentication tokens, etc.

Some of the secrets are user-oriented. For example, our developer wants to be able to use their mobile phone to look up a password to a service.

Some of the secrets are system-oriented. For example, our continuous delivery pipeline needs to be able to look up the credentials for our cloud hosting.
","Bitwarden for user-oriented secrets

Vault by HashiCorp for system-oriented secrets."
"In naming conventions for REST APIs, there are two popular formats: snake_case and camelCase. The snake_case format is where each word in the name is separated by underscores, whereas camelCase is where the first word of the name is in lower case, and the subsequent words have their first letter capitalized. This decision will determine which naming convention should be used for a REST API.
","rivers

- Consistency with existing naming conventions in the project

- Readability and clarity for anyone who may be working on the API

- Alignment with industry best practices for REST API naming conventions

- Ease of implementation and maintenance

## Decision

The snake_case naming convention will be used for REST API endpoints. This choice is driven by the following factors:

1. **Consistency**: The project already uses snake_case naming convention for all endpoints, and it would be beneficial to maintain this convention to ensure consistency across the entire project.

2. **Readability and clarity**: The snake_case convention is more readable and easier to understand. The underscores provide a clear separation between words, making it easier to parse and understand the meaning of the name.

3. **Alignment with industry best practices**: The snake_case convention is widely used in the industry and is considered to be a best practice for REST APIs, making it a good choice for the project.

4. **Ease of implementation and maintenance**: Keeping with the existing naming convention is easier to implement and maintain as all existing code and documentation would need to be updated if a new convention was chosen.
"
"We are developing a web application using a modern front-end JavaScript framework. We want to choose a library that is lightweight, fast, and easy to use. We have evaluated several libraries, including React, Vue, and Angular. We have also heard about Svelte, which claims to be a new kind of lightweight framework. We need to evaluate Svelte and decide whether it is a good fit for our application.","After considering the benefits and drawbacks of each library, we have decided to use Svelte as our front-end JavaScript library for the following reasons:

1. **Lightweight:**  Svelte is one of the lightest libraries available, which will help us keep our application size small and improve page load times.

2. **Fast:**  Svelte is known for its fast rendering speed, which will make our application feel responsive and snappy.

3. **Easy to use:**  Svelte's syntax is simple and easy to understand, which will make it easier for our team to adopt it and write clean code.

4. **Optimized for UI rendering:**  Svelte is optimized for rendering UI components, which will make it easier for us to create dynamic and reusable UI elements.

5. **Good documentation and community support:**  Svelte has a growing community and good documentation, which will help us get started quickly and resolve any issues that arise."
"We are building a new web application that requires a scalable and efficient front-end architecture. After conducting a thorough analysis of various front-end frameworks, we have chosen SvelteKit as our preferred framework. ","We have decided to implement the SvelteKit front-end architecture for our web application. SvelteKit provides an advanced front-end development system that is fast, efficient, and flexible. It can be used for optimizing large and small applications."
"We are building a web application for a client that requires a responsive and customizable user interface. The client has asked for consistent and modern styling. The development team has experience using custom CSS but wants to explore CSS frameworks that can save time and effort. We are looking for a framework that is easy to use, customizable, and has a large community for support.
","rivers

- **Easy to use:**  We want a framework that is simple to learn and can be integrated into our development workflow.

- **Customizable:**  We want a framework that can be customized to match the client's brand guidelines and theme.

- **Large community support:**  We want a framework that has an active community of developers and designers that can provide help and resources when needed.

- **Time-saving:**  We want a framework that can save time and effort in styling the UI.

## Considered Options

- **Bootstrap:**  A popular CSS framework with a lot of pre-built components and a large community of developers.

- **Foundation:**  Another popular CSS framework that was designed for mobile-first development.

- **Materialize:**  A CSS framework based on Google's Material Design language with pre-built components for UI development.

- **Tailwind CSS:**  A utility-first CSS framework with a focus on customization and efficiency.

## Decision

After considering the options, we decided to use Tailwind CSS for the following reasons:

- **Easy to use:**  Tailwind's approach is simple and easy to understand. Its utility classes make building a UI faster.

- **Customizable:**  Tailwind allows for customization by providing a set of configuration files that allow developers to modify the framework's default styles with ease.

- **Large community support:**  Tailwind has a very active community of developers and designers that are consistently creating new resources, plugins, and tools.

- **Time-saving:**  Tailwind can save time, enabling developers to focus on other areas of the project.

We will use Tailwind CSS's utility classes to build the UI for our project. The development team will use its atomic and modular design system to write efficient and scalable CSS code. We will leverage Tailwind's pre-built templates and visual components to speed up the development process while utilizing custom styles to match the branding and theme of the client."
"We want to be able to track when things happen by using timestamps and by using a consistent timestamp format that works well across all our systems and third-party systems.

We interact with systems that have different timestamp formats:

* JSON messages do not have a native timestamp format, so we need to choose how to convert a timestamp to a string, and convert a string to a timestamp, i.e. how to serialize/deserialize.

* Some applications are set to use local time, rather than UTC time. This can be convenient for projects that must adjust to local time, such as projects that trigger events that are based on local time.

* Some systems have different time precision needs and capabilities, such as using a time resolution of seconds vs. milliseconds vs. nanoseconds. For example, the Linux operating system `date` command uses a default time precision of seconds, whereas the Nasdaq stock exchange wants a default time precision of nanoseconds.
","We choose the timestamp standard format ISO 8601 with nanosecond precision, specifically ""YYYY-MM-DDTHH:MM:SS.NNNNNNNNNZ"".

The format shows the year, month, day, hour, minute, second, nanoseconds, and Zulu time zone a.k.a. UTC, GMT."
"Our web application requires a front-end JavaScript library to provide dynamic and responsive user interfaces. We have evaluated several popular libraries that include React, Angular, and Vue. Based on the evaluation, we have decided to use the Vue library as the primary front-end library for our web application.",We have decided to adopt the Vue front-end JavaScript library
"Due to the ongoing COVID-19 pandemic, many employees have been working remotely from home. This arrangement has provided benefits such as increased flexibility, improved work-life balance and a decrease in the spread of the virus. Furthermore, numerous employees have expressed that they would like to continue working from home even after the pandemic has ended.","After weighing the pros and cons of allowing employees to work from home, the management team has decided to implement a permanent work from home policy for eligible employees. The policy applies to all full-time employees who can effectively perform their duties from home and have demonstrated success working remotely during the pandemic.

Eligible employees can choose to work from home up to three days per week, while in-office work will be required for the remaining two days. To ensure effective communication and collaboration, regular check-ins with team members and managers will be required. Additionally, employees will be provided with the necessary equipment and resources to perform their duties effectively from home.

The implementation of this policy will be reviewed and revisited regularly to ensure that it continues to benefit both the employees and the company."
"We are in search for a means to document our architectural and design decisions for all of our components. In order to do that, there is practice called architectural decision records (ADR), that we can integrate into our workflow.

This does not replace actual architecture documentation, but provides people who are contributing:

    the means to understand architectural and design decisions that were made

    a framework for proposing changes to the current architecture

For each decision, it is important to consider the following factors:

    what we have decided to do

    why we have made this decision

    what we expect the impact of this decision to be

    what we have learned in the process

As were already using rST, Sphinxdoc and readthedocs, it would be practical to integrate these ADRs as part of our current documentation infrastructure.","We will use ADRs to document, propose and discuss any important or significant architectural and design decisions.

    The ADR format will follow the format described in Implications section.

    We will follow the convention of storing those ADRs as rST or Markdown formatted documents stored under the docs/adr directory, as exemplified in Nat Pryces adr-tools. This does not imply that we will be using adr-tools itself, as we might diverge from the proposed structure.

    We will keep rejected ADRs

    We will strive, if possible, to create an ADR as early as possible in relation to the actual implementation."
"Since the genesis of the plutus-apps repository, the components have been historically using the types in the plutus-ledger-api package (which is now part of the plutus repository) in the off-chain part of a Plutus application.

This was desirable in order to start designing a way to build Plutus applications before the cardano-ledger actually supported the Alonzo-era features. Of course, this resulted in the unintended consequence that we used TxInfo types (types that are designed to be used in Plutus scripts) in off-chain code. This wouldnt have been a problem if there was a 1:1 relationship between on-chain and off-chain types. However, that presumption is wrong.

Lets take the example of the TxOut representation of plutus-ledger-api for PlutusV2.

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe ScriptHash
  }

As we can see, the TxOut can optionally store the ScriptHash of the referenced script. However, that is not the adequate representation of a TxOut in a transaction given the cardano-ledger specification. The off-chain TxOut should instead be:

data TxOut = TxOut {
  ...
  txOutReferenceScript :: Maybe Script
  }

where the reference script field can store that actual script, not just the hash.

This proved that we need to start moving away from plutus-ledger-api types in the off-chain part of Plutus applications, especially in components like the emulator and the chain indexer.","We will create a cardano-api-extended cabal project, which will contain features and utilities on top of the cardano-api package. A similar idea has emerged with hydra-cardano-api. This package will contain:

            type synonyms for working with the latest era

            a simplified and working transaction balancing function (mainly the Ledger.Fee.makeAutoBalancedTransaction in plutus-apps)

            validation rules (most of whats in the current Ledger.Validation)

    The cardano-api-extended package will re-export the modules from the hydra-cardano-api package which contain type synonyms for working with the latest era.

    We will remove our data type representation of a Cardano transaction (Ledger.Tx.Internal.Tx in plutus-ledger) and fully commit to Cardano.Api.Tx.Tx era (or Cardano.Api.Tx.TxLatestEra) in the codebase.

    We will replace any use of plutus-ledger-api types by cardano-api and cardano-api-extended types whenever we work with the off-chain part of Plutus applications. For instance, the plutus-contract emulator and types in the Plutus.Contract.Request module of plutus-contract will be updated to use cardano-api types. However, the data types in Ledger.Tx.Constraints.TxConstraints will continue to use plutus-ledger-api types because the constraints are used to generate both Plutus scripts and transactions. Therefore, there should be no breaking change on the API for writing Plutus applications.

    We will improve cardano-api through cardano-api-extended and regularly push changes upstream when possible.

    We will restructure the Ledger.Tx.CardanoApi module in plutus-ledger and move functions in cardano-api-extended.

    We will enhance the plutus-contract emulator by being able to balance and submit cardano-api transactions.

    We will modify the plutus-contract emulator to fully use the cardano-ledger transaction validation rules, and we will remove our custom validation rules (module Ledger.Index in plutus-ledger)."
"The following code samples were executed with cabal repl plutus-ledger on the plutus-apps commit hash 172873e87789d8aac623e014eff9a39364c719ae.

Currently, the plutus-ledger-constraint library has the MustValidateIn constraint which

    validates that a given POSIXTimeRange` contains the TxInfos validity range

    creates a transaction with the provided POSIXTimeRange

The implementation of 1) is trivial. However, a major issue arises for the implementation of 2). Setting the validity interval of a Cardano transaction is done by specifing the slot of the lower bound and the slot of the upper bound. Therefore, the MustValidateIn constraint needs to convert the provided POSIXTimeRange to essentially a (Maybe Slot, Maybe Slot). The problem is that there are many ways to convert a POSIXTime to a Slot.

Currently, provided a POSIXTimeRange, plutus-contract does the following:

    convert the time range to a slot range with Ledger.TimeSlot.posixTimeRangeToContainedSlotRange :: POSIXTimeRange -> SlotRange

    convert the SlotRange to (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) (essentially a (Maybe Slot, Maybe Slot))

The issue with these conversion is that the POSIXTimeRange and SlotRange intervals are type synonyms of the Plutus.V1.Ledger.Api.Interval.Interval a datatype which has has a Closure flag for each of the bounds.

Therefore, the conversions yields a discrepency when cardano-ledger converts the (Cardano.Api.TxValidityLowerBound, Cardano.Api.TxValidityUpperBound) to a POSIXTimeRange when creating the TxInfo.

Lets show some examples to showcase the issue.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound (Finite 999) False) (UpperBound PosInf True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound (Finite (Slot {getSlot = 0})) False, ivTo = UpperBound PosInf True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityLowerBound ValidityLowerBoundInBabbageEra (SlotNo 1),TxValidityNoUpperBound ValidityNoUpperBoundInBabbageEra)

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound (Finite 1000) True) (UpperBound PosInf True))

In practical reasoning, LowerBound (Finite 999) False and LowerBound (Finite 1000) True are equal considering the precision of 1000 milliseconds per slot. However, given Interval semantics, these are not the same values. Therefore, if the constraint mustValidateIn interval is used both to create a transaction and inside a Plutus script (corresponds to the check interval `contains` txInfoValidRange scriptContextTxInfo), then the Plutus script will yield False.

We can identify a similar behavior with the upper bound.

> let sc = SlotConfig 1000 0
> let interval = (Interval (LowerBound NegInf True) (UpperBound (Finite 999) True))
> let r = posixTimeRangeToContainedSlotRange sc interval
> r
Interval {ivFrom = LowerBound NegInf True, ivTo = UpperBound (Finite (Slot {getSlot = 0})) True}
> let txValidRange = toCardanoValidityRange r
> txValidRange
Right (TxValidityNoLowerBound,TxValidityUpperBound ValidityUpperBoundInBabbageEra (SlotNo 1))

When creating the TxInfo, cardano-ledger will convert the previous cardano-api validity slot range to:

(Interval (LowerBound NegInf True) (UpperBound (Finite 1000) False))

Again, a Plutus script with interval `contains` txInfoValidRange scriptContextTxInfo will yield False.

Additionnaly, the current behavior makes it hard to reason about how a POSIXTime gets translated into a Slot when creating a transaction. Ultimately, a DApp developer should have control over how his POSIXTime gets translated to a Slot.","We will create the following datatype:

    -- | ValidityInterval is a half open interval. Closed (inclusive) on the bottom, open
    -- (exclusive) on the top. A 'Nothing' on the bottom is negative infinity, and a 'Nothing'
    -- on the top is positive infinity.
    data ValidityInterval a = ValidityInterval
      { invalidBefore :: !(Maybe a) -- ^ Inclusive lower bound or negative infinity
      , invalidHereafter :: !(Maybe a) -- ^ Exclusive upper bound or positive infinity
      }

    We will add the following constraint and smart constructor:

    data TxConstraint =
      ...
      MustValidateInTimeRange !(ValidityInterval POSIXTime)

    mustValidateInTimeRange :: !(ValidityInterval POSIXTime) -> TxConstraints

    We will remove the MustValidateIn constraint and deprecate the the mustValidateIn smart constructor which will be replaced by mustValidateInTimeRange.

    We will create the smart constructor

    mustValidateInSlotRange :: !(ValidityInterval Slot) -> TxConstraints

    which will translate the provide validity slot range into a POSIXTimeRange using Ledger.TimeSlot.posixTimeRangeToContainedSlotRange."
"Currently, PAB users need to provide the SlotConfig in the configuration file, which is passed through to the Contract API, which users can use to convert between a Slot and a POSIXTime. However, the current SlotConfig representation supposes that the slot length is the same for all epochs in the Cardano blockchain, which is not the case. For example, during the Byron era, the slot length was 20s, while from Shelley era and onwards, the slot length is 1s. Therefore, the functions from the Ledger.TimeSlot module in plutus-ledger do not compute the conversion between Slot and POSIXTime the right way. The current easiest way to compute the time conversions is to query the local Cardano node on the consensus layer, which requires the ouroboros-consensus dependency.","We will deprecate the Ledger.TimeSlot.SlotConfig type and all functions in the Ledger.TimeSlot module using the SlotConfig. The only viable functions are the ones that convert between Data.Time types and plutus types (types related to TxInfo).

    We will copy the Ledger.TimeSlot module in the emulator (ideally rename it) and keep it as an internal module. Any functions not used by the emulator will be removed.

    We will move the Ledger.Params module inside the emulator as an internal module and modify the Params datatype name to EmulatorParams.

    We will modify the Plutus.Contract.Request.getParams function to Plutus.Contract.Request.getProtocolParameters. This implies modifying the name of Contract effect GetParamsReq/GetParamsResp.

    We will create two pairs of effects in Plutus.Contract.Effect:

    data PABReq =
      ...
      | SlotToUTCTimeIntervalReq SlotNo
      | UTCTimeToSlotReq UTCTime
      ...

     data PABResp =
      ...
      | SlotToUTCTimeIntervalResp (UTCTime, SlotLength) -- An alternative can be (UTCTime, UTCTime)
      | UTCTimeToSlotResp SlotNo
      ...

    We will implement the emulator effect interpreter by simply using the SlotConfig for the conversions.

    We will implement the PAB effect interpreter by using the local node. There are multiple steps to implement this:

        At startup, the PAB will query the EraHistory from the local node and store it in its local environment.

        We will implement the PAB interpreter by using the EraHistory alongside the consensus functions wallclockToSlot and slotToWallclock. Heres an example function of how to use them:

        -- Calculate slot number which contains a given timestamp
        utcTimeToSlotNo
          :: SystemStart
          -> EraHistory CardanoMode
          -> Time.UTCTime
          -> Either PastHorizonException SlotNo
        utcTimeToSlotNo systemStart (EraHistory _ interpreter) time = do
          let relativeTime = toRelativeTime systemStart time
          (slotNo, _, _) <- interpretQuery interpreter $ wallclockToSlot relativeTime
          pure slotNo

        slotStart
          :: SystemStart
          -> EraHistory CardanoMode
          -> SlotNo
          -> Either PastHorizonException Time.UTCTime
        slotStart systemStart (EraHistory _ interpreter) slotNo = do
          (relativeTime, _) <- interpretQuery interpreter $ slotToWallclock slotNo
          pure $ fromRelativeTime systemStart relativeTime

        However, we will also add an additional step. If the conversion returns PastHorizonException, then there is a good probability that the EraHistory is out of date. The reason is that EraHistory only encodes era information from the moment the user ran the query and it cannot predict the future. In that case, if the PastHorizonException is returned, we will re-query the EraHistory of the local node, replace the old value in the PAB environment, and retry the conversion. If it fails again, we return the error message."
"The current plutus-contract Haskell package allows developers to write Plutus applications using the Contract Monad. On top of that, the package contains a Plutus application contract emulator, a way to run those contracts on a simulated environment so that they can be tested. The emulator in question contains multiple components like the testing framework (including the ContractModel), the wallet emulator, a chain-index emulator and the node emulator.:

 +-------------------+     +-------------------+     +-----------------+
 | Testing framework |<----+ Contract Emulator +---->| Wallet emulator |
 +-------------------+     +-----+--------+----+     +--------+--------+
                                 |        |                   |
                                 |        |                   v
+----------------------+         |        |           +---------------+
| Chain index emulator |<--------+        +---------->| Node emulator |
+----------------------+                              +---------------+

The main reason we cant use a real wallet or node backend is because they are not fast enough to be able to run many test cases using property tests with the ContractModel.

Now, we believe the node emulator to be a useful separate component that other testing framework can leverage for being able to write fast test cases.","We will create a new Haskell package named cardano-node-emulator.

    We will move node related functionality into this new package. Here are some modules (or parts of a module) that will need to be moved over.

        The Ledger.Validation module which validates transactions using cardano-ledger should only be used by cardano-node-emulator and should not be exposed.

        The Ledger.Fee module, which calculates the fees for a given transaction, should be internal to cardano-node-emulator.

        The Ledger.Generators module, which contains generators for constructing blockchains and transactions for use in property-based testing, should be internal to cardano-node-emulator.

        The Ledger.TimeSlot.SlotConfig datatype should only be used by the cardano-node-emulator. An end user should not use this representation in a real world scenario. See ADR 12: Time conversion semantic change for more details.

        The Wallet.Emulator.Chain module in plutus-contract should be moved in cardano-node-emulator.

        The Ledger.Params which allows to configuration the network parameters should be moved over to cardano-node-emulator."
"End-to-end testing of Plutus Core functionality is currently performed by a combination of automation and exploratory approaches, both are performed on public preview and pre-prod testnets using a real node. Automation test scenarios for Plutus are currently being run as part of the wider cardano-node-tests test suite, which uses a Python wrapper for cardano-cli. Those tests focus on general ledger/node/cli functionality and only cover a few key scenarios for Plutus functionality, such as TxInfo and SECP256k1 builtins.

There is also ongoing development work to separate the functionality of cardano-api out into two packages:

    cardano-ledger-api handles the building and balancing of transactions.

    cardano-node-client will live in cardano-node and handle the submitting of balanced transactions and querying the ledger state.

Both of these packages are in early stage development and will require end-to-end test coverage.

This document outlines the decisions and arguments for an additional approach to end-to-end test automation using a framework written in Haskell.

The exploratory testing approach is not in the scope of this document.","We will create a new end-to-end testing framework written in Haskell called plutus-e2e-tests that will initially be a package in plutus-apps, see argument 1.

    We will use cardano-testnet for configuring and initialising local test network environments, see argument 2.

    We will initially use cardano-api for building and balancing transactions, and to submit balanced transactions and for querying the ledger, see argument 3.

    When available, we will use cardano-ledger-api instead of cardano-api for building and balancing transactions, see argument 4.

    When available, we will use cardano-node-client instead of cardano-api to submit balanced transactions and for querying the ledger state to make test assertions, see argument 5.

    We will prioritise Plutus test coverage over cardano-node, see argument 6.

    We will start by creating a few tests with the node/ledger apis without depending on plutus-apps and then assess whether we want to use the Contract API and other off-chain tooling going forwards, see argument 7.

    We will continue adding a subset of Plutus tests to cardano-node-tests, see argument 8."
"IOG is undertaking a company-wide effort to restructure and standardize its repositories, favoring mono-repos and enforcing shared GitOps and DevOps processes. Parallel to this, a new CI infrastructure is being developed.

Examples of this are:

    input-output-hk/cardano-world

    input-output-hk/ci-world

    input-output-hk/atala-world

This initiative appears to be championed by the SRE team who are the creators of divnix/std. Indeed std is at the heart of the standardization dream.","Standardization of the repositories has been deemed a worthwhile endeavour, though of very low priority.

    Phase 1 of the standardization process will be carried out in parallel with Move Marconi to a separate repository. A separate repository will be created for Marconi, and from the very beginning it will use std. This way the benefits, limitations and integration costs of std can be experienced and measured, and an informed, definitive decision on standardizing plutus-core and plutus-apps themselves can be made."
"Marconi is a Haskell executable and library that lives in plutus-chain-index.

It is desirable to move it into a separate repository for the following reasons:

    Better visibility and easier to discover

    It wants to update the version of its cardano-api dependency independently of the version used by plutus-apps

    It is a farily independent component, therefore it warrants its own repository

However, creating a separate repository would be rather costly. It would involve a great deal of duplication, due to the way our current nix code is structured, not to mention the added complexity and overhead inherent in maintaining a separate codebase.","We will put Marconi in a separate Github repository

    Until we resolve the issues with creating a separate Github repository (see Context), we will keep Marconi as a separate project in plutus-apps"
"Lets start with the problematic example (copy-paste of the current PubKey contract in plutus-use-cases).

-- | Lock some funds in a 'PayToPubKey' contract, returning the output's address
--   and a 'TxIn' transaction input that can spend it.
pubKeyContract
    :: forall w s e.
    ( AsPubKeyError e
    )
    => PaymentPubKeyHash
    -> Value
    -> Contract w s e (TxOutRef, Maybe ChainIndexTxOut, TypedValidator PubKeyContract)
pubKeyContract pk vl = mapError (review _PubKeyError   ) $ do
    -- Step 1
    let inst = typedValidator pk
        address = Scripts.validatorAddress inst
        tx = Constraints.mustPayToTheScriptWithDatumHash () vl
    ledgerTx <- mkTxConstraints (Constraints.typedValidatorLookups inst) tx
               >>= submitUnbalancedTx . Constraints.adjustUnbalancedTx

    -- Step 2
    _ <- awaitTxConfirmed (getCardanoTxId ledgerTx)

    -- Step 3
    let refs = Map.keys
               $ Map.filter ((==) address . txOutAddress)
               $ getCardanoTxProducedOutputs ledgerTx
    case refs of
        []                   -> throwing _ScriptOutputMissing pk
        [outRef] -> do
            -- Step 4
            ciTxOut <- unspentTxOutFromRef outRef
            pure (outRef, ciTxOut, inst)
        _                    -> throwing _MultipleScriptOutputs pk

Heres an outline of the contracts steps:

    Creates a transaction and submits it to the node

    Waits for transaction to be confirmed

    Finds the first UTXO of that transaction (return type TxOutRef)

    Queries the plutus-chain-index to get the ChainIndexTxOut out of that TxOutRef

The problem is that the ciTxOut variable in step 4 will almost always result in Nothing.

Why? Heres some context.

The PAB listens to the local node and stores blockchain information in memory such as the status of transactions, the status of transaction outputs, the last synced slot, the current slot, etc., in a variable of type BlockchainEnv. The awaitTxConfirmed is actually querying the state of BlockchainEnv and waits until the status of the transaction transitions to Confirmed.

Meanwhile, plutus-chain-index (our main indexing component at the time of this writing) is also listening to incoming blocks from the local node and indexes them into a database. The indexed data can be queried using the REST API interface.

This brings up the main issue: the PAB and plutus-chain-index each listen to the same source of information (a local Cardano node), but each index the information at different speeds. For a dApp developer writing off-chain code using the Contract API, there is no abstraction for handling multiple sources of truth.

Currently, in the best case scenario (fully synced PAB and plutus-chain-index), plutus-chain-index will always trail behind the in-memory storage of the PAB by a few seconds. Therefore, even in this scenario, querying the plutus-chain-index with unspentTxOutFromRef in the above contract has a high probability of returning Nothing.","The best solution is probably a combination of the Alternative solutions described below. However, we will mainly choose the Query functions should interact with a single source of truth solution.

    We will replace plutus-chain-index with Marconi as PABs indexing component

    We will move out the blockchain information indexed by PAB in Marconi

    We will add new indexers in Marconi in order to replicate the information indexed by plutus-chain-index

    We will adapt the architecture of Marconi (which will become our new indexing component) to support waiting queries

    Since we suppose that indexing component should be in the same machine as the PAB, then we will use Marconi as a library to index and query the indexed blockchain information without relying on an HTTP API"
"There are multiple implementations of a Plutus Application Backend (PAB) external of IO Global, and also other tools related to Plutus smart contracts. Some of them are using the same contract interface as the official implementation, but some of them use a different interface. However, as the ecosystem evolves, it would be beneficial to create a well defined standard, that other off-chain tools can use as a reference, or as an interface to implement.

Currently, as we are getting close to the Vasil hardfork, testing tools and Plutus Application backend tools are at a hurry to update their dependencies and get to a Vasil compliant/compatible state. However, tools that are depending on plutus-apps are blocked by the PAB development. This initiative was born out of this context, but could solve other problems as well.

The Contract API (defined in plutus-apps/plutus-contract) is using the freer-simple effect system to define all the contract effects. This already allows us to separate the interface from the implementation, and to have multiple implementations/interpreters for one interface. Currently, there are two implementations for the Contract API:

    one for the plutus-apps emulator (inside plutus-apps/plutus-contract)

    one for plutus-apps Plutus Application Backend (inside plutus-apps/plutus-pab)

Therefore, we can leverage this separatation of interface and implementation in order to move the interface out of plutus-apps.","We will split the plutus-apps/plutus-contract package into two parts: the Contract API (plutus-contract) and the emulator (plutus-contract-emulator).

    We will create effects for the constraints-based transaction builder library (plutus-apps/plutus-ledger-constraints) in the Contract API. Currently, the interface and the implementation in the transaction builder library are tightly coupled. Therefore, we need to decouple them.

    We will create a separate repository with the contract effects and types (the splitted plutus-contract). By moving the Contract API out of the plutus-apps monorepository, any tool could update to newer version to their discretion. Without many dependencies, many tools could utilize the Contract API without having to depend on the whole plutus-apps monorepo.

    We (the Plutus Tools at IO Global) will continue to be the main maintainers of this new repository. However, a new ADR will need to be created if we ever decide to make this a community driven project.

    TODO: What about governance? How do we decide which interface changes are accepted? ADRs? Who ultimately accepts and rejects them?"
"After the Vasil HF, the Cardano blockchain will support reference inputs by adding a new field in the transaction data type. With reference inputs, transactions can take a look at UTXOs without actually spending them.

Thus, we need to adapt our transaction constraint data type (TxConstraints) to support referencing UTXOs.","We will add the data constuctor MustReferenceOutput TxOutRef to the TxConstraints data type.

    The PlutusV1 on-chain implementation of this new constraint will simply return False. However, cardano-ledger throws a phase-1 validation error if transactions that use the some of the new features (reference inputs, inline datums and reference scripts) try to execute PlutusV1 scripts. See the Babbage era ledger specification. Therefore, the only way to get a phase-2 validation error would be to use this constraint on-chain in a PlutusV1 script, without using any of the new Babbage era features off-chain.

    The PlutusV2 on-chain implementation of this new constraint will check that the provided TxOutRef is part of the ScriptContexts reference inputs."
"In Babbage era (available after the Vasil HF), the Cardano blockchain will support inline datums by changing the TxOut data type.

In Alonzo era, a TxOut was able to store arbitrary data called the datum. However, only the hash of the datum was stored, not the actual datum.

With inline datums available in Babbage era, transaction outputs can either contain the hash of the datum or the actual datum. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","We will replace the Datum parameter in TxConstraintss data constructor MustPayToPubKeyAddress with Plutus.V2.Ledger.Api.OutputDatum. In the offchain implementation of the constraint, we will use this new data constructor parameter to support either adding the datum in the datum witness set (by using the datum lookups to resolve the hash) or inline it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if the datum value matches OutputDatum Datum because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.

    We will modify the data constructor interface, on-chain implementation and off-chain implementation of MustPayToOtherScript similarly to MustPayToPubKeyAddress.

    We will modify the off-chain implementation of the data constructor MustSpendScriptOutput in order to support inline datums. Currently, the script outputs datum is added in the transactions datum witness set. However, if the datum is inlined in the script output, then it is already witnessed. Therefore, we dont need to add it in the datum witness set."
"In Babbage era (available after the Vasil HF), the Cardano blockchain will support reference scripts by changing the TxOut data type. Reference scripts are used to attach arbitrary scripts to transaction outputs and are used to satisfy script requirements during validation, rather than requiring the spending transaction to do so. Thus, we need to adapt our transaction constraint data type (TxConstraints) to support this new feature.","We will add Maybe ScriptHash as a new data constructor parameter for the constraints MustPayToPubKeyAddress, MustPayToOtherScript, ScriptOutputConstraint in TxConstraints. In the off-chain implementation of those constraints, if a reference script hash is provided, we will need to find the actual script in the lookups table so that we can include it in the transaction output. In the PlutusV1 on-chain implementation of the constraint, we will return False if a reference script is provided because the ledger forbids using Babbage era features with PlutusV1. The PlutusV2 on-chain implementation of the constraint is trivial.

    We will modify the off-chain implementation of MustSpendScriptOutput and ScriptInputConstraint in order to add support for witnessing a script by actually providing it, or by pointing to the reference input which contains the script."
"In Babbage era (available after the Vasil HF), Cardano transactions will contain new collateral related fields: return collateral and total collateral collateral. Return collateral (also called collateral output) and total collateral are detailed in CIP-40.

In summary, return collateral is a special output (basically of type TxOut) that becomes available in case there is a failed phase-2 validation. In addition, we have the new total collateral field which explicitly says how much collateral (in lovelace) is going to be actually consumed in the case of phase-2 validation failure.","We will add the txReturnCollateral and the txTotalCollateral fields in the Ledger.Tx.Internal.Tx data type.

    We will modify the Wallet.Emulator.Wallet.handleBalance function in plutus-contract to set the correct return and total collateral for an UnbalancedTx (of type Either CardanoBuildTx EmulatorTx). In either type of transaction, we would compute the txTotalCollateral while estimating the fee with the formula quot (txfee txb * (collateralPercent pp)) * 100 and then set txReturnCollateral with the formula sum collateralInputs - txTotalCollateral."
We want to record architectural decisions made in this project. Which format and structure should these records follow?,"Chosen option: ""MADR 2.1.0"", because
Implicit assumptions should be made explicit.Design documentation is important to enable people understanding the decisions later on.See also A rational design process: How and why to fake it.The MADR format is lean and fits our development style.The MADR structure is comprehensible and facilitates usage & maintenance.The MADR project is vivid.Version 2.1.0 is the latest one available when starting to document ADRs."
"We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.
Decision Drivers
Low complexity and overhead in development.Fit for our stack.Optimize CI/CD with dependency graphs and/or caching.Flexible.","Chosen option: ""Nx"", because:
It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).It's relatively easy to learn with focused documentation.It has schematics to generate apps, libraries and components that includes all of our tools.It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended."
"We need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a monorepo we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.
Terms
code integration - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.artefact building - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after code integration process finishes successfully.continuous integration - the practice of running the code integration process triggered by events such aspushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.We also run the artefact building process after a successful code integration process to have artefacts ready for deployment at all times.continuous integration platform (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your continuous integration and publish the results
pushing new revisions of the code to the code repositoryopening a Pull Requestfixed scheduleetc.
Decision Drivers (Policy)
The code integration process needs to be independent from CI platform integrationBenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experienceWe use Docker as much as possible to implement the steps in the integration processBenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in generalWe only build the code affected by the change but re-tag all unchanged code artefactsBenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)We support only Linux as a target operating system when we cannot use DockerBenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
BenefitsEasier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development processDrawbacksNeeds more knowledge and experience
Easier troubleshooting and development of the processEasier migration to a different CI platformEasier experimentationEasier to run as part of the development process
Needs more knowledge and experience
BenefitsPlatform independenceRepeatabilitySecurityDrawbacks:Needs expertise in Dockerfile and Docker in general
Platform independenceRepeatabilitySecurity
Needs expertise in Dockerfile and Docker in general
BenefitsBe able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideologyDrawbacksCan be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hashSupports the monorepo benefits and ideology
Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)
BenefitsSame as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and WindowsDrawbacks:Devs that use non-Linux OS might need to install additional software and customizations
Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differencesWe minimize the effort and complexity of supporting different operating systemsLinux tooling works on all major other OSs today - macOS and Windows
Devs that use non-Linux OS might need to install additional software and customizations
CI Platform ","GitHub Actions
Number 1 CI platform on GitHub at the time of this writingEasy customization of which parts of the CI process to run depending on branching patterns and pull requestsGood integration of code health with the pull request processAs a GitHub open-source project, we have an unlimited number of ""compute""-minutes that come as a part of the packageSupports parallelisation of the process which can be pretty important in the context of monorepoSupport using own runners which can be helpful to maximize speed, minimize costs and increase security."
"We're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?
Decision Drivers
Should be performant, with code splitting, caching and minimal runtime overhead.Needs to have easy access to our design system constants. These should optimally be shared with JS logic.Should be type-safe to catch issues when refactoring.Reusable components should be closed, not accepting arbitrary styles/classes.We want a pattern for responsive props with atomic layout components.","Chosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.
We'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.
Example:
// Good:<Box padding""small"" /><Box padding={{xs: 'small', md: 'medium'}} /><Input large /><Text preset=""heading3"" as=""p"" />
// Bad:<Box className={customLayout} /><Input style={{ height: 50, padding: 16 }} /><Text className={styles.heading} />
Positive Consequences
Treat is statically extracted at build time, so it has minimal runtime.Styles load in parallel with JS, also when code splitting.Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.
Negative Consequences
We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles."
"How do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?
Decision Drivers
We need to have confidence in our releases.We want more structured releases while we're still getting our footing in a shared monorepo.We need simplicity and clear takt time so different teams can plan for what is going out the door from them.It should work well with our agile work environment.","Chosen option: ""OneFlow"" because it provides a single eternal branch with well structured releases.
We'll implement OneFlow with these details:
Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.Release and quality managers from each team are responsible for reviewing and approving releases.Releases apply to all apps in the monorepo.Releases are versioned like this: {cycle}.{sprint}.{hotfix}. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.Feature branches are merged using ""Squash and merge"", so they can be easily reverted.There are two ways to build larger features.If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.
If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.
Future strategy
With time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.
Hosting environments
We'll set up continuous delivery to different hosting environments:
EnvironmentGit sourceDatabases/servicesFeaturessandboxfeature branchTestAlldevmainTestAllstagingmainProdAllpre-prodrelease/hotfix branchProdFinishedprodlatest release tagProdFinished
We'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage."
"Know it before they do! We need a tool to discover, triage, and prioritize errors in real-time.","Chosen option: Sentry, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it."
"Viskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?
Decision Drivers
Should use NodeJS and React as outlined in S technical directionShould be able to support markdown content rendered to HTMLShould be open sourceShould be customizable to island.is UI design","Chosen option: NextJS + NestJS
NextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using Remark library.
NestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing."
"What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.
Decision Drivers
SecureWell defined and well reviewed standardEasy to implement by client and resource systemsSupport for non web client systems i.e. mobile devices","Chosen option: ""OAuth 2.0 + OpenID Connect"", because it is secure and well examined and and has support libraries for our tech stack."
"As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.
Decision Drivers
Provide consistency when navigating the codebaseThe earlier we decide on this, the better","Chosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.
Naming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of islandUi: import { Box } from '@island.is/island-ui/core'
Use PascalCase for directories only containing React components:
components/CtaButton/index.tsimport 'components/CtaButton'
or:
components/CtaButton/CtaButton.tsximport 'components/CtaButton/CtaButton'
rather than
components/cta-button/CtaButton.tsx
In all other cases, use camelCase.
Positive Consequences
Easier to navigate the codebaseFile names are more readable, and developers know what to expectThis approach is the most common practice, and something most JS and TS developers are familiar with."
"island.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.
Agencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.
Which CMS system would best suit the needs of island.is?
Decision Drivers
Content needs to be editable by non technical usersContent needs to be accessible across multiple domains and platformsSetup should be simple for developers new to the projectThe system should manage flexible content structures to limit systems impact on designThe system should be user friendly and easy to use for a non technical personThe system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute","Devs narrowed the choice down to two options Contentful and Contentstack.
Both systems meet the required featureset.
A decision from management was made to use Contentful. Contentful is deemed to have a larger presence in the Icelandic dev community. Contentful is also believed to have a stronger funding base. Contentful is already implemented in some of our projects."
"It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.
There are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).
Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.
Decision Drivers
The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.It is important to build on the experience of similar government led inititives in other countries.Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.","The MIT license was chosen, for the following reasons:
It is the least restrictive of the licenses.It is very consise, simple and easy to understand and therefore should be clear to users and developers.Digital Iceland does not require protection of patents or existing intelletual property.Well known government lead initiatives like uk.gov and X-Road use the MIT license.The MIT license is the best known and most widely used free and open-source license in the world."
"Multiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.
Requirements
The charting library should:
support rendering all standard charts, i.e. bar, line, pie,support custom styling of elements (colors, fonts, tooltips, legends, axis)support lazy/dynamic loading to minimize js bundlesTypescript support
Decision Drivers
Meet all requirements listed aboveAPI qualityPricingBundle sizeTypescript support","Chosen option: ""Recharts"", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.
Positive Consequences
We can start implementing charts and graphs as needed in island.is
Negative Consequences
It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase."
"We want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.
Decision Drivers
Ease of setupEase of maintenanceCostDeveloper experienceUsability/UXOperational concernsHandling of PII","Chosen option: ""ConfigCat"", because:
We can probably get away with using it for very low costWe can start using it almost right away with little configuration
If we decide later that we would like some of the features of LaunchDarkly, we want to be able to quickly swap. Thus, it is vital that we write some kind of service-agnostic wrapper.
Positive Consequences
We can start using feature flags across our stack.
Negative Consequences
Complexity of applications will increase"
"We need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.
Decision Drivers
ReliabilityFeature-richnessCostMaintenanceDevelopmentVendor lock level","SaaS solution (DataDog) is a clear winner due to its hastle-free service usage, continuous improvement as well as its competitive pricing.
Positive Consequences
Easy access to observability for devs and devopsWell known integration target for third-party servicesMonitoring setup is well known in the industryNo special knowledge about setup of the plaftorm
Negative Consequences
None."
