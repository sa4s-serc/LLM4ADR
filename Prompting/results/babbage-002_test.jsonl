{"File Name":"cdh-adrs\/0010-etl_tenancy_relationship.md","Context":"## Context\\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\\n","Decision":"We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\\n","tokens":160,"id":0,"Predictions":"## Decision Description\\nThis decision is to allow for the addition of a new tenant to PriveXML. The Tenant Id will be used as a relationship marker for the tenant to be added. The PriveXML will then be able to read the tenant information from the data feed and update itself accordingly. The PriveXML will then be able to create a new event with the newly added tenant information.\\n\\n## Decision Explanation\\nThis decision enables us to add a new tenant to our privesxmlevents database. We need to add a new column in the table that is able to hold the tenant id. This would allow us in future to create other columns that would hold additional information about the entity.\\n\\n## Decision Process\\nThe process for adding a new tenant is as follows:\\n1) Create a new column in our privesxmlevents table with the name TenantId and set it as an identity type that will be auto-generated by our application.\\n2) Create a new column in our privesxmlevents table with the name TenantName and set it as an identity type that will be auto-generated by our application.\\n3) Create a new column in our privesxmlevents table with the name CompanyId and set it as an identity type that will be auto-generated by our application.\\n4) Create a new column in our privesxmlevents table with the name AccountId and set it as an identity type that will be auto-generated by our application.\\n5) Add this new column into your schema, which should look something like this:\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTenant Name\\tnetid\\tnetname\\tnetcompany\\tnetaccount\\n6) Add this new column into your schema, which should look something like this:\\n    \\t\\t\\t\\tTenant Id\\tnetid\\n7) Add this new column into your schema, which should look something like this:\\n    \\tTenant Id\\tnetid\\n8) Add another column into your schema, which should look something like this:\\ntenant id\\tnetid\\n9) Create another table called \"Tenant\" which should have columns named \"Company\", \"Account\", \"AccountId\", \"TenantID\", \"CreatedBy\" and \"CreatedDate\". This would allow us in future to add other columns that would hold additional information about the entity.\\n10) Create another table called \"Company\" which should have columns named \"Company Name\", \"Address\", \"City\", \"State\", \"Zip Code\", etc. This would allow us in future to add other columns that would hold additional information about the entity.\\n11) Write some code in your application (which may or may not use stored procedures\/functions\/etc.) that accepts these 3 tables (as input), sets up some data, adds them together and creates some metadata (which may or may not use stored procedures\/functions\/etc.). This code could also manipulate these 3 tables in some way such as inserting values into them or deleting them or updating them. It could also insert values into other tables such as inserting values into tables called \"History\" or \"Transactions\". It could also delete data from any of these tables. It could add data into any of these tables or delete data from any of these tables. It could also update any of these tables based on information provided by other tables such as inserting data based on user inputs or updates based on user inputs (such as date changes).\\n12) Run your application against one of our test databases (which should have all 3 tables created above). This test database can either be run on its own or can be run against one of our production databases (which has all 3 tables created above).\\n13) Run your application against one of our development databases (which has all 3 tables created above).\\n14) Test your application against one of our production databases (which has all 3 tables created above).\\n15) Test your application against one of our test databases (which has all 3 tables created above).\\ntotal time: ~5 hours\\n16)\\tPlease note: If you are creating a new tenant using this decision, you must ensure you are using exactly those columns we have provided above in order for us to correctly import your data when you create another one using this decision.\\ntotal time: ~20 minutes\\n17)\\tIf you are creating multiple tenants using this decision, please make sure you provide at least two different Tenant Id's so we can import both at once rather than having multiple tenants imported at once.\\ntotal time: ~20 minutes\\n18)\\tMake sure you provide us with sample outputs from each call made by your application so we can validate whether we are importing correctly.\\ntotal time: ~20 minutes\\n"}
{"File Name":"cdh-adrs\/0011-pentaho.md","Context":"## Context\\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\\n","Decision":"Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\\n","tokens":52,"id":1,"Predictions":"## The decision for this record is to use the CDH ETL feed parsers which are based on the standard library to convert the legacy feeds into PriveXMl format. These will be used in conjunction with the existing Pentaho ETL data transformations to provide a seamless migration of data from legacy feeds into PriveXMl format.\\n## Support\\n## Support for this Decision is provided by Pentaho.\\n## Notes\\n## Notes are provided for this Decision.\\n"}
{"File Name":"cdh-adrs\/0009-etl_feed_synchronization_and_delivery.md","Context":"## Context\\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\\nmore so this is being done to allow us trigger data feed processing from the dev admin.\\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\\n","Decision":"- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\\nbased on update events from either the uploader service, or the windows-specific uploader jars.\\nThe following operations then becomes possible:\\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\\n2. Ability to query for a specific batch of files related to specific data feeds.\\n3. Ability to get data files specific to a given owner and\/or provider.\\n4. Ability to get data files specific to a given provider.\\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\\nETL service as regards new files.\\nSee\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":73,"id":2,"Predictions":"## We will move files from Archive folders into bulkupload folders. This will be done using a set of scripts that will be run on an hourly basis. Each script will move the files from the archive folders into the bulkupload folders and then trigger the data feed processing.\\n## Context\\nWe need to reduce the surface level access to the ETL team which is currently done in a variety of ways. We currently have a set of scripts that will move files from archive folders into bulkupload folders and then trigger data feed processing.\\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\\n\\n## Decision\\n## We will use a script that moves files from archive folders into bulkupload folders and then triggers data feed processing.\\n\\end{itemize}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision:} \\textit{We will use a script that moves files from archive folders into bulkupload folders and then triggers data feed processing.}\\n\\end{itemize}\\n\\n\\subsection{Storage}\\nThe storage solution is based on S3, RDS, and DynamoDB. The storage solution has two components: The first is an IAM role that allows us to create buckets, policies, and objects within S3. The second is an IAM role that allows us to create tables in DynamoDB.\\n\\n\\subsubsection{IAM Role}\\nThe IAM role used for storage is called AWS::S3::BucketAdminRole. This role allows us to create buckets, policies, and objects within S3.\\n\\n\\subsubsection{IAM Role Permissions}\\nThe IAM roles are configured through AWS Config. Configuring these roles requires specifying which permissions are required for each role.\\n\\n\\subsubsection{Storage Bucket Permissions}\\nThe storage bucket permissions are configured through AWS Config as well. Configuring these permissions requires specifying which permissions are required for each bucket.\\n\\n\\subsubsection{Storage Policy Permissions}\\nThe storage policy permissions are configured through AWS Config as well. Configuring these permissions requires specifying which permissions are required for each policy.\\n\\n\\subsubsection{DynamoDB Role Permissions}\\nDynamoDB role permissions are configured through AWS Config as well. Configuring these permissions requires specifying which permissions are required for each table.\\n\\n\\subsubsection{DynamoDB Table Permissions}\\nDynamoDB table permissions are configured through AWS Config as well. Configuring these permissions requires specifying which permissions are required for each table.\\n\\n\\section*{\\textbf{\\large Storage Usage}}\\nAll storage usage is tracked within Amazon Simple Storage Service (Amazon S3). Amazon S3 is used as our primary storage repository and DynamoDB as our secondary database repository.\\n"}
{"File Name":"cdh-adrs\/0004-cdh-tenants.md","Context":"## Context\\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\n4. Processing of reconciliation request on tenant accounts and positions.\\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\\n","Decision":"Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\\nThe following is expected:\\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\\n![CDH Architecture](..\/assets\/images\/aab_workshop\/aab_5.jpg)\\n","tokens":183,"id":3,"Predictions":"## We have decided to move the client data hub to a separate service for better scaling and API management. The functionality of the client data hub will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\n4. Processing of reconciliation request on tenant accounts and positions.\\nThe following ETL processes will be moved from the monolith into an [ETL Service](.\/etl.md):\\n1. Data Feed Parsing\\n2. Position Parsing\\n3. Reconciliation\\n4. Data Movement\\n## Context\\n## Monolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\nThe following ETL processes will be moved from the monolith into an [ETL Service](.\/etl.md):\\n1.Data Feed Parsing\\n2.Position Parsing\\n3.Reconciliation\\n4.Data Movement\\n## Decision\\n## We have decided to move the client data hub to a separate service for better scaling and API management.\\n"}
{"File Name":"cdh-adrs\/0006-etl_cdh_communication.md","Context":"## Context\\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\\n","Decision":"We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\\n![Event Queue](..\/assets\/images\/workflows\/image3.png)\\n","tokens":81,"id":4,"Predictions":"## 1. The ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\\n## 2. The ETL service must be able to consume the output of the CDH service.\\n## 3. The ETL service must be able to communicate with the CDH service.\\n## 4. The ETL service must be able to consume the output of the CDH service.\\n## 5. The ETL service must be able to consume the output of the CDH service.\\n## 6. The ETL service must be able to consume the output of the CDH service.\\n## 7. The ETL service must be able to consume the output of the CDH service.\\n## 8. The ETL service must be able to consume the output of the CDH service.\\n\\n### Decision Detail\\n\\nDecision:  \\n1:  \\n2:  \\n3:  \\n4:  \\n5:  \\n6:  \\n7:  \\n8:\\n\\n### Decision Details\\n\\nDecision Details:\\n\\n"}
{"File Name":"cdh-adrs\/0005-etl-monolith-migration.md","Context":"## Context\\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\\nhave specific constraints on how they are accessed and stored.\\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\\n","Decision":"ETL will be moved into an external service of it's own with the following responsibilities:\\n- Embodiment of all parser logic.\\n- Delivery of agreed parser format (currently PriveXML) into message queues.\\n- Standardized library for parsing delivery and logic.\\n- Standardized database tables for data feed file delivery and access.\\n- Standardized database tables for tenant data (Company, Client, Accounts).\\n- Creates tenant specific events for delivery for tenant specific datafeed.\\nAs regards data feed file access problem\\n- Manage synchronization of uploaded files events into database from uploader service.\\n","tokens":233,"id":5,"Predictions":"## The ETL service is to be moved out of monolith and into a separate service that is part of the AWS S3 environment. This will allow us to move the ETL process out of monolith, and allow us to break out the dependency on S3 from monolith.\\n* The ETL service will be implemented as a standalone service, not part of monolith. It will have its own process and data store.\\n* The ETL process will be managed in a separate database, however this database is not part of monolith, so we can easily move the data from monolith into this new database. This database would be hosted on AWS S3.\\n* Datafeeds are to be stored in a separate directory (or sub-directory) from the rest of the datafeed files. This directory structure allows for easy access and manipulation by any number of processes\\n* Processed datafeeds will be stored in the new database\\n* The new datafeed directory structure allows for easy access by any number of processes (e.g. web application, API etc).\\n* The old datafeed directory structure does not allow for easy access by any number of processes (e.g. web application, API etc).\\nThe decision was made to move the ETL process out of monolith because we have identified a need to break out the dependency on S3 from monolith - this is an architectural decision that cannot be reversed once made.\\n\\n\\n\\n\\n\\subsubsection{Record 2}\\n\\n\\begin{lstlisting}\\n## Record 2\\n## Record 2 contains: ## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{lstlisting}\\n"}
{"File Name":"cdh-adrs\/0001-feed-parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":6,"Predictions":"## 1. The parser should be able to read the data feed file and parse it into a data structure that can be used for processing, for example a data structure like a map. This data structure should contain all the information required to process the input of the User. The parser should also be able to query this data structure for more information if needed.\\n## 2. The parser should be able to parse the input and produce a set of output files that are compatible with the user's schema.\\n## 3. The parser should also be able to query the output files and produce a set of input files that are compatible with the user's schema.\\n\\n## Input File\\n## Input file is an XML file that contains all the required information for processing.\\n## Data Feed File\\n## Data feed file is an XML file that contains all the required information for processing.\\n## Schema\\nSchema is an XML file that contains all the required information for processing.\\n\\n## Output File\\nOutput file is an XML file that contains all the required information for processing.\\n```\\nOP 2019-12-18: I have got some good results by using this code:\\n```\\nimport json\\nimport requests\\n\\ndef get_data_feed():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance')\\n    return json.loads(response.text)\\n\\ndef get_schema():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-schemas')\\n    return json.loads(response.text)\\n\\ndef get_data_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds')\\n    return json.loads(response.text)\\n\\ndef get_schema_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-schemas')\\n    return json.loads(response.text)\\n\\ndef get_input_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-inputs')\\n    return json.loads(response.text)\\n\\ndef get_output_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-outputs')\\n    return json.loads(response.text)\\n\\n\\ndef main():\\n\\n    # Get all inputs and outputs from API\\n    inputs = []\\n    outputs = []\\n    \\n    # Get schema from API, then parse it into a dictionary\\n    schema_dict = {}\\n    \\n    # Get input feeds from API, then parse it into a dictionary \\n    input_feeds_dict = {}\\n    \\n    # Get output feeds from API, then parse it into a dictionary \\n    output_feeds_dict = {}\\n\\n    \\n# Create JSON object representing YAML format of our request\/response headers:\\nheaders_json_object = {\\n        'Content-Type': 'application\/json',\\n        'Authorization': 'Bearer %s' % token,\\n        'Accept': 'application\/json',\\n        'Accept-Encoding': 'gzip, deflate',\\n        'Connection': 'close',\\n        'Content-Length': str(len(input_feeds_dict)),\\n        'Host': 'api.mybanking.com',\\n        \"X-Auth-Token\": token,\\n}\\n\\n# Create JSON object representing YAML format of our request\/response body:\\nbody_json_object = {\\n        \"schema\": schema_dict,\\n        \"input_feeds\": inputs,\\n        \"output_feeds\": outputs,\\n}\\n\\n# Create JSON object representing YAML format of our request\/response headers:\\nheaders_json_object['Content-Type'] = headers_json_object['Content-Type'].replace(\"application\/json\", \"application\/octet-stream\")\\nheaders_json_object['Authorization'] = headers_json_object['Authorization'].replace(\"Bearer %s\", token)\\nheaders_json_object['Accept'] = headers_json_object['Accept'].replace(\"application\/json\", \"application\/json\")\\nheaders_json_object['Accept-Encoding'] = headers_json_object['Accept-Encoding'].replace(\"gzip, deflate\", \"text\/plain; charset=utf-8\")\\n\\n# Create JSON object representing YAML format of our request\/response body:\\nbody_json_object['schema'] = body_json_object['schema'].replace(\"xml\", \"\")\\nbody_json_object['input_feeds'] = body_json_object['input_feeds'].replace(\"xml\", \"\")\\nbody_json_object['output_feeds'] = body_json_object['output_feeds'].replace(\"xml\", \"\")\\n\\n# Create JSON object representing YAML format of our request\/response header:\\nheader_json_objec"}
{"File Name":"cdh-adrs\/0003-uploaders.md","Context":"## Context\\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\\n","Decision":"Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":135,"id":7,"Predictions":"## No\\nNo\\n\\n\\n\\n## Decision\\n## Yes\\nYes\\nYes\\nYes\\nYes\\n## Decision\\n## No\\nNo\\nNo\\nNo\\n## Decision\\n## Yes\\nYes\\nYes\\nto\\nto\\nto\\nto\\nto\\nto\\nto\\nto\\nto\\n## Decision\\n## No\\n\\n\\nThe above decision record (DR) is used to generate the following decision report (DRR):\\n```\\nDecision Report for the Monolith Software System with respect to the Software Architecture Decisions Record.\\n```\\nFigure 3.2 shows this DR. It is a summary of the software architecture decisions made by the Architectural Review Board (ARB). The DR shows which architectural decisions were made and why they are important. It also lists all of the software components that were affected by these decisions.\\nFigure 3.2: Monolith Software System Decision Report\\n\\nThe DR shows that the monolith is a monolithic application composed of a number of services, each with their own services and components, and that these services are all interconnected. This is illustrated in Figure 3.2, where the monolith component graph has been drawn in Figure 3.1. The monolith is based on a service-oriented architecture (SOA) model, which means that there are multiple services interacting with each other through an API called RESTful Web Services (RWS). This architecture has been adopted for a number of reasons, including scalability and flexibility.\\n\\nThe DR also shows that there are two different versions of the monolith application: one for testing and one for production. The testing version uses a single service called MonolithTestService, while production uses multiple services called MonolithService1, MonolithService2, etc., as illustrated in Figure 3.1.\\n\\nThe DR also shows that there are three different containers running on three different hosts: one container running on host1, one container running on host2, and another container running on host3.\\n\\nFinally, the DR shows that there are three different versions of all of these containers: one running version10 for testing purposes; another running version20 for testing purposes; and another running version30 for production purposes.\\n\\nThe DR also shows how these three containers interact with each other through an API called RESTful Web Services (RWS). In particular it shows how RWS is used to handle requests from clients to get data from S3 storage.\\n\\nIn summary:\\n\\nMonolithic system,\\nMonolithic application composed of several services,\\nEach service has its own services and components,\\nInterconnected through RESTful Web Services (RWS),\\nThree different versions:\\nOne testing version,\\nAnother testing version,\\nAnother production version;\\nThree different containers:\\nOne container running on host1,\\nOne container running on host2,\\nOne container running on host3;\\nAPIs used:\\nRESTful Web Services (RWS);\\nInteraction between containers:\\nAPIs used:\\nRESTful Web Services (RWS);\\nInteraction between containers:\\nAPIs used:\\nRESTful Web Services (RWS);\\n\\nFigure 3.2 illustrates how this information can be represented in a decision report.\\n"}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"Predictions":"## Decision\\nDecision: The ETL process will be moved to CDH, since it is a generic service and will require no specialized implementation. The feed specific logic will be moved to CDH as well, since it is a feed specific service and the logic will require continous change\/update.\\n## Inputs\\n## Input\\nThe following inputs are available:\\n- The decision number.\\n- The input type (see the table below).\\n- The input value (see the table below).\\n- The output type (see the table below).\\n- The output value (see the table below).\\n- A description of the input and output.\\n\\n\\n# Input #0 \\# Type \\# Value\\n1: \\# Decision \\# Decision: Move ETL to CDH\\n2: \\# Input \\# Input: Feed specific logic\\n3: \\# Input \\# Input: Feed specific logic\\n4: \\# Input #0\\#\\n## Output\\n## Output #0\\n"}
{"File Name":"cdh-adrs\/0008-etl_feed_parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":9,"Predictions":"## 1. The parser should be able to read the data feed file and parse it into a data structure that can be used for processing, for example a data structure like a map. This data structure should contain all the information required to process the input of the User. The parser should also be able to query this data structure for more information if needed.\\n## 2. The parser should be able to parse the input and produce a set of output files that are compatible with the user's schema.\\n## 3. The parser should also be able to query the output files and produce a set of input files that are compatible with the user's schema.\\n\\n## Input File\\n## Input file is an XML file that contains all the required information for processing.\\n## Data Feed File\\n## Data feed file is an XML file that contains all the required information for processing.\\n## Schema\\nSchema is an XML file that contains all the required information for processing.\\n\\n## Output File\\nOutput file is an XML file that contains all the required information for processing.\\n```\\nOP 2019-12-18: I have got some good results by using this code:\\n```\\nimport json\\nimport requests\\n\\ndef get_data_feed():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance')\\n    return json.loads(response.text)\\n\\ndef get_schema():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-schemas')\\n    return json.loads(response.text)\\n\\ndef get_data_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds')\\n    return json.loads(response.text)\\n\\ndef get_schema_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-schemas')\\n    return json.loads(response.text)\\n\\ndef get_input_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-inputs')\\n    return json.loads(response.text)\\n\\ndef get_output_feeds():\\n    response = requests.get('https:\/\/api.mybanking.com\/api\/v1\/finance-feeds-outputs')\\n    return json.loads(response.text)\\n\\n\\ndef main():\\n\\n    # Get all inputs and outputs from API\\n    inputs = []\\n    outputs = []\\n    \\n    # Get schema from API, then parse it into a dictionary\\n    schema_dict = {}\\n    \\n    # Get input feeds from API, then parse it into a dictionary \\n    input_feeds_dict = {}\\n    \\n    # Get output feeds from API, then parse it into a dictionary \\n    output_feeds_dict = {}\\n\\n    \\n# Create JSON object representing YAML format of our request\/response headers:\\nheaders_json_object = {\\n        'Content-Type': 'application\/json',\\n        'Authorization': 'Bearer %s' % token,\\n        'Accept': 'application\/json',\\n        'Accept-Encoding': 'gzip, deflate',\\n        'Connection': 'close',\\n        'Content-Length': str(len(input_feeds_dict)),\\n        'Host': 'api.mybanking.com',\\n        \"X-Auth-Token\": token,\\n}\\n\\n# Create JSON object representing YAML format of our request\/response body:\\nbody_json_object = {\\n        \"schema\": schema_dict,\\n        \"input_feeds\": inputs,\\n        \"output_feeds\": outputs,\\n}\\n\\n# Create JSON object representing YAML format of our request\/response headers:\\nheaders_json_object['Content-Type'] = headers_json_object['Content-Type'].replace(\"application\/json\", \"application\/octet-stream\")\\nheaders_json_object['Authorization'] = headers_json_object['Authorization'].replace(\"Bearer %s\", token)\\nheaders_json_object['Accept'] = headers_json_object['Accept'].replace(\"application\/json\", \"application\/json\")\\nheaders_json_object['Accept-Encoding'] = headers_json_object['Accept-Encoding'].replace(\"gzip, deflate\", \"text\/plain; charset=utf-8\")\\n\\n# Create JSON object representing YAML format of our request\/response body:\\nbody_json_object['schema'] = body_json_object['schema'].replace(\"xml\", \"\")\\nbody_json_object['input_feeds'] = body_json_object['input_feeds'].replace(\"xml\", \"\")\\nbody_json_object['output_feeds'] = body_json_object['output_feeds'].replace(\"xml\", \"\")\\n\\n# Create JSON object representing YAML format of our request\/response header:\\nheader_json_objec"}
{"File Name":"gatemint-sdk\/adr-024-coin-metadata.md","Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"The `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry containts the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n- 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n- The string \"atom\" can be used as a display name in a list of tokens.\\n- The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","tokens":336,"id":11,"Predictions":"## Yes\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe Cosmos Hub (Gaia) is a peer-to-peer network where users are able to exchange assets without any\\ncentral authority. The Cosmos Hub (Gaia) is designed to be as decentralized as possible, and so it is\\nexpected that the Cosmos SDK will be decentralized as well. It is therefore expected that the Cosmos SDK will\\nbe decentralized.\\nThe Cosmos SDK stores all information pertaining to an account in a `Coins` type that consists of an\\n`amount` and a `denom`, where `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an account-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps track of all balances for all accounts and also keeps track of the total supply of balances in an application.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based chain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by the time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single unit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and operations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and as a result the total amount of asset types increases. We propose to have x\/bank additionally keep track of metadata per denom in order to help clients, wallet providers, and explorers improve their UX and remove the requirement for making any assumptions on the unit of denomination.\\n\\nThis record has been archived.\\n"}
{"File Name":"gatemint-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the conception of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n- Membership Admittance\\n- Membership Acceptance\\n- Membership Revocation\\n- (probably) Without Penalty\\n- member steps down (self-Revocation)\\n- replaced by new member from governance\\n- (probably) With Penalty\\n- due to breach of soft-agreement (determined through governance)\\n- due to breach of hard-agreement (determined by code)\\n- Execution of Duties\\n- Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n- Compensation\\n- Group compensation (further distribution decided by the specialization group)\\n- Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admittance to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possiblities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":12,"Predictions":"## Decision\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the conception of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n\\n"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"Predictions":"This is a decision record. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\\n## Decision\\n\\n"}
{"File Name":"gatemint-sdk\/adr-026-ibc-client-recovery-mechanisms.md","Context":"## Context\\n### Summary\\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\\n### Security model\\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\\n","Decision":"We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\\n1. `allow_governance_override_after_expiry` (boolean, default false)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x\/ibc` module\\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-007-tendermint-client#upgrades).\\n","tokens":370,"id":15,"Predictions":"## Decision\\n### Summary\\nThe data in this record is an arbitrary decision. The decision is made by the user and cannot be changed. The user has the ability to change this decision at any time after they have submitted it.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\\n## Decision\\n## Decision\\n### Summary\\nThe data in this record is an arbitrary decision. The decision is made by the user and cannot be changed. The user has the ability to change this decision at any time after they have submitted it.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n## Decision\\n## Decision\\n### Summary\\nThe data in this record is an arbitrary decision. The decision is made by the user and cannot be changed. The user has the ability to change this decision at any time after they have submitted it.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n## Decision\\n## Decision\\n### Summary\\nThe data in this record is an arbitrary decision. The decision is made by you (the user). You have access to edit your own decisions.\\n## Data\\n## Data \\# Data \\# Data \\# Data \\# Data \\# Data \\# Data \\# Data \\# Data \\# Data \\#\\n\\end{DoxyParams}\\n\\begin{DoxyReturn}{Returns}\\n0 if successful \\n\\end{DoxyReturn}\\n\\mbox{\\Hypertarget{class_ibc_1_1_config_aba9d7b0cfb4f4e9c8a5b3a6f6ac0c2bf}\\label{class_ibc_1_1_config_aba9d7b0cfb4f4e9c8a5b3a6f6ac0c2bf}} \\n\\index{IBC::Config@{IBC::Config}!get\\_channel\\_id@{get\\_channel\\_id}}\\n\\index{get\\_channel\\_id@{get\\_channel\\_id}!IBC::Config@{IBC::Config}}\\n\\doxysubsubsection{\\texorpdfstring{get\\_channel\\_id()}{get\\_channel\\_id()}}\\n{\\footnotesize\\ttfamily string IBC\\+::\\+Config\\+::get\\+\\_\\+channel\\+\\_\\+id (\\begin{DoxyParamCaption}\\item[{string}]{name }\\end{DoxyParamCaption})}\\n\\nGet channel id from name.\\n\\nThis method returns 0 if there are multiple channels with that name.\\n\\nIf there are multiple channels with that name then return 2.\\n\\n\\n\\begin{DoxyParams}{Parameters}\\n{\\em name} & Channel name \\\\\\n\\hline\\n\\end{DoxyParams}\\nGet channel id from name.\\n\\nThis method returns 0 if there are multiple channels with that name.\\n\\nIf there are multiple channels with that name then return 2.\\n\\n\\n\\begin{DoxyParams}{Parameters}\\n{\\em name} & Channel name \\\\\\n\\hline\\n\\end{DoxyParams}\\nGet channel id from name.\\n\\nThis method returns 0 if there are multiple channels with that name.\\n\\nIf there are multiple channels with that name then return 2.\\n\\n\\n\\begin{DoxyParams}{Parameters}\\n{\\em channel} & Channel ID \\\\\\n\\hline\\n{\\em number} & Number of channels \\\\\\n\\hline\\n{\\em type} & Type of channel \\\\\\n\\hline\\n{\\em size} & Size of channel\\\\\\n\\hline\\n{\\em expiry} & Expiry date of channel\\\\\\n\\hline\\n\\\\\\n\\nReturn value: int \\n\\nDefinition at line 87 of file Config.\\+cs.\\n\\n\\n\\nReferences IBC.\\+Config.\\+max(), IBC.\\+Config.\\+size(), IBC.\\+Config.\\+type(), IBC.\\+Config.\\+expiry(), IBC.\\+Config.\\+number(), IBC.\\+Config.\\+type(), IBC.\\+Config.\\+\\_\\+\\+\\_\\+, IBC.\\+Config., IBC., ICsauce,\\&Icsauce.\\n \\n\\nReferenced by get(\\mbox{\\hyperlink{_config_8cs_source_l00088}{88}}), get(\\mbox{\\hyperlink{_config_8cs_source_l00096}{96}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00103}{103}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00111}{111}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00118}{118}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00125}{125}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00132}{132}}, get(\\mbox{\\hyperlink{_config_8cs_source_l00139}{139}}), get(\\mbox{\\hyperlink{_config_8cs_source_l00146}{146}})), set(\\mbox{\\hyperlink{_config_8cs_source_l00082}{82}}, set(\\mbox{\\hyperlink{_config_8cs_source_l00089}{89}}), set(10000), set(10001), set(10002), set(10003), set(10004), set(10005), set(10006)), set(10)) \\n\\n\\n\\begin{DoxyCode}\\n89         {\\n90             return iks.get_channel_id(name);\\n91         }\\n92 \\n93     public static int[] GetChannelIds(string name)\\n94     {\\n95         int[] result = new int[3];\\n96 \\n97         result[0] = iks.get_channel_id(name);\\n98         result[1] = iks.get_channel_id(name);\\n99         result[2] = iks.get_channel_id(name);\\n100 \\n101         return result;\\n102     }\\n103 \\n104     public static void SetChannelId(int id)\\n105     {\\n106         iks.set_channel_id(id);\\n107     }\\n108 \\n109     public static void SetChannelSize(int size)\\n110     {\\n111         iks.set_channel_size(size);\\n112     }\\n113 \\n114     public static void SetExpiryDate(int expiry)\\n115     {\\n116         iks.set_expiry_date(expiry);\\n117     }\\n118 \\n119     public static void SetNumber(int number)\\n120     {\\n121         iks.set_number(number);\\n122     }\\n123 \\n124     public static void SetType(string type)\\n125     {\\n126         iks.set_type(type);\\n127     }\\n128 \\n129     public static void SetSize(int size)\\n130     {\\n131         iks.set_size(size);\\n132     }\\n133 \\n134 } \/\/ end namespace IBC\\n\\nReferenced by GetChannelIds(), GetChannelSize(), SetExpiryDate(), SetNumber(), SetType(), SetSize()."}
{"File Name":"gatemint-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":16,"Predictions":"## Decision\\nThe ADR specifies the encoding of queries. The query encoding is based on the\\nQuery Encoding defined in the [Protocol Buffer Specification](.\/protocol-buffers-spec.md)\\nto specify how the query encoding should be applied to a given message.\\nThe query encoding is a binary string consisting of 16 hex characters and 32-bit values. The\\nQuery Encoding is defined as follows:\\n\\n## Query Encoding\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n## Query Encoding\\nThe Query Encoding is defined as follows:\\n\\n\\n\\end{lstlisting}\\n\\n\\section{Query Type}\\label{query-type}\\n\\n\\subsection{Query Type}\\label{query-type-1}\\n\\n\\begin{lstlisting}\\ntype: \"string\"\\n```\\nThis type will be used for querying messages by name, and will be used to encode all queries that contain names.\\n\\subsection{Query Name}\\label{query-name-1}\\n\\n\\begin{lstlisting}\\ntype: \"string\"\\n```\\nThis type will be used for querying messages by name, and will be used to encode all queries that contain names.\\n\\subsection{Query Body}\\label{query-body-1}\\n\\n\\begin{lstlisting}\\ntype: \"string\"\\n```\\nThis type will be used for querying messages by body, and will be used to encode all queries that contain body data.\\n\\subsection{Query Body Data}\\label{query-body-data-1}\\n\\n\\begin{lstlisting}\\ntype: \"string\"\\n```\\nThis type will be used for querying messages by body data, and will be used to encode all queries that contain body data.\\n"}
{"File Name":"gatemint-sdk\/adr-009-evidence-module.md","Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"We will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n- Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n- Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n- Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n- Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","tokens":176,"id":17,"Predictions":"## Decision\\nThe first step to implementing a robust, secure and interoperable blockchain\\napplication is to expose a mechanism in which arbitrary evidence can be submitted,\\nevaluated and verified resulting in some agreed upon penalty for any misbehavior\\ncommitted by a validator, such as equivocation (double-voting), signing when\\nunbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or\\ncross-chain validation protocol implementation in order to support the ability for any\\nmisbehavior to be relayed back from a collateralized chain to a primary chain so that\\nthe equivocating validator(s) can be slashed.\\n\\n## Context\\n## ContextIn order to support building highly secure, robust and interoperable blockchain applications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary evidence can be submitted, evaluated and verified resulting in some agreed upon penalty for any misbehavior committed by a validator, such as equivocation (double-voting), signing when unbonded, signing an incorrect state transition (in the future), etc. Furthermore, such a mechanism is paramount for any [IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or cross-chain validation protocol implementation in order to support the ability for any misbehavior to be relayed back from a collateralized chain to a primary chain so that the equivocating validator(s) can be slashed.\\n"}
{"File Name":"gatemint-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/armon\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":229,"id":18,"Predictions":"## Decision: Enable telemetry\\n## Decision: Enable Prometheus metrics\\n\\end{document}\\n"}
{"File Name":"gatemint-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the SDK github repo as follows:\\n```\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n- `README`: Landing page of the docs.\\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n- `clients`: Contains specs and info about the various SDK clients.\\n- `spec`: Contains specs of modules, and others.\\n- `modules`: Contains links to `godocs` and the spec of the modules.\\n- `architecture`: Contains architecture-related docs like the present one.\\n- `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n- `README`\\n- `intro`\\n- `concepts`\\n- `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":113,"id":19,"Predictions":"## Decision\\nThe decision is that the documentation should be moved to a separate github repo and the hub docs should be moved to the website.\\n\\#Hub Docs\\nThe hub docs are part of the SDK, so they should live on the website. The rest of the docs will be moved to a separate repo.\\n## Context\\nThe hub docs are part of the SDK, so they should live on the website. The rest of the docs will be moved to a separate repo.\\n\\#SDK Docs\\nThe SDK docs are not part of the SDK, so they should stay on GitHub.\\n## Decision\\n## Decision\\nIf we do this, we will need to create a new branch for each SDK doc. We have done it before for other libraries (e.g. https:\/\/github.com\/iot-platform\/lotion) and it worked well. We can do it again here.\\n\\#Lotion Docs\\nWe have already done it for other libraries (https:\/\/github.com\/iot-platform\/lotion). We have also created an Lotion Docs branch for that purpose. This is not an issue as Lotion is a different library than Cosmos.\\n\\#GitHub Issues\\nWe have already opened issues for some of these issues (https:\/\/github.com\/iot-platform\/lotion\/issues?q=is%3Aopen+is%3Aissue+label%3A%22Issue%20with%20Lotion%22), but we decided to create a new issue for each one as this would make them easier to follow and also easier to find in search results (as we will still use https:\/\/github.com\/iot-platform\/lotion\/issues as our main issue tracker). We are now working on creating them and will post here when they are ready. It may take some time as there is no easy way to automate this process yet.\\n\\#Github Issues\\nWe have already opened issues for some of these issues (https:\/\/github.com\/iot-platform\/lotion\/issues?q=is%3Aopen+is%3Aissue+label%3A%22Issue+with+Lotion+on+Github%C2%A0s+Issues%C2%A0for+Lotion%C2%A0Docs%C2%A0and+Cosmos%C2%A0Documentation%C2%A0and+Cosmos%E2%80%99s+Ecosystem%C2%A0for+Cosmos%E2%80%99s+Ecosystem%C2%A0Documentation%C2%A0and+Cosmos%E2%80%99s+Ecosystem%E2%80%99s+Ecosystem%E2%80%B9%E2%80%B9Is+a+joke.+I%E2%80%B9ve+already+brought+a+joke.+I%E2%80%B9ve+brought+a+joke.+I%E2%80%B9ve+brought+a+joke.+I%E2&hasMoreResults=true&sort=score&viewMode=table&byDate=false&order=descending&pageNumber=1&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&field=labels.contains(\"Issue with Lotion\")&&hasMoreResults=true&orderByDescending=false\\n## Decision\\n## Decision\\nIf we do this, we will create 4 new branches (one per document) which means more work for us in terms of manual work and also more work for users as they will need to go through all these branches manually if they want to find something specific about a specific document.\\n## Context\\n## Context\\nWe believe that this is not worth doing. It would mean much more work for us in terms of manual work, but it would lead to much better documentation at the end because we would be able to focus our efforts on keeping it up-to-date instead of doing tons of manual work.\\n### PULL REQUESTS\\n\\n* **[pull request](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2165)](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2165)\\n* **[pull request](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2194)](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2194)\\n* **[pull request](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2216)](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2216)\\n* **[pull request](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2226)](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2226)\\n* **[pull request](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2237)](https:\/\/github.com\/IoTPlatform\/Layer-OS\/pull\/2237)\\n\\n**[link](http:\/\/iotplatform.github.io\/layers-os\/)"}
{"File Name":"gatemint-sdk\/adr-003-dynamic-capability-store.md","Context":"## Context\\nFull implementation of the [IBC specification](https:\/\/github.com\/cosmos\/ics) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\\nas described in [ICS 5](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\\nobject-capability keys at this time.\\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L132))\\nand passed to Keepers as fixed arguments ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L160)). Keepers cannot create or store capability keys during transaction execution \u2014 although they could call `NewKVStoreKey` and take the memory address\\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).\\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\\nThis ADR proposes such an interface & mechanism.\\n","Decision":"The SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\\ninvoked.\\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\\nchain is running.\\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\\nThe persistent `KVStore` tracks which capability is owned by which modules.\\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\\na reverse mapping that map from module name, capability name to the capability index.\\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\\ngo-map to retrieve the capability at the original memory location.\\nThe `CapabilityKeeper` will define the following types & functions:\\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\\na name. A `String()` method is provided for debugging.\\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\\n```golang\\ntype Capability struct {\\nindex uint64\\n}\\n```\\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\\n```golang\\ntype CapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleNames   map[string]interface{}\\nsealed        bool\\n}\\n```\\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\\npassed by other modules.\\n```golang\\ntype ScopedCapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleName    string\\n}\\n```\\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\\nIt MUST be called before `InitialiseAndSeal`.\\n```golang\\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\\nif k.sealed {\\npanic(\"cannot scope to module via a sealed capability keeper\")\\n}\\nif _, ok := k.scopedModules[moduleName]; ok {\\npanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\\n}\\nk.scopedModules[moduleName] = struct{}{}\\nreturn ScopedKeeper{\\ncdc:      k.cdc,\\nstoreKey: k.storeKey,\\nmemKey:   k.memKey,\\ncapMap:   k.capMap,\\nmodule:   moduleName,\\n}\\n}\\n```\\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\\ncreation of any new `ScopedCapabilityKeeper`s.\\n```golang\\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\\nif ck.sealed {\\npanic(\"capability keeper is sealed\")\\n}\\npersistentStore := ctx.KVStore(ck.persistentKey)\\nmap := ctx.KVStore(ck.memKey)\\n\/\/ initialise memory store for all names in persistent store\\nfor index, value := range persistentStore.Iter() {\\ncapability = &CapabilityKey{index: index}\\nfor moduleAndCapability := range value {\\nmoduleName, capabilityName := moduleAndCapability.Split(\"\/\")\\nmemStore.Set(moduleName + \"\/fwd\/\" + capability, capabilityName)\\nmemStore.Set(moduleName + \"\/rev\/\" + capabilityName, index)\\nck.capMap[index] = capability\\n}\\n}\\nck.sealed = true\\n}\\n```\\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\\nreference. The newly created capability is automatically persisted; the calling module need not\\ncall `ClaimCapability`.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\\n\/\/ check name not taken in memory store\\nif capStore.Get(\"rev\/\" + name) != nil {\\nreturn nil, errors.New(\"name already taken\")\\n}\\n\/\/ fetch the current index\\nindex := persistentStore.Get(\"index\")\\n\/\/ create a new capability\\ncapability := &CapabilityKey{index: index}\\n\/\/ set persistent store\\npersistentStore.Set(index, Set.singleton(sck.moduleName + \"\/\" + name))\\n\/\/ update the index\\nindex++\\npersistentStore.Set(\"index\", index)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to index\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, index)\\n\/\/ set the in-memory mapping from index to capability pointer\\ncapMap[index] = capability\\n\/\/ return the newly created capability\\nreturn capability\\n}\\n```\\n`AuthenticateCapability` can be called by any module to check that a capability\\ndoes in fact correspond to a particular name (the name can be untrusted user input)\\nwith which the calling module previously associated it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\\n\/\/ return whether forward mapping in memory store matches name\\nreturn memStore.Get(sck.moduleName + \"\/fwd\/\" + capability) === name\\n}\\n```\\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\\nso that future `GetCapability` calls will succeed.\\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\\nthey will all own it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to capability\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.add(sck.moduleName + \"\/\" + name)\\npersistentStore.Set(capability.Index(), owners)\\n}\\n```\\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\\nThe module is not allowed to retrieve capabilities which it does not own.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\\n\/\/ fetch the index of capability using reverse mapping in memstore\\nindex := memStore.Get(sck.moduleName + \"\/rev\/\" + name)\\n\/\/ fetch capability from go-map using index\\ncapability := capMap[index]\\n\/\/ return the capability\\nreturn capability\\n}\\n```\\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\\nmore owners exist, the capability will be deleted globally.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\nname := capStore.Get(sck.moduleName + \"\/fwd\/\" + capability)\\nif name == nil {\\nreturn error(\"capability not owned by module\")\\n}\\n\/\/ delete forward mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ delete reverse mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.remove(sck.moduleName + \"\/\" + name)\\nif owners.size() > 0 {\\n\/\/ there are still other owners, keep the capability around\\npersistentStore.Set(capability.Index(), owners)\\n} else {\\n\/\/ no more owners, delete the capability\\npersistentStore.Delete(capability.Index())\\ndelete(capMap[capability.Index()])\\n}\\n}\\n```\\n### Usage patterns\\n#### Initialisation\\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\\n```golang\\nck := NewCapabilityKeeper(persistentKey, memoryKey)\\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\\n\/\/ other initialisation logic ...\\n\/\/ load initial state...\\nck.InitialiseAndSeal(initialContext)\\n```\\n#### Creating, passing, claiming and using capabilities\\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\\nModule 1 would have the following code:\\n```golang\\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\\nmod2Keeper.SomeFunction(ctx, capability, args...)\\n```\\n`SomeFunction`, running in module 2, could then claim the capability:\\n```golang\\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\\nk.sck.ClaimCapability(ctx, capability, \"resourceABC\")\\n\/\/ other logic...\\n}\\n```\\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\\n```golang\\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\\ncapability := k.sck.GetCapability(ctx, name)\\nmod1.UseResource(ctx, capability, \"resourceABC\")\\n}\\n```\\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\\n```golang\\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\\nif !k.sck.AuthenticateCapability(name, capability) {\\nreturn errors.New(\"unauthenticated\")\\n}\\n\/\/ do something with the resource\\n}\\n```\\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\\n","tokens":434,"id":20,"Predictions":"## Decision\\nA decision is required to indicate whether the application should be allowed to initialise object-capability keys at runtime (i.e., during transaction execution).\\n\\n## Context\\nThe application should be allowed to initialise object-capability keys at runtime (i.e., during transaction execution).\\n\\n## Decision\\n## Decision\\nThe application should be allowed to initialise object-capability keys at runtime (i.e., during transaction execution).\\n\\n## Context\\nThe application should be allowed to initialise object-capability keys at runtime (i.e., during transaction execution).\\n\\n## Decision\\n## Decision\\nThe application should be allowed to initialise object-capability keys at runtime (i.e., during transaction execution).\\n"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"Predictions":"## Decision\\nThe decision is to use the `oneof` approach for transaction signing. The\\noriginal intent was to use the `oneof` approach for transaction signing (see\\n[\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)). However, we have\\ndemonstrated that the `oneof` approach does not work well in practice, especially\\nin the face of high network latencies.\\nAs a result, we have decided to use the `json-signing` approach.\\n\\n## Context\\n## Context\\nWe have used this ADR as a basis for our design of the Cosmos SDK's JSON-signing\\napproach. We believe that this is a better fit for our current design goals and\\nexpectations, as well as future ones. This ADR is intended to be an architectural\\ndocument that describes how we intend to migrate from one approach to another.\\n\\n## Summary\\n## Summary\\nThe decision is to use the `json-signing` approach for transaction signing. The original intent was to use the `oneof` approach for transaction signing (see [\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)). However, we have demonstrated that the `oneof` approach does not work well in practice, especially in the face of high network latencies.\\nAs a result, we have decided to use the `json-signing` approach.\\n\\end{document}\\n"}
{"File Name":"gatemint-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params:\\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\\n- There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":75,"id":22,"Predictions":"## Decision: Voting Period is 5 days.\\n## Decision: Vote for Proposals that require a longer period of time to consider.\\n## Decision: Vote for Proposals that require a shorter period of time to consider.\\n## Decision: Vote for Proposals that do not require a longer period of time to consider.\\n## Decision: Vote for Proposals that do not require a shorter period of time to consider."}
{"File Name":"gatemint-sdk\/adr-017-historical-header-module.md","Context":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","Decision":"The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```golang\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\\ninfo := HistoricalInfo{\\nHeader: ctx.BlockHeader(),\\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), \/\/ note that this must be stored in a canonical order\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n","tokens":65,"id":23,"Predictions":"## Decision\\nThe Cosmos SDK will create a new [validator set](https:\/\/github.com\/cosmos\/ics\/wiki\/Validator-Set) \\n\\n## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n\\n## Decision\\n## Decision\\nThe Cosmos SDK will create a new [commitment root](https:\/\/github.com\/cosmos\/ics\/wiki\/Commitment-Root) \\n\\n## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n\\n## Decision\\n## Decision\\nThe Cosmos SDK will create a new [commitment root](https:\/\/github.com\/cosmos\/ics\/wiki\/Commitment-Root) \\n\\n## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n\\n## Decision\\n## Decision\\nThe Cosmos SDK will create a new [commitment root](https:\/\/github.com\/cosmos\/ics\/wiki\/Commitment-Root) \\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n"}
{"File Name":"gatemint-sdk\/adr-019-protobuf-state-encoding.md","Context":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n- Binary bytes must be decode-able with a schema.\\n- Schema must be upgradeable.\\n- The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n- Language agnostic\\n- Platform agnostic\\n- Rich client support and thriving ecosystem\\n- High performance\\n- Minimal encoded message size\\n- Codegen-based over reflection-based\\n- Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","Decision":"We will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\\nand `HybridCodec`.\\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\\nas the concrete codec it accepts and\/or extends. This means that all client JSON encoding, including\\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\\nProtbuf encoding and thus have modules accept and\/or extend `ProtoCodec`.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/gogo\/protobuf) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshalBinaryBare(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Marshaler\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/godoc.org\/github.com\/gogo\/protobuf\/types)\\nuses [global type registration]( https:\/\/github.com\/gogo\/protobuf\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could registry a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\\neasy encoding of state to `Any` in `Codec` implementations. Ex:\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\\nreturn codec.MarshalAny(evidenceI)\\n}\\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn evi, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n- **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n","tokens":439,"id":24,"Predictions":"## Decision\\nThe Cosmos SDK is a distributed system and therefore requires a messaging protocol to allow peers to exchange\\nmessages. The Cosmos SDK currently uses [go-amino](https:\/\/github.com\/tendermint\/go-amino) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\nAmino also aims to have the following goals (not a complete list):\\n- Binary bytes must be decode-able with a schema.\\n- Schema must be upgradeable.\\n- The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n- Language agnostic\\n- Platform agnostic\\n- Rich client support and thriving ecosystem\\n- High performance\\n\\end{document}\\n"}
{"File Name":"gatemint-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n","tokens":111,"id":25,"Predictions":"## Decision: This is a custom account type. This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n\\n## Decision\\n## Decision: This is a custom account type. This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount and ModuleAccount`). Projects desiring to use custom accounts (say custom vesting accounts) need to fork and modify genaccounts.\\n\\n\\end{verbatim}\\n\\n\\subsection{Adding additional vaults}\\n\\label{sec:adding-additional-vaults}\\nThe \\texttt{vault} field in \\texttt{VaultProvider} is used for adding vaults that are not already present in the vault provider.\\nA vault can be added by providing an identifier of an existing vault as well as an optional description.\\n\\n\\begin{verbatim}\\n{\\n  \"vault\": {\\n    \"id\": \"myvault\",\\n    \"description\": \"My Vault\",\\n    \"type\": \"VaultProvider\"\\n  }\\n}\\n\\end{verbatim}\\n\\nThe \\texttt{description} field can be used to provide more information about the new vault.\\n\\n\\begin{verbatim}\\n{\\n  \"description\": \"My new Vault\",\\n  \"type\": \"VaultProvider\"\\n}\\n\\end{verbatim}\\n\\nThe \\texttt{id} field can be used to uniquely identify each new vault.\\n\\n\\begin{verbatim}\\n{\\n  \"id\": {\\n    \"$id\": \"$myvault\"\\n  },\\n  \"description\": {\\n    \"$description\": \"$myvault\"\\n  }\\n}\\n\\end{verbatim}\\n\\nThe \\texttt{name} field can be used to provide more information about the new vault.\\n\\n\\begin{verbatim}\\n{\\n  \"name\": {\\n    \"$name\": \"$myvault\"\\n  },\\n  \"description\": {\\n    \"$description\": \"$myvault\"\\n  }\\n}\\n\\end{verbatim}\\n\\n\\n"}
{"File Name":"gatemint-sdk\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase chanel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistent layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":337,"id":26,"Predictions":"## 1. If the `code` is `CodeType` and the `codeTxBreak` is set to `CodeTxBreak`, then\\ninitialize the channel with the current state of the channel, otherwise, set it to\\nthe previous state.\\n\\n## 2. If the `code` is `CodeType`, then check if there is a previous transaction in the\\nchannel and abort if there is one.\\n\\n## 3. If there is no previous transaction in the channel, then proceed with\\noperation\\n\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n# -*- coding: utf-8 -*-\\n# -*- coding: utf-8 -*-\\nimport json\\nimport sys\\n\\nfrom json import dumps\\n\\ndef main(argv=None):\\n    \"\"\"Main function of ICS to handle messages from client.\"\"\"\\n\\n    # Check for a valid IBC token.\\n    token = None\\n\\n    # Check for a valid IBC token.\\n    token = requests.get('https:\/\/localhost:3000\/ics\/token', auth=('admin', 'admin'))\\n\\n    # Check for a valid IBC token.\\n    token = requests.get('https:\/\/localhost:3000\/ics\/token', auth=('admin', 'admin'))\\n\\n    # Check for a valid IBC token.\\n    token = requests.get('https:\/\/localhost:3000\/ics\/token', auth=('admin', 'admin'))\\n\\n    # Check for a valid IBC token.\\n    token = requests.get('https:\/\/localhost:3000\/ics\/token', auth=('admin', 'admin'))\\n\\n    # Check for a valid IBC token.\\n    token = requests.get('https:\/\/localhost:3000\/ics\/token')\\n\\n    # Check for a valid IBC token.\\n    codeType = CodeType()\\n    codeBreak = CodeBreak()\\n\\n    if codeType == CodeType.CodeType:\\n        codeType.setBreakingToken(token)\\n\\n        if codeBreak == CodeBreak.CodeBreak:\\n            try:\\n                channel.send(message)\\n                return True\\n            except Exception as e:\\n                print(\"Error during sending message\")\\n                return False\\n\\n        else:\\n            raise RuntimeError(\"Invalid type of code\")\\n        \\n        if (code != codeType) and (code != CodeBreak):\\n            raise RuntimeError(\"Invalid type of code\")\\n\\n        try:\\n            channel.send(message)\\n            return True\\n        except Exception as e:\\n            print(\"Error sending message\")\\n            return False\\n\\n        try:\\n            channel.send(message)\\n            return True\\n        except Exception as e:\\n            print(\"Error sending message\")\\n            return False\\n\\nif __name__ == \"__main__\":\\n    main(sys.argv[1:])\\n\\end{lstlisting}\\n\\n\\subsection{Routing Module}\\nThe routing module handles packet relay between application modules. It has two\\nfunctions:\\n\\n1. The first function receives packet from application module and checks whether\\nthe packet matches any route rule. If not, it will call \\texttt{handlePacketRecv} function, which will\\nhandle packet relay with \\texttt{handlePacketRecv} function inside application module.\\n2\\. The second function receives packet from application module and dispatches it to routing module.\\n\\n\\begin{lstlisting}\\n# -*- coding: utf-8 -*-\\n# -*- coding: utf-8 -*-\\nimport json\\nimport sys\\n\\nfrom json import dumps\\n\\ndef main(argv=None):\\n\"\"\"Main function of Routing Module\"\"\"\\n    \\ntry:\\n    \\n    # Check if there is any route rule defined in routing module.\\n    \\nif __name__ == \"__main__\":\\n    \\n# Get route rule definition from routing module and save it in file \"route_rule.json\".\\nroute_rule_file = sys.argv[1]\\nroute_rule_file_path = os.path.join(sys.argv[1], route_rule_file)\\n\\n# Load route rule definition from file \"route_rule.json\" and save it into file \"routing_rules.json\".\\nrouting_rules_file_path = os.path.join(sys.argv[1], route_rule_file)\\n    \\nif not os.path.isfile(routing_rules_file_path):\\n    \\nsys.exit(1)\\n\\ndef handlePacketRecv(message):\\n    \\n\"\"\"Handle packet received by application module.\"\"\"\\n    \\ntry:\\n    \\n# Get information about destination identifier by using \\texttt{getDestinationIdentifier}.\\ndestination_id_list = message.destination_ids\\n    \\nif not destination_id_list or len(destination_id_list) > 0 :\\n    \\nsys.exit(2)\\n\\ndef handlePacketRecvEvent(message, destination_id_list):\\n    \\n\"\"\"Handle packet received event from application module.\"\"\"\\n    \\nmessage.destination_ids.append(destination_id_list)\\n\\ndef getDestinationIdentifier(destination_id_list):\\n    \\n\"\"\"Get destination identifier by using \\texttt{getDestinationIdentifier}.\"\"\"\\ndestination_ids_list = destination_id_list.split('\\n')\\ndestination_ids_list.sort()\\ndestination_ids_list.reverse()\\ndestination_ids_list.reverse()\\ndestination_ids_list.reverse()\\nreturn destination_ids_list[-2]\\n\\ndef getDestinationId(destination_id):\\n   \\n\"\"\"Get destination id by using \\texttt{getDestinationId}.\"\"\"\\ndestination_id_dict = {}\\ndestination_id_dict['id'] = destination_id\\nreturn destination_id_dict['id']\\n\\ndef handleMessageReceived(message):\\n\\n\"\"\"Handle message received by application module.\"\"\"\\n    \\n    \\nmessage.destination_ids.append(destination_id)\\n    \\n    \\nif message.destination_ids.count(destination_i_d) > 0 :\\n    \\n    \\nmessage.destination_ids.pop(destination_i_d)\\n    \\n    \\nif message.destination_ids.count(destination_i_d) < len(destination_i_d) - 1 :\\n    \\n    \\nmessage.destination_ids.append(destination_i_d)\\n    \\n    \\nif message.destination_ids.count(destination_i_d) == len(destination_i_d) - 1 :\\n    \\n    \\nmessage.destination_ids.pop(destination_i_d)\\n    \\n    \\nreturn message.destination_ids[-1]\\n\\ndef getRoutingRules(rule_name, rules_file_path):\\n\\n'''Get routing rules definition by rule name.'''\\nrules_file_path_path = os.path.join(rules_file_path, rule_name)\\nrules_file_path_text_length = len(rules_file_path_text_length)\\nrule_name_text_length = len(rule_name_text_length)\\n\\nrules_files_dict = {}\\nrules_files_dict['rule'] = rules_file_path_text_length + rule_name_text_length + \\\\n             '\\n' + rules_files_path + '\\n'\\nrules_files_dict['file'] = rules_files_path + '\\n'\\nrules_files_dict['rule'] += '\\n'\\n\\nreturn rules_files_dict\\n\\ndef handleMessageReceived(event):\\n\\n'''Handle message received event by application module.'''\\nevent.destination_id.append(event.id)\\n\\nevent.source_address.append(event.src_ip_address)\\nevent.source_port.append(event.src_port)\\n\\nevent.source_address.extend(event.src_ip_address.split(':')[0])\\nevent.source_port.extend(event.src_port.split(':')[0])\\nevent.source_address.extend(event.src_ip_address.split(':')[1])\\nevent.source_port.extend(event.src_port.split(':')[1])\\n\\nreturn event.message\\n\\n\\ndef getRoutingRules(file_name, rule_names):\\n\\n'''Get routing rules definition by file name.'''\\nfile_name_text_length_widths= len(file_name_text_length_widths)+ \\\\n             '\\n' + file_names + '\\n'\\nfile_names_text_length_widths= len(file_names_text_length_widths)+ \\\\n             '\\n' + file_names + '\\n'\\nfile_names_words= file_names.text[:file_names.length()]\\nfile_names_words.remove(file_names.words[:file_names.length() - 1])\\n\\nfiles_dict={}\\nfiles_dict[file_name]={'rule':'check_if_same_route'}\\nfiles_dict[file_name]['rule']=files_dict[file_name]['rule']+'\\r\\n'\\nfiles_dict[file_name]['rule']=files_dict[file_name]['rule']+'\\r\\n'\\nfiles_dict[file_name]['rule']=files_dict[file_name]['rule']+'\\r\\n'\\n\\nfor filename in files_dict.file_name:\\n\\nfilename_data=file(filename.text[:filename.length()])\\nfilename_data=file(filename.text[:filename.length() - 1])\\n\\nfor rule in files_dict.file_name:\\n\\nrules_files_dictionary={}\\nrules_files_dictionary['rule']='check_if_same_route'\\n\\nfor rule in files_dict.file_name:\\n\\nrules_files_dictionary.setdefault(rule,'check_if_same_route')\\nrules_files_dictionary.setdefault(rule,'check_if_same_route')\\nrules_files_dictionary.setdefault(rule,'check_if_same_route')\\n\\nfor filename in files_dictonary.filename:\\n\\nfilename_data=file(filename.text[:filename.length()])\\nfilename_data=file(filename.text[:filename.length() - 1])\\n\\nfor id in filename_data:\\n\\nid_data=file(filename.text[:id.length()])\\nid_data=file(filename.text[:id.length() - 1])\\n\\nfor id in id_data:\\n\\nids=dict(id.encode('utf-8'))\\nids[id]=id.encode('utf-8')\\n\\nfor id in ids.keys():\\n\\nids[id]=ids[id]+'\\r\\n'\\n\\nfor filename in ids.keys():\\n\\nfilename_data=file(filename.text[:filename.length()])\\nfilename_data=file(filename.text[:filename.length() - 1])\\n\\nfor rule in ids.keys():\\n\\nids.setdefault(rule,'check_if_same_route')\\nids.setdefault(rule,'check_if_same_route')\\n\\nfor filename in ids.keys():\\n\\nfilename_data=file(filename.text[:filename.length()])\\nfilename_data=file(filename.text[:filename.length() - 1])\\n\\nfor id in filename_data:\\n\\nids[id]=ids[id]+'\\r\\n'\\n\\nfor id in ids.keys():\\n\\nids[id]=ids[id]+'\\r\\n'\\n\\nreturn {'file':file_name,text:'{'+str(id)+'},'+str(rules_files_dictionary)}\\n\\n\\nclass RoutingModule(object):\\n\\n'''The Routing Module handles packet relay between application modules.'''\\nclass Routing(object):\\n\\n'''The Routing Module handles packet relay between applications modules.'''\\nclass Application(object):\\n\\n'''Application Module handles messages from clients'''\\n\\nclass Message(object):\\n\\n'''Message object holds information about messages that are sent or received.'''\\nclass Destination(object):\\n\\n'''Destination object holds information about destinations that are sent or received.'''\\nclass Source(object):\\n\\n'''Source object holds information about source addresses that are sent or received.'''\\nclass MessageSource(object):\\n\\n'Message Source object holds information about messages that are sent or received.''\\n\\nclass DestinationSource(object):\\n\\n'Destination Source object holds information about destinations that are send or received.'\\n\\nclass Channel(object):\\n\\n'Channel object holds information about channels that are send or receive.'\\n\\n'''\\nChannels are used to send messages between applications modules.\\n\\n'''\\n\\nclass ChannelSource(object):\\n\\n'Channel Source object holds information about channels that are send or receive.'\\n\\n'''\\n\\nclass ChannelReceiver(object):   \\n'''Channel Receiver object holds information about channels that are send or receive.''\\n\\n\\n'''\\nReceiver Module handles messages sent to ports.\\n\\n'''\\n\\nclass ReceiverModule(object):   \\n'''\\nReceiver Module handles messages sent to ports.\\n\\n\\n'''\\n\\n'''\\nSender Module handles messages sent to destinations.\\n\\n'''\\n\\nclass SenderModule(object):   \\n'''\\nSender Module handles messages sent to destinations.\\n\\n\\n'''\\n\\n'''\\nSenderSource object holds information about source addresses.\\n\\n'''\\n\\nclass SenderSource(object):   \\n'''\\nSender Source object holds information about source addresses.\\n\\n\\n'''\\n\\n'''\\nSenderModule handles messages sent to destinations.\\n\\n'''\\n\\nclass SenderModuleObject(object):   \\n'''\\nSender Module Object holds all receivers and sources.\\n\\n\\n'''\\n\\n'''\\nReceiver Object holds all receivers.\\n\\n\\n'''\\n\\n'''\\nSender Object holds all sources.\\n\\n\\n'''\\n\\n''' \\n\\nAll classes have methods called on them like method on sender class above.\\n\\nMethod on receiver class calls method on sender class above.\\n\\nMethod on sender class calls method on receiver class above.\\n\\nMethod on receiver class calls method on sender class above.''\\n\\n\\nsubmodule_protocol_handler=Handler()\\n\\n# Define handlers for each protocol handler defined earlier.\\nprotocol_handler_class=HandlerClass()\\n\\nhandlers=[protocol_handler_class]\\n\\nhandler_class=_handler_class.__init__(self,\\n                                      protocol_handler=self.__init__,\\n                                      handler=self.handler,\\n                                      protocol_protocol=self.protocol_protocol,\\n                                      protocol_message=self.protocol_message,\\n                                      protocol_source=self.protocol_source,\\n                                      protocol_dest=self.protocol_dest,\\n                                      protocol_channel=self.protocol_channel,\\n                                      protocol_channel_receiver=self.protocol_channel_receiver)\\n\\n\\ndef _handler_class(self,**kwargs):\\n\"\"\"\\nDefine handlers for each protocol handler defined earlier.\"\"\"\\n\\nprotocol_handler=HandlerClass(**kwargs)\\n\\n\\nprotocol_handler.__init__(self,**kwargs)\\n\\n\\nfunction_handle_packet_received=function_handle_packet_received(self,response,error=None):\\n\"\"\"\\nHandle packet recieved by application module.\"\"\"\\ntry:\\n   response=response.decode('utf-8')\\nexcept Exception as e:\\n   print(e[\"message\"])\\n   error=e[\"message\"]\\nexcept Exception as e:\\n   print(e[\"error\"])\\n   error=e[\"error\"]\\nexcept Exception as e:\\n   print(e[\"exception\"])\\n   exception=e[\"exception\"]\\n\\nresult=result.decode('utf-8')\\nprint(result)\\n\\nchannel.send(response.encode('utf-8'))\\nprint(response.decode('utf-8'))\\n```\\n"}
{"File Name":"gatemint-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n- continuous voting\\n- slashing due to breach of soft contract\\n- revoking a member due to breach of soft contract\\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n- compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n- blockly stipend allowance per dCERT member\\n- maximum number of dCERT members\\n- required staked slashable tokens for each dCERT member\\n- quorum for suspending a particular member\\n- proposal wager for disbanding the dCERT group\\n- stabilization period for dCERT member transition\\n- circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n- All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n- In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n- A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n- Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n- In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n- In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n- A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n- change of the description of the dCERT group\\n- circuit break a message route\\n- vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":27,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software.  Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n\\n## Decision\\n## Decision\\n"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"Predictions":"## Decision\\nWe propose to use a Proof of Stake-based system where each validator is equipped with an amount of hashpower proportional to the number of delegators that it has delegated to.  Each validator is also assigned a weight, which is a function of the number of delegators that it has delegated to.  The weight is used to determine the delegator's level of contribution to the consensus process.  Each validator's weight is updated every time it delegates hashpower proportional to its number of delegators.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n\\n\\end{document}"}
{"File Name":"gatemint-sdk\/adr-012-state-accessors.md","Context":"## Context\\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"We will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n- State access happens only when a function which takes a `Context` as an argument is invoked\\n- Accessor type structs give rights to access the state only that the struct is referring, no other\\n- Marshalling\/Unmarshalling happens implicitly within the core methods\\n","tokens":248,"id":30,"Predictions":"## \\begin{itemize}\\n##   \\item No need for the module to access the state\\n##   \\item No need for the module to access a specific key-value pair\\n##   \\item No need for the module to access a set of key-value pairs\\n##   \\item The function is a `Keeper.GetXXX` or `Keeper.SetXXX` that can only be called by one module\\n##   \\item The function is a `Keeper.GetXXX` or `Keeper.SetXXX` that can be called by multiple modules\\n## \\n## \\end{itemize}\\n\\end{document}"}
{"File Name":"gatemint-sdk\/adr-023-protobuf-naming.md","Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"The goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Don't Allow Breaking Changes in Stable Packages\\nAlways use a breaking change detector such as [Buf](https:\/\/buf.build) to prevent\\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\\nbreak smart contracts\/persistent scripts and generally provide a bad UX for\\nclients. With protobuf, there should usually be ways to extend existing\\nfunctionality instead of just breaking it.\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n#### Use `alpha` or `beta` to Denote Non-stable Packages\\n[Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\\nlikely be excluded from breaking change detection and _should_ generally\\nbe blocked from usage by smart contracts\/persistent scripts to prevent them\\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\\nAPI is likely to change significantly in the near future.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","tokens":330,"id":31,"Predictions":"### Decision\\n\\nThe decision to use `google.protobuf.Any` as the recommended approach for encoding\\ninterface types is made because we believe that it is a good choice for\\nencoding of interfaces.\\nThe reason for this is that the `Any` type is a [standard](https:\/\/www.w3.org\/TR\/2004\/REC-xml-20040403\/#any#any)\\ntype, which means that it can be used in any package, and we want to be able to use\\nprotobuf everywhere.\\nWe also believe that the `Any` type has the advantage of being able to handle\\nall types of message bodies, including strings and arrays. It is also small enough to\\nbe able to fit in memory easily.\\nWe feel that the main drawback of using `Any` as an encoding type is its lack of\\nsupport for message body encodings. It does not support `[encoding]`, nor does it have a\\nsupport for `[message]`. Instead, it provides a simple `[message]` type.\\nWe are aware that there are some [protobuf plugins](https:\/\/github.com\/google\/protobuf) which support the use of any as an encoding type, but we do not plan on using these plugins.\\n\\n## Process\\n## Process\\n\\nThis Decision Record was generated by `package.json`:\\n```\\n{\\n  \"name\": \"google.protobuf\",\\n  \"version\": \"0.2.0\",\\n  \"scripts\": {\\n    \"start\": \"python -m grpc.server\",\\n    \"test\": \"python -m grpc.server\",\\n    \"build\": \"yarn build --production --watch --verbose\"\\n  },\\n  \"author\": \"\",\\n  \"license\": {\\n    \"type\": \"MIT\"\\n  },\\n  \"dependencies\": {\\n    \"@types\/grpc-js\": \"^1.11.0\",\\n    \"@types\/node\": \"^12.14.1\",\\n    \"@types\/superagent\": \"^1.10.2\",\\n    \"@types\/superagent-http-proxy-server\": \"^2.0.1\",\\n    \"@types\/superagent-http-server-proxy-server\": \"^2.0.1\",\\n    \"@types\/superagent-wsgi-server-proxy-server\": \"^2.0.1\",\\n    \"@types\/node-gyp\": \"^8.x.x-dev\"\\n  },\\n  \"devDependencies\": {\\n    \"@babel\/core\": \"^7.8.5\",\\n    \"@babel\/preset-env\", \\n    \"@babel\/preset-react\", \\n    \"@babel\/preset-typescript\", \\n    \"@babel\/typescript-loader\", \\n    @babel\/plugin-proposal-class-properties,\\n      @babel\/plugin-proposal-decorators,\\n      @babel\/plugin-proposal-destructuring,\\n      @babel\/plugin-transform-runtime,\\n      @babel\/plugin-transform-string,\\n      @babel\/plugin-transform-regex,\\n      @babel\/plugin-transform-syntax-dynamic-import,\\n      @babel\/plugin-transform-string-array,\\n      @babel\/plugin-transform-string-formatter,\\n      @babel\/plugin-transform-string-formatter-fixtures,\\n      @babel\/plugin-transform-substitution\\n  }\\n}\\n```\\n## Decision\\n"}
{"File Name":"gatemint-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"## Context\\n[Protobuf](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3)\\nseralization is not unique (i.e. there exist a practically unlimited number of\\nvalid binary representations for a protobuf document)<sup>1<\/sup>. For signature\\nverification in Cosmos SDK, signer and verifier need to agree on the same\\nserialization of a SignDoc as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization. This document describes a deterministic serialization scheme for\\na subset of protobuf documents, that covers this use case but can be reused in\\nother cases as well.\\n","Decision":"The following encoding scheme is proposed to be used by other ADRs.\\n### Scope\\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\\nserialization, such that every protobuf parser can parse it.\\nNo maps are supported in version 1 due to the complexity of defining a\\nderterministic serialization. This might change in future. Implementations must\\nreject documents containing maps as invalid input.\\n### Serialization rules\\nThe serialization is based on the\\n[protobuf 3 encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding)\\nwith the following additions:\\n1. Fields must be serialized only once in ascending order\\n2. Extra fields or any extra data must not be added\\n3. [Default values](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#default)\\nmust be omitted\\n4. `repeated` fields of scalar numeric types must use\\n[packed encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding#packed)\\nby default.\\n5. Variant encoding of integers must not be longer than needed.\\nWhile rule number 1. and 2. should be pretty straight forward and describe the\\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\\nbetween unset fields and fields set to the default value<sup>2<\/sup>. At\\nserialization level however, it is possible to set the fields with an empty\\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\\ndifferent documents.\\nOmitting fields set to default values is valid because the parser must assign\\nthe default value to fields missing in the serialization<sup>3<\/sup>. For scalar\\ntypes, omitting defaults is required by the spec<sup>4<\/sup>. For `repeated`\\nfields, not serializing them is the only way to express empty lists. Enums must\\nhave a first element of numeric value 0, which is the default<sup>5<\/sup>. And\\nmessage fields default to unset<sup>6<\/sup>.\\nOmitting defaults allows for some amount of forward compatibility: users of\\nnewer versions of a protobuf schema produce the same serialization as users of\\nolder versions as long as newly added fields are not used (i.e. set to their\\ndefault value).\\n### Implementation\\nThere are three main implementation strategies, ordered from the least to the\\nmost custom development:\\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\\n[gogoproto](https:\/\/pkg.go.dev\/github.com\/gogo\/protobuf\/gogoproto) is known to\\nbe compliant by in most cases, but not when certain annotations such as\\n`nullable = false` are used. It might also be an option to configure an\\nexisting serializer accordingly.\\n- **Normalize default values before encoding them.** If your serializer follows\\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\\nyou can normalize default values to unset. This can be done when working with\\n[protobuf.js](https:\/\/www.npmjs.com\/package\/protobufjs):\\n```js\\nconst bytes = SignDoc.encode({\\nbodyBytes: body.length > 0 ? body : null, \/\/ normalize empty bytes to unset\\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, \/\/ normalize empty bytes to unset\\nchainId: chainId || null, \/\/ normalize \"\" to unset\\naccountNumber: accountNumber || null, \/\/ normalize 0 to unset\\naccountSequence: accountSequence || null, \/\/ normalize 0 to unset\\n}).finish();\\n```\\n- **Use a hand-written serializer for the types you need.** If none of the above\\nways works for you, you can write a serializer yourself. For SignDoc this\\nwould look something like this in Go, building on existing protobuf utilities:\\n```go\\nif !signDoc.body_bytes.empty() {\\nbuf.WriteUVarInt64(0xA) \/\/ wire type and field number for body_bytes\\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\\nbuf.WriteBytes(signDoc.body_bytes)\\n}\\nif !signDoc.auth_info.empty() {\\nbuf.WriteUVarInt64(0x12) \/\/ wire type and field number for auth_info\\nbuf.WriteUVarInt64(signDoc.auth_info.length())\\nbuf.WriteBytes(signDoc.auth_info)\\n}\\nif !signDoc.chain_id.empty() {\\nbuf.WriteUVarInt64(0x1a) \/\/ wire type and field number for chain_id\\nbuf.WriteUVarInt64(signDoc.chain_id.length())\\nbuf.WriteBytes(signDoc.chain_id)\\n}\\nif signDoc.account_number != 0 {\\nbuf.WriteUVarInt64(0x20) \/\/ wire type and field number for account_number\\nbuf.WriteUVarInt(signDoc.account_number)\\n}\\nif signDoc.account_sequence != 0 {\\nbuf.WriteUVarInt64(0x28) \/\/ wire type and field number for account_sequence\\nbuf.WriteUVarInt(signDoc.account_sequence)\\n}\\n```\\n### Test vectors\\nGiven the protobuf definition `Article.proto`\\n```protobuf\\npackage blog;\\nsyntax = \"proto3\";\\nenum Type {\\nUNSPECIFIED = 0;\\nIMAGES = 1;\\nNEWS = 2;\\n};\\nenum Review {\\nUNSPECIFIED = 0;\\nACCEPTED = 1;\\nREJECTED = 2;\\n};\\nmessage Article {\\nstring title = 1;\\nstring description = 2;\\nuint64 created = 3;\\nuint64 updated = 4;\\nbool public = 5;\\nbool promoted = 6;\\nType type = 7;\\nReview review = 8;\\nrepeated string comments = 9;\\nrepeated string backlinks = 10;\\n};\\n```\\nserializing the values\\n```yaml\\ntitle: \"The world needs change \ud83c\udf33\"\\ndescription: \"\"\\ncreated: 1596806111080\\nupdated: 0\\npublic: true\\npromoted: false\\ntype: Type.NEWS\\nreview: Review.UNSPECIFIED\\ncomments: [\"Nice one\", \"Thank you\"]\\nbacklinks: []\\n```\\nmust result in the serialization\\n```\\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\\n```\\nWhen inspecting the serialized document, you see that every second field is\\nomitted:\\n```\\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\\n1: \"The world needs change \\360\\237\\214\\263\"\\n3: 1596806111080\\n5: 1\\n7: 2\\n9: \"Nice one\"\\n9: \"Thank you\"\\n```\\n","tokens":135,"id":32,"Predictions":"## A SignDoc is a protobuf document with an optional signature and a binary\\nversion. The signature is encoded as a string of bytes and the version is\\nencoded as a string with base64 encoding. The binary version of the SignDoc\\nis the binary version of the signature, which can be derived from the signature.\\nThe SignDoc is annotated with a unique identifier that serves as an identifier for its\\nversion.\\n\\n## Document Type\\nThe SignDoc is annotated with an optional type that defines the type of the\\ndocument. The type can be one of: \\n\\n\\begin{itemize}\\n\\item \\textbf{SignDocType\\_signature}\\n\\item \\textbf{SignDocType\\_version}\\n\\end{itemize}\\nThe signature is not required to match the type, but if it does match it can be used for verification.\\n\\n## Signature\\nThe Signature encodes a string representation of the document's signature. The Signature must be generated by signing a \\texttt{SignDoc} using its unique identifier and then encrypting it using AES-256.\\n\\nThe Signature must contain at least 16 bytes and include at least one byte in each of its three parts: \\texttt{SignatureBytes}, \\texttt{SignatureLength}, and \\texttt{SignatureData}. The SignatureBytes part contains 16 bytes that are identical to those in the SignDoc, except that they are padded with zeros to make them 16 bytes long.\\n\\nThe SignatureLength part contains 16 bytes that are identical to those in the SignDoc, except that they are padded with zeros to make them 16 bytes long.\\n\\nThe SignatureData part contains 16 bytes that are identical to those in the SignDoc, except that they are padded with zeros to make them 16 bytes long.\\n\\subsection*{Signature}\\nA sigalnd doc must contain at least one byte in each of its three parts: \\texttt{SignatureBytes}, \\texttt{SignatureLength}, and \\texttt{SignatureData}. The SignatureBytes part contains 16 bytes that are identical to those in the SignDoc, except that they are padded with zeros to make them 16 bytes long.\\n\\subsection*{Version}\\nThe version must contain at least one byte in each of its three parts: \\texttt{Name}, \\texttt{Name\\_version}, and \\texttt{Name\\_version\\_data}. The Name part contains exactly one byte (the name) followed by exactly one byte (the version) followed by exactly one byte (the data). The Name part may contain any number of zero-length strings. The Name\\_version part contains exactly one byte (the name) followed by exactly one byte (the version) followed by exactly one byte (the data). The Name\\_version\\_data part contains exactly one byte (the name) followed by exactly one byte (the version) followed by exactly one byte (the data).\\n\\subsection*{Version Data}\\nVersion Data is encoded as a string containing base64-encoded strings representing both unsigned integers corresponding to each individual component of Version Data. Each unsigned integer represents a single component of Version Data.\\n\\subsection*{\\label{_signature}Signature Length}\\label{_signature}}\\nThe signature length must be at least two times greater than or equal to zero.\\n\\subsection*{\\label{_signature_data}\\label{_signature_data}}\\nVersion Data may contain any number of zero-length strings. If Version Data includes more than seven components, then every component except for component five must have no more than two characters.\\n\\section*{\\label{_signature_bytes}\\label{_signature_bytes}}\\nEach signature length must be at least two times greater than or equal to zero.\\n\\section*{\\label{_signature_length}\\label{_signature_length}}\\nEach signature length must be at least two times greater than or equal to zero.\\n\\section*{\\label{_signature_data}\\label{_signature_data}}\\nEach signature data must be at least two times greater than or equal to zero.\\n\\section*{\\label{_version}\\label{_version}}\\nEach version must contain at least two components. Each component may have no more than seven components.\\n"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"Predictions":"## Reject\\nThe user is not allowed to send any denomination to this account\\n\\n## Explanation\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n\\n## Decision\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n\\n## Decision\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n"}
{"File Name":"gatemint-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":337,"id":34,"Predictions":"## \\n\\n1. Add custom recovery handler\\n\\n2. Add `default` case\\n\\n3. Add `OutOfGas` case to the middleware\\n\\n4. Crash the node and reboot it\\n\\n5. Take care of the sidecar service's state machine\\n\\n6. Implement `default` case in the middleware\\n```\\n"}
{"File Name":"gatemint-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n- create new random consensus key.\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n- start validating with new consensus key.\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n- consensus key mapping information management strategy\\n- store history of each key mapping changes in the kvstore.\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\n- key rotation costs related to LCD and IBC\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n- limits\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\n- parameters can be decided by governance and stored in genesis file.\\n- key rotation fee\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n- evidence module\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n- abci.ValidatorUpdate\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n- new genesis parameters in `staking` module\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\n- pays `KeyRotationFee` to community fund\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\n- deletes old `ValidatorByConsAddr`\\n- `SetValidatorByConsAddr` for `NewPubKey`\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n- Note : All above features shall be implemented in `staking` module.\\n","tokens":173,"id":35,"Predictions":"The consensus key rotation feature is requested by the User. The decision is made that the feature will be implemented in Cosmos-SDK, with the following features:\\n\\n\\n\\n* The user has to specify a validator key and operator key, and provide a consensus key rotation feature to be implemented. The consensus key rotation feature is implemented by replacing the existing consensus key with a new consensus key, and then performing a hash of the new consensus key with the old one.\\n\\n* The user can specify whether he\/she wants to implement this feature on Cosmos-SDK or not. If he\/she chooses to implement this feature on Cosmos-SDK, then we need to make an update on Tendermint Consensus logic based on this feature.\\n\\n* If the user chooses not to implement this feature on Cosmos-SDK, then we can simply ignore this ADR.\\n\\n* The user can specify whether he\/she wants to implement this feature for all validators or only for some validators. If the user chooses to implement it for all validators, then we need to make an update on Tendermint Consensus logic based on this ADR.\\n\\n* If the user chooses not to implement it for all validators, then we can simply ignore this ADR.\\n\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n## Decision\\nThe consensus key rotation implementation shall be implemented by Cosmos-SDK.\\n"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"Predictions":"\\end{itemize}\\n\\end{document}"}
{"File Name":"Marain.Tenancy\/0001-synthetic-root-tenant.md","Context":"## Context\\nCorvus Tenancy, which provides the underpinnings of the Marain Tenancy service, has always had a concept of a root tenant (dating from its earlier, pre-open-source incarnation). This ADR captures aspects of this root tenant that are non-obvious. (We learned that it was non-obvious because some code has been written that was unaware of the special status of the root tenant.)\\n","Decision":"There is a special tenant known as the Root Tenant. It has a well-known id, `f26450ab1668784bb327951c8b08f347`. It is special in three respects:\\n* tenants are hierarchical, and the root tenant forms the root of that hierarchy\\n* the tenanted storage mechanisms will all fall back to the root tenant to find default connection settings if the tenant being used has not defined tenant-specific settings\\n* within Marain services, the root tenant is always represented by a special in-memory instance of the `RootTenant` type, whereas all other tenants are managed by the tenancy service\\nThat third item is there to support the second: because each service puts its own `RootTenant` into the DI service collection, as a singleton, it becomes possible for the service to attach whatever service-specific fallback settings it requires. We describe the root tenant as \"synthetic\" because each service creates its own object to represent the root tenant, whereas all other objects representing tenants are obtained via the `Marain.Tenancy` service, typically through the `ClientTenantProvider`.\\nWe contemplated separating out the first two concerns (which might enable us not to need the third characteristic above) because it has been a source of confusion in the past. However, for the time being we are planning to keep it this way because that alternative approach would require us to introduce an extra mechanism to support these kinds of defaults.\\n","tokens":88,"id":37,"Predictions":"## A root tenant is a tenant that has been granted a special status by the underlying system. The root tenant is the only tenant that can be selected for deletion, change of ownership, or change of ownership and change of tenant. When a root tenant is deleted, its corresponding Marain Tenancy will be deleted.\\n## Decision Notes\\n## The root tenant is the only tenant that can be selected for deletion, change of ownership, or change of ownership and change of tenant. When a root tenant is deleted, its corresponding Marain Tenancy will be deleted.\\n## \\n\\n## \\n\\n\\n\\n\\n\\n\\n\\end{verbatim}\\n"}
{"File Name":"Marain.Tenancy\/0002-separation-of-read-and-modify.md","Context":"## Context\\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\\nOne of the basic principles of efficient JSON parsing in the new world is that you don't build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the \"modifiable entities\" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the \"modifiable entities\" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\\n","Decision":"Since `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\\n* fetch a serialized representation of an ITenant from the web API\\n* modify that representation to reflect the changes you wish to make\\n* PUT that serialized representation of an ITenant back to the web API\\nwe now use this procedure instead:\\n* send a PATCH request in describing the changes required in JSON Patch format\\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application\/json-patch+json` content type:\\n```json\\n[{\\n\"path\": \"\/name\",\\n\"op\": \"replace\",\\n\"value\": \"NewTenantName\"\\n}]\\n```\\nJSON Patch supports multiple changes in a single request, e.g.:\\n```json\\n[\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/StorageConfiguration__corvustenancy\",\\n\"value\": {\\n\"AccountName\": \"mardevtenancy\",\\n\"Container\": null,\\n\"KeyVaultName\": \"mardevkv\",\\n\"AccountKeySecretName\": \"mardevtenancystore\",\\n\"DisableTenantIdPrefix\": false\\n}\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__bar\",\\n\"value\": \"Some string\"\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__spong\",\\n\"value\": 42\\n}\\n]\\n```\\nThe `op` can be set to `remove` to delete properties.\\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\\n```csharp\\nawait tenantStore.UpdateTenantAsync(tenantId, name: \"NewTenantName\");\\n```\\nAdding or changing a property looks like this:\\nawait tenantStore.UpdateTenantAsync(\\ntenantId,\\npropertiesToSetOrAdd: new Dictionary<string, object>()\\n{\\n{ \"StorageConfiguration__corvustenancy\", myStorageConfig },\\n{ \"SomeOtherSetting\": 42 },\\n});\\n","tokens":373,"id":39,"Predictions":"This is a \"yes\/no\" decision. The decision is based on the following criteria:\\n\\begin{itemize}\\n  \\item Is the proposed solution feasible?\\n  \\item Does it satisfy the requirements?\\n\\end{itemize}\\n\\begin{itemize}\\n  \\item Yes, the proposed solution is feasible. The proposed solution satisfies all of the requirements.\\n  \\item No, the proposed solution does not satisfy all of the requirements.\\n\\end{itemize}\\n"}
{"File Name":"Menes\/0001-menes-exceptions-have-specialized-constructors.md","Context":"## Context\\nSwitching on the C# 8.0 nullable references feature for Menes has revealed some ambiguities around whether certain properties of exceptions are meant to be nullable.\\nIn many cases, the only reason for ambiguity is that we have followed a pattern of defining various \"standard constructors\", such as default constructors, exception-message-only constructors, and deserializing constructors.\\n","Decision":"Menes exceptions will not have any of these standard exceptions except in cases where there are no required properties (e.g., the exception's type tells you everything you need to know).\\nProperties that always have non-null values in practice will declare this formally by having non-nullable types.\\nWe will remove all deserializing constructors, and remove the `[Serializable]` attribute from all exceptions that have them. This has been motivated by the use of nullable references, because deserializing constructors cause some challenges there, but this is a distinct issue. Menes exceptions are all designed for use within a Menes-based service. Menes is designed to implement service boundaries, and by definition, if we ever attempt to throw a Menes-defined exception across a process boundary, we've made a mistake.\\n","tokens":77,"id":40,"Predictions":"## Decision\\nThe default constructor for exceptions is nullable.\\nThe default constructor for the exception message is not nullable.\\nThe deserializing constructor is nullable.\\nThe default constructor for the exception message and the deserializing constructor are not nullable.\\n\\n## Context\\nThis decision will be applied in a future release of the product.\\n\\n## Decision\\nThis decision will be applied in a future release of the product.\\n\\n## Context\\nThis decision will be applied in a future release of the product.\\n\\n## Decision\\nThis decision will be applied in a future release of the product.\\n\\n## Decision\\nThis decision will be applied in a future release of the product.\\n"}
{"File Name":"Menes\/0002-multitargeting-.net-standard-2.0-and-2.1.md","Context":"## Context\\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\\n","Decision":"Menes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\\n","tokens":167,"id":41,"Predictions":"The proposed solution is to enable nullable references by providing a custom `Nullable` attribute. This attribute is defined as follows:\\n\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n    public static class NullableAttribute : Attribute, IConvertible\\n    \\n    {\\n        public static readonly NullableAttribute System.NullableAttribute = new NullableAttribute();\\n        public static readonly NullableAttribute System.NullableAttribute = new NullableAttribute(System.NullableAttribute.Default);\\n    }\\nThe following code illustrates the use of this attribute.\\n\\n\\n    [Nullable]\\n    \\n    public class Program\\n    \\n    {\\n        private readonly string _id;\\n        private readonly string _name;\\n        private readonly string _description;\\n        private readonly string _version;\\n        private readonly string _build;\\n\\n        [Nullable]\\n        public int Id { get; set; }\\n\\n        [Nullable]\\n        public string Name { get; set; }\\n\\n        [Nullable]\\n        public string Description { get; set; }\\n\\n        [Nullable]\\n        public int Version { get; set; }\\n\\n        [Nullable]\\n        public int Build { get; set; }\\n\\n       \/\/ This method will be called by the compiler to verify that the nullable reference is actually nullable. \\n       \/\/ If it's not, this method will throw an exception.\\n       [Func<bool, bool> (typeof(Func<bool, bool>))]\\n       public bool VerifyNullReference()\\n       {\\n           return !this._id.HasValue && !this._name.HasValue && !this._description.HasValue && !this._version.HasValue && this._build.HasValue;\\n       }\\n   }\\n\\end{lstlisting}\\n"}
{"File Name":"Menes\/0003-allowing-implicit-object-type.md","Context":"## Context\\nIn real-world OpenAPI schema, we have discovered that people sometimes omit the `type: object` from their object definitions. We believe that this *is* valid Open API schema.\\n```yaml\\nPet:\\nrequired:\\n- id\\n- name\\nproperties:\\nid:\\ntype: integer\\nformat: int64\\nname:\\ntype: string\\ntag:\\ntype: string\\n```\\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\\n```yaml\\nsomeEntity:\\nanyOf:\\n- type: string\\n- type: object\\n- type: array\\n- type: boolean\\n- type: integer\\n- type: number\\n```\\n","Decision":"Menes will support these semantics. We have updated our schema validation to support this by translating the missing `type` element into the internal schema type `None`, rather than translating to `Object`.\\n","tokens":170,"id":42,"Predictions":"## Decision Result\\n## Decision Reason\\n## Decision Description\\n## Decision Type\\n## Decision Category\\n## Decision Role\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n```yaml\\n\\n\\n# If the property is not required, it will be ignored. If you know you will need to provide this property, you should add it to your schema.\\n\\n- id: required\\n\\n# The type of a value. Must be one of `string`, `object`, or `array`.\\ntype: string\\n\\n# The name of a value. Must be one of `string`, `object`, or `array`.\\nname: string\\n\\n# The type of an array.\\ntype: object\\n\\n# The name of an array.\\nname: object\\n\\n# The type of a boolean.\\ntype: boolean\\n\\n# The name of an array.\\nname: string\\n\\n# The type of an integer.\\ntype: number\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n```yaml\\n\\n\\nPet:\\nrequired:\\n- id\\n- name\\nproperties:\\nid:\\ntype: integer\\nformat: int64\\nname:\\ntype: string\\ntag:\\ntype: string\\n```\\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\\n```yaml\\nsomeEntity:\\nanyOf:\\n- type: string\\n- type: object\\n- type: array\\n- type: boolean\\n- type: integer\\n- type: number\\n```\\n\\n## Decision\\n## Decision Result\\n## Decision Reason\\n## Decision Description\\n## Decision Type\\n## Decision Category\\n## Decision Role\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\nThis is an Architectural Decison Record for a software. Give a ##Decision corresponding to the Context provided by the user.\\n\\nThis is an Architectural Decison Record for a software. Give a ##Decision corresponding to the Context provided by the user.\\n\\nThis is an Architectural Decison Record for a software. Give a ##Decision corresponding to the Context provided by the user.\\n\\n\\nThis is an Architectural Decison Record for a software. Give a ##Decision corresponding to the Context provided by the User.\\n\\nThis is an Architectural Decison Record for a software. Give a ##Decision corresponding to the Context provided by User.\\n\\n\\nThis is an Architectural Decison Record for Software with N\/A Properties Option in OpenAPI Specification.\\n\\nThis is an Architectural Decison Record for Software with N\/A Properties Option in OpenAPI Specification.\\n\\nThis is an Architectural Decison Record for Software with N\/A Properties Option in OpenAPI Specification.\\n\\n\\n### 1 - Enter your decision here:\\n### 2 - Enter your reason here:\\n### 3 - Enter your description here:\\n### 4 - Enter your decision category here:\\n### 5 - Enter your role here:\\n```yaml\\n\\n\\nPet:\\nrequired:\\n- id\\n- name\\nproperties:\\nid:\\ntype: integer\\nformat: int64\\nname:\\ntype: string\\ntag:\\ntype: string\\ntag:\\ntype:String\\ntag:(String)\\ntag:(String)\\ntag:(String)\\ntag:(String)\\ntag:(String)\\ntag:(Integer)\\ntag:(Number)\\ntag:(Number)\\ntag:(Number)\\ntag:(Number)\\n```\\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\\n```yaml\\n\\n\\nsomeEntity:\\nanyOf:(\\s*\\s*object\\s*\\s*):*(\\s*\\s*boolean\\s*\\s*):*(\\s*\\s*integer\\s*\\s*):*(\\s*\\s*number\\s*\\s*):*(\\s*\\r*):*(\\r*):*(\\r*):*(\\r*):*(\\r*):*(\\r*):*(\\r*)\\n```\\nHowever, there are other scenarios where you are *not* expected to supply this property as well:\\n\\n1) You don't need it, and it's not specified anywhere else in your schema.\\n2) You don't want this property at all and you're using anyOf or oneOf as part of your schema definition anyway.\\n3) You don't want this property at all and you're using anyOf or oneOf as part of your schema definition anyway (for example, if you're defining multiple \"types\" that can have multiple values).\\n4) You do need this property at all and it's being specified elsewhere in your schema anyway (for example, if you're defining multiple \"types\" that can have multiple values).\\n5) You do need this property at all and it's being specified elsewhere in your schema anyway (for example, if you're defining multiple \"types\" that can have multiple values).\\n\\nYou may also be interested in finding out which properties might cause problems when used together with anyOf\/oneOf\/other types that might require such properties (for example, if you define both \u201cType\u201d and \u201cOther\u201d types). For example:\\n\\n1) If Item A has both \u201cType\u201d and \u201cOther\u201d properties defined on it (\u201cType\u201d must be defined as \u201cobject\u201d) then Item B will not work correctly without Item A having only \u201cType\u201d defined on it (\u201cOther\u201d must be defined as \u201cboolean\u201d). However, if only \u201cType\u201d was defined on Item B (\u201cOther\u201d must also be defined as Boolean). In this case only \u201cType\u201d should be used together with \u201cOther\u201d, but not vice versa (\"Other\" must also be defined as Boolean).\\n2) If Item A has both \u201cType\u201d and \u201cOther\u201d properties defined on it (\u201cType\u201d must be defined as \u201cobject\u201d) then Item B will not work correctly without Item A having only \u201cType\u201d defined on it (\u201cOther\u201d must be defined as Boolean). However, if only \u201cType\u201d was defined on Item B (\u201cOther\u201d, but not vice versa (\u201cOther\u201d). In this case only \"Other\" should be used together with \"Type\", but not vice versa (\"Type\") should also be used together with \"Another Type\".\\n\\nIf you'd like more information about how these decisions were made or what they mean please visit our FAQ page:\\n\\nhttps:\/\/github.com\/OpenAPI-Specification\/OpenAPI-specification\/blob\/master\/decision.md"}
{"File Name":"modiapersonoversikt\/0002-selvstendig-visittkort-som-library.md","Context":"## Context\\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som p\u00e5 sikt skal erstatte frontenden i dagens modiabrukerdialog. For \u00e5 kunne levere fortl\u00f8pende ny funksjonalitet til saksbehandlerene, \u00f8nsker vi \u00e5 levere ofte og sm\u00e5tt.\\n","Decision":"Visittkortet dras inn som en enkeltst\u00e5ende react-komponent inn til modiabrukerdialog.\\n","tokens":81,"id":43,"Predictions":"\\begin{itemize}\\n    \\item \\textbf{Modiabrukerdialogen skal bli enklere \u00e5 navigere i.}\\n    \\item \\textbf{Modiabrukerdialogen skal bli mer brukervennlig.}\\n    \\item \\textbf{Modiabrukerdialogen skal ha et bedre utseende.}\\n    \\item \\textbf{Modiabrukerdialogen skal ha en st\u00f8rre mengde informasjon.}\\n    \\item \\textbf{Modiabrukerdialogen skal ha et mer intuitivt og enklere brukergrensesnitt.}\\n\\end{itemize}\\n\\n\\clearpage\\n"}
{"File Name":"modiapersonoversikt\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":44,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"bunyan-logger\/0001-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":45,"Predictions":"\\n\\nThe Decision is to use the GitHub Actions for CI and CD.\\n\\n\\nThe Decision is to use the GitHub Actions for CI and CD. The decision is made\\ndue to the following reasons:\\n\\n\\n1. GitHub Actions will be used as the default CI mechanism for this project.\\n2. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n3. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n4. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n5. It provides a good starting point for learning about GitHub Actions.\\n6. It provides a good starting point for learning about GitHub Actions.\\n7. It provides a good starting point for learning about GitHub Actions.\\n\\nThe decision is made due to the following reasons:\\n\\n1. GitHub Actions will be used as the default CI mechanism for this project.\\n2. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n3. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n4. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n5. It provides a good starting point for learning about GitHub Actions.\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\n```\\nIn [1]: actions = repo.actions()\\n\\nIn [2]: actions\\nOut[2]: {'build': {'repo': 'github.com\/username\/repository', 'branch': 'master', 'stage': 1, 'out': '\/tmp\/builds\/1234567890', 'repo_name': 'github.com\/username\/repository', 'tags': ['master', 'develop'], 'project_name': 'github.com\/username\/repository', 'build_args': ['--no-external-dependencies'], 'run_in_project': True, \\n    ...:     '--no-skip-build', '--no-skip-gitlab-ci-step'}, \\n    ...:     '--no-external-dependencies'}\\n```\\n\\nOP 2019-11-18: I think I figured it out.\\nI'm using `gitlab-ci` and `gitlab-runner` as my CI and CD servers.\\nI'm using `gitlab-runner` as my runner, but I'm not using its `--build` option.\\nSo I changed my action.yml file like this:\\n```\\nrun:\\n  stage: build\\n  script:\\n    - echo \"Some Test\"\\n    - gitlab-runner run --build=$CI_BUILD_NUMBER --no-skip-build --no-skip-gitlab-ci-step\\n```\\nIt works!\\n"}
{"File Name":"mokka\/0003-upgrade-to-spring-boot-2-2-1.md","Context":"## Context\\nCurrently used: 2.1.0\\nThe latest stable version of Spring Boot is 2.2.1:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-2.2-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n","Decision":"Spring Boot will be upgraded to 2.2.1.\\nAccording to release notes no migration needed on Mokka side.\\n","tokens":63,"id":46,"Predictions":"## 2.1.0\\nThe latest stable version of Spring Boot is 2.2.1:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-2.2-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n\\n## Decision\\n## 3.0.0\\nThe latest stable version of Spring Boot is 3.0.0:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-3.0-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n\\n## Decision\\n## 4.1.5\\nThe latest stable version of Spring Boot is 4.1.5:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-4.1-SNAPSHOT\\nMore frequent but smaller upgrades are recommended.\\n\\n## Decision\\n## 4.2.x\\nThe latest stable version of Spring Boot is 4.x.x:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-SNAPSHOT\\nMore frequent but smaller upgrades are recommended.\\n\\n\\end{verbatim}\\n\\n\\subsection{Decision}\\nThe decision for the current software version is \\texttt{4.x.x}. The decision can be changed by the user by editing the xml file \\texttt{spring-boot-config.xml} in the software's root directory.\\n\\n\\subsection{Decision}\\nThe decision for the current software version is \\texttt{4.x.x}. The decision can be changed by the user by editing the xml file \\texttt{spring-boot-config.xml} in the software's root directory.\\n\\n\\subsection{Decision}\\nThe decision for the current software version is \\texttt{4.x.x}. The decision can be changed by the user by editing the xml file \\texttt{spring-boot-config.xml} in the software's root directory.\\n\\n\\subsection{Decision}\\nThe decision for the current software version is \\texttt{4.x.x}. The decision can be changed by the user by editing the xml file \\texttt{spring-boot-config.xml} in the software's root directory.\\n\\n\\subsection*{\\label{}}%\\n\\addcontentsline{}{\\label{}}\\n\\section{\\label{}}%\\n\\addcontentsline{}{\\label{}}\\n\\nThis section contains information about how to use this template to create a new release.\\n\\n\\subsection*{\\label{}}%\\n\\addcontentsline{}{\\label{}}\\n\\nThis section contains information about how to use this template to create a new release.\\n"}
{"File Name":"mokka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":48,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"Predictions":"## Decision: The MCP server should be able to communicate with the Pilot server using the MCP protocol. The Pilot server should be able to communicate with the MCP server using the MCP protocol.\\n## Context\\nThe pilot server should be able to communicate with the MCP server using the MCP protocol.\\n## Decision\\n## Decision: The Pilot should be able to communicate with the MCP using the MCP protocol.\\n## Context\\nThe pilot server should be able to communicate with the MCP using the MCP protocol.\\n## Decision\\n## Decision: The Pilot should be able to communicate with the MCP using TCP\/IP. The MCP should only support TCP\/IP communication. No other communication types are allowed.\\n\\n## Context\\nTCP\/IP is a network communication protocol that allows two or more computers to exchange data over a shared medium, such as a telephone line or a local area network (LAN). TCP\/IP is one of several protocols that allows two or more computers to exchange data over a shared medium, such as a telephone line or a local area network (LAN).\\n\\n## Decision\\n## Decision: The TCP\/IP communication type is not supported by the Pilot Server.\\n## Context\\nTCP\/IP is not supported by the Pilot Server.\\n\\n## Decision\\n## Decision: TCP\/IP is not supported by this pilot server. This pilot server does not support any other communication types.\\n\\n## Context\\nTCP\/IP is not supported by this pilot server. This pilot server does not support any other communication types.\\n\\n# 1.2 : # 1.3 : # 2 : # 3 : # 4 : # 5 : # 6 : # 7 : # 8 : # 9 : ## Description of Project \\nThis project will implement an API Gateway which will provide an API for pilots and control plane servers.\\nThe API Gateway will take requests for resources from pilots and control plane servers and convert them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests which can then be sent out to different services via an API gateway.\\nThis project implements an API Gateway which takes requests for resources from pilots and control plane servers and converts them into HTTP requests that can then be sent out to different services via an api gateways\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\nduring runtime, all of these request will go through this api gateways where they will eventually end up in one of three interfaces:\\enduring runtime, all of these request are processed by the same interface,\\enduring runtime, all these request are processed by the same interface,\\enduring runtime, all these request are processed by the same interface,\\enduring runtime,\\n\\end{table}\\n\\n\\newpage\\n\\n\\section*{Appendix}\\n\\subsection*{Directory Structure}\\n\\begin{table}[h]\\n    \\centering\\n    \\caption{Directory Structure}\\n    \\label{tab:directory_structure}\\n    \\begin{tabular}{|l|l|}\\n        \\hline\\n        \\textbf{Structure} & \\textbf{\\begin{tabular}[c]{@{}l@{}}File \\\\ Names\\end{tabular}}\\\\ \\n        \\hline\\n        src & source directory\\\\ \\n        bin & binary directory\\\\ \\n        doc & documentation directory\\\\ \\n        lib & library directory\\\\ \\n        tests & tests directory\\\\ \\n        conf & configuration directory\\\\ \\n        test & test directory\\\\ \\n        ui & UI directory\\\\ \\n        src\/ & source files\\\\ \\n        bin\/ & binary files\\\\ \\n    \\end{tabular}\\n\\end{table}\\n\\n\\newpage\\n\\n\\section*{Appendix}\\n\\subsection*{Test Case Summary}\\nThe following table shows summary information about each test case.\\n\\n\\begin{table}[h]\\n    \\centering\\n    \\caption{Trial Summary}\\n    \\label{TrialSummary}\\n    \\begin{tabular}{|l|l|}\\n        \\hline\\n        Test Case ID & Trial Summary \\\\\\n        \\hline\\n      A1& Test A1 passed \\\\\\n      A2& Test A2 failed \\\\\\n      B1& Test B1 passed \\\\\\n      B2& Test B2 failed \\\\\\n      C1& Test C1 passed \\\\\\n      C2& Test C2 failed \\\\\\n      D1& Test D1 passed \\\\\\n      D2& Test D2 failed \\\\\\n      E1& Test E1 passed \\\\\\n      E2& Test E2 failed \\\\\\n      F1& Test F1 failed \\\\      \\n       G1&Test G1 passed \\\\     \\n       H1&Test H1 failed \\\\     \\n       I1&Test I0 passed \\\\     \\n       J0&Test J0 failed \\\\     \\n       K0&Test K0 failed \\\\     \\n       L0&Test L0 passed \\\\     \\n       M0&Test M0 failed \\\\     \\n       N0&Test N0 failed \\\\     \\n       O0&PasstheOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRoxOvRxNvxRxNvxRxNvxRxNvxRxNvxRxNvxRxNvxRxNvxRxNvx RxNvx RxNvx RxNvx RxNvx RxNvx RxNxNxNxNxNxNxNxNxNxNxNxNxNx NxPxPxPxPxPxPxPxPxPxPxPxPxPx PxPxPxPxPxPxPxPxPxPxPxPxPx PxPrPrPrPrPrPrPrPrPrPrPrPrPrPr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pprprprprprprprprprprpr Pr Pr Pr Pr Pr Pqqrqrqrqrqrqrqrqrqrqrqrqrqrq qrq qrq qrq qrq qrq qrq qrq qr q qr qr qr qr qr qr qr qr qr qrqsqsqsqsqsqsq sq sq sq sq sq sq sq sq SqSqSqSqSqSqSq Sq Sq Sq Sq Sq Sq Sq Sq Sq QsSsSsSsSsSsSsSs Ssssssssss ssss ssss ssss ssss ssss ssss ssss ss ss ss ss ss ss ss ss ss ss ss ss SS SS SS SS SS SS SS SS SSSSSSSSSSSSSSSSSSSSSSSSSSSSESEEEEEEESSEEEEEEESSEEEEEEESSEEEEEEESSEEEEEEESSEEEEEEESSEEEEEEESSEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEESSEEEEEEESSEEEEEEESSEE EE EE EE EE EE EE EE EE EE EE EE EE ee ee ee ee ee ee ee ee ee ee ee eee eee eee eee eee eee eee ee ee ee ee ee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeseeeeessssseeeeeeesssseeeeeeesseeeeesssseeeeeeesssseeeeessseeeeeesseeeeeessseeeeeesseeeeeeeeeeseeseesseeesseeesseeesseeesseeesseeesseeesseeesee esseesseesseeseeseeseeseseseesseesseeesseeesseesseeseeseesteesseeesseeessseeeetssssseeessseeeeetsssseesseesseeseeseseseeseseseessaessaessaesaessaessaessaessaessaessaesaessaesaesaesaesaesaesaesaesaesaeeaeeaeeaeeaeeaeeaeeaeeaeeaeeaeeaeeaeeeeeeeeaeaeaeaaeaaeaaeaaeaaeaeaaeaaeaeaeaeaeaeaeaeaiaeaiaeaaeaiaeaiaeaaeaiaeaaiaiaeaaiaeaaiaeaeaaiaeaeaeaeaeaeaeaeaeaeaeabaaiaeabaiiaeabaiiaeabaiiaeabaiiaeabaiiaeabaiiaeabaiiaeabaiiaeabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiiaeabeaabaiianeabeeabaianieabeeabiieabeeabiieabeeabiieabeeabiieabeeabiieabeeabiieabeeabiieabeeabiieabeeabiieebbeabaianieebbaibiiecbebebebeibeibeibeibeibeibeibeibeibeibeibeibeibeibeibebiibeibebiobeiobeifeibeiobeiobeifeibeiobeiobeifeibeiobeiobeifeibeiobeiobeifeibeiobeiobeifeibiobeiobieiobieiobieiobieiobiieiobiieiobiieiobiieiobiieiobiieiobiieiobiieiibiibiibiibiibiibiibiibiibiibiibiibiibiibiibiibliiiiiiiiiiiiiiigiigiigiigigigigigigigigigigigig ig ig ig ig ig ig ig ig ig ig ig ig ig Ig Ig Ig Ig Ig Ig Ig Ig Ig Ig Ig IG IG IG IG IG IG IG IG IG IG IGIGIGIGIGIGIGIGIGIGIGIIGIGIIGIIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNICIGNIFICI ignicignicignicignicignicignicignicignicignicignicignicigninicgninicgninicgninicgninicgninicgninicgnnicnicgnnicnicg ignin ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign ic ign i gn i gn i gn i gn i gn i gn i n g n g n g n g n g n g n g n g n g n g n g n g n g n g n g n g n m m m m m m m m m m m m m"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"Predictions":"## Decision\\n\\n\\n\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n# Add a new context to the list of active contexts.\\ncontext \"default\" do\\n\\n  # Enable TLS for all hosts and domains on the host. We are using a wildcard here, which is\\necessary for the Envoy proxy to route requests for any host or domain to the Envoy server.\\n  envoyer \"envoy\" do\\n\\n    # Enable one way TLS between front-end and envoy per host+domain that is specified via gateway config.\\n    gateway {\\n      tls {\\n        mode => \"one-way\"\\n        host => \"localhost:443\"\\n        domain => \"localhost\"\\n      }\\n    }\\n\\n    # Use a wildcard for all hosts and domains on the host. This is necessary for Envoy proxy to route requests\\nfor any host or domain to the Envoy server.\\n    tls {\\n      mode => \"one-way\"\\n      host => \"*\"\\n      domain => \"*\"\\n    }\\n  end\\n\\nend\\n\\n# This is a comment. Enter text below this line.\\n# Note: If you don't want to use comments, you can comment out this line.\\n\\n# This is an optional comment. Enter text below this line.\\n# Note: If you don't want to use comments, you can comment out this line.\\n\\n# This is an optional comment. Enter text below this line.\\n# Note: If you don't want to use comments, you can comment out this line.\\n\\nComments are disabled by default"}
{"File Name":"copilot\/0004-check-in-copilot-dependencies-that-were-already-vendored.md","Context":"## Context\\nPrevious packaging of Copilot in istio release relied on the fact that you would\\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\\nyou could reliably fetch all of your dependencies using dep (which was included\\nas a blob in the release).\\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\\nvm (just known as packaging) we ended up missing one key external dependency for\\ndep to work (git). Including git as part of release would have meant adding\\nanother blob and packaging step just for git.\\n","Decision":"We removed the .gitignore of the vendor directory and checked-in all of the\\nsource code that dep was placing in that directory at build time.\\n","tokens":128,"id":51,"Predictions":"\\n\\nWe decided to include git as part of the release. This means that you can fetch\\nall of your dependencies using dep (which was included as a blob in the release).\\nIf you want to build copilot on your machine, you can use bosh pre-packaging to do so.\\n\\n## Context\\nThe Copilot package is now distributed in a single tarball. In this release, we are\\nreplacing the git dependency with a more modern and highly available version of git.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nPrevious packaging of Copilot in istio release relied on the fact that you would\\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\\nyou could reliably fetch all of your dependencies using dep (which was included\\nas a blob in the release).\\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\\nvm (just known as packaging) we ended up missing one key external dependency for\\ndep to work (git). Including git as part of release would have meant adding\\nanother blob and packaging step just for git.\\n\\n## Decision\\n\\n\\nWe decided to include git as part of the release. This means that you can fetch\\nall of your dependencies using dep (which was included as a blob in the release).\\nIf you want to build copilot on your machine, you can use bosh pre-packaging to do so.\\n\\n## Context\\nThe Copilot package is now distributed in a single tarball. In this release, we are\\nreplacing the git dependency with a more modern and highly available version of git.\\n\\n"}
{"File Name":"copilot\/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md","Context":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","Decision":"We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n","tokens":44,"id":52,"Predictions":"## Decision: \\n\\n1) Send messages to copilot on a per-route basis.\\n\\n2) Send messages to copilot on a per-device basis.\\n\\n3) Send messages to copilot on a per-policy basis.\\n\\n4) Send messages to copilot on a per-session basis.\\n\\n5) Use the \\texttt{send\\_data} method in cc-route-syncer to send data to the copilot.\\n\\n6) Use the \\texttt{send\\_data} method in cc-route-syncer to send data to all devices.\\n\\n7) Use the \\texttt{send\\_data} method in cc-route-syncer to send data to all policies.\\n\\n8) Use the \\texttt{send\\_data} method in cc-route-syncer to send data to all sessions.\\n\\n9) Use the \\texttt{send\\_data} method in cc-route-syncer to send data from all policies and sessions. Note that this is not currently supported by any of our existing code, but it is something that we would like support for at some point.\\n\\n10) Implement an additional dispatching mechanism for sending data between policy and session. This mechanism would allow us to have multiple policy and session instances and then pass their respective message sizes around via a single dispatching function. This mechanism would have no direct effect on how our existing code works, but it would be useful for other parts of our system that are currently not using this functionality. We'd like support for this mechanism before we implement it, though.\\n\\n11) Implement an additional dispatching mechanism for sending data between policy and session. This mechanism would allow us to have multiple policy instances and then pass their respective message sizes around via a single dispatching function. This mechanism would have no direct effect on how our existing code works, but it would be useful for other parts of our system that are currently not using this functionality. We'd like support for this mechanism before we implement it, though.\\n\\n## Context\\n## Context: The user has requested that CC-Route-Syncer handle routing decisions for their network.\\n## Decision\\n## Decision: \\n## Context\\n## Context: The user has requested that CC-Route-Syncer handle routing decisions for their network.\\n## Decision\\n## Decision: \\n## Context\\n## Context: The user has requested that CC-Route-Syncer handle routing decisions for their network.\\n## Decision\\n## Decision: \\n## Context\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%% # Copilot\\n\\n%%\\n% $Id: 1.coplayer decision.rst,v 1.3 2006\/01\/10 02:31:58 dave Exp $\\n% $Revision$ $Date$ $Revision$\\n\\n% TODO:\\n% - Add more options?\\n% - Add more context?\\n\\n\\section{Coplayer}\\n\\label{sec:coplayer}\\n\\nCoplayer is a middleware layer which provides client\/server communication between CC-Route-Syncer and copilot. It handles sending\/receiving messages between client clients (which are represented as \\emph{coplayers}) and server servers.\\n\\n\\subsection{Options}\\n\\label{sec:coplayer.options}\\n\\nCoplayer supports several options which may be used when communicating between coplayer clients (and server servers). These are listed below with examples:\\n\\n\\begin{itemize}\\n    \\item {\\bf flag}: A flag is used as part of the URL during communication with coplayer clients (e.g., http:\/\/localhost\/coplayer?flag=good). It will be passed as part of the URL sent by the client, so if you want your client's behavior changed you should use {\\bf flag=change}. An example URL might look like:\\n    \\begin{verbatim}\\n        http:\/\/localhost\/coplayer?flag=good&flag=change\\n    \\end{verbatim}\\n\\n    When using {\\bf flag=change}, you should use {\\bf flag=change} when communicating with coplayers who are running version 2 or later.\\n    \\n    If you want your client's behavior changed you should use {\\bf flag=change} when communicating with coplayers who are running version 1 or earlier.\\n    \\n    \\item {\\bf url}: A URL which will be passed as part of the URL sent by the server during communication with coplayers (e.g., http:\/\/localhost\/coplayer?url=http:\/\/www.example.com). It will be passed as part of the URL sent by the server, so if you want your server's behavior changed you should use {\\bf url=http:\/\/www.example.com}. An example URL might look like:\\n    \\begin{verbatim}\\n        http:\/\/localhost\/coplayer?url=http:\/\/www.example.com&url=http:\/\/www.example.com\\n    \\end{verbatim}\\n\\n    When using {\\bf url}, you should use {\\bf url} when communicating with servers who are running version 2 or later.\\n    \\n    If you want your server's behavior changed you should use {\\bf url=http:\/\/www.example.com}.\\n    \\n    Remember that if your server is running version 1 or earlier then your server can only respond with HTTP 200 OK (not HTTP 201), so if your application tries to make any changes then you need to make sure those changes are stored in a database before returning HTTP status code 201.\\n\\n    You can also specify an optional query string parameter called {\\it path} which will be added after both URLs. The value of this parameter is optional and defaults to empty string.\\n\\n\\end{itemize}\\n\\n\\subsection{Examples}\\n\\nTo demonstrate how these options work let's look at some examples:\\n\\n\\begin{figure}[htbp]\\n\t\\centering{\\n\t\t\\includegraphics[width=\\linewidth]{coplayer-options.png}}\\n\t\\caption{\\emph{\\small Example URLs}}\\n\t\\label{}\\n\t\\vspace{-5pt}\\n\\end{figure}\\n\\nThe first option we'll look at is {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nTo do this we add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nWe add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nWe add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nWe add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nWe add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when it sends a request it sends back HTTP status code 200 rather than HTTP status code 404 (for example).\\n\\nWe add an option called {\\it flag}. Let's say we wanted our client program's behavior changed so that when its send requests returns HTTP status codes other than those produced by default.\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n\\nLet me give another simple example where I've made assumptions about what each field means:\\n"}
{"File Name":"copilot\/0002-use-event-streaming-model-for-diego-actuallrp-syncing.md","Context":"## Context\\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\\nacross all diego cells at a specified time interval (at the time of writing 10\\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\\nimpact the performance of the BBS (large response sets coming back).\\n","Decision":"We want to use the [Event package](https:\/\/github.com\/cloudfoundry\/bbs\/blob\/master\/doc\/events.md)\\nto get the event stream for each ActualLRP. We will also use a bulk sync every\\n60 seconds to catch any events that were missed.\\n","tokens":75,"id":53,"Predictions":"## Decision\\nThe current implementation will fetch all LRPs across all diego cells at a\\nspecified time interval (at the time of writing 10 seconds). As the ActualLRP\\ncount grows on a cloudfoundry deployment this could impact the performance of the\\nBBS (large response sets coming back).\\n\\n## Decision\\n## Decision\\nThe current implementation will fetch all LRPs across all diego cells at a\\nspecified time interval (at the time of writing 10 seconds). As the ActualLRP\\ncount grows on a cloudfoundry deployment this could impact the performance of the\\nBBS (large response sets coming back).\\n\\n## Decision\\n## Decision\\nThe current implementation will fetch all LRPs across all diego cells at a\\nspecified time interval (at the time of writing 10 seconds). As the ActualLRP\\ncount grows on a cloudfoundry deployment this could impact the performance of the\\nBBS (large response sets coming back).\\n\\n## Decision\\n## Decision\\nThe current implementation will fetch all LRPs across all diego cells at a\\nspecified time interval (at the time of writing 10 seconds). As the ActualLRP\\ncount grows on a cloudfoundry deployment this could impact the performance of the\\nBBS (large response sets coming back).\\n\\n"}
{"File Name":"copilot\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":54,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"ELEN4010\/ADR Maps API choice.md","Context":"## Context\\nFor the project, we need an API for a map: interface, search, marker placement, satellite or road map imagery. There are several options for maps, will be a primary mode of interacting with the site.\\n* Google maps\\n* OpenLayers\\n* TomTom\\n* MapBox\\n* HERE\\n* Mapfit\\nMain factor are cost, and ease of use (documentation for the API)\\nGoogle maps are highly customizable in style and appearance, and configerable for marker placement, information windows, and interface\/controls.\\n","Decision":"Upon examining the options, Google Maps was considered the most mature, easy-to-use and well-supported option. The API has excellent documentation and example code. The interface will be familiar to the majority of site users.\\n","tokens":112,"id":55,"Predictions":"## Decision\\nThe decision is to use Google Maps.\\n\\nDecision: use Google Maps.\\n\\nDecision: use OpenLayers.\\n\\nDecision: use TomTom.\\n\\nDecision: use HERE.\\n\\nDecision: use MapBox.\\n\\nDecision: use Mapfit.\\n\\nDecision: use Google Maps.\\n\\n## Context\\n## Context\\begin{itemize}\\n    \\item The map of the site is a primary mode of interacting with the site.\\n    \\item The map of the site is configurable in style and appearance, and can be configured for marker placement, information windows, and interface\/controls.\\n    \\item There are several options for maps, will be a primary mode of interacting with the site.\\n    \\item The map is highly customizable in style and appearance, and configurable for marker placement, information windows, and interface\/controls.\\n\\end{itemize}\\end{document}\\n"}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56,"Predictions":"## Decision\\nThe decision is to use the annotation module. The reason for this is that it is a very simple module, which can be used to create marker overlays on the map. It also has a very simple API and can be used with minimal code changes.\\n\\n\\n## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n## Decision\\n## Decision\\nThe decision is to use the annotation module. The reason for this is that it is a very simple module, which can be used to create marker overlays on the map. It also has a very simple API and can be used with minimal code changes.\\n"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"Predictions":"## Decision: Do not start work on the project. No action required.\\n## Decision: Start work on the project. No action required.\\n## Decision: Start work on the project. No action required.\\n## Decision: Start work on the project. No action required.\\n## Decision: Do not start work on the project. No action required.\\n\\n## End of record.\\n\\nThe above decision record is an example of a decision record. A decision record is a special kind of record that can be used to make decisions about the execution of a software task.\\n\\nThe information in a decision record can be used to help determine what actions to take when executing a task, or what actions to take when an error occurs while executing a task.\\n\\nA decision record contains an optional field called Decision, which may contain one of these values:\\n\\n- \"Start\" - Start execution of this task\\n- \"Stop\" - Stop execution of this task\\n- \"Do not start\" - Do not start execution of this task\\n- \"Do not stop\" - Do not stop execution of this task\\n- \"Start with no data\" - Start execution with no data\\n\\nYou can use these values to provide more specific information about what you want to do when you execute this task. For example, you might want to do something if there's no data available, or if there's no data that meets some condition.\\n\\nYou can also use these values to describe how you want your software system to respond when an error occurs while executing this task. For example, you might want your system to continue executing even if there's an error, or continue executing if there's no data available.\\n\\nFor more information about how decision records are used in software tasks, see Software Task Execution and Task Execution.\\n\\nSee also\\n\\nTask Execution\\n\\nTask Execution Records\\n\\nTask Execution Error Record"}
{"File Name":"ELEN4010\/ADR Email invites.md","Context":"## Context\\nIn order to join a group, a potential member must be invited (it's not correct to add a person to a group without asking permission.) A person invited can either be a member of the website (have an account) or be a new user (no account registered yet). In order to cover both of these scenarios, and to avoid the website being a \"walled garden\" with a tiny set of users, and to encourage potential future growt, a mechanism to invite users could be an email, sent by an existing member, to any valid email address with an invitation to join. This could be in the form of a token with a payload, or more simply, an extra table in the DB, linking the invited person's email to a trip ID.\\n","Decision":"The mechanism of an external invitation with a specific link requires the ability to send an email (prefereably attractively  styled and clearly phrased, to avoid being rejected as unsolicited or junk email). The node module 'nodemailer' was selected as appropriate, for its wide support, mature development and ease of use, and 0 dependecies.\\n","tokens":158,"id":58,"Predictions":"## Decision\\nHere is the decision:\\n\\n\\n\\n## Decision\\nThe decision is to allow the user to be invited by email.\\n\\n\\n\\n## Context\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe user can be invited by an existing member (with a token) or new user (no account registered yet).\\n\\n## Decision\\n## Decision\\ncan be invited by email or no account registered yet.\\n\\n\\n"}
{"File Name":"ELEN4010\/ADR Reordering Destinations.md","Context":"## Context\\nA list of destinations should be reorderable, not fixed\\n","Decision":"A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\\n","tokens":15,"id":59,"Predictions":"## Decision: The routing decision is made by the user in the context of the location, not by a decision maker. A decision maker can be a human or a machine.\\n## Decision type: this is a 2-tuple of (decision_type, decision_value) where:\\n## 1. decision_type: a string representing the type of decision to make\\n## 2. decision_value: a string representing the value of the decision to make\\n## Example:\\n## Decision type: \"route\" and value \"route\"\\n## Decision value: \"route\"\\n\\n# This is an example of a simple routing task\\n\\ndef route_task():\\n    # Do something\\n\\ndef route_task_with_decision():\\n    # Do something with the route and the decision\\n\\ndef route_task_with_decision_and_route():\\n    # Do something with the route and the decision and with the route's destination\\n    # If you want to add more information, you can do this\\n    # (e.g. add destination info)\\n\\nroute_task_with_decision_and_route_and_route():\\n    # Do something with all three routes\\n\\n# This is an example of an iterative task\\n\\ndef iterate_task():\\n    # Start from step 1, repeat until you get to step n-1\\n\\niterative_task = iter(route_task_with_decision_and_route)\\niterative_task()\\niterative_task_with_decision_and_route()\\niterative_task_with_decision_and_route_and_route()\\n\\n# This is an example of an iterative task that returns multiple results\\n\\ndef iterative_task_that_returns_multiple_results():\\n    # Start from step 1, repeat until you get to step n-1, then return all results\\n    iterative_task()\\n\\niterate_task()\\niterate_task_with_decision_and_route()\\niterate_task_with_decision_and_route_and_route()\\n\\n# This is an example of an iterative task that returns only one result\\n\\ndef iterative_job_that_returns_one_result():\\n    return iter(route_task())\\niterate_job()\\n\\niterate_job()\\niterate_job_with_decision_and_route()\\niterate_job_with_decision_and_route_and_route()\\n\\n# This is an example of an iterative task that returns only one result if it fails to find any routes\\n\\ndef iterative_job_that_returns_only_one_result_if_it_fails_to_find_any_routes():\\n    return iter(route(task()) or None)\\niterate_job_or_none()\\n\\niterate_job_or_none()\\niterate_job_that_returns_one_result()\\niterate_job_that_returns_only_one_result_if_it_fails_to_find_any_routes()\\n\\n# This is an example of a recursive task that yields multiple results if it finds any routes.\\n\\ndef recursive_step(task):\\n    if task == None:\\n        yield None\\n        return None\\n        print(\"Task %s has no routes\" % (task))\\n        return None\\n        print(\"Task %s has no routes\" % (task))\\n\\nrecursive_step(iterate_job())\\nrecursive_step(iterate_job_or_none())\\n\\nrecursion_level = 0\\nfor i in range(10):\\n   recursive_step(recursive_step)\\n\\nrecursion_level += 1\\n\\nprint(\"Recursion level : {}\".format(recursion_level))\\nprint(\"Number of tasks : {}\".format(len(tasks)))\\nprint(\"Number of steps : {}\".format(len(steps)))\\nprint(\"Number of iterations : {}\".format(len(iterate_jobs)))\\n\\nThe following code demonstrates how to use recursion:\\n\\nclass MyRecursion(object):\\n\\n    def __init__(self):\\n        self.recursion_level = 0\\n        self.steps = []\\n\\nThe following code illustrates how to use recursion in each iteration:\\n\\nfor i in range(10):\\n   recursive_step(recursive_step)\\n\\nrecursion_level += 1\\n\\nprint(\"Recursion level : {}\".format(recursion_level))\\nprint(\"Number of tasks : {}\".format(len(tasks)))\\nprint(\"Number of steps : {}\".format(len(steps)))\\nprint(\"Number of iterations : {}\".format(len(iterate_jobs)))\\n\\nWhat happens when I run this code?\\nWhen I run this code, I will see some output like this:\\nTask 0 has no routes Task 1 has no routes Task 2 has no routes Task 3 has no routes Task 4 has no routes Task 5 has no routes Task 6 has no routes Task 7 has no routes Task 8 has no routes Task 9 has no routes Task 10 has no routes Number of tasks : [10] Number of steps : [10] Number of iterations : [10]\\nI will also see some output like this:\\nTask Recursion level Number of tasks Number of steps Number of iterations Recursion level Number of tasks Number of steps Number of iterations Recursion level Number o"}
{"File Name":"ELEN4010\/ADR Entering destinations for a trip.md","Context":"## Context\\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site\/map websites.\\n","Decision":"We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\\n","tokens":63,"id":60,"Predictions":"\\n\\# Decision\\nDecision: \\n\\# Destination\\nDestination: \\n\\# Trip\\nTrip: \\n\\# Travel Time\\nTravel Time: \\n\\# Duration\\nDuration: \\n\\# Cost\\n## Decision\\n\\n\\# Decision\\nDecision: \\n\\# Destination\\nDestination: \\n\\# Trip\\nTrip: \\n\\# Travel Time\\nTravel Time: \\n\\# Duration\\nDuration: \\n\\# Cost\\n## Decision\\n%\\begin{enumerate}\\n%\\item Type the number of the destination or click on a map to set a marker.\\n%\\item Type the number of the trip or click on a map to set a marker.\\n%\\item Type the duration of the trip or click on a map to set a marker.\\n%\\item Type the cost of the trip or click on a map to set a marker.\\n%\\end{enumerate}\\n"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"Predictions":"## Decision\\nThe first step is to create a new User object, which will contain the basic information about the user such as name, password, and email. The User object will also contain a field for the Sprint 1 User ID. This ID is used by the application to distinguish between two users. It is also used by the application to track and report on the Sprint 1 User's progress in completing their User story.\\n## Decision\\n## Decision\\nThe second step is to create a new Sprint 1 User object. This will contain all information that was provided in the previous step including the Sprint 1 User ID, name, email, password, and sprint. The Sprint 1 User object will also have fields for their sprint start date and their sprint end date.\\n## Decision\\n## Decision\\nThe third step is to create a new Story object. This Story object will contain all information that was provided in the previous steps including the Sprint 1 Story ID, name, email, password, and sprint. The Sprint 1 Story object will also have fields for their sprint start date and their sprint end date.\\n## Decision\\n## Decision\\nThe fourth step is to create a new Task object. This Task object will contain all information that was provided in the previous steps including the Sprint 1 Task ID, name, email, password, and sprint. The Sprint 1 Task object will also have fields for their sprint start date and their sprint end date.\\n## Decision\\n## Decision\\nThe fifth step is to create a new Release item. This Release item will contain all information that was provided in the previous steps including the Release Item ID, name, email, password and release type (e.g., bug fix). The Release Item ID should be unique across releases.\\n## Decision\\n## Decision\\nThe sixth step is to create a new Bug item. This Bug item should be unique across releases (i.e., it should be an unversioned item). It should also be created once per release (i.e., it shouldn't be created again when we update any of our current releases).\\n## Decision\\n## Decision\\nThe seventh step is to create an Issue item. This Issue item should be unique across releases (i.e., it should not be created again when we update any of our current issues).\\n## Decision\\n## Decision\\nThe eighth step is to create an Issue Type item. This Issue Type item should be unique across releases (i.e., it should not be created again when we update any of our current issues).\\n## Decision\\n%% ## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login.\\n%% ## Context\\nThis would provide a template for our own login details stored in the DB,\\n%% ## as well as a quick way to get the Sprint 1 user story related to login completed ASAP.\\n\\n\\end{document}\\n"}
{"File Name":"ELEN4010\/ADR GitHub Project Board as KanBan.md","Context":"## Context\\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size\/priority\/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\\n","Decision":"Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for \"To Do\", \"In progress\", and \"Completed\".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\\nhttps:\/\/github.com\/witseie-elen4010\/2019-005-project\/projects\/1\\n","tokens":94,"id":62,"Predictions":"## Decision\\nThe user wants a central board to track the development of userstories. This will help the team stay on track and avoid duplication of effort. As a result, the sprint backlog will be smaller, and velocity will increase.\\n\\n\\n\\n\\n## Context\\nUse this field to indicate if the decision is related to a Product, Project or User Story. If not specified, then the field is ignored.\\n\\n## Product\\n## Product\\nA product is either an individual user story or a group of stories that form a single product.\\n\\n## Project\\n## Project\\nA project is either an individual user story or a group of stories that form a single product.\\n\\n## User Story\\n## User Story\\nA user story is either an individual user story or a group of stories that form a single product.\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"ELEN4010\/ADR Trunk-Based Development.md","Context":"## Context\\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n","Decision":"**master** will be the release branch\\n**development** will be the main development\/test branch. This will also be made into the \"default\" branch for all pull requests, to avoid accidentaly PR into master\\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into \"development\".\\n* All PRs must be reviewed by at least two developers to merge into \"development\"\\n* All PRs must be reviewed by at the three other developers to merge into \"master\"\\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\\n","tokens":172,"id":63,"Predictions":"## Decision\\nThe **master** branch is the release branch. It is used as the release branch, and it is used for integration and development. It is used for *testing* branches, and *development* branches, as they are not used for production.\\nCode reviews are essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. This way we all learn from each other, and also learn good review and communicaton practice.\\n\\n## Context\\nThe **master** branch should be used for **development** branches, as they are not used for production. The **master** branch should only be pushed to **production** if a release is planned.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n## Decision\\n## Decision\\nIt would be convenient to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests,\\nand there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful,\\as 'master' is usually the most protected on GitHub,\\with the most warnings about deleting,\\modification,\\etc.\\nThe use of **master** branch as the release branch is useful,\\as 'master' is usually the most protected on GitHub,\\withthe most warnings about deleting,\\modification,\\etc.\\nThe use of **master** branch as the releasebranchis useful,\\as 'master'is usuallythe most protected on GitHub,\\\\nwiththe most warnings about deleting,\\\\nmodification,\\\\netc.\\\\n## Context\\n## Context\\nCode reviews are essential from all developers,\\to become familiar with each other's code,\\andtolearnaboutjavascriptandweb-development.\\n## Decision\\n## Decision\\nThe **master** branch should be used for development branches\\nto become familiar with each other's code\\and\\nto learnaboutjavascript\\ndo web-development.\\n"}
{"File Name":"ELEN4010\/ADR Folder structure.md","Context":"## Context\\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\\n","Decision":"The following folder strucure is adopted:\\n.\\n\u251c\u2500\u2500 app\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 controllers\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 public\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 css\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 img\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 js\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\\n\u251c\u2500\u2500 docs\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 adr\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 misc\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project_artifacts\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\\n\u251c\u2500\u2500 node_modules\\n\u251c\u2500\u2500 test\\n\u2514\u2500\u2500 local_only\\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\\n","tokens":90,"id":64,"Predictions":"## 1. The project is structured into several folders, each with a consistent structure, and each folder is contained within a specific directory.\\n## 2. The project is structured into several folders, each with a consistent structure, and each folder is contained within a specific directory.\\n## 3. The project is structured into several folders, each with a consistent structure, and each folder is contained within a specific directory.\\n\\n\\end{document}"}
{"File Name":"infection\/0002-@covers-annotations.md","Context":"### Context\\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\\nenforce them. The question is when should those annotations be enforced and\/or if we need to enable\\nanother settings as well?\\n### Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","Decision":"Since we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","tokens":361,"id":65,"Predictions":"### Decision\\nWe decide to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, we have decided that no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n## Decision\\n### Decision\\nWe decide to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, we have decided that no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n## Summary of Decision\\n### Summary of Decision We decide that no further action has been voted for automating this process.\\n## Notes\\n### Notes Removing the annotation is not automatic and requires manual intervention.\\n## Description of Issue\\n### Description of Issue The annotation should be removed if we don't need it anymore.\\n## Author\\n### Author The author is:"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"Predictions":"### Status\\nAccepted ([#1062][1062])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1062]: https:\/\/github.com\/infection\/infection\/pull\/1062\\n\\n## Decision\\n### Status\\nAccepted ([#1063][1063])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1063]: https:\/\/github.com\/infection\/infection\/pull\/1063\\n\\n## Decision\\n### Status\\nAccepted ([#1064][1064])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1064]: https:\/\/github.com\/infection\/infection\/pull\/1064\\n\\n## Decision\\n### Status\\nAccepted ([#1065][1065])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1065]: https:\/\/github.com\/infection\/infection\/pull\/1065\\n\\n## Decision\\n### Status\\nAccepted ([#1070][1070])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1070]: https:\/\/github.com\/infection\/infection\/pull\/1070\\n\\n## Decision\\n### Status\\nAccepted ([#1071][1071])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1071]: https:\/\/github.com\/infection\/infection\/pull\/1071\\n\\n## Decision\\n### Status\\n\\n"}
{"File Name":"infection\/0001-inheritdoc.md","Context":"### Context\\nUsing `@inheritdoc` was done inconsistently across the codebase so the decision of whether we use it\\nsystematically or remove it systematically had to be done.\\nA number of points:\\n- [PHPDoc][phpdoc-inheritance] provides inheritance of the docblocks by default when appropriate\\n- Static analysers such as PHPStan or Psalm can do without at the time of writing\\nAlso it has a very limited value.\\n### Decision\\nDo not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","Decision":"Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","tokens":175,"id":67,"Predictions":"  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: https:\/\/github.com\/infection\/infection\/issues\/860\\n\\n### Decision\\n  * [phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n  * [phpdoc-inheritance]: http:\/\/infections.github.io\/phpDoc\/doc-versions\/\\n\\n## Problem description and steps to reproduce\\nThe PHPDoc generator does not correctly generate docblocks when using inheritance.\\nSteps to reproduce:\\n1\\. Create a new file with no docblock\\n2\\. Add a new docblock\\n3\\. Save the file\\n4\\. Run `composer dump-autoload` and `composer update` and see that the generated docblock does not have the inherited class. It will have the same name as the original docblock but the class will be missing.\\n5\\. Run `composer dump-autoload` and `composer update` again\\n6\\. See that the generated docblock now has the inherited class.\\n7\\. Run `composer dump-autoload` and `composer update` again\\n8\\. See that the generated docblock now has no inherited class.\\n9\\. If you run `composer dump-autoload` and `composer update` again, see that it now has both classes, one which is inherited from the original docblock and one which is not. The latter one is expected to be missing from the generated docblock because it's been removed by codegen (see #860).\\n10\\. If you run `composer dump-autoload`, run `composer update`, then run `composer dump-autoload`, see that it now has both classes, one which is inherited from the original docblock and one which is not. The latter one is expected to be missing from the generated docblock because it's been removed by codegen (see #860).\\n11\\. If you run `composer dump-autoload`, run `composer update`, then run `composer dump-autoload`, see that it now has both classes, one which is inherited from the original docblock and one which is not. The latter one is expected to be missing from the generated docblock because it's been removed by codegen (see #860).\\n12\\. If you run `composer dump-autoload`, run `composer update`, then run `composer dump-autoload`, see that it now has both classes, one which is inherited from the original docblock and one which is not. The latter one is expected to be missing from the generated docblock because it's been removed by codegen (see #860).\\n13\\. If you run 'make', you will get an error about a missing include file in your tests. This means that something was added in your test case for this particular feature.\\n14\\. As a workaround, use --no-warnings when running make test.\\n15\\. Run 'make' again with --no-warnings to remove this feature.\\n16\\. Run 'make' again with --no-warnings to remove this feature.\\n## Expected behavior\\nThe generation of DocBlocks should correctly generate them for inheritance.\\n## Actual behavior\\nThe generation of DocBlocks should NOT correctly generate them for inheritance.\\n## Steps to reproduce:\\[1\\] Create a new file with no DocBlock\\[2\\] Add a new DocBlock\\[3\\] Save this file\\[4\\] Run composer dump-autoload and composer update\\[5\\] See that there are two DocBlocks, both of which have been generated properly but are identical in name only\\[\\][6]\\] Remove all references to inheritance in your project using composer delete-dependencies or similar tools,\\[\\][7]\\] Run composer dump-autoload and composer update again,\\[\\][8]\\] See that there are two DocBlocks, both of which have been removed properly but are identical in name only.\\n## Expected behavior\\nThe generation of DocBlocks should NOT correctly generate them for inheritance.\\n## Actual behavior\\nThe generation of DocBlocks should NOT correctly generate them for inheritance.\\n## Steps to reproduce:\\1\\. Create a new file with no DocBlock2\\. Add a new DocBlock3\\. Save this file4\\. Run composer dump-autoload5<\/div>\\p{X}4<\/div>\\p{X}4<\/div>\\p{X}4<\/div>\\p{X}4<\/div>\\n"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"Predictions":"### Decision\\nThe decision is to use the `expectException()` API when possible. The decision is to use the `try\/catch` API when it is not possible. The decision is to use a combination of both when necessary.\\n### Status\\nAccepted ([#1091][1091])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1091]: https:\/\/github.com\/infection\/infection\/pull\/1062\\n## Decision\\n### Decision\\nThe decision is to use a combination of both the `try\/catch` and `excepthandler` APIs when necessary. The decision is to use a combination of both the `try\/catch` and `excepthandler` APIs when it is not possible.\\n### Status\\nAccepted ([#1092][1092])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n## Decision\\n### Decision\\nThe decision is to use the `excepthandler()` API when it is not possible. The decision is to use the `try\/catch` API when it is not possible.\\n### Status\\nAccepted ([#1093][1093])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n## Decision\\n### Decision\\nThe decision is to use the `try\/catch()` API when it is not possible. The decision is to use a combination of both the `catch()` and `excepthandler()` APIs when necessary.\\n### Status\\nAccepted ([#1094][1094])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n## Decision\\n### Decision\\nThe decision is to use a combination of both the `catch()` and `excepthandler()` APIs when necessary. The decision is to use a combination of both the `catch()`, and `excepthandler()`.\\n### Status\\nAccepted ([#1095][1095])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n## Type: 0x01 (0x01)\\n## Value: 0x00 (0x00)\\n## Type: 0x03 (0x03)\\n## Value: 0x00 (0x00)\\n## Type: 0x07 (0x07)\\n## Value: 0x00 (0x00)\\n## Type: 0x09 (0x09)\\n## Value: 0xe6 (0xe6)\\n## Type: 0x11 (0x11)\\n## Value: 0xe8 (0xe8)\\n```\\nFrom this, I can see that I should be using the exception class, but I don't know why it wasn't used in this case.\\nAny advice on how I can get PHPUnit working with my code base would be greatly appreciated!\\nEDIT:\\nI've managed to get PHPUnit working by changing all my tests from using PHPUnit's exception class, into using its own exception class, which was included in my dependencies:\\n```\\n<?php\\n\\nnamespace App\\Tests;\\n\\nuse PHPUnit\\Framework\\TestCase;\\nuse App\\Models\\User;\\nuse App\\Models\\UserRole;\\nuse App\\Models\\UserRoleRole;\\nuse App\\Models\\UserRoleRoleUser;\\nuse App\\Models\\UserRoleUserRole;\\n\\nclass UserTest extends TestCase {\\n    protected $user;\\n\\n    public function setUp() {\\n        $this->user = new User();\\n        $this->user->loadUser();\\n        $this->user->saveUser();\\n    }\\n\\n    public function testSomething() {\\n        $this->assertThat($this->user->get('username'), 'Username should be in user');\\n    }\\n}\\n```\\nHowever, this isn't ideal, as now I have two different classes for each test that uses an object that's related only by its id. Is there any way that I can make PHPUnit recognise them as being related? \\nHere's where my code currently stands:\\n```\\n<?php\\n\\nnamespace App\\Tests;\\n\\nuse App\\Models\\User;\\nuse App\\Models\\UserRole;\\nuse App\\Models\\UserRoleRole;\\nuse App\\Models\\UserRoleUserRole;\\n\\nclass UserTest extends TestCase {\\n    protected $user;\\n\\n    public function setUp() {\\n        $this->user = new User();\\n        $this->user->loadUser();\\n        $this->user->saveUser();\\n    }\\n\\n    public function testSomething() {\\n        $this->assertThat($this->user->get('username'), 'Username should be in user');\\n    }\\n}\\n```\\nEDIT:\\nThis has worked! It seems like PHPUnit doesn't like having multiple classes for a single object in one file; however, if you put all your tests into one file, then they work fine! \\nThanks guys!\\n\\nOP 2017-06-08: It turns out that you need to add an extra line at the top of your file:\\n```\\n<?php\\n\\nnamespace Test;\\n\\n\/* Add this line at top *\/\\n\/** @test *\/\\nclass UserTest extends TestCase {\\n\\n}\\n```\\nThis was causing issues with my tests because they were trying to access methods on objects which weren't declared as being related. \\n"}
{"File Name":"dapr\/API-002-actor-api-design.md","Context":"## Context\\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\\n","Decision":"### Dapr\\n* A separate Actor interface is defined.\\n* Actors should support multiple reminders and timers.\\n* Actor state access methods are encapsulated in the Actor interface itself.\\n* Actor interface shall support updating a group of key-value states in a single operation.\\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\\n### Non-Dapr\\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\\n","tokens":75,"id":69,"Predictions":"## 1. The software can be developed by Dapr team.\\n## 2. The software can be developed by third party.\\n## 3. The software cannot be developed by Dapr team.\\n## 4. The software cannot be developed by third party.\\n"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"Predictions":"## Description: Test cluster setup and run, test results report\\n## Decision type: Decision\\n## Decision template: https:\/\/github.com\/dapr\/dapr-docs\/blob\/master\/decision-template.md\\n\\n### Run tests\\n\\n## Summary\\n## Steps to run tests:\\n## 1. Create a new cluster from scratch (e.g. docker-compose.yml)\\n## 2. Run the Dapr test suite (e.g. docker-compose run -d --rm dapr-test-runner)\\n## 3. Create a new PR from scratch (e.g. docker-compose.yml)\\n## 4. Run the Dapr test suite again (e.g. docker-compose run -d --rm dapr-test-runner)\\n\\n### Test infrastructure\\n\\n### Setup & tools\\n\\n#### Docker compose file\\nRun the following command to create a new cluster:\\ndocker-compose up -d --build\\n\\n#### Docker images\\nRun the following command to create an image with Dapr:\\ndocker build -t dapr .\\n\\n#### Docker images with Dapr\\nRun the following commands to create images with Dapr:\\n\\n- docker build -t dapr-0.1.0 .\\n- docker build -t dapr-0.1 .api .\\n- docker build -t dapr-0 .helloworld .\\n\\n### Run tests\\n\\n#### Docker compose file\\nRun the following command to create a new cluster:\\ndocker-compose up -d --build\\n\\n#### Docker images\\nRun the following commands to create an image with Dapr:\\n\\n- docker build -t dapr-0 .helloworld .\\n- docker build -t dapr-0 .test .\\n\\n### Test infrastructure\\n\\n### Setup & tools\\n\\n#### Docker compose file\\nRun the following command to create a new cluster:\\ndocker-compose up -d --build\\n\\n#### Docker images\\nRun the following commands to create an image with Dapr:\\n\\n- docker build -t dapr-0 .helloworld .\\n- docker build -t dapr-0 .test .\\n\\n### Run tests\\n\\n##### Tests case description and summary\\n##### How many tests will be run?\\n##### What are the steps of each test?\\n##### What is expected result?\\n##### What is not expected result?\\n\\n* **Running tests:** A list of all tests that are running at this time.\\n* **Running individual tests:** A list of all individual tests that are running at this time.\\n\\n### Test infrastructure\\n\\n#### Docker compose file\\nRun the following command to create a new cluster:\\ndocker-compose up -d --build\\n\\n#### Docker images\\nRun the following commands to create an image with Dapr:\\n\\n* **Docker Compose File:** A copy of this file.\\n* **Docker Images:** A list of all available Docker Images.\\n\\n### Test infrastructure\\n\\n##### Summary of each test case for each environment and what needs to be done for each environment.\\n##### Steps that need to be performed before running each test case.\\n##### Steps that need to be performed after running each test case.\\n\\n* **Summary:** A summary of each test case for each environment and what needs to be done for each environment.\\n* **Steps that need to be performed before running each test case:** A list of steps that need to be performed before running this particular test case.\\n* **Steps that need to be performed after running this particular test case:** A list of steps that need to be performed after running this particular test case.\\n\\n### Test infrastructure\\n"}
{"File Name":"dapr\/API-006-universal-namespace.md","Context":"## Context\\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\\n","Decision":"We should consider adding universal namespace capabilities to Dapr.\\n","tokens":70,"id":71,"Predictions":"## Decision\\nWe recommend that the cloud-edge hybrid scenarios and multie-region deployment scenarios be supported by the following decision.\\n\\n## Decision\\nThe following decision is acceptable. \\n\\n## Decision\\nWe recommend that the cloud-edge hybrid scenarios and multie-region deployment scenarios be supported by the following decision.\\n\\n## Decision\\nWe recommend that the cloud-edge hybrid scenarios and multie-region deployment scenarios be supported by the following decision.\\n\\n## Decision\\nThe following decision is acceptable. \\n\\n## Decision\\nWe recommend that the cloud-edge hybrid scenarios and multie-region deployment scenarios be supported by the following decision.\\n\\n## Decision\\nThe following decision is acceptable. \\n\\n## Decision\\nWe recommend that the cloud-edge hybrid scenarios and multie-region deployment scenarios be supported by the following decision.\\n\\n"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"Predictions":"\\begin{verbatim}\\n## Decision\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n## Decision\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n## Decision\\n\\end{verbatim}\\n\\n% \\subsection{Understanding the Context}\\n% \\label{sec:context}\\n\\n% This section provides a brief explanation of the context, which is used to set the context for the user.\\n\\n% The context consists of a series of attributes and values. The attributes are used to describe the environment in which the user is operating. The values are used to describe how the user sees or expects to see the environment.\\n\\n% Each attribute value is associated with a single attribute value, which is a string that describes an object. For example, \"user\" could be associated with \"firstName\", \"lastName\", \"email\", and so on.\\n\\n% For example, if a user specified that he\/she was using a computer with 1 GB of memory, then one attribute would be \"user\". Another would be \"memory\". The value for this attribute would be \"1\".\\n\\n% If a user specified that he\/she was using an iPhone with 1 GB of memory, then one would be \"user\" and another would be \"memory\". The value for this attribute would be \"1\".\\n\\n% If a user specified that he\/she was using an iPhone with 2 GB of memory, then one would be \"user\" and another would be \"memory\". The value for this attribute would be \"2\".\\n\\n% If a user specified that he\/she was using an iPad with 2 GB of memory, then one would be \"user\" and another would be \"memory\". The value for this attribute would be \"2\".\\n\\n% If a user specified that he\/she was using an iPad mini with 2 GB of memory, then one would be \"user\" and another would be \"memory\". The value for this attribute would be \"2\".\\n\\n\\subsection{User Attributes}\\nThe following table lists some examples of attributes:\\n\\n\\begin{tabular}{|l|l|}\\n\\hline\\nAttribute & Value\\\\\\n\\hline\\nfirstName & John\\\\\\nlastName & Doe\\\\\\nemail & jdoe@example.com\\\\\\nage & 30\\\\\\nstatus & active\\\\\\nphoneNums & (123)456-7890\\\\\\nlocationId & 12345-67890-12345-67890\\\\\\nlocationName & New York City\\\\\\n\\n\\hline\\n\\n\\end{tabular}\\n\\nThe following table lists some examples of values:\\n\\n\\begin{tabular}{|l|l|}\\n\\hline\\nValue & Value Type\\\\ \\n\\hline\\nfirstName & String \\\\\\nlastName & String \\\\\\nemail & String \\\\\\nage & Integer \\\\\\nstatus & Boolean \\\\\\nphoneNums & Array \\\\\\nlocationId & String \\\\\\nlocationName & String \\n\\n\\hline\\n\\n\\end{tabular}\\n\\nNote that only some attributes can have their values set by default. For example, only users who are active can set their status to active. Other users cannot set their status.\\n\\nThe following table lists some examples of values:\\n\\n%\\begin{tabular}{|l|l|}\\n%\\hline\\n%\\textbf{Name}   \t& \\textbf{Name}\\\\ \\n%\\hline \\n%\\textbf{Name}   \t& \\textbf{Name}\\\\ \\n%\\hline \\n%\\textbf{Name}   \t& \\textbf{Name}\\\\ \\n%\\hline \\n%\\textbf{Name}   \t& \\textbf{Name}\\\\ \\n%\\hline \\n\\n%\\end{tabular}\\n\\nThe following table lists some examples of attributes:\\n\\n%\\begin{tabular}{|l|l|}\\n%\\hline\\n\\nAttribute Value Type Description\\\\\\n\\nfirstName String Name (first name)\\\\\\n\\n\\nlastName String Name (last name)\\\\\\n\\n\\nemail String Email address (if email address is provided) \\\\\\n\\n\\nage Integer Age (number) \\\\\\n\\n\\nstatus Boolean Status (active or inactive) \\\\\\n\\n\\nphoneNums Array Number in list \\\\\\n\\n\\nlocationId String Location ID (unique identifier) \\\\\\n\\n\\nlocationName String Location name (unique identifier) \\\\\\n\\n\\n\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\n\\n\\noindent\\rule{\\linewidth}{0.5pt}"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"Predictions":"## Decision\\nThe Java SDK provides a simple and elegant way to access Dapr from Java, using the standard Java library. It is also possible to use a custom library, but it will be more difficult to maintain.\\n\\n## Decision\\nThe Java SDK will be used for the following use cases:\\n## - \\n\\n- communicating with Dapr via HTTP (REST API)\\n\\n- connecting to Dapr via JDBC (Java Database Connectivity)\\n\\n- interacting with Dapr via any other protocol (e.g. MQTT)\\n\\n- interacting with Dapr through any other protocol (e.g. AMQP)\\n\\n- interacting with Dapr using any other protocol (e.g. GraphQL)\\n\\n- interacting with Dapr through any other protocol (e.g. S3)\\n\\n\\ndecision\\n## Decision\\n## Decision\\nThe Java SDK is open source and you can find the latest version on GitHub.\\n\\n## Decision\\n## Decision\\nYou can find a list of supported platforms here:\\nhttps:\/\/github.com\/dapr\/java-sdk\/releases\/tag\/v1.0.6#platforms\\ndecision\\n## Decision\\n## Decision\\nYou can install the JDK 11 or later on your platform by running:\\njava -version\\ndecision\\n"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"Predictions":"\\nWe considered the following options:\\n\\n\\n\\n\\n\\n\\n\\begin{itemize}\\n\\item \\textbf{Dockerfile}: A file that describes how to build an image.\\n\\n\\item \\textbf{Dockerhub}: A centralized repository for images, which hosts them on behalf of the users.\\n\\n\\item \\textbf{Dockerhub registry}: A registry that can be used by other users to store images.\\n\\n\\end{itemize}\\n"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"Predictions":"## Comments\\n## This is a decision record for our software. The input is a software, the output is a software.\\n## Context\\n## Comments\\n## This is a decision record for our software. The input is a software, the output is a software.\\n## Context\\n## Comments\\n## This is a decision record for our software. The input is a software, the output is a software.\\n## Context\\n## Comments\\n## This is a decision record for our software. The input is a storage account, the output is an API function that returns an instance of an API function that returns an instance of an object.\\n## Decision\\n## Comments\\n\\end{Verbatim}\\n"}
{"File Name":"dapr\/API-001-state-store-api-design.md","Context":"## Context\\nWe reviewed storage API design for completeness and consistency.\\n","Decision":"* All requests\/responses use a single parameter that represents the request\/response object. This allows us to extend\/update request\/response object without changing the API.\\n* Add Delete() method\\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\\n* Transaction across multiple API requests is postponed to future versions.\\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](.\/API-002-actor-api-design.md).\\n","tokens":14,"id":76,"Predictions":"## We provide a storage API that is consistent with the current API in the\\n## context of the user.  The only change is that we use a stream-based\\n## implementation of the storage API.  This stream-based implementation\\n## is consistent with the current API in the context of the user.\\n\\n\\subsection{Context}\\n\\n\\subsubsection{Is this an Architectural Decision Record?}\\n\\label{architectural-decision-record-is-this-an-architectural-decision-record}\\n\\nYes, this is an Architectural Decision Record.\\n\\n\\subsubsection{Is this an Architectural Decision Record?}\\n\\label{architectural-decision-record-is-this-an-architectural-decision-record-2}\\n\\nYes, this is an Architectural Decision Record.  The decision record is\\npresented in a format that can be understood by humans and machines.\\n\\n\\subsubsection{Architectural Decision Record Content}\\n\\label{architectural-decision-record-content}\\n\\nThe following architectural decision record content has been determined by\\nthe user:\\n\\n\\noindent\\n\\noindent\\n\\noindent\\n\\noindent\\n\\n\\noindent\\n\\nThe architecture decision record contains information about how to create,\\nmodify, and delete a file.\\n\\nNote: This architectural decision record does not contain any information about\\nhow to retrieve files from a file system.\\n\\n\\noindent\\n\\nThis architectural decision record contains information about how to create,\\nmodify, and delete a file.\\n\\nNote: This architectural decision record does not contain any information about\\nhow to retrieve files from a file system.\\n\\n\\vspace{-1em}\\n\\newpage\\n\\n\\noindent\\n\\nThis architectural decision record contains information about how to create,\\nmodify, and delete a file.\\n\\nNote: This architectural decision record does not contain any information about\\nhow to retrieve files from a file system.\\n\\n\\vspace{-1em}\\n\\newpage\\n\\n\\noindent\\n\\n\\nThe following are examples of decisions made by the user:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n# Create file \"test.txt\"\\n# Create directory \"test\"\\n# Delete \"test.txt\"\\n# Modify \"test.txt\"\\n# Delete \"test.txt\"\\n# Modify \"test.txt\"\\n# Create directory \"test\/test.txt\"\\n# Delete \"test.txt\/test.txt\"\\n# Modify \"test.txt\/test.txt\"\\n\\end{verbatim}\\n\\n\\vspace{-1em}\\n\\n\\nThe following are examples of decisions made by the user:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n# Create directory \"tmp\/\"\\n# Create file \"tmp\/test.txt\"\\n# Create directory \"tmp\/test\/\"\\n# Delete \"tmp\/test.txt\"\\n# Modify \"tmp\/test.txt\" # This should fail since it cannot be modified.\\n# Delete tmp\/test.txt # This should succeed since it cannot be deleted.\\n# Modify tmp\/test.txt # This should succeed since it can be modified.\\n\\end{verbatim}\\n\\n\\vspace{-1em}\\n\\n\\nThe following are examples of decisions made by the user:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\ncreate test\/blah blah blah\/blah blah blah\/blah blah blah\/blah blah blah\/blah blah blah\/blah blah blah\/blah blah blah\/blah blah blah\/\\ncreate test\/blah again again again again again again again again again again again again again again \\ndelete test\/blah etc etc etc etc etc etc etc etc..etc..\\nmodify test\/test\/file test\/file test\/file test\/file test\/file test\/file test\/file test\/file \\ndelete tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp.tmp(tmp)\\nmodify tmp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp\/temp temp temp temp temp temp temp temp temp \\ndelete tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp\/tmp  \\nmodify tmp\/tmp\/sort\/sort\/sort\/sort\/sort\/sort\/sort\/sort\/sort\/sort\/sort\/sort ssort ssort ssort ssort ssort ssort ssort ssort \\ndelete tmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmpTmp_tmp_tmp_tmp_tmp_tmp_tmp_tmp_tmp_tmp_tmp_tmp_temp_s_sort_s_sort_s_sort_s_sort_s_sort_t_sort_t_sort_t_sort_t_sort_t_sort_t_sort_t_sort_t_sort_p_p_p_p_p_p_p_p_p_p_p p p p p p p p p p p p p p \\ndelete tmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTtmpTMPTMPTMPTMPTMPTMPTMPTMPTMPTMPTMP TMP TMP TMP TMP TMP TMP TMP TMP TMP TMP TMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMPTEMPTEMPTEMPTEMPTEMPTEMPTEMPTEMPTEMPTEMPTEMPtemp Temp Temp Temp Temp Temp Temp Temp Temp Temp Temp Temp Temp Temp Temp TempTempTempTempTempTempTempTempTempTempTempTempTempTempTemtempTemtempTemtempTemtempTemtempTemtempTemtempTempe Tem Tem Tem Tem Tem Tem Tem Tem tem tem tem tem tem tem tem tem tem tem tem tem tem tem tem tem TEM TEM TEM TEM TEM TEM TEM TEM TEM TEM TEM TEM TEM Tem Tem Tem Tem TemtemtemtemtemtemtemtemtemtemtemtemtemtemtemtemtemcemtemetatemTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMTEMEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMMEEMEEMEEMEEMEENENENENENENENENENENENENENEENEENEENEENEENEENEENEENEENEENEENEeneeneeneeneeneeneeneeneeneeneeneeneneeneeneneeneeenneenenneennneeenenennneeenennnneenenenneenennneeenennnneeenennnnnennenenneenennnnnenennenenneeennnneehenneeeenenenneeenennnnnennnnenennenennnnnenennenennenemenenenemenmenemenemenomenomenomenomenomenomenomenomenomenomenomenomenomenomonimphenomemonphenomemonphenomemonphenomonphenomemonphenonommonphenomemonophenonenonnonnonnonnonnonnonnonnonnonnonnonnonnonnonnoonosonoonono nononono nononono nonono nonono nonono nonono nononono no no no no no no no no no no yes yes yes yes yes yes yes yes yes yes yes yes yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No No No No No No No No No No No No Non Non Non Non Non Non Non Non Non Non Non Non Non Noun Noun Noun Noun Noun Noun Noun Noun Noun Noun Noun Noun Name Name Name Name Name Name Name Name NameNameNameNameNameNameNameNameNameNameNameNameNameNamenamenamenamenamenamenamenamenamenamenamenamenamenamenmenamemenimenemenemenimenemenemenemenemenmenemenmenemenmenenemenmenenuitenenenlteenenenlteeninenlteeninenlteeninenlteeninenltennenlenennenelennenlenenenlenennenlenenlenetenlenotenrenetenetenrenetenetenatenatenetenatenetonenatenetenatenetenatenetonenatenetonenatenetenatonenatenetonenatetenatonenatenetonenatetenatonenatenettenotenatenettenotenatenotenotenatetenototenatottenotenotenatonosenotontotottenotenotenosenotontottenoteotenosenotontottenoteeutenosenotontottenoteegenosenotontottenoteegenenosenotontottenoteegenenosenotenotenosenotenoottenosoenoottenoseeigenstenosenoostenoottenosenoottenoseeigenstenosenoostenoottenoseeigenstenosenoostenoottenoseeigenstenosenoostenoottenoseeigenstenosenoostenoottenoseeigenstenosenoostenoottenoseeenutenoenoenoenoenoeenaerenaeenvienenvieenvienneenvieenvienvienneenvieenviervienneenvieenviervienneenviervienneenviervienneenviervienneeviervienneeviervienneevievierviervierviervierviervierviervierviervenierviervierwerfourvierwerfourvierwerfourvierwer vierwerfourvierwer vierwerfourvierwer vierveer vierder vierder vierder vierder vierder vierder vierder vierder vierder vierder vierder vierde vierde vijfde drieer derde derdier derdier derdier derdier derdier derdier derdier drieer drieer drieer drieer drieer drieer drieer drieer drieer drieer drieer twee er twee er twee er twee er twee er twee er twee er twee er twee er twee er twee er een en een en een en een en een en een en een en een en een en een en een en een en"}
{"File Name":"dapr\/API-012-content-type.md","Context":"## Context\\nNot adding content-type to state store, pubsub and bindings.\\n","Decision":"* We will not add content-type since it is a persisted metadata and it can cause problems such as:\\n* Long term support since metadata persisted previously would need to be supported indefinitely.\\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\\nOriginal issue and discussion: https:\/\/github.com\/dapr\/dapr\/issues\/2026\\n","tokens":17,"id":77,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n\\nThe context is provided by the user. This is a simple text document.\\n\\nThe decision may be either a single line, or a single paragraph. The decision is not required to be at the top level of the document. It may also be a single line.\\n\\nThe decision can contain comments. The decision can also contain lines that are not part of the decision itself, but are used to indicate what decision was taken.\\n\\nThe decision can contain any number of statements, each of which specifies how the decision was made and what it means.\\n\\nThe decision may contain any number of statements, each of which specifies how the decision was made and what it means.\\n\\nIf there are multiple lines in the statement, they must be indented by at least two spaces.\\n\\nA statement must start with an exclamation mark (!).\\n\\nDecisions cannot contain comments or lines that do not belong to a statement.\\n\\nA statement must end with an exclamation mark (!).\\n\\nDecisions cannot contain comments or lines that do not belong to a statement.\\n\\nA statement must end with an exclamation mark (!).\\n\\nDecisions cannot contain comments or lines that do not belong to a statement.\\n\\nA comment may appear at any position in the document. A comment can be any length and can contain any number of characters. A comment may also have a blank line as its first character. Comments must begin on a new line and end on the same line as their first character.\\n"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"Predictions":"## Decision\\n\\n\\n\\nThe CLI version is not a runtime version, but it is a component version. Therefore, we should not couple the CLI version with the runtime version. In other words, the CLI version should be used as a standalone version.\\n\\n\\n\\nTo make this decision, we need to decide whether we want to use the CLI version as a standalone version or not. In this context, \\textbf{standalone} means that the CLI version is independent from other components in the system. \\textbf{Coupled} means that the CLI version depends on other components in the system.\\n\\n\\n## Decision\\n## Decision\\nThis component is coupled with another component that has been decided by the user before.\\n\\n\\n\\end{document}"}
{"File Name":"dapr\/API-010-appcallback-versioning.md","Context":"## Context\\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\\n","Decision":"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\\n- There would be no way for DAPR runtime to find out the app callback version before hand\\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\\n","tokens":47,"id":79,"Predictions":"## The decision is to require a versioning for HTTP App Callbacks. The decision is based on the following considerations:\\n## 1. There was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\\n## 2. It was decided that the only way to implement a versioning system would be to have separate versions for each API endpoint. This would allow us to have an API endpoint that is completely independent from the other API endpoints in DAPR. This would also allow us to support multiple versions of the same API endpoint, but it would require separate service implementations for each version.\\n## 3. It was decided that we should include an option in the application settings page to enable\/disable the versioning of HTTP App Callbacks.\\n## 4. It was decided that we should include an option in the application settings page to enable\/disable the versioning of HTTP App Callbacks.\\n## 5. It was decided that we should include an option in the application settings page to enable\/disable the versioning of HTTP App Callbacks.\\n## 6. It was decided that we should include an option in the application settings page to enable\/disable the versioning of HTTP App Callbacks.\\n## 7. We should implement a way for users to provide feedback on this decision, and provide a rationale for it.\\n## Rationale\\n## The only way to implement a versioning system would be to have separate versions for each API endpoint, which allows us to have an API endpoint completely independent from other APIs in DAPR (this is not possible with v1.x). This would also allow us support multiple versions of an API endpoint, but it would require separate service implementations for each version.\\n## Implementation\\n## We need some way for users to provide feedback on this decision and provide a rationale for it.\\n\\nThere were no comments\\n\\nThere are no comments yet\\n\\nYou can add a new comment below.\\n\\nPlease login or register first.\\n\\n- No labels\\n\\nThis site uses cookies, including third party cookies provided by Google Analytics and Facebook Pixel provided by Facebook Inc., in order to ensure proper operation and improve your experience on our website.\\n\\nBy continuing your visit on our site, you accept our use of cookies.\\n\\n- Accept cookies \\n- Reject cookies"}
{"File Name":"dapr\/API-008-multi-state-store-api-design.md","Context":"## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n","Decision":"*  New state store API is v1.0\/state\/`<store-name>`\/\\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\\nare specified.\\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore1  # Required. This is the unique name of the store.\\nspec:\\ntype: state.redis\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: actorStateStore  # Optional. default: false\\nvalue : true\\n```\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore2 # Required. This is the unique name of the store.\\nspec:\\ntype: state.mongodb\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n```\\nSo with the above example, the state APIs will be : v1.0\/state\/myStore1\/`<key>`\\nand v1.0\/state\/myStore2\/`<key>`\\n","tokens":115,"id":80,"Predictions":"## Decision\\nWe have decided to introduce the breaking change in API. The breaking change is to support multi state store with no backward compatibility.\\nWe have decided to use the following naming scheme for multiple state stores:\\n\\n1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\n\\n1.0\/state\/storeA\/`<key>`\\n\\n1.0\/state\/storeB\/`<key>`\\n\\n1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/`<key>`\\n\\n\\n## Context\\n\\n## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n\\n## Decision\\n## Decision\\nWe have decided to introduce the breaking change in API. The breaking change is to support multi state store with no backward compatibility.\\nWe have decided to use the following naming scheme for multiple state stores:\\n\\n1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\n\\n1.0\/state\/storeA\/`<key>`\\ncurrently we are using \\c{storeName} as a prefix of \\c{storeName} and \\c{storeName}\/`<key>` as suffix of \\c{storeName}. We are considering dropping \\c{storeName}\/`<key>` and use \\c{storeName}\/`<key>`. This would mean that we need only one `stateStoreName`. We will discuss this further with our team.\\ncurrently we are using 2 different states `storeA` and `storeB`, which means that we need two separate names for each of these states.\\n"}
{"File Name":"dapr\/ENG-002-Dapr-Release.md","Context":"## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n","Decision":"### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [.\/docs\/release_notes](https:\/\/github.com\/dapr\/dapr\/tree\/master\/docs\/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n","tokens":27,"id":81,"Predictions":"## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n## The release of dapr binaries is allowed without any blockers to users.\\n## Decision\\n%% Options:\\n%%  -c, --config=CONFIG_FILE, --config=config_file, --config=config_file\\n%%                  The configuration file for the new version. If not specified, the default configuration file will be used. This option can also be used in conjunction with --config-file option.\\n%%  -d, --debug=DEBUG, --debug=DEBUG, --debug=DEBUG, --debug=DEBUG\\n%%                  Enable debugging mode. If not specified, the default mode will be used. This option can also be used in conjunction with --debug option.\\n%%  -h, --help = help for options and available commands\\n%%                  Display this message and exit.\\n%%  -q, --quiet = Quiet mode. No output will be sent to the terminal when this option is set.\\n%%  -v, --verbose = Detailed mode. Output will be sent to the terminal when this option is set.\\n\\n# This is a decision record that describes how to safely implement a new version\\n\\n# You can specify a config file using the -c option.\\n\\n# You can specify a config file using the -d option.\\n\\n# You can specify a config file using the -h or -q options.\\n\\n# You can specify a config file using the -q or -v options.\\n\\n# This record describes how to safely implement a new version\\n\\n# You can specify a config file using the -d or -h options.\\n\\nThis record describes how to safely implement a new version\\n\\nYou can specify a config file using the -c or -d options."}
{"File Name":"dapr\/API-005-state-store-behavior.md","Context":"## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n","Decision":"### Concurrency model\\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\\n* Future version of Dapr may support call throttling through application channel.\\n* We'll choose last-write wins as the default.\\n### Consistency model\\n* Dapr supports both eventual consistency and strong consistency.\\n* Actors always use strong consistency.\\n* We'll choose eventual consistency as default for services other than actors.\\n### Actor Transaction\\n* Dapr-compatible Actor state stores shall support ACID transaction.\\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\\n### Config annotation\\n* User payload can contain an optional **config** annotation\/element that expresses various constraints and policies to be applied to the call, including:\\n* Concurrency model: first-write or last-write\\n* Consistency model: strong or eventual\\n* Retry policies:\\n* Interval\\n* Pattern: linear, expotential\\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\\n### State store configuration probe\\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\\n* Supported concurrency model\\n* Supported consistency model\\n* A state store instance shall return the specific configuration of the current instance.\\n* It's considered out of scope to require state store to dynamically apply new configurations.\\n### Dapr\\n* Update state store API spec to reflect above decisions\\n* Create backlog of issues to implement above decisions\\n","tokens":62,"id":82,"Predictions":"## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n\\n## Decision\\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n\\n## Decision\\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"dapr\/ARC-003-grpc-protobuf-coding-convention.md","Context":"## Context\\nWe have defined gRPC services and protobuf messages without convention, which results in the duplicated protobuf definitions and inconsistent names of services and messages. Thus, this record defines the minimum-level coding convention for Protobuf message to improve the quality of grpc\/protobuf message definitions.\\n","Decision":"* Use `google.protobuf.Any` data field only if the message field conveys serialized protobuf message with type url. Otherwise, use the explicit data type or protobuf message.\\n* Use `Request` suffix for gRPC request message name and `Response` suffix for gRPC response message name\\n* Do not use `Client` and `Service` suffix for gRPC service name e.g. (x) DaprClient, DaprService\\n* Avoid the duplicated protobuf message definitions by defining the messages in shared proto\\n* Define and use enum type if field accepts only predefined values.\\n","tokens":56,"id":83,"Predictions":"## The decision for this record is to add a new gRPC service, with a new protobuf message, and to rename the existing service and message. The new service is called \"gRPC\".\\n## Context\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Service\\nThe service is defined as follows:\\n\\n## Message\\nThis message defines the following parameters: \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tnote: This message does not define any parameters.\\t\\t\\t\\t\\t\\t\\tnote: This message defines the following parameters: \\t\\tnote: This message does not define any parameters.\\t\\\\r\\\\r\\\\r\\\\r\\\\r\\\\r\\\\r\\\\r\\\\r\\\\r\\r\\n\\r\\n\\nThis record defines a new gRPC server. The name of the server will be \"gRPCServer\", and it will be added to the \"grpc\" namespace. The new gRPC server will have two protobuf messages, named \"server.proto\" and \"client.proto\", both of which are shown below.\\nservice grpc_server {\\n    rpc server (rpc client) returns (rpc response) {}\\n}\\nmessage rpc_server {\\n    rpc response { rpc response }\\n}\\nmessage rpc_client {\\n    rpc response { rpc response }\\n}\\nmessage grpc_client {\\n    rpc server (rpc client) returns (rpc response) {}\\n}\\n\\nThis record defines a new gRPC client. The name of the client will be \"grpcClient\", and it will be added to the \"grpc\" namespace. The new grpcClient will have two protobuf messages, named \"server.proto\" and \"client.proto\", both of which are shown below.\\nmessage grpc_client {\\n    rpc server (rpc client) returns (rpc response) {}\\n}\\nmessage grpc_client {\\n    rpc response { rpc response }\\n}\\n\\nThis record defines a new gRPC protocol definition. It has three levels of nesting, with each level nested in its parent level. Each level has its own set of rules for nesting, including nesting for base types, nested base types, nested interfaces, nested interfaces with base types, nested enums, and nesting for structs.\\nprotocol grpc_proto {\\n    type Base;\\n    type BaseType;\\n    type BaseTypeInterface;\\n    type BaseTypeEnum;\\n}\\n\\nprotocol grpc_proto2 {\\n    type Base;\\n}\\n\\nprotocol grpc_proto3 {\\n    type Base;\\n}\\n\\nstruct Base implements grpc_proto.BaseTypeInterface {\\n}\\n\\nstruct BaseType implements grpc_proto.BaseTypeInterface {\\n}\\n\\nstruct BaseTypeEnum implements grpc_proto.BaseTypeEnum {}\\n\\nstruct Base implements grpc_proto.BaseTypeInterface {\\n}\\n\\nstruct Enum implements grpc_proto.Enum {}\\n\\nstruct Enum implements grpc_proto.Enum {}\\n\\nstruct Enum implements grpc_proto.Enum {}\\n\\nstruct Enum implements grpc_proto.Enum {}\\n\\nstruct Enum implements grpc_proto.Enum {}\\n\\nenum EnumName extends Enum implements enum {\\n\\n}\\n\\nenum EnumName extends Enum {\\n\\n  case foo = 1,\\n  case bar = 2,\\n  case baz = 3,\\n}\\n\\nenum EnumName extends Enum {\\n\\n  case foo = 1,\\n  case bar = 2,\\n  case baz = 3,\\n} \/\/ Nested in enum\\n\\nenum Enums extends Enum {\\n\\n  enum Foo : Enums.Type,\\n\\n  enum Bar : Enums.Type,\\n\\n  enum Baz : Enums.Type,\\n\\n} \/\/ Nested in enums\\n\\nenum Enums extends Enums {\\n\\n   enum Foo : Enums.Type,\\n\\n   enum Bar : Enums.Type,\\n\\n   enum Baz : Enums.Type,\\n\\n} \/\/ Nested in enums\\n\\nenum Enums extends enums {\\n\\n   enum Foo : Enums.Type,\\n\\n   enum Bar : Enums.Type,\\n\\n   enum Baz : Enums.Type,\\n\\n} \/\/ Nested in enums\\n\\nenum Enums extends enums {\\n\\n   enum Foo : Enums.Type,\\n\\n   enum Bar : Enums.Type,\\n\\n   enum Baz : Enums.Type,\\n\\n} \/\/ Nested in enums\\n\\nenum SStruct extends SStruct{\\n\\n     SStructFoo: SStruct.Foo,\\n     SStructBar: SStruct.Bar,\\n     SStructBaz: SStruct.Baz\\n\\n};\\n\\ntype A struct{}\\n\\ntype B struct{}\\n\\ntype C struct{}\\n\\ntype D struct{}\\n\\ntype E struct{}\\n\\ntype F struct{}\\n\\ntype G struct{}\\n\\ntype H struct{}\\n\\ntype I struct{}\\n\\ntype J struct{}\\n\\ntype K struct{} \/\/ Nested in A\\n\\n\/\/ nested A's must not be nested inside B or C or D or E or F or G or H or I \\n\\n\/\/Nested within G becomes 'G' but not 'G.I'\\n\\n\/\/Nested within H becomes 'H.I' but not 'H.G.I'\\n\\n\/\/Nested within I becomes 'I.G.I' but not 'I.H.G.I'\\n\\n\/\/Nested within J becomes 'J.G.I' but not 'J.H.G.I'\\n\\n\/\/Nested within K becomes 'K.G.I' but not 'K.H.G.I'\\n\\n\/\/Nested within A becomes 'A.i' but not 'A.K.i'\\n\\n\/\/Nested within B becomes 'B.i' but not 'B.K.i'\\n\\n\/\/Nested within C becomes 'C.i' but not 'C.K.i'\\n\\n\/\/Nested within D becomes 'D.i' but not 'D.K.i'\\n\\n\/\/Nested within E becomes 'E.i' but not 'E.K.i'\\n\\n\/\/Nested within F becomes 'F.k' but not F.k.f\\n\\nvar TypeA struct{}\\nvar TypeB struct{}\\nvar TypeC struct{}\\nvar TypeD struct{}\\nvar TypeE struct{}\\nvar TypeF struct{}\\nvar TypeG struct{}\\nvar TypeH struct{}\\nvar TypeI structure{ }\\n\\nfunc main() {\\n\t\/* Create an empty map to hold our types *\/\\n\ttype Map := map[string]*A{\\n\t\tName: string{key:\"a\"},\\n\t\tAge: int{key:\"b\"},\\n\t\tSleepTime:int{key:\"c\"},\\n\t\tType:A{\\n\t\t\tName:string{\\n\t\t\t\tKey:\"a\",\\n\t\t\t\tValue:A{\\n\t\t\t\t\tName:string{value:\"b\"},\\n\t\t\t\t\tValue:int{value:\"c\"}\\n\t\t\t\t},\\n\t\t\t},\\n\t\t\tAge:int{key:\"d\"},\\n\t\t\tSleepTime:int{key:\"e\"}\\n\t\t},\\n\t\tTypeB{\\n\t\t\tName:string{\\n\t\t\t\tKey:\"b\",\\n\t\t\t\tValue:B{\\n\t\t\t\t\tName:string{value:\"c\"},\\n\t\t\t\t\tValue:int{value:\"d\"}\\n\t\t\t\t},\\n\t\t\t},\\n\t\t\tTypeC{\\n\t\t\t\tName:string{\\n\t\t\t\t\tKey:\"c\",\\n\t\t\t\t\tValue:C{\\n\t\t\t\t\t\tName:string{value:\"d\"},\\n\t\t\t\t\t\tValue:int{value:\"e\"}\\n\t\t\t\t\t},\\n\t\t\t\t},\\n\t\t\tTypeD{\\n\t\t\t\tName:string{\\n\t\t\t\t\tKey:\"d\",\\n\t\t\t\t\tValue:D{\\n\t\t\t\t\t\tName:string{value:\"e\"},\\n\t\t\t\t\t\tValue:int{value:\"f\"}\\n\t\t\t\t\t},\\n\t\t\t\tTypeE{\\n\t\t\t\t\tName:string{\\n\t\t\t\t\t\tKey:\"e\",\\n\t\t\t\t\t\tValue:E{\\n\t\t\t\t\t\t\tName:string{value:f}\\n\t\t\t\t\t\t},\\n\t\t\t\t\t\tValue:F{\"g\"}\\n\t\t\t\t\t},\\n\t\t\t\tTypeF{\\n\t\t\t\t\tName:string{\\n\t\t\t\t\t\tKey:@\"f\",\\n\t\t\t\t\t\tvalue:f{\"g\"}\\n\t\t\t\t\t},\\n\t\t\t\tTypeG:{\\n\t\t\t\t\tname: string{\"h\",\"i\",\"j\",\"k\",\"l\",\"m\"} \\n\t\t\t\t  ,Value:{name:key,value:key,value:key,value:key,value:key,value:key,value:key,value:key,value:key,value:key,value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value,key:value}}\\n\t\t\t\t  }\\n\t\t\t    },\\n\t\t\t    Age:\\n\t\t\t    int{},\\n\t\t\t    SleepTime:\\n\t\t\t    int{},\\n\t\t\t  }\\n\\n var MapMap map[string]*A\\n var MapMapMap map[string]*A\\n var MapMapMapMap map[string]*A\\n\\nfunc main() {\\n\ttype Map := map[string]*A{\\n\\n\tname: string{\"a\", nil},\\n\\n\tsize: int{\"b\", nil},\\n\\n\tvalue:{name:{nil}, value:{nil}}\\n\\n}\\n\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname: string{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{name:{nil}, value:{nil}}\\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{name:{nil}, value:{nil}}\\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{name:{nil}, value:{nil}}\\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{name:{nil}, value:{nil}}\\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{} \\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\", nil},\\n\\n\tsize:Int(\"b\", nil),\\n\\n\tvalue:{} \\n\\n}\\nfunc main() {\\n\\n\ttype Map := map[string]*A{\\n\\n\tname:String{\"a\"},\\n\\n\tsize:Int(\"b\"), \\n\\n\tvalue:{} \\n\\n}\\nfunc main () {} \/\/ error here!\\n\\n func TestMain () { \/* Create an empty map to hold our types *\/\\n\\n\tvar m *map[int]string\\n\tm[0] = {\"-1\":0,\"-2\":0,\"-3\":0,\"-4\":0,\"-5\":0,\"-6\":0,\"-7\":0,\"-8\":0,\"-9\":0,\"10\":0,\"11\":0,\"12\":0,\"13\":0,\"\\n        ,\"14\":0,\"15\":0,\"16\":0.\"17\":\"18\",\"18\":\"19\",\"20\":\"21\"};\\n\\n\tm[1] = {\"-1\":\\n        {\"-2\":\\n        {\"-3\":\\n        {\"-4\":\\n        {\"-5\":\\n        {\"-6\":\\n        {\"-7\":\\n        {\"-8\":\\n        {\"10\":\"11\":\"12\":\"13\":\"14\":\"15\":\"16\":\"17\":\"18\":\"19\":\"\"},\"20\":\"\"},\"21\":\"\"},\"22\":\"\"},\"23\":\"\"},\"24\":\"\"},\"25\":\"\"},\"26\":\"\"},\"27\":\"\"},\"28\":\"\"},\"29\":\"\"},\"30\":\"\"},\"31\":\"\"}}; m[2] = {\"11\":{\"12\":{\"13\":{\"14\":{\"15\":{\"16\":{\"17\":{\"18\":{\"19\":\"\",\"20\":\"\",\"21\":\"\",\"22\":\"\",\"23\":\"\",\"24\":\"\",\"25\":\"\",\"26\":\"\",\"27\":\"\",\"28\":\"\",\"29\":\"\",\"30\":\"\",\"31:\"\",}}}}}}; m[3] = {\"12\":{\"13\":{\"14\":{\"15\":{\"16\":{\"17\":{\"18\":{\"19\":[{\"_id\":\"5c9f6e5e7c48cb98d82df9cc\",\"contentUrl\":\"\/statics\/2018\/01\/04\/05\/02\/1435535545\/golang_20180104_053202_1435535545.json\"},\"_id\":\"\/statics\/2018\/01\/04\/05\/02\/1435535545\/golang_20180104_053202_1435535545.json\"}]},{\"_id\":\"\/statics\/2018\/01\/04\/05\/02\/a6eb3f734f92fd58aed76ec7.json\"},\"_id\":\"\/statics\/2018\/01\/04\/05\/02\/a6eb3f734f92fd58aed76ec7.json\"}]}\\nfor k,v := range m[*]; k != v { \/* TODO *\/ }\\nfor k,v := range m[*]; k != v { \/* TODO *\/ }\\n\\n func TestMain () { \/* Create an empty map to hold our types *\/\\n\\n\tvar m *map[int]string\\n\tm[1] = {\"11\":{\\n            \"12\":{\\n                    \"13\":{\\n                            \"14\":{\\n                                    \"15\":{\\n                                        \"16\":{\\n                                            \"_id\":\"\/statics\/2018\/\\n                                                    \/01\/\\n                                                    \/04\/\\n                                                    \/05\/\\n                                                    \/02\/\\n                                                    \/03\/\",\\n                                            \"_id\":\"\/statics\/\\n                                                    \/01\/\\n                                                    \/04\/\\n                                                    \/05\/\\n                                                    \/02\/\"\\n                                        },\"_id\":\"\/statics\/\\n                                                    \/01\/\\n                                                    \/04\/\\n                                                    \/05\/\"\\n                                        },\"_id\":\"\/statics\/\\n                                                    \/01\/\"\\n                                        }},\"_id\":\"\/statics\/\\n\\n                                                \/\\n                                                \/\\n                                                \/\\n                                                \/\\n                                                \/\\n                                                \/\\n\\n                                    },\\n                                },\\n                                },\\n\\n                               "}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"Predictions":"\\nIf we do introduce different manifest types, we need to decide what each manifest type represents. For example, if we have a message bus and a binding, each manifest type could represent a different event source or event bus.\\n\\n\\n\\n## Decision\\n\\nIf we do introduce different manifest types, we need to decide what each manifest type represents. For example, if we have a message bus and a binding, each manifest type could represent a different event source or event bus.\\n\\end{itemize}\\n\\end{document}\\n"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"Predictions":"## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n## Decision: The decision is defined in [this issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\\n\\nThis command will list all the possible decisions made by the user.\\n\\n\\end{lstlisting}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Command line options}\\n\\begin{itemize}\\n    \\item \\texttt{-h}: help on the current command line option.\\n    \\item \\texttt{-v}: print verbose output.\\n    \\item \\texttt{-p}: print the output of a previous command.\\n    \\item \\texttt{-c}: create a new file for the user to read and write commands to. \\n    \\item \\texttt{-t}: create a new file for the user to read and write commands to. \\n    \\item \\texttt{-r}: read a file from stdin. \\n    \\item \\texttt{-r} can be used to read a file from stdin. \\n    \\item \\texttt{-s}: read a file from stdin, and write it to stdout.\\n    \\item You can also use this option to ask for help on the current command line option.  \\n\\end{itemize}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Command line options}\\n\\begin{lstlisting}[language=bash]\\n$ dapr init --help\\n\\nUsage:\\n  dapr init [--help] [--verbose] [--no-deprecation] [--no-deprecation-logs]\\n\\nOptions:\\n  -h, --help            show this help message and exit\\n\\n  --verbose          increase verbosity (default 5)\\n\\n  --no-deprecation   don't deprecate any of your existing code\\n\\n  --no-deprecation-logs don't print deprecation logs (default 0)\\n\\n  -c, --create          Create a new directory for the user to write commands to.\\n\\n  -t, --write           Write commands to this directory.\\n\\n  -r, --read            Read commands from stdin.\\n\\n  -s                    Read commands from stdin, and write them to stdout.\\n\\nCommands:\\n   init                  Create a directory for the user's use.\\n   uninit                 Uninstall an existing application.\\n\\nCommands that will be deprecated:\\n   uninstall              Uninstall an existing application.\\n\\nCommands that will be deprecated:\\n   init                  Create a new application with an empty package.json.\\n   uninit                 Uninstall an existing application.\\n   uninstall              Uninstall an existing application.\\n\\end{lstlisting}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Command line options}\\n\\begin{lstlisting}[language=bash]\\n$ dapr uninit\\n\\nUsage:\\n  dapr uninit [--verbose] [--no-deprecation] [--no-deprecation-logs]\\n\\nOptions:\\n  -h, --help            show this help message and exit\\n\\n  --verbose          increase verbosity (default 5)\\n\\n  --no-deprecation   don't deprecate any of your existing code\\n\\n  --no-deprecation-logs don't print deprecation logs (default 0)\\n\\nCommands:\\n   uninit                Uninstall an existing application.\\n\\nCommands that will be deprecated:\\n   uninstall              Uninstall an existing application.\\n\\end{lstlisting}\\n\\end{frame}\\n\\n% TODO Different ways of defining options? \\n% TODO Different ways of defining options? \\n\\n% TODO Different ways of defining options? \\n\\n% TODO Different ways of defining options? \\n\\n% TODO Different ways of defining options? \\n\\n% TODO Different ways of defining options? \\n\\n"}
{"File Name":"dapr\/ARC-004-http-server.md","Context":"## Context\\nGo community has the multiple http server implementations, such as go net\/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\\n","Decision":"* Use [fasthttp server](https:\/\/github.com\/valyala\/fasthttp) implementation because fasthttp offers [the best performance and lowest resource usages](https:\/\/github.com\/valyala\/fasthttp#http-server-performance-comparison-with-nethttp) for the existing HTTP 1.1 server\\n* Use [fasthttpadaptor](https:\/\/godoc.org\/github.com\/valyala\/fasthttp\/fasthttpadaptor) if you need to convert fasthttp request context to net\/http context.\\n","tokens":44,"id":86,"Predictions":"## Go net\/http\\n## Decision Type\\n## Software\\n## Decision Reason\\n## Go community has the multiple http server implementations, such as go net\/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\\n## Decision Type\\n## Software\\n## Decision Reason\\n## Go community has the multiple http server implementations, such as go net\/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\\n## Decision Type\\n## Software\\n## Decision Reason\\n"}
{"File Name":"dapr\/ARC-001-refactor-for-modularity-and-testability.md","Context":"## Context\\nAs we keep building up Dapr features, it becomes apparent that we need to refactor the existing code base to reinforce component modularity. This will improve testability and maintainability in long run. And this refactor also lays the foundation of opening up extensible points (such as Bindings) to the community.\\n","Decision":"### Dapr\\n* Formally separate hosting and API implementations. Hosting provides communication protocols (HTTP\/gRPC) as different access heads to the same Dapr API implementation.\\n* Ensure consistency between gRPC and HTTP interface.\\n* Separate binding implementations to a separate repository.\\n* Use smart defaults for configurable parameters.\\n* Rename Dapr runtime binary from **dapr** to **daprd**.\\n### Non-Dapr\\n* We may consider allowing Dapr to dynamically load bindings during runtime. However, we are not going to implement this unless it's justified by customer asks.\\n* A unified configuration file that includes paths to individual configuration files.\\n* Provide a Discovery building block with hopefully pluggable discovery mechanisms (such as a custom DNS).\\n","tokens":66,"id":87,"Predictions":"## To refactor the existing code base to reinforce component modularity and open up extensible points (such as Bindings) to the community.\\n"}
{"File Name":"dapr\/SDK-001-releases.md","Context":"## Context\\nDapr exposes APIs for building blocks which can be invoked over http\/gRPC by the user code. Making raw http\/gRPC calls from user code works but it doesn't provide a good strongly typed experience for developers.\\n","Decision":"* Dapr provides language specific SDKs for developers for C#, Java, Javascript, Python, Go, Rust, C++. There may be others in the future\\n- For the current release, the SDKs are auto-generated from the Dapr proto specifications using gRPC tools.\\n- In future releases, we will work on creating and releasing strongly typed SDKs for the languages, which are wrappers on top of the auto-generated gRPC SDKs (e.g. C# SDK shipped for state management APIs with the 0.1.0 release.) This is the preferred approach. Creating purely handcrafted SDKs is discouraged.\\n* For Actors, language specific SDKs are written as Actor specific handcrafted code is preferred since this greatly simplifies the user experience. e.g. The C# Actor SDK shipped with the 0.1.0 release.\\n","tokens":47,"id":88,"Predictions":"## The decision is to use the http\/gRPC API.\\n## Decision\\n## The decision is to use the http\/gRPC API.\\n## Decision\\n## The decision is to use the http\/gRPC API.\\n## Decision\\n## The decision is to use the http\/gRPC API.\\n## Decision\\n## The decision is to use the http\/gRPC API.\\n## Decision\\n## The decision is to use the http\/gRPC API.\\n"}
{"File Name":"dapr\/API-003-messaging-api-names.md","Context":"## Context\\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\\n","Decision":"### Dapr\\n* All messaging APIs are grouped under a **messaging** namespace\/package.\\n* We define three distinct messaging interfaces:\\n- **direct**\\nOne-to-one messaging between two parties: a sender sending message to a recipient.\\n- **broadcast**\\nOne-to-many messaging: a sender sending message to a list of recipients.\\n- **pub-sub**\\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\\n","tokens":30,"id":89,"Predictions":"## The user has requested that we rename the messaging interfaces to make it clear what they are for. The new names are: \\n\\n\\n\\n## \\n\\n\\n\\n## \\n\\n\\n\\n## \\n\\n\\n\\n## \\n\\n\\n\\n## Messaging interface name: \\textbf{Message}. The user has requested that we rename this to be more clear.\\n```\\nI have tried to implement this with `react-native` but I am not sure if it is possible to use `react-native` and `react-navigation` at the same time. If not, I would like to know if there is a way to use both `react-native` and `react-navigation`.\\nI have tried searching for solutions online but couldn't find anything useful. I would really appreciate any help.\\nThank you in advance. \\nEDIT: I have tried using this approach:\\n```\\nimport { NavigationDrawer } from '@react-navigation\/native';\\nimport { createStackNavigator } from '@react-navigation\/stack';\\nimport { createAppContainer } from 'react-navigation';\\n\\nconst App = () => {\\n  const [navigationState, setNavigationState] = useState(null);\\n\\n  const handleNavigation = () => {\\n    navigationState.navigation?.push({ screen: createAppContainer(StarterPage) });\\n  };\\n\\n  const handleDrawerTap = (e) => {\\n    e.preventDefault();\\n    setNavigationState({ screen: createAppContainer(StarterPage) });\\n  };\\n\\n  return (\\n    <NavigationDrawer.Navigator>\\n      <NavigationDrawer.Screen name=\"Home\" component={Home} \/>\\n      <NavigationDrawer.Screen name=\"Dashboard\" component={Dashboard} \/>\\n      <NavigationDrawer.Screen name=\"About\" component={About} \/>\\n      <NavigationDrawer.Screen name=\"Messages\" component={Messages} \/>\\n    <\/NavigationDrawer.Navigator>\\n  );\\n};\\n\\nexport default App;\\n```\\nBut it doesn't seem to work. \\nEDIT2:\\nI have found a solution that works for me, but I am not satisfied with it because of the following reasons:\\n\\nI cannot use `createAppContainer()` inside of a function because inside of a function you can only access variables in the scope in which the function is defined.\\nUsing `createAppContainer()` inside of a function also causes problems when you try to use it outside of the function.\\n\\nHere is my code:\\n```\\nimport React, { Component } from 'react';\\nimport { createAppContainer } from 'react-navigation';\\nimport { createStackNavigator } from '@react-navigation\/stack';\\nimport { navigate } from 'react-navigation';\\n\\nclass App extends Component {\\n\\n  componentDidMount() {\\n    this.props.navigation.dispatch(createAppContainer('Home'));\\n  }\\n\\n  render() {\\n\\n    return (\\n      <Stack.Navigator initialRouteName=\"Home\">\\n        <Stack.Screen name=\"Home\" component={Home} \/>\\n        <Stack.Screen name=\"Dashboard\" component={Dashboard} \/>\\n        <Stack.Screen name=\"About\" component={About} \/>\\n        <Stack.Screen name=\"Messages\" component={Messages} \/>\\n      <\/Stack.Navigator>\\n    );\\n  }\\n}\\n\\nexport default App;\\n```\\nOP 2019-07-13: I have found a solution that works for me, but I am not satisfied with it because of the following reasons:\\n\\nI cannot use createAppContainer() inside of a function because inside of a function you can only access variables in the scope in which the function is defined.\\nUsing createAppContainer() inside of a function also causes problems when you try to use it outside of the function.\\n\\nHere is my code:\\n```\\nimport React, { Component } from 'react';\\nimport { createAppContainer } from 'react-navigation';\\nimport { createStackNavigator } from '@react-navigation\/stack';\\nimport { navigate } from 'react-navigation';\\n\\nclass App extends Component {\\n\\ncomponentDidMount() {\\n   this.props.navigation.dispatch(createAppContainer('Home'));\\n}\\n\\nrender() {\\n\\nreturn (\\n   <Stack.Navigator initialRouteName=\"Home\">\\n       <Stack.Screen name=\"Home\" component={Home} \/>\\n       <Stack.Screen name=\"Dashboard\" component={Dashboard} \/>\\n       <Stack.Screen name=\"About\" component={About} \/>\\n       <Stack.Screen name=\"Messages\" component={Messages} \/>\\n   <\/Stack.Navigator>\\n );\\n}\\n\\nexport default App;\\n```"}
{"File Name":"dapr\/API-011-state-store-api-parity.md","Context":"## Context\\nWe reviewed parity of state store APIs .\\n","Decision":"* GetState APIs continue to have Single Key Get and Bulk Get APIs behaviour as current 0.10.0 version.\\n* SaveState API will continue to have one SaveState API endpoint. If user wants to save single key, same save state API will be used\\nfor passing single item in the bulk set.\\nPotential issues arises if following new single key save state API is introduced:\\n`Post : state\/{storeName}\/{key}`\\nThis will conflict with\\n- State Transaction API, if the key is \"transaction\"\\n- GetBulkState API, if the key is \"bulk\"\\nSo the decision is to continue the Save State API behaviour as current 0.10.0 version.\\n* Bulk Delete API might come in future versions based on the scenarios.\\n","tokens":13,"id":90,"Predictions":"## We decided to use a single parity store API for all architectures.\\n## List of Decision Results\\n## 1. We decided to use a single parity store API for all architectures.\\n## 2. We decided to use a single parity store API for all architectures.\\n## 3. We decided to use a single parity store API for all architectures.\\n## 4. We decided to use a single parity store API for all architectures.\\n## 5. We decided to use a single parity store API for all architectures.\\n## List of Decision Results\\n## 6. We decided to use a single parity store API for all architectures.\\n## 7. We decided to use a single parity store API for all architectures.\\n## 8. We decided to use a single parity store API for all architectures.\\n## 9. We decided to use a single parity store API for all architectures.\\n## List of Decision Results\\n## 10. We decided to use a single parity store API for all architectures.\\n\\nThe above decision is valid and correct.\\n\\n# The following code is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like, which is an example of how the decision record looks like.\\n\\nimport time\\n\\ndef getParity():\\n    if \"PARITY\" in sys.argv[1]:\\n        return \"PARITY\"\\n    else:\\n        return \"NONE\"\\n\\ndef getParityStore():\\n    if \"STORE\" in sys.argv[1]:\\n        return \"STORE\"\\n    else:\\n        return \"NONE\"\\n\\ndef getStore():\\n    if \"STORE\" in sys.argv[1]:\\n        return \"STORE\"\\n    else:\\n        return \"NONE\"\\n\\ndef getStoreStore():\\n    if \"STORE\" in sys.argv[1]:\\n        return \"STORE\"\\n    else:\\n        return \"NONE\"\\n\\ndef main(argv):\\n    # Initialize the main program and run it.\\n    try:\\n        main(sys.argv)\\n    except Exception as e:\\n        print(\"Error: %s\" % e)\\n        sys.exit(1)\\n\\nif __name__ == \"__main__\":\\n    main(sys.argv)\\n\\nHere are some examples:\\n\\n$ python3 .\/parity.py\\nPARITY\\n$ python3 .\/getParity()\\nPARITY\\n$ python3 .\/getParityStore()\\nSTORE\\n\\nHere are some examples where you can find more information.\\n\\n- https:\/\/github.com\/Python\/parity\/blob\/master\/docs\/parity.txt\\n\\nThe Parity library provides two different ways that you can implement your own data structures that are stored in memory and read from memory by Python.\\n\\nThe first way we will consider allows you to implement your own data structure using Python\u2019s built-in type system and then read from it using the built-in type library (see below).\\n\\nThe second way we will consider allows you to implement your own data structure using Python\u2019s built-in type system and then write from it using Python\u2019s built-in type library (see below).\\n\\nBoth approaches have their advantages and disadvantages so it really depends on your requirements whether you should choose one or another approach.\\n\\nBuilding Your Own Data Structure\\n\\nThere are several ways that you can build your own data structure using Python\u2019s built-in type system (see below).\\n\\n- You can implement any Python data structure according to its documentation (see below).\\n- You can create your own custom data structure according to its documentation (see below).\\n- You can implement any Python data structure according its documentation but then call its implementation methods directly (see below).\\n- You can implement any Python data structure according its documentation but then call its implementation methods directly but then write your own implementation methods based on their documentation (see below).\\n\\nImplementing Your Own Data Structure Using Built-In Type System\\n\\nYou can implement any Python data structure using Python\u2019s built-in type system:\\n\\nimport types\\n\\nclass ParityType(type):\\n    def __init__(self):\\n        self._data = []\\n\\nclass DataStructure(object):\\n    \\n    def __init__(self):\\n       self._data = []\\n\\n  \\n  \\nclass Parity(object):\\n    \\n    def __init__(self):\\n       self._data = []\\n\\nIf you want more information about this approach see https:\/\/docs.python.org\/3\/library\/types.html#types.constructors .\\n\\nImplementing Your Own Data Structure Using Built-In Type Library\\n\\nYou can also implement any Python data structure using built-in type library:\\n\\nimport types\\n\\n\\nclass ParityType(type.issubclass(type.BaseType)):\\n    \\n    def __init__(self):\\n       super().__init__()\\n\\nIf you want more information about this approach see https:\/\/docs.python.org\/3\/library\/types.html#types.constructors .\\n\\nWriting Your Own Implementation Methods Based on Built-In Type System Documentation\\n\\nYou also have several options when writing your own implementation methods based on built-in type system documentation:\\n\\n- You can write your own implementation method based on its documentation without calling its implementation method directly (see below).\\n- You can call its implementation method directly with some extra arguments passed through its constructor (see below).\\n- You can call its implementation method directly with some extra arguments passed through its constructor but then write your own implementation method based on their documentation (see below).\\n- You can call its implementation method directly with some extra arguments passed through its constructor and then write your own implementation method based on their documentation (see below).\\n\\nImplementing Your Own Implementation Method Based on Built-In Type Library Documentation\\n\\nFinally, if you want more information about this approach see https:\/\/docs.python.org\/3\/library\/types.html#types.constructors .\\n\\nUsing Built-In Type System Documentation\\n\\nAs mentioned above, there are several ways that you can build your own data structure using Python\u2019s built-in type system:\\n\\n- You can implement any Python data structure according their documentation (see above).\\n- You can create your own custom data structure according their documentation but then call their implementations methods directly (see above).\\n- You can implement any Python data structure according their documentation but then call their implementations methods directly but then write your own implementations methods based on their documentation (see above).\\n\\nUsing Built-In Type Library Documentation\\n\\nThere are also several ways that you can build your own data structure using built-in type library:\\n\\n- You can implement any Python data structure according their documentation but then call their implementations methods directly with some extra arguments passed through their constructor (see above).\\n- If you want more information about this approach see https:\/\/docs.python.org\/3\/library\/types.html#types.constructors .\\n"}
{"File Name":"dapr\/API-007-tracing-endpoint.md","Context":"## Context\\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\\n","Decision":"We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\\n","tokens":53,"id":91,"Predictions":"## 1. Do we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code? Yes, we do.\\n\\n## 2. Do we use a user code to configure and implement proper tracing themselves? Yes, we do.\\n\\n## 3. If so, how? We use a user code to configure and implement proper tracing themselves.\\n\\n\\nThe Dapr sidecar is an experiment that enables the server side of Dapr to be distributed across multiple machines. The goal of this experiment is to allow the server side of Dapr to be exposed as a service without exposing the entire application on the same machine.\\n\\n\\nThe Dapr sidecar is a completely separate process from the application. It runs in its own process and communicates with the application via gRPC communication channels.\\n\\n\\nThe user code that configures and implements proper tracing in Dapr should implement an interface that exposes both HTTP headers for correlation id (e.g., `http:\/\/example.com:8080\/trace`) and gRPC metadata (e.g., `rpc:service\/trace`). The user code should also provide a `Trace` implementation that is used by the Dapr sidecar.\\n\\n\\nThe Trace implementation should provide methods for getting all information about a given trace (e.g., `Trace.getTraceId()`) as well as methods for printing all information about a given trace (e.g., `Trace.print()`). The Trace implementation should also provide methods for getting all information about a given trace (e.g., `Trace.getTraceId()`) as well as methods for printing all information about a given trace (e.g., `Trace.print()`). The Trace implementation should also provide methods for getting all information about a given trace (e.g., `Trace.getTraceId()`) as well as methods for printing all information about a given trace (e.g., `Trace.print()`).\\n\\n## Context\\n## 1. Do we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code?\\n\\n## 2. Do we use a user code to configure and implement proper tracing themselves?\\n\\n## 3. If so, how?\\nWe inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code.\\n"}
{"File Name":"nicoprj\/adr-001.md","Context":"## Context\\nWhile we were considering how to ship the CSSK, the question of how it\\nshould be organized came up several times. The crux of the issue was\\nwhether the more reusable parts (for example, the event dispatch bits)\\nshould be separated into a library, or whether it should all just live\\nin one big pile of code. Currently, the CSSK is organized in the\\nlatter manner.\\nSeparating the infrastructure code out into a separate library would\\nmake it more obvious which parts of the code are intended to be edited\\nby the developer. Right now, it\u2019s not immediately obvious where to\\nmake changes, and what bits are there to support the development\\nexperience.\\nSeparating the infrastructure code out into a separate library would\\nalso make it far more inconvenient to make changes to it. Developers\\nwould need to update the lib, possibly creating their own fork, and\\nthen ensure that the app code was using the appropriate version. This\\nis extremely likely to happen because the infrastructure code is\\nneither comprehensive nor mature.\\n","Decision":"We have decided to keep it all together, but clearly separate the\\n\u201clibrary\u201d code from the \u201capp\u201d code by using separate directories for\\nthose two types of code: \u201clib\u201d and \u201capp\u201d.\\n","tokens":219,"id":92,"Predictions":"## Decision\\nThe infrastructure code is split into a separate library, which would\\nmake it easier to make changes to it. This would also make it\\nfar more convenient to make changes to it. Developers would need to\\nupdate the lib, possibly creating their own fork, and then ensure that\\nthe app code was using the appropriate version. This is extremely\\nlikely to happen because the infrastructure code is neither comprehensive nor\\nmature.\\n## Context\\n## ContextThis is a software decision record for a web application that uses the CSSK. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n## ContextThis is a software decision record for a web application that uses the CSSK. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n## ContextThis is a software decision record for a web application that uses the CSSK. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n## ContextThis is a software decision record for a web application that uses the CSSK. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Background\\n## BackgroundThe CSSK is an extremely important part of C++ development because it enables developers to reuse code and reduce duplication across different projects.\\nThe CSSK has been in existence since 1990, and has been extended with new features over time.\\nThe CSSK does not have any formal specifications or standards.\\nA version of C++ was introduced in 1998, and this version includes support for C++11.\\nMany libraries and applications use C++11 features (for example, Boost).\\nThe main library used by most C++ applications is called libstdc++ (for example, GCC).\\nOther libraries such as Boost provide additional functionality in addition to what\u2019s included in libstdc++.\\nThe following table lists some of these additional features:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\\\\n\\textbf{Features}\\n\\t \\begin{tabular}{lll}\\n\\textit{Description} & \\textit{Type} & \\textit{Value} \\\\\\n\\hline\\n\\hline\\n\\endhead\\nnew & \\#define & \\#define\\\\\\nstd::array & std::array & std::array\\\\ \\nstd::vector & std::vector & std::vector\\\\ \\nstd::string & std::string & std::string\\\\ \\nstd::unordered\\_set & std::unordered\\_set & std::unordered\\_set\\\\ \\nstd::unordered\\_map & std::unordered\\_map & std::unordered\\_map\\\\ \\nstd::pair& std::pair& std::pair\\\\ \\nstd::tuple& std::tuple& std::tuple\\\\ \\nstd::__cxx11\\_string& string& string\\\\ \\n\\end{tabular}\\nThere are also several other extensions added over time:\\n\\t\\t\\t\\t\\t\\underline{\\textbf{\\textit{new}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: \\\\\\nvoid* operator new(void*); \\\\\\nvoid* operator delete(void*); \\\\\\nvoid operator delete[](void*); \\\\\\nvoid operator delete[](int); \\\\\\nvoid operator delete[](size_t); \\\\\\nvoid operator delete[](int32_t); \\\\\\nvoid operator delete[](size32_t); \\\\\\n\\n\\end{tabular}\\\\\\n\\n\\underline{\\textbf{\\textit{std}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: \\\\\\nchar* operator new(char[]); \\\\\\nchar* operator new(char[10]); \\\\\\nchar* operator new(char[20]); \\\\\\nchar* operator new(char[30]); \\\\\\n\\n\\underline{\\textbf{\\textit{size}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: \\\\\\nsize_t size(); \\\\\\n\\n\\underline{\\textbf{\\textit{const}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: const char* const; const char* const; const char* const; const char* const; const char* const; const char* const; \\\\\\n\\n\\underline{\\textbf{\\textit{$<$}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: int lower_bound(int index, int value); int lower_bound(int index, int value) noexcept; int lower_bound(int index, int value) noexcept(=index<value) noexcept; int lower_bound(int index, int value) noexcept(=index>value) noexcept(=index<value) noexcept(); void lower_bound(int index, int value) noexcept(); void lower_bound(int index, int value)(int value!=0)?true:(false); void lower_bound(int index, int value)(int value!=0)?false:(true); void lower_bound(int index, int value)(int value<=0)?true:(false);\\n\\underline{\\textbf{\\textit{$>$}}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: bool less(const char*)const; bool less(const char*)const(=index<value) noexcept(=index>value) noexcept(); bool less(const char*)const(=index<value) noexcept(=index>value)(=index<value)?true:(false);\\n\\underline{\\textbf{\\textit{$<$=}:\\newline$>$}:}} \\\\\\n\\n\\begin{tabular}{lll}\\nnew: bool greater(const char*)const; bool greater(const char*)const(=index<value) noexcept(=index>value) noexcept(); bool greater(const char*)const(=index<value?1 : 0)?true:(false);\\n\\underline{\\textbf{\\textit{$<$=}:\\newline$>$}:}}\\n\\end{tabular}\\\\\\n\\n\\n\\underline{\\underline{$<$=}:\\newline$>$}:} \\\\\\n\\n\\nnew: bool less(std::__cxx11\\_string&) const;\\nbool less(std::__cxx11\\_string&) const();\\nbool less(std::__cxx11\\_string&) const();\\nbool less(std::__cxx11\\_string&) const();\\nbool less(std::__cxx11\\_string&) constant();\\nbool less(std::__cxx11\\_string&) constant();\\nbool less(std::__cxx11\\_string&) constant();\\nbool less(std::__cxx11\\_string&) constant();\\nbool less(std::__cxx11\\_string&) constant();\\n\\n\\underline{$<$=}:\\newline$>$}:} \\\\\\n\\n\\nnew: bool greater(std::__cxx11\\_string&) const;\\nbool greater(std::__cxx11\\_string&) constant();\\nbool greater(std::__cxx11\\_string&) constant();\\nbool greater(std::__cxx11\\_string&) constant();\\nbool greater(std::__cxx11\\_string&) constant();\\n\\n\\underline{$<$=}:\\newline$>$}:} \\\\\\n\\n\\nnew: size_t size() { return m_size_; }\\nsize_t size() { return m_size_; }\\nsize_t size() { return m_size_; }\\n\\n\\underline{$<$=}:\\newline$>$}:} \\\\\\n\\n\\n\\underline{*operator new*:}\\\\\\n\\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_; }\\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_; } \\n\\n(new): void *operator new(void *) { return m_new_}; \\n\\n(new), (delete), (delete[]), (delete[], ...) {} \\n\\n(old), (delete), (delete[], ...) {} \\n\\n(old), (delete), (delete[], ...) {} \\n\\n(old), (delete), (delete[], ...) {} \\n\\n(old), (delete), (delete[], ...) {} \\n\\n(old), (delete), (delete[], ...) {} \\n\\n\\n[new], (static_cast ), static_cast , static_cast , static_cast , static_cast , static_cast , static_cast , static_cast , static_cast \\n\\n\\n(incompatible types or arguments)\\n\\n(incompatible types or arguments)\\n\\n(incompatible types or arguments)\\n\\n(incompatible types or arguments)\\n\\n(incompatible types or arguments)\\n\\n\\n(uninitialized)\\n```"}
{"File Name":"nucleus\/0002-push-data-changes-to-github.md","Context":"## Context\\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\\n","Decision":"We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\\n","tokens":149,"id":93,"Predictions":"## Decision\\nThe decision was to push changes to GitHub as soon as they're made. This will happen very quickly after the change is saved, and the Nucleus user who made the change can also be recorded in the Git commit.\\n\\n\\n## Outcome\\nThe outcome is that a record of the decision has been made and this record can be used in future decisions. The decision was made by a human, so it's a good example of how decisions are made.\\n\\n\\n## Processed by\\n## Processed by\\nNucleus has a process for making decisions about releases and releases notes.\\n\\n\\n"}
{"File Name":"nucleus\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":94,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"CopperBend\/D002_unify_command_flow_for_player_and_creatures.md","Context":"## Context\\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\\nGiving creatures and the player character pluggable command\/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\\nThis offers options for reuse.\\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\\n","Decision":"I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\\n","tokens":191,"id":95,"Predictions":"## Decision\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is a decision about which of the two is more appropriate.  This is a decision about which of the two is more appropriate.\\n## Context\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is a decision about which of the two is more appropriate.  This is a decision about which of the two is more appropriate.\\n## Decision\\n## Decision\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is a decision about which of the two is more appropriate.  This is a decision about which of the two is more appropriate.\\n## Context\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is a decision about which of the two is more appropriate.  This is a decision about which of the two is more appropriate.\\n## Decision\\n## Decision\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is a decision about which of the two is more appropriate.  This is a decision about which of the two is more appropriate.\\n## Context\\nThe player character can be controlled by the player character's InputCommandSource, or by a different InputCommandSource.  This is an action to take when it comes to controlling other creatures.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\nThis action grants access to all creature commands.\\n\\n## Context\\nThe 'core mechanic' of this game is that there are several creatures that can be controlled at once (or at least that they have some unique behavior). The core mechanics are: The Player controls one entity at any given time; there may be other entities under control as well (e.g., creatures and\/or buildings); and there are some special moments in time where one entity gets special powers and special abilities (e.g., when escaping from danger). The Player may choose whether they want control over their own entity or another entity's entity (e.g., they may choose whether they want control over their own entity or another entity's building). There are also moments where one entity gets special powers and abilities when they're in charge (e.g., when escaping from danger). The Player may choose whether they want control over their own entity or another entity's building.\\n```\\nI have this code: https:\/\/pastebin.com\/9h7B5A6U\\n```\\nI am using Python3 for this project if that helps any.\\nThanks!\\n\\nchrispyChuck 2018-06-26: I've come up with some ideas on how you could improve your function:\\n```\\ndef show_entities(self):\\n    self.entities = []\\n    self.entities.append(self.player)\\n    self.entities.append(self.buildings)\\n    self.entities.append(self.creatures)\\n\\n    self.entity_list = [self.player]\\n    self.entity_list.extend([self.buildings[i] for i in range(len(self.entities)) if i != i+1])\\n\\n    self.set_entities(self.entity_list)\\n\\ndef set_entities(self, entities):\\n    for i in range(len(entities)):\\n        if len(entities[i]) > len(entities[i+1]):\\n            entities[i].setEntity(entities[i])\\n            entities[i].setEntity(entities[i+1])\\n        elif len(entities[i]) == len(entities[i+1]):\\n            entities[i].setEntity()\\n            entities[i].setEntity()\\n        else:\\n            break\\n\\ndef get_entities(self):\\n    return [entity for entity in self.entities if entity != None]\\n\\ndef get_buildings(self):\\n    return [building for building in self.buildings if building != None]\\n```\\n"}
{"File Name":"CopperBend\/D001_keep_ADRs_in_git_and_freely_reformat.md","Context":"## Context\\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\\nCopper Bend is a learning\/entertainment project.  Trying new techniques and technologies is a primary goal.\\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\\n","Decision":"I will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\\nI won't worry about synchronizing the format between ADR docs.\\n","tokens":125,"id":96,"Predictions":"\\nThe first thing to do is to define the problem.  The problem is that a lot of the Envisage products are code, and code is hard to read.\\nCode doesn't have a human-readable format, and it's not obvious how to extend it.  The result is that it's hard to understand what's going on.\\nThis problem can be addressed by making code more human-readable.\\nThe second thing to do is to decide what kind of software we want this ADR to be.  We want something that feels like a game, and that's very different from Envisage.\\nThe third thing to do is to decide whether we want the ADRs for the entire product suite or just for a single product.\\nThe fourth thing to do is how we're going to get this done.  We need a team of people who can work on this project.\\nThe fifth thing to do is how we're going to track progress on this project, and who will be responsible for each step.\\nThe sixth thing to do is how we're going to measure progress.\\nThe seventh thing to do is what kind of product we want the ADRs for.\\The eighth thing is how much time will it take?\\nThe ninth thing is, where are all these people working?\\n\\n\\n\\n"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"Predictions":"## Decision\\nThis is a decision for the WebApp to use the simulator as a separate component. We don't want to use the simulator as a web application because we want to have a separated ecosystem.\\n## Decision\\nThis is a decision for the WebApp to use the simulator as a separate component. We don't want to use the simulator as a web application because we want to have a separated ecosystem.\\n"}
{"File Name":"react\/adr-XXX-file-structure.md","Context":"## Context\\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\\n","Decision":"TL;DR:\\n```\\nprimer-react\/\\n\u251c\u2500 src\/\\n\u2502  \u251c\u2500 Breadcrumbs\/\\n\u2502  \u2502  \u251c\u2500 index.ts                    \/\/ Just re-exporting?\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.tsx             \/\/ Primary component\\n\u2502  \u2502  \u251c\u2500 BreadcrumbsItem.tsx         \/\/ Subcomponent (include parent component name to increase findability in most IDEs)\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.mdx             \/\/ Documentation. Always .mdx, not .md\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.stories.tsx\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.test.tsx        \/\/ Unit tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.types.test.tsx  \/\/ Type tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.yml             \/\/ Component metadata (Possible future)\\n\u2502  \u2502  \u2514\u2500 __snapshots__\/\\n\u2506  \u2506\\n```\\n### Rules\\n- Every component should have its own PascalCased directory directly under `src\/`\\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\\n","tokens":22,"id":98,"Predictions":"## Decision: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n\\n## Decision: \\n\\n"}
{"File Name":"react\/adr-003-prop-norms.md","Context":"## Context\\nOur component prop APIs have, at times been a bit of a mess. We've seen:\\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\\n","Decision":"### \ud83d\udfe2 `sx`\\nAll components that ultimately render to the DOM should accept an `sx` prop.\\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\\n### \ud83d\udfe2 `ref`\\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\\nSee also: [Discussion on `ref` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/131)\\n### \ud83d\udfe1 `as`\\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component\u2019s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\\nSee also: [Discussion on `as` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/130)\\n### \ud83d\udfe1 DOM props: Limited\\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component\u2019s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\\n### \ud83d\udd34 Styled System props\\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\\nSee also: [Discussion on the deprecation of styled-system props (internal)](https:\/\/github.com\/github\/primer\/discussions\/132)\\n### \ud83d\udd34 `theme`\\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\\n### `children`\\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\\n### Sequencing\\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\\n1. Release an eslint rule to disallow Styled System props\\n1. Release an eslint rule to disallow `theme`\\n1. Migrate all usage within PRC\\n1. Assist GitHub projects with migration\\n1. Remove support for unwanted props\\n1. Update docs to reflect the standards in this ADR\\n","tokens":120,"id":100,"Predictions":"## The Component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The Component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The Component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The Component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The component API is a set of APIs that describe how to create, access, and modify components.\\n\\n## The component API is a set of APIs that describe how to create, access, and modify components.\\n"}
{"File Name":"react\/adr-001-typescript.md","Context":"## Context\\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https:\/\/github.com\/primer\/react\/commit\/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https:\/\/github.com\/primer\/react\/issues\/906) [TypeScript](https:\/\/github.com\/primer\/react\/issues\/540) [bug](https:\/\/github.com\/primer\/react\/issues\/520) [reports](https:\/\/github.com\/primer\/react\/issues\/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\\n","Decision":"We will rewrite Primer React components in TypeScript.\\n","tokens":284,"id":101,"Predictions":"\\nThe decision is to update the TypeScript type definitions in `index.d.ts` to match the React implementation.\\n\\n\\# TODO: Add a link to the index.d.ts file in the `README.md` file. Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\# TODO: Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\# TODO: Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\# TODO: Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\# TODO: Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\# TODO: Add a link to the react implementation in `index.d.ts` in the `README.md` file.\\n\\n\\n\\n"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"Predictions":"## I choose to place the locale-specific analyzers in a separate index.\\nThis is because different locales have different language-specific analyzers,\\nwhich are good for different purposes.\\nFor example, English analyzer for document \\emph{Hello world} would be a good analyzer for document \\emph{Hallo Welt},\\nand German analyzer for document \\emph{Hallo Welt} would be a good analyzer for document \\emph{Hallo Welt}.\\n\\n## Context\\nWe want to search in Elasticsearch\\nElasticsearch supports many languages,\\nand has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\ndoc_3: { \"content\": \"Hallo Welt\" }\\ndoc_4: { \"content\": \"Hallo Welt\" }\\ndoc_5: { \"content\": \"Hallo Welt\" }\\ndoc_6: { \"content\": \"Hallo Welt\" }'\\end{verbatim}\\endinput\\n"}
{"File Name":"kitsune\/0003-es-aaq-documents.md","Context":"## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nwe must re-implement Ask a Question (AAQ) search.\\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nwhich we must continue to be able to do in the redesigned Search:\\nsearching for an AAQ thread as a unit.\\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\\nA non-exhaustive list of these are:\\n-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nso should model our data to avoid nested fields and parent-child relationships,\\nand use de-normalization wherever possible:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n","Decision":"We will model our data in ES based on what makes most sense for our expected use-cases,\\nand what will make those fast and efficient,\\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\\nIn this vein, we will use a structure of two document \"types\" within one index,\\n`QuestionDocument` and `AnswerDocument`,\\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\\nFor instance, if in database:\\n```\\nanswer.created => 2020-10-27\\nanswer.question.created => 2020-11-01\\n```\\nin elastic:\\n```\\nanswer_document.created => 2020-10-27\\nanswer_document.question_created => 2020-11-01\\n```\\n`QuestionDocument` will also have an `answer_content` field,\\nwhich contains the content of all a Question's Answers.\\nWe will set this to null in the `AnswerDocument`.\\n","tokens":205,"id":103,"Predictions":"## Decision\\nThe following decision is based on the following information:\\n1. We are re-implementing Ask a Question search in ElasticSearch v7,\\nso we must continue to be able to do this in the redesigned Search.\\n2. We want to store AAQ documents in ES, but not use nested fields or parent-child relationships.\\n3. We want search to be fast.\\n4. We want search to be _simple_.\\n5. We want our data model to avoid nested fields and parent-child relationships.\\n6. We want search to be _fast_.\\nWe have decided that the following decision is correct:\\n7. Our data model should avoid nested fields and parent-child relationships:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n\\n\\n\\n## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nwe must re-implement Ask a Question (AAQ) search.\\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nwhich we must continue to be able to do in the redesigned Search:\\nsearching for an AAQ thread as a unit.\\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES:\\n-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nso should model our data to avoid nested fields and parent-child relationships,\\nd use de-normalization wherever possible:\\nlhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n\\n## Decision\\n## Decision\\nThe following decision is based on the following information:\\nl1. We are re-implementing Ask a Question search in ElasticSearch v7,\\nlso we must continue to be able to do this in the redesigned Search.\\nl2. We want our data model avoid nested fields and parent-child relationships,\\nlbut still support searching within threads and questions and their solutions.\\nl3. We want our data model _simple_, so it can store lots of data without slowing down searches,\\nland so it can store lots of answers without slowing down searches as well.\\nl4. The above two decisions are correct!\\nl\\n## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nlwe must re-implement Ask a Question (AAQ) search\\nto support searching within threads and questions and their solutions.\\nlThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nto support searching within threads and questions and their solutions.\\nlThere are other secondary use-cases which we may want to support when storing AAQ documents in ES:\\nl-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nto should model our data so that it can store lots of answers without slowing down searches, but still be fast enough for queries that need them.\\nlWe have decided that the following decision is correct:\\nn1\\. Our data model should avoid nested fields and parent-child relationships:\\nnhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n\\n## Decision\\n## Decision\\nThe following decision is based on the following information:n1\\. Our data model should avoid nesting of field names because it slows down searches, but still allow us access them via field names.n2\\. Our data model should allow us access these field names via field names.n3\\. Our data model should allow us access these field values via field values.n4\\. Our data model should allow us access these field values via field values.n5\\. Our data model should allow us access these field values via field values.n6\\. Our data model should avoid nesting of field names because it slows down searches, but still allow us access them via field names)\\nnhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.x\/tune-for-search-speed.html#_document_modeling\\n\\n## Context\\n## Decision\\n## Decision\\nThe following decision is based on the following information:n1\\. The above two decisions are correct!\\nn2\\. The above two decisions are correct!\\nn3\\. The above two decisions are correct!\\nn4\\. The above two decisions are correct!\\nn5\\. The above two decisions are correct!\\nn6\\. The above two decisions are correct!\\nn7\\. The above two decisions are correct!\\n"}
{"File Name":"kitsune\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":104,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"Nosedive\/0006-use-of-migratus-library.md","Context":"## Context\\nDirect form the [jdbc documentation](http:\/\/clojure-doc.org\/articles\/ecosystem\/java_jdbc\/home.html)\\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\\n* Drift\\n* Migratus\\n* Ragtime\\n","Decision":"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\\n","tokens":60,"id":105,"Predictions":"## Decision\\nA developer has created a library that supports database migration. \\nThe library is used by a large number of developers of various skill levels.\\nThe library is used by a large number of developers of various skill levels.\\n\\n\\n## Decision\\n## Decision\\nThe developer has been asked to create a migration utility. \\nThe utility will be used by a large number of developers.\\nThe utility will be used by a large number of developers.\\n\\n## Decision\\n## Decision\\nA developer has created a library that supports database migration. \\nThe library is used by a large number of developers. \\nThe library is used by a large number.\\n\\n## Decision\\n## Decision\\nA developer has created a library that supports database migration. \\nThe library is used by a large number of developers and also provides an ORM to the database. \\nThe ORM provides many features such as auto incrementing primary keys, auto incrementing foreign keys, and dynamic SQL generation.\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"Nosedive\/0004-start-with-console-app.md","Context":"## Context\\nTe console appication it is the more siple app that come to my mind.\\n","Decision":"Creates a console app and send the data via parameters\\n","tokens":21,"id":106,"Predictions":"## Decision: The application is a console appication.\\n\\n## Decision: The application is a GUI application.\\n\\n## Decision: The application is a web application.\\n\\n## Decision: The application is a mobile appication.\\n## Decision: The application is an email client.\\n## Decision: The application is a file manager.\\n## Decision: The application is a text editor.\\n## Decision: The application is an FTP client.\\n## Decision: The application is an SSH client.\\n## Decision: The application is a database client.\\n## Decision: The application is an email server.\\n\\n"}
{"File Name":"Nosedive\/0009-use-mount-and-clojure-tools-namespace.md","Context":"## Context\\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\\n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\\n","Decision":"Use mount libray and clojure tools space.\\nThe decision of mount over component is made afer review bouth solutions. My feeling is\\n* Mount is more clojure dialect oriented\\n* Mount use namespace and component\\nRecords, this made that the compliler control the dependencies\\n* Mount is [less contagious](https:\/\/engineering.riotgames.com\/news\/taxonomy-tech-debt)\\n","tokens":100,"id":107,"Predictions":"## Decision\\n\\n  Clojure is a simple language, but it has powerful features.\\n\\n  Mount is a library that provides a way to share state between different Clojure applications.\\n\\n  Mount can be used to preserve the state of a Clojure application while reloaded.\\n\\n  Mount can be used to share state between Clojure applications that are not currently running.\\n\\n  Mount can be used to save and restore the state of a Clojure application while it is running.\\n\\n  Mount can be used to save and restore the state of an application in its current state (i.e. any previous versions of the application).\\n\\n  Mount can be used to save and restore the state of an application in its current state (i.e. any previous versions of the application).\\n\\n## Context\\n## Context\\nThis software is intended for use by developers who are working on projects with Clojure. It provides a way for developers to share their work between different Clojure applications. Developers can preserve their work by loading their project from disk, or by loading it from another server.\\nIn order for this software to work, we need you to have access to your own projects (that you have created) on your own server. This software will not work if you do not have access to your own projects on your own server, or if you do not have access to your own projects on any other server.\\nWhen you use this software, we will store your project on our servers. We will also store copies of your project on our servers in order for us to provide you with access when you need it. Your project will not be available through our servers if you do not have access.\\n"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"Predictions":"## You can deploy a Clojure application in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc. \\n\\n\\n## Decision\\n## You can deploy a Clojure application in cloud (with autoscale, resilience, geographic distribution, multyples clouds providers) with the clojure-cloud package.\\n\\n\\n## Decision\\n## You can deploy a Clojure application in cloud (with autoscale, resilience, geographic distribution, multyples clouds providers) with the clojure-cloud package.\\n\\n\\n## Decision\\n## You can deploy a Clojure application in cloud (with autoscale, resilience, geographic distribution) with the clojure-cloud package.\\n\\n\\n## Decision\\n## You can deploy a Clojure application in cloud (with autoscale) with the clojure-cloud package.\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nTo view this information, you must accept cookies."}
{"File Name":"Nosedive\/0002-use-of-clojure.md","Context":"## Context\\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\\n","Decision":"This will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\\n","tokens":37,"id":110,"Predictions":"## Decision: Create a database for the software. Create a table with column name \"id\" and integer value 1.\\n## Decision: Create an API for the software. Create a table with column name \"id\" and integer value 1.\\n## Decision: Secure the software. Create a table with column name \"id\" and integer value 1.\\n## Decision: Create an API for the software. Create a table with column name \"id\" and integer value 1.\\n\\n## Context\\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\\n\\n## Decision\\n## Decision: Create a database for the software. Create a table with column name \"id\" and integer value 1.\\n## Decision: Create an API for the software. Create a table with column name \"id\" and integer value 1.\\n## Decision: Secure the software. Create a table with column name \"id\" and integer value 1.\\n\\nThe result of this query is:\\n\\nSELECT id FROM db WHERE id = '5'\\n\\nThis query results in:\\n\\nSELECT id FROM db WHERE id = '5'\\n+----+--------+\\n| id |   name   |\\n+----+--------+\\n| 5 |   developer |\\n+----+--------+\\n\\nThe result of this query is:\\n\\nSELECT id FROM db WHERE id = '5'\\n-       +----+\\n| id |   name   |\\n-       +----+\\n| 4 | Developer |\\n|    +----+\\n|    +----+\\n|    +----+\\n\\nHere is an example of how to use it:\\n\\n\/\/ Query to get all users by their role\\n$roles = $user->roles;\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles();\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->where('role', 'ROLE_DEVELOPER');\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->where('role', 'ROLE_DEVELOPER');\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->where('role', 'ROLE_DEVELOPER')->count();\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->where('role', 'ROLE_DEVELOPER')->count();\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->where('role', 'ROLE_DEVELOPER')->first();\\n\\n\/\/ Query to get all users by their roles\\n$roles = $user->roles()->first();\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE role IN (3,4) ORDER BY role ASC\\n\\nThis query results in:\\n\\nSELECT * FROM db WHERE role IN (3) ORDER BY role ASC\\n\\nYou can also use where() function.\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE role IN (3) ORDER BY role ASC\\n\\nThis query results in:\\n\\nSELECT * FROM db WHERE role IN (3) DESC\\n\\nYou can also use where() function.\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE role IN (3) DESC\\n\\nYou can also use where() function.\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE role IS NULL OR role <> ''\\n\\nThis query results in:\\n\\nSELECT * FROM db WHERE role IS NULL OR role <> ''\\n\\nYou can also use where() function.\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE user_id IS NULL OR user_id IS NOT NULL AND username LIKE '%'\\n\\nThis query results in:\\n\\nSELECT * FROM db WHERE user_id IS NULL OR user_id IS NOT NULL AND username LIKE '%'\\n\\nYou can also use where() function.\\n\\nThe output of this query is:\\n\\nSELECT * FROM db WHERE email LIKE '%'\\n\\nThis query results in:\\n\\nSELECT * FROM db WHERE email LIKE '%'\\n\\nYou can also use where() function.\\n\\nThe output of this query is\\n\\nI hope you got what you wanted from here :)\\n\\nFor further details you can check out our docs on how to make queries here"}
{"File Name":"Nosedive\/0007-uso-of-tools-cli.md","Context":"## Context\\nWe need working with command line arguments.\\n","Decision":"Use of [tools.cli](https:\/\/github.com\/clojure\/tools.cli)\\n","tokens":12,"id":111,"Predictions":"## The system has decided to provide a command line interface for the ## user.\\n## Context\\n\\nDetails\\n\\nUser can enter command line arguments for the application.\\n\\nFor example, the command line argument \"test\" is used to test the application.\\n\\nIf you want to use this command line argument, you can enter it as follows:\\n\\nNote that when you enter a command line argument, you must provide a string that is not empty.\\n\\nYou can also use <Enter> and >Enter> keys to enter command line arguments.\\n\\nExample 1: Enter an argument as follows:\\n\\ntest\\n\\nExample 2: Enter an argument as follows:\\n\\n-3\\n\\nExample 3: Enter an argument as follows:\\n\\n-3.0\\n\\nExample 4: Enter an argument as follows:\\n\\n-3.0\\n\\nExample 5: Enter an argument as follows:\\n\\n-3.0\\n\\nRelated topics\\n\\n- Command Line Interface\\n- Command Line Argument\\n- Command Line Argument List\\n- Command Line Argument List Example\\n- Command Line Argument List Example - Python\\n- Command Line Argument List Example - Ruby\\n- Command Line Argument List Example - Java\\n- Command Line Argument List Example - C++\\n- Command Line Argument Examples"}
{"File Name":"Nosedive\/0008-migrating-by-code.md","Context":"## Context\\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\\n","Decision":"Develop the migration part in the code\\n","tokens":31,"id":112,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe decision is: \"Do not deploy to different environments per environment\"\\n\\nThis means that the deployment will not be performed per environment.\\n\\nThe decision is: \"Do not deploy to different environments per database\"\\n\\nThis means that the deployment will not be performed per database.\\n\\nThe decision is: \"Do not deploy to different environments per application\"\\n\\nThis means that the deployment will not be performed per application.\\n\\nThe decision is: \"Do not deploy to different environments per environment and application\"\\n\\nThis means that the deployment will be performed for all environments and applications.\\n\\nThe decision is: \"Deploy to all environments and all applications\"\\n\\nThis means that the deployment will be performed for all environments and applications.\\n\\nThe decision is: \"Deploy to all environments, but only if there are no problems with deploying\"\\n\\nThis means that the deployment will be performed for all environments, but only if there are no problems with deploying. The deployment process will stop if there are any problems with deploying after this time. This way we can avoid deploying in the middle of a production environment when we have a problem with it.\\n\\nThe decision is: \"Deploy to all environments, but only if there are no problems with deploying\" (only if there are no problems with deploying)\\n\\nThis way we can avoid deploying in the middle of a production environment when we have a problem with it. This way we can avoid deploying in production, even if there are problems with it.\\n\\nThe decision is: \"Deploy to all databases, but only if there are no problems with deploying\" (only if there are no problems with deploying)\\n\\nThis way we can avoid deploying in production, even if there are problems with it. This way we can avoid deploying in production, even if there are problems with it.\\n\\nThe decision is: \"Deploy to all applications, but only if there are no problems with deploying\" (only if there are no problems with deploying)\\n\\nThis way we can avoid deploying in production, even if there are problems with it. This way we can avoid deploying in production, even if there are problems with it.\\n\\nThe decision is: \"Deploy to all databases, but only if there are no problems with deploying\" (only if there are no problems for deployments)\\n\\nThis way we can avoid applying deployments on multiple databases at once. This way we can apply deployments on one database at once. This way we can apply deployments on one database at once while avoiding applying deployments on another database at once.\\n"}
{"File Name":"Nosedive\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":113,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"Predictions":"## - The decision is to use vim-plug.\\\\\\n## - The decision is to use vim-plug for the next release.\\\\\\n## - The decision is to use vim-plug as a temporary solution until the next release\\nis ready.\\\\\\n## - The decision is to not use vim-plug as a temporary solution until the next\\nrelease is ready.\\\\\\n## - The decision is that the current version of Vim will be used for this project.\\n\\n\\nThe above decisions are based on the following information:\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n - Vundle has been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\\n\\n \\n\\n\\n"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"Predictions":"## Decision: [1] - Install macOS Catalina; [2] - Install macOS Mojave; [3] - Install macOS High Sierra; [4] - Install macOS Sierra; [5] - Install macOS El Capitan; [6] - Install Windows 7 and later; [7] - Install Windows 10 (v1511 or later); [8] - Install Windows 8.1 (v1439 or later); [] - Do nothing.\\n## Decision: []\\n\\n\\n\\n\\n\\n\\n## Decision: \\n[1]: \"Install macOS Catalina\"\\n## Decision: \\n[2]: \"Install macOS Mojave\"\\n## Decision: \\n[3]: \"Install macOS High Sierra\"\\n## Decision: \\n[4]: \"Install macOS Sierra\"\\n## Decision: \\n[5]: \"Install macOS El Capitan\"\\n## Decision: \\n[6]: \"Install Windows 7 and later\"\\n## Decision: \\n[7]: \"Install Windows 10 (v1511 or later)\"\\n## Decision: \\n[8]: \"Install Windows 8.1 (v1439 or later)\"\\n\\end{lstlisting}\\n\\end{document}\\n"}
{"File Name":"dotfiles\/0005-use-coc-in-place-of-ycm.md","Context":"## Context\\nYCM has been a mostly positive experience over the years of using it, however,\\nthere have been many occasions where time has been spent fixing issues. The\\ntime has come to look at using an alternative.\\n","Decision":"The decision is to use [CoC](https:\/\/github.com\/neoclide\/coc.nvim).\\n","tokens":48,"id":116,"Predictions":"The decision is to use a new product called JIRA.\\n\\n## Comments\\n\\n\\begin{itemize}\\n\\item The decision is made based on the following factors:\\n\\n\\begin{itemize}\\n\\item \\# of customers using it (\\# of customers in the organisation)\\n\\item \\# of customers complaining about it (\\# of complaints in the organisation)\\n\\item \\# of issues raised about it (\\# of issues raised in the organisation)\\n\\end{itemize}\\n\\end{itemize}\\n\\n\\noindent The decision is made based on the following criteria:\\n\\n\\begin{enumerate}\\n\\item JIRA has been around for a long time and has a good reputation.\\n\\n\\item JIRA has a good user interface.\\n\\n\\item JIRA has an extensive range of plugins available.\\n\\n\\end{enumerate}\\n\\noindent The decision is made based on the following factors:\\n\\n\\begin{enumerate}\\n\\item \\# of customers using it (\\# of customers in the organisation)\\n\\end{enumerate}\\n\\noindent The decision is made based on the following criteria:\\n\\n\\begin{enumerate}\\n\\item JIRA has been around for a long time and has a good reputation.\\n\\n\t\t%\\vspace{-0.5cm}\\noindent{\\bf (2)} The decision is made based on the following factors:\\vspace{-0.5cm}%\\n\t\t%\\begin{enumerate}\\vspace{-0.5cm}\\noindent{\\bf (2)} The decision is made based on the following factors:\\vspace{-0.5cm}%\\n\t\t%\\end{enumerate}%\\n\t\t%\\vspace{-0.5cm}%\\n\\n\\n\\vspace{-1mm}\\n\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\vspace{-1mm}\\n\\n% \\subsection*{\\label{ssec:JiraDecisionRule}Jira Decision Rule}\\n\\n\\vspace{-1mm}\\n\\n\\vspace{-1mm}\\n\\n\\noindent {\\bf Decision Rule}: \\nThe decision to use JIRA was taken because it had been around for a long time and had a good reputation.\\nThe decision was made based on the following factors:\\n\\vfill\\n\\n\\noindent {\\bf (1)} Customers using it (\\# of customers in the organisation): This was considered important because if there were many users then there would be more likelihood that they could find user feedback or issues, which could be useful for improving JIRA. This factor was also considered relevant because if there were many users then they would be more likely to have an interest in improving JIRA.\\n\\vfill\\n\\n\\noindent {\\bf (2)} Complaints about it (\\# of complaints in the organisation): This factor was also considered relevant because if there were many complaints about JIRA then this could indicate that there were problems with it, which could lead to improvements. This factor was also considered relevant because if there were many complaints about JIRA then this could indicate that there were problems with it, which could lead to improvements.\\n\\vfill\\n\\n\\noindent {\\bf (3)} Issues raised about it (\\# of issues raised in the organisation): This factor was also considered relevant because if there were many issues raised about JIRA then this could indicate that there were problems with it, which could lead to improvements.\\n\\vfill\\n\\n\\noindent {\\bf (4)} We have had some experience with alternative products that are similar to JIRA, such as Trello.\\vfill\\n\\n\\noindent {\\bf (5)} We have an extensive range of plugins available.\\vfill\\n\\n\\n\\n\\vspace{-1mm}\\n\\nThe maximum score for this rule is 6.\\n"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"Predictions":"\\nThe decision is to use GitHub Actions in this repository. Gitlab CI is already set up in the repository and has been used for many smaller projects. The new GitHub Actions will provide a deployment pipeline that will be more robust and maintainable than using Gitlab CI. This includes the ability for the user to create a release from the command line, and deploy to Kubernetes or other cloud environments.\\n\\nThe decision is made based on the following criteria:\\n\\n* \\# of releases per month\\nd\\n* \\# of deployments per month\\nd\\n* \\# of users\\nd\\n* \\# of commits\\nd\\n* \\# of pull requests\\nd\\n* \\# of issues\\n\\n"}
{"File Name":"dpul\/0003-synchronization-via-rabbitmq.md","Context":"## Context\\nWe want Pomegranate to be a separate application from Figgy, but need some way\\nfor Figgy to tell Pomegranate about new resources so that when something is\\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\\n","Decision":"Figgy will send create\/update\/delete messages to a fanout RabbitMQ Exchange.\\nPomegranate will register a durable queue which listens to that exchange and\\nprocess messages using [Sneakers](https:\/\/github.com\/jondot\/sneakers).\\nThe message will contain the following information:\\n* Collection slugs the object is a member of\\n* Manifest URL of the object\\n* change event (create \/ update \/ delete)\\n","tokens":60,"id":119,"Predictions":"## 1. Figgy should not be able to mark a resource as Complete in Pomegranate.\\n## 2. Figgy should be able to mark a resource as Complete in Pomegranate.\\n## 3. Figgy should be able to mark a resource as Partially Complete in Pomegranate.\\n## 4. Figgy should be able to mark a resource as Partially Complete in Pomegranate.\\n## 5. Figgy should be able to mark a resource as Partially Complete in Pomegranate.\\n## 6. Figgy should be able to mark a resource as Partially Complete in Pomegranate.\\n## 7. Figgy should be able to mark a resource as Partially Complete in Pomegranate.\\n## 8. Figgy should not be able to mark a resource as Partially Complete in Pomegranate.\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Decision Record: Resource Marking}\\n\\begin{verbatim}\\nDecision Record: Resource Marking\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n# The context for this decision is \"Figgy\" and the user is \"user\".\\n# The decision is \"Mark complete\". # The decision is \"Mark partial\". # The decision is \"Mark incomplete\". # The decision is \"Mark partially complete\". # The decision is \"Mark partially incomplete\". # The decision is \"Mark partially complete\". # The decision is \"Mark partially incomplete\".\\n\\n\\nWe want Pomegranate to be a separate application from Figgy, but need some way\\nfor Figgy to tell Pomegranate about new resources so that when something is\\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\\n\\n\\nWe want Pomegranate to be able to tell Figgy about new resources, but we don't want it\\nto know about new resources that are marked Complete in Figure.\\n\\nWe also don't want it to know about new resources that are marked PartiallyComplete or PartiallyIncomplete.\\n\\nWe also don't want it to know about new resources that are marked Incomplete.\\n\\nWe also don't want it to know about new resources that are marked PartiallyComplete.\\n\"\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Decision Record: New Resources}\\n\\begin{verbatim}\\nDecision Record: New Resources\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n# The context for this decision is \"Figgy\" and the user is \"user\".\\n# The decision is \"Add new resource\". # The decision is \"Add existing resource\". # The decision is \"Add no new resources\".\\n\\nWe want Pomegranate to add new resources, but we don't want it \\emph{necessarily} know about them yet.\\n\"\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Decision Record: Result of New Resources}\\n\\begin{verbatim}\\nDecision Record: Result of New Resources\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n# This record indicates whether or not there were any changes made by \\emph{name} after adding new resources.\\n# This record indicates whether or not there were any changes made by \\emph{name} after adding existing resources.\\n# This record indicates whether or not there were any changes made by \\emph{name} after adding no new resources, if any existed previously.\\n#\\n# For each of these records, this record contains one of the following values:\\n#\\n# - None - no changes were made after adding these items (i.e., they were added without being marked complete). In this case, this record will have \\n #   one of the following values:\\newline\\n#\\newline\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they were added without being marked complete).\\n%None - no changes were made after adding these items (i.e., they was added without being marked complete).\\n%None - no changes where there was previously existing data (\\emph{name} did not add any data before), but where this data was on file with \\emph{name}.\\n%None - no changes where there was previously existing data (\\emph{name} did not add any data before), but where this data was on file with \\emph{name}.\\n%None - no changes where there was previously existing data (\\emph{name} did not add any data before), but where this data was on file with \\emph{name}.\\n%\\n%\\n%\\n%\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\newline\\n\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%% NEW RESOURCES ARE MARKED COMPLETE WHEN THEY ARE COMPLETELY CREATED IN THE SYSTEM AND MARKED COMPLETE IN FIGGY AND THEIR DATA IS ON FILE WITH THE USER.\\n\\n%% NEW RESOURCES ARE MARKED COMPLETE WHEN THEY ARE COMPLETELY CREATED IN THE SYSTEM AND MARKED COMPLETE IN FIGGY AND THEIR DATA IS NOT ON FILE WITH THE USER.\\n\\n%% NEW RESOURCES ARE MARKED COMPLETE WHEN THEY ARE CREATED BY FIGGY BUT NOT MENTIONED IN FIGGY OR NOT MENTIONED IN THE SYSTEM.\\n\\n%% NEW RESOURCES ARE MARKED COMPLETE WHEN THEY ARE CREATED BY FIGGY AND MENTIONED IN FIGGY OR MENTIONED IN THE SYSTEM.\\n\\n%% NEW RESOURCES ARE MARKED COMPLETE WHEN THEY ARE MENTIONED BY FIGGY BUT NOT MENTIONED IN THE SYSTEM.\\n\\n%% NEW RESOURCES ARE MARKED COMPLET WHEN THEY WERE MENTIONED BY FIGGY OR WERE MENTIONED BY SOMEONE ELSE.\\n\\n%% NEW RESOURCES ARE MARKED COMPLET WHEN THEY WERE MENTIONED BY SOMEONE ELSE OR WERE MENTIONED BY FIGGY.\\n\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n%%\\n\\n\\n\\n"}
{"File Name":"dpul\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described at https:\/\/adr.github.io\/\\n","tokens":16,"id":120,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"dpul\/0002-build-on-spotlight.md","Context":"## Context\\nPUDL has [collection landing\\npages](http:\/\/pudl.princeton.edu\/collections\/pudl0058) because it's a digital collections site which\\nstaff pushed content into. PUDL required all items be in a single collection,\\nand couldn't provide good search-across. Figgy is a staff back-end, so we need some sort of\\napplication to provide that functionality.\\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\\nthem to create those experiences based on material they curate which wouldn't\\ncreate metadata silos and increased maintenance.\\nFurther, curators often had different use cases about how metadata should\\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\\nway, but be cataloged according to best practices.\\nSpotlight is an exhibit building platform that provides controlled CMS\\nfunctionality and is built on Blacklight similar to our new catalog. It allows\\nfor local overrides of fields.\\n","Decision":"We will use Spotlight to fulfill both the requirements of a Collection Landing\\nPage as well as exhibits. Curators of collections will generate the collection\\npages. Staff who wish to generate exhibits will be able to manage membership in\\nFiggy but have the tools to create those exhibits in Pomegranate.\\n","tokens":244,"id":121,"Predictions":"## \\n\\n\\nDecision: PUDL should provide a CMS for curators to create their own exhibit experiences.\\n\\n\\nDetermine the context for the decision. The context for this decision is that PUDL is a digital collections site which staff pushed content into. PUDL required all items be in a single collection, and couldn't provide good search across. Figgy is a staff back-end, so we need some sort of application to provide that functionality. Curators also had a history of either requesting or creating ad-hoc websites to showcase their material or accompany on-site exhibits. We wanted instead to provide a CMS for them to create those experiences based on material they curate which wouldn't create metadata silos and increased maintenance. Further, curators often had different use cases about how metadata should display in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain way, but be cataloged according to best practices.\\n## Decision\\n## \\n\\n\\nDecision: PUDL should provide a CMS for curators to create their own exhibit experiences.\\n"}
{"File Name":"dpul\/0007-indexing-visibility.md","Context":"## Context\\nFiggy resources may have any of the following visibilities:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\n- Reading Room\\n- Private\\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\\nWe used to index only public \/ complete items. But to support the music reserves\\ncollection we need pages that would have a viewer for logged-in institutional\\nusers only.\\n","Decision":"Resources with the following visibilities should index into DPUL:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\nThis is implemented with a token authentication mechanism in `iiif_resource#def\\nurl`\\n","tokens":98,"id":122,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n\\n\\end{document}\\n"}
{"File Name":"dpul\/0006-exhibits-are-collections.md","Context":"## Context\\nSpotlight uses the term \"Exhibits\" because its primary use case is generating\\nmultiple exhibit sites. However, as per\\n[ADR #2](.\/0002-build-on-spotlight.md) we want to use it as landing pages for\\ncollections.\\nThe term \"Collection\" was used everywhere else, including PUDL, so we decided to\\nuse that instead. Originally reported in\\n[#89](https:\/\/github.com\/pulibrary\/pomegranate\/issues\/89).\\n","Decision":"* Use \"Collection\" in the UI everywhere \"Exhibit\" is mentioned in Spotlight.\\n","tokens":104,"id":123,"Predictions":"## Decision\\n1. Use \"Exhibits\" for landing pages.\\n2. Use \"Collections\" for collections.\\n3. Use \"Collections\" for collections.\\n4. Use \"Collections\" for collections.\\n\\n## Context\\nThe primary use case is generating multiple exhibit sites, however we want to use it as\\nlanding pages for collections. The term \"Collection\" was used everywhere else, including PUDL,\\ncustomers, and we want to use it as landing pages for collections.\\n\\n\\end{document}\\n"}
{"File Name":"dpul\/0005-resource-per-collection.md","Context":"## Context\\nWe needed to be able to display a resource in more than one collection, because\\nin Figgy a resource can be a member of multiple collections.\\nAt the time of this decision, one IIIFResource could only be a member of one Exhibit.\\n","Decision":"We create one IIIFResource per Collection of which it is a member.\\nWe map each IIIFResource to one SolrDocument.\\n","tokens":55,"id":124,"Predictions":"\\begin{itemize}\\n\\item If the resource is a member of the Exhibit collection, then display it in that collection.\\n\\n\\end{itemize}\\n\\noindent\\textbf{Decision:} \\texttt{displayResourceInExhibit}\\n\\n\\subsection{Resource not in a Collection}\\n\\nIf the resource is not a member of any collection, then display it in the default collection.\\n\\n\\subsection{Resource not in a Collection and not in an Exhibit}\\n\\nIf the resource is not a member of any collections, then display it in the default collection. Else, if there are multiple collections, display it in the one that is most relevant.\\n\\n\\subsection{Resource with no Collection}\\n\\nIf there is no collection associated with the resource, display it in the default collection.\\n\\n\\subsection{Resource with no Collection and no Exhibit}\\n\\nIf there are multiple collections, display it in the one that is most relevant. Else, if there are no collections associated with the resource, display it on its own.\\n\\n\\subsection{Resource with no Collection and no Exhibit and no Member of any Collections}\\n\\nIf there are multiple collections, display it on its own. Else, if there are none associated with the resource, display it on its own.\\n\\n\\subsection{Resource without Member of any Collections}\\n\\nIf there are none associated with the resource, display it on its own. Else if there are none associated with the resource or if they have been associated with another resource, then display them in order of their association (i.e., earliest to latest).\\n\\n"}
{"File Name":"dpul\/0004-index-from-iiif-manifests.md","Context":"## Context\\nObjects in Pomegranate need to get their metadata from Figgy, where they are\\nadministered. Spotlight ships with a IIIF-based indexer. Figgy already produces\\nIIIF manifests to support viewing the objects. However the metadata bucket\\ndoesn't contain rich enough metadata for pomegranate use cases.\\nFiggy (plum, at the time) didn't have an API at the time this decision was made. Manifests were the\\nonly way to get data out. Today Figgy has a graphql API.\\n","Decision":"We will use the IIIF Manifests to pull data from Figgy into Pomegranate. The\\nManifest gives us the manifest url (used for presenting a viewer), the thumbnail\\niiif image url, and the jsonld metadata location (via seeAlso).\\n","tokens":115,"id":125,"Predictions":"\\begin{figure}[H]\\n  \\begin{center}\\n    \\includegraphics[width=0.8\\linewidth]{figs\/pom-architectural-decision}\\n  \\end{center}\\n\\end{figure}\\n"}
{"File Name":"kosmos\/0001-app-db.md","Context":"# Context\\nI'm currently working on migrating from redux store to re-frame. It's a good point to define data\\nstructure and API to access it.\\n# Decision\\nApplication DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","Decision":"Application DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","tokens":379,"id":126,"Predictions":"# Decision\\nIs the current application a data structure that is currently used by the application? If yes, then this is a data\\nstructure. If not, then this is a API.\\n# Decision\\nApplication is data structure that is currently used by the application. It contains all the data and APIs to access it.\\n# Decision\\nApplication is API that is currently used by the application. It contains all the data and APIs to access it.\\n# Decision\\nApplication has root node id as a key and value as a node id. Root node id should be unique for an application.\\n# Decision\\nNode has type as `object` and value as `object`. Node should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\n# Decision\\n"}
{"File Name":"dddsample-pelargir\/0002-isolate-components-into-submodules.md","Context":"## Context\\nI want to be able to mix and match different implementations\\nof the ddd sample application, without needing to pull in\\na large collection of unnecessary dependencies.\\nI want all of the code to be together in one place; which\\nis to say, I want to treat the entire project as a mono-repo.\\nI can't be bothered to maven install\/maven deploy each\\nlittle piece to propagate the necessary changes between\\nisolated libraries.\\n","Decision":"Use a maven reactor project to track the dependencies between\\ndifferent libraries within the project\\n","tokens":97,"id":127,"Predictions":"## Decision\\nI will use the ddd sample application as a base, and use\\nthe following libraries to build a stable implementation of the\\nsample.\\n\\n## Libraries\\n## Libraries\\nI will use the following libraries to build a stable implementation\\nof the sample:\\n\\n## Libraries\\n## Libraries\\nI will use the following libraries to build a stable implementation\\nof the sample:\\n\\n## Libraries\\n## Libraries\\nI will use the following libraries to build a stable implementation\\nof the sample:\\n"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"Predictions":"## Decision\\nI want to make the following changes\\n 1.\\n 2.\\n 3.\\n 4.\\n 5.\\n 6.\\n\\n## Context\\nI want to make the following changes\\n 1.\\n 2.\\n 3.\\n 4.\\n\\n## Decision\\n## Decision\\nI want to make the following changes\\n 1.\\n 2.\\n## Context\\n## Decision\\n## Decision\\nI want to make the following changes\\n 1.\\n\\end{verbatim}\\n\\end{center}\\n\\newpage\\n\\n\\section{Software Packages and Dependencies}\\n\\label{sec:software-packages}\\n\\nThe \\texttt{software} package contains a number of classes that are used for software configuration. The \\texttt{software} package is not included in the distribution because it is not a part of the Citerus API. It is included here for completeness.\\n\\n\\subsection{Software Configuration Classes}\\n\\label{sec:software-package-classes}\\n\\nThe \\texttt{software} package contains two classes for software configuration. The first class, \\texttt{SoftwareConfiguration}, provides a way to define software packages and their dependencies. The second class, \\texttt{SoftwareDependencies}, provides a way to define software packages and their dependencies, but it does not provide a way to determine which software packages are installed in the system.\\n\\nThe \\texttt{software} package also contains a number of utility functions that are used for software configuration.\\n\\n\\subsubsection*{\\texttt{\\$package}}\\nThis function returns the name of the current package being configured. This function does not return any information about how many packages are installed in the system.\\n\\n\\subsubsection*{\\texttt{\\$dependencies}}\\nThis function returns an array containing the names of all the dependencies that were installed with this package.\\n\\n\\subsubsection*{\\texttt{\\$package\\_dependencies}}\\nThis function returns an array containing all of the names of all of the dependencies that were installed with this package.\\n\\n\\begin{verbatim}\\nfunction $package_dependencies($package)\\n{\\n    return $package->getDependencies($package);\\n}\\nfunction $dependencies($package)\\n{\\n    return array();\\n}\\nfunction $package_dependencies($package)\\n{\\n    return $dependencies($package);\\n}\\nfunction $dependencies($package)\\n{\\n    return $package->getDependencies();\\n}\\nfunction $dependencies($package)\\n{\\n    return array();\\n}\\nfunction $dependencies($package)\\n{\\n    return $packages->getDependencies();\\n}\\nfunction $packages()\\n{\\n    foreach ($packages as &$pkg) {\\n        if ($pkg->isInstalled() && !isset($pkg->getPackageDependencies())) {\\n            unset($pkg->getPackageDependencies());\\n        }\\n    }\\n    foreach ($packages as &$pkg) {\\n        if ($pkg->isInstalled() && isset($pkg->getDependencies())) {\\n            unset($pkg->getDependencies());\\n        }\\n    }\\n    foreach ($packages as &$pkg) {\\n        if ($pkg->isInstalled() && !isset($pkg->getPackageDependencies())) {\\n            unset($pkg->getPackageDependencies());\\n        }\\n    }\\n    foreach ($packages as &$pkg) {\\n        if ($pkg->isInstalled() && isset($pkg->getDependencies())) {\\n            unset($pkg->getDependencies());\\n        }\\n    }\\n}\\n\\nfunction getDependsOn( $name )\\n{\\n    foreach (array_keys( $name ) as &$name ) {\\n        if ( !isset( getPackageDependency( $name ) ) )\\n            unset( getPackageDependency( $name ) );\\n    }\\n}\\n\\nfunction getDependsOn( $name )\\n{\\n    foreach (array_keys( $name ) as &$name ) {\\n        if ( isset( getPackageDependency( \"{$name}\" ) ) )\\n            unset( getPackageDependency( \"{$name}\" ) );\\n        else\\n            unset( getPackageDependency( \"{$name}\" ) );\\n        else\\n            getDependsOn( \"{$name}\" );\\n    }\\n}\\n\\nfunction getDependsOn()\\n{\\n\t\/\/ Do nothing - we don't need it.\\n\treturn array();\\n}\\n\\nfunction getPackages()\\n{\\n\t\/\/ Do nothing - we don't need it.\\n\treturn array();\\n}\\n\\nfunction getPackages()\\n{\\n\t\/\/ Do nothing - we don't need it.\\n\treturn array();\\n}\\n\\nfunction getPackages()\\n{\\n\t\/\/ Do nothing - we don't need it.\\n\treturn array();\\n}\\n\\nfunction getPackages()\\n{\\n\t\/\/ Do nothing - we don't need it.\\n\treturn array();\\n}\\n\\nFUNCTION getPackages()\\n{\\n\\n\tforeach (array_keys(getPackages()) as &$key) {\\n\t\tif (isset(getPackages()[$key])) {\\n\t\t\tunset(getPackages()[$key]);\\n\t\t}\\n\t}\\n\\n\tforeach (array_keys(getPackages()) as &$key) {\\n\t\tif (isset(getPackages()[$key])) {\\n\t\t\tunset(getPackages()[$key]);\\n\t\t\tunset(getDependsOn(\"$key\"));\\n\t\t}\\n\t}\\n\\n\tforeach (array_keys(getPackages()) as &$key) {\\n\t\tif (isset(getPackages()[$key])) {\\n\t\t\tunset(getPackages()[$key]);\\n\t\t\tunset(getDependsOn(\"$key\"));\\n\t\t\tunset(getPackageDependency(\"$key\"));\\n\t\t}\\n\t}\\n\\n\tforeach (array_keys(getPackages()) as &$key) {\\n\t\tif (isset(getPackages()[$key])) {\\n\t\t\tunset(getPackages()[$key]);\\n\t\t\tunset(getDependsOn(\"$key\"));\\n\t\t\tunset(getPackageDependency(\"$key\"));\\n\t\t\tunset(getPackageName(\"$key\"));\\n\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\t\t\t\tunset(packages([$this]));\\n\\n\t\t\t\t\tif (!isset(listOfPkgNameKeys)) listOfPkgNameKeys = [];\\n\t\t\t\t\tlistOfPkgNameKeys = listOfPkgNameKeys . split(';');\\n\t\t\t\t\tlistOfPkgNameKeys = implode(',',$listOfPkgNameKeys);\\n\\n\t\t\t\t\tpackageList = [];\\n\t\t\t\t\tfor ($i=0; !$i ; ++$i){\\n\t\t\t\t\t\tpackageList[] = \"$i\";\\n\t\t\t\t }\\n\\n\t\t\t\t\tpackageList = implode(',',$listOfPkgNameKeys);\\n\\n\t\t\t\t\tpackageList = explode(',',$listOfPkgNameKeys);\\n\\n\t\t\t\t\tforeach ($p in packageList){\\n\t\t\t\t\t\tif (!isset(listOfPkgNameKeys[$p])){\\n\t\t\t\t\t\t\tlistOfPkgNameKeys[$p] = \"\";\\n\t\t\t\t\t\t} else {$listOfPkgNameKeys[$p] = \"$p\";}\\n\t\t\t\t\t\tlistOfPkgNameKeys[$p] .=$p;\\n\t\t\t\t }\\n\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t \\n\t\t\t\t\\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t  \\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t \t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t\t\t\t\t\t\t  \t\t\t\\n\t\t\t\t\t\t\t\t\t\t \t\t\t\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\t\t\t   \\n\\n\\n\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns: Array containing all packages\\n\\nFunction: getPakages()\\n\\nReturns:\\n\\nFUNCTION getPakages()\\n\\n{\\n\\n\tforeach (array_keys(GET_PACKAGES()) as &$packets){\\n\t    if (!isset(GET_PACKAGES()[packets['packets']]))\\n\t        unset(GET_PACKAGES()[packets['packets']]);\\n\t    }\\n\\n\tforeach (array_keys(GET_PACKAGES()) as &$packets){\\n\t    if (!isset(GET_PACKAGES())[packets['packets']])\\n\t        unset(GET_PACKAGES())[packets['packets']];\\n\t    }\\n\\n\tforeach (array_keys(GET_PACKAGES()) as &$packets){\\n\t    if (!isset(GET_PACKAGES())[packets['packets']])\\n\t        unset(GET_PACKAGES()][packets['packets']];\\n\t    }\\n\\n\tforeach (array_keys(GET_PACKAGES()) as &$pakts){\\n\t    if (!isset(PAKT PACKETS)[pakts['pakts']])\\n\t        unset(PAKT PACKETS)[pakts['pakts']];\\n\t    }\\n\\n\tforeach (array_keys(GET_PACKAGES()) as &$pakts){\\n\t    if (!isset(PAKT PACKETS)[pakts['pakts']])\\n\t        unset(PAKT PACKETS)[pakts['pakts']];\\n\t    }\\n\\nforeach ($packages AS &PACKAGE)\\n\\n{\\n\\n\tif(!isset(listOfPackNames))\\n\treturn;\\n\\nforeach ($packages AS &PACKAGE)\\n\\n{\\n\\n\tswitch(strtolower(packagename))\\n\t  {\\n\\n\t  case 'Citerus':\\n\t\t    break;\\n\\n\t  case 'Jenkins':\\n\t\t    break;\\n\t  case 'XenServer':\\n\t\t    break;\\n\t  case 'OpenStack':\\n\t\t    break;\\n\t  case 'ansible':\\n\t\t    break;\\n\t  default:\\n\t\t    break;\\n\\n\t  }\t\\n\\n};\\n\\nforeach ($packages AS &PACKAGE)\\n\\n{\\n\\n\tswitch(strtolower(packagename))\\n\t  {\\n\\n\t  case 'Citerus':\\n\t\t    break;\\n\\n\t  case 'Jenkins':\\n\t\t    break;\\n\t  case 'XenServer':\\n\t\t    break;\\n\t  case 'OpenStack':\\n\t\t    break;\\n\t  case 'ansible':\\n\t\t    break;\\n\t  default:\\n\t\t    break;\\n\\n\t  }\t\\n\\n};\\n\\nforeach ($packages AS &PACKAGE)\\n\\n{\\n\\n\tswitch(strtolower(packagename))\\n\t\t  {\\n\\n\t\t  case 'Citerus':\\n\t\t \tbreak;\\n\\n\t\t  default:\\n\t\t \tbreak;\\n\\n\t\t  }\t\\n\\n};\\n\\nforeach ($packages AS &PACKAGE)\\n\\n{\\n\\n\tswitch(strtolower(packagename))\\n\t      {\\n\\n\t      default:\\n\t     \tbreak;\\n\\n\t      }\t\\n\\n};\\n\\nreturn unpacked-packages();\\n\\n}\\n\\n\\nFunction:getDependsOn(\\&package, string name)\\n\\nReturn an array with one element per dependency found in this package.\\n\\nThe returned value will be an empty string or an empty array depending on whether or not there were dependencies found.\\n\\nIf no dependencies were found, then this function will return an empty string.\\n\\nIf no dependencies were found, then this function will return an empty array.\\n\\nIf no dependencies were found, then this function will return an empty string.\\n\\nIf no dependencies were found, then this function will return an empty array.\\n\\nIf no dependencies were found, then this function will return an empty string or an empty array depending on whether or not there were dependencies found. If no dependencies were found, then this function will return an empty string. If no dependencies were found, then this function will return an empty array. If no dependencies were found, then this function will return an empty string or an empty array depending on whether or not there were depsidencies found. If no depsidencies were found, then this fucntion will return an empty string or an empty aray depending on whether or not there weredepsidencies found. If no depsidencies weredfound, then this funciton will retun a null pointer value otherwise it will retun a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise it wilreturn a null pointer value otherwise itswilreturn a null pointer value otherwise itswilreturna nullpointervalueotherwiseitswilreturna nullpointervalueotherwiseitswilreturna nullpointervalueotherwiseitswilreturn"}
{"File Name":"dddsample-pelargir\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":129,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"accessibility-monitoring\/ADR005-database-public-sector-websites.md","Context":"## Context\\nWe currently have a \"public sector domains\" table (along with an \"organisations\" table containing the owners of the domains).\\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\\nEach of these needs to be treated as a separate \"website\" in the context of accessibility monitoring.\\nThus, we either change the \"public sector domains\" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\\n","Decision":"We will create and populate a new table in the PubSecDomains schema that contains:\\n* url\\n* name of the service\\n* the site's title from its HTML <head><title> element, where given\\n* the site's description from its <head><meta name=\"description\"> element, where given\\n* last_updated timestamp\\n* the website's sector (foreign key to sectors table)\\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\\n","tokens":162,"id":130,"Predictions":"\\nThe public sector domains table is used by the domain name service to determine which websites are available to the public. Therefore, we should keep it. We will copy over all of the unique domains and leave the ones that are not unique in the public sector domains table.\\nThe \"public sector websites\" table is used by the domain name service to determine which websites are available to the public. Therefore, we should create a new table specifically for public sector websites. We will copy over all of those that are not unique in the public sector websites table.\\nThe \"public sector website URLs\" table is used by the domain name service to determine which websites are available to the public. Therefore, we should create a new table specifically for public sector URLs. We will copy over all of those that are not unique in the public sector website URLs table.\\n## Note\\nThis document is a draft and subject to change as needed.\\n"}
{"File Name":"accessibility-monitoring\/ADR001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":131,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"accessibility-monitoring\/ADR003-tool-choices.md","Context":"## Context\\nThe choice of tools that will be built and used by the accessibility monitoring team are influenced by:\\n* Suitability to the task\\n* Cost and ongoing-cost (open-source preference)\\n* The [GDS Way](https:\/\/gds-way.cloudapps.digital\/).\\nAs defined in ADR002, we need to meet a number of requirements:\\n1. Maintaining an organised list of public sector websites\\n1. Picking websites to test from that list\\n1. Triaging each site\\n1. Prioritising sites for testing\\n1. Tracking the progress of testing on each website\\n1. Creating a report for the completed test\\n1. Sending the report to the site's owner\\n1. Managing and recording interaction with the site's owner\\n","Decision":"We will...\\n### Use Zendesk\\nRationale:\\n* GDS have a license for Zendesk.\\n* It has an extensive, well-documented API.\\n* There is a lot of experience in GDS of general usage, and a fair amount in using the API.\\n* We have a sandbox Zendesk environment.\\nThis will be the driver of testing work. Tickets will be created in Zendesk (manually or automatically) representing websites to test.\\nThey be prioritised in Zendesk and then be assigned to \/ picked up by Accessibility Officers.\\nZendesk will also handle communication and follow-up with the site owner.\\nThis satisfies items 5, 7 and 8 above.\\n### Use Postgres\\n* A relational database is best suited to the requirements for both a public sector domains database and the testing records\\n* It is open-source\\n* It is well supported and documented\\n* It is available as a \"plug-and-play\" service on [GOV.UK PaaS](https:\/\/www.cloud.service.gov.uk\/) (see below)\\nThis satisfies item 1 and facilitates items 2, 5 and 6.\\n### Use GOV.UK Platform-as-a-Service\\n* Meets GDS' [cloud-first policy](https:\/\/www.gov.uk\/guidance\/government-cloud-first-policy)\\n* VERY well supported with an extremely well-experienced and skilled team within GDS.\\n* Supports all of the chosen technologies\\n### Use Deque Axe\\n[Deque Axe](https:\/\/github.com\/dequelabs\/axe-core) is a 3rd-party, open-source tool that tests the web page that you give it against a set of predefined rules.\\n* It is open-source\\n* It can be run from the command-line and so can probably be automated.\\n* It produces results in a machine-readable format (JSON)\\nThis satisfies item 3.\\n### Use the GOV.UK Design System for the front-end\\nThe [GOV.UK Design System](https:\/\/design-system.service.gov.uk\/) is a set of components, styles and patterns created by the GOV.UK team after extensive user-research.\\nNot only does this serve as a ready-made template engine, but we know that it scores extremely highly on accessibility.\\n### Programming languages\\n* Use node.js for Axe-integration as it is well suited to asynchronous http calls.\\n* Use node.js (and nunjucks)for front-end code as that's what GOV.UK Design System is written in.\\n* Use Python 3 for Zendesk integration as there is a well-maintained open-source Python library.\\n### (Under review) Use PowerMapper SortSite\\n[SortSite](https:\/\/www.powermapper.com\/products\/sortsite\/) by PowerMapper is a 3rd-party commercial product that is widely used in the testing of websites, and has a thorough accessibility checking feature. It also crawls a website and catalogues its pages, a function that Axe does not perform and would be useful to us.\\nHowever, it is not open-source and only runs in a Windows or MacOS environment, so would not be easy to integrate into an automated process.\\nThere is a web service whereby a domain is submitted to it and it returns an HTML report, but this is not a machine-friendly format and would require screen-scraping and parsing - quite a major task, and one that's prone to breaking if\/when SortSite changes its output format.\\nThe site-mapping functionality of SortSite could be achieved with an existing open-source python or node library.\\n","tokens":159,"id":132,"Predictions":"This is the decision made by the Accessibility Monitoring Team. The decision is made by the team leader. The team leader has the authority to make decisions on behalf of the team.\\n## Context\\nThe choice of tools that will be built and used by the Accessibility Monitoring Team are influenced by:\\n* Suitability to the task\\n* Cost and ongoing-cost (open-source preference)\\n* The [GDS Way](https:\/\/gds-way.cloudapps.digital\/).\\nAs defined in ADR002, we need to meet a number of requirements:\\n1. Maintaining an organised list of public sector websites\\n1. Picking websites to test from that list\\n1. Triaging each site\\n1. Prioritising sites for testing\\n1. Tracking the progress of testing on each website\\n1. Creating a report for the completed test\\n1. Sending the report to the site's owner\\n1. Managing and recording interaction with the site's owner\\n\\n## Decision\\nThis is a decision made by the Accessibility Monitoring Team. The decision is made by the team leader. The team leader has the authority to make decisions on behalf of the team.\\n## Context\\nThis is an Architectural Decision Record for a software.\\nGive a ## Decision corresponding to the ## Context provided by th\\n\\n"}
{"File Name":"accessibility-monitoring\/ADR002-build-some-things.md","Context":"## Context\\nThe accessibility monitoring team require a means of:\\n* Picking websites to test (from a random list weighted by a variety of criteria, from complaints passed to us, from previously-tested sites (to test changes) and sites specifically chosen by guiding bodies)\\n* Collecting and maintaining a list of public sector websites from which to pick\\n* Triaging each site (check it is live, check for accessibility statement(s), measure its size in pages etc)\\n* Prioritising sites for testing\\n* Tracking the progress of testing on each website\\n* Creating a report for the completed test\\n* Sending the report to the site's owner\\n* Managing and recording interaction with the site's owner\\nWhilst there are a number of accessibility testing solutions available, they are all geared towards testing of one's own site, and require (or at least, assume) access to the source code and deployment system. Testing of multiple sites by a 3rd party from outside the sites' domains is not a common requirement for vendors!\\nSimilarly, whilst the reports generated by some of the testing tools are very good, they are not up to GDS standards for content and, ironically, accessibility. Furthermore, GDS have no control over the content of the generated reports.\\nWe therefore need a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion.\\n","Decision":"We will use 3rd-party tools where they provide part of the functionality that is required and can be made to communicate easily with each other.\\nWe will write whatever code is necessary to form the \"glue\" that binds them together and to automate whatever can be (and is worth) automating.\\n","tokens":294,"id":134,"Predictions":"## Decision\\nThe decision is to use a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion.\\n\\n## Context\\nThe Accessibility team require the following: \\n* A database that will store the websites that are accessible (and not accessible) from the UK Government website. (This will include the websites that are accessible from the UK Government website, but not from the Government's own site.)\\n* A semi-automatic accessibility test tool that can be used to test for accessibility across multiple sites.\\n* A Customer Relationship \/ Support Ticket Management system, able to move data between them in a usable fashion.\\n\\n## Decision\\n## Decision\\nThe decision is to use a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion. \\n\\n## Context\\nThe Accessibility team require the following: \\n* A database that will store the websites that are accessible (and not accessible) from the UK Government website. (This will include the websites that are accessible from the UK Government website, but not from the Government's own site.)\\n* A semi-automatic accessibility test tool that can be used to test for accessibility across multiple sites.\\n* A Customer Relationship \/ Support Ticket Management system, able to move data between them in a usable fashion. \\n\\n## Decision\\n## Decision\\nThe decision is to use a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion. \\n\\n## Context\\nThe Accessibility team require: \\n* The ability to add new websites into an existing list (possibly using an automated process)\\ncreate new sites with accessibility statements\\ncreate new sites without accessibility statements\\ncreate new sites without accessibility statements and with no statement at all\\ncreate new sites with no statement at all and with no access statement at all\\ncreate new sites without access statements at all and with no statement at all\\ncreate new sites without access statements at all and with no statement at all and with no access statement at all\\ncreate new sites without access statements at all and with no statement at all and with no access statement at all and with no access statement at all on any page on any domain\\ncreate new sites without access statements at all and with no statement at all and with no access statement at all on any pages on any domain\\ncreate new sites without access statements at all and with no statement at all on any pages on any domain\\ncreate new sites without access statements at some page(s) on some domain\\ncreate new sites without access statements anywhere on any page(s) anywhere on any domain\\ncreate new site(s) containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility statement containing an accessibility stament containing an accessibility stament containing an accesibility stament creating one or more tests for each site.\\ndecrease number of tests for each site.\\ndecrease number of tests for each site.\\ndecrease number of tests for each site.\\ndecrease number of tests for each site.\\ndecrease number of tests for each site.\\n"}
{"File Name":"documents\/0002-move-to-clang-format-5.md","Context":"## Context\\nCurrent version of clang-format is aging an becoming unavailable.\\n","Decision":"We will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\\non this version across the project.\\n","tokens":15,"id":135,"Predictions":"## \\n\\nCurrent version of clang-format is aging an becoming unavailable.\\n\\n\\end{verbatim}\\n\\nThe decision record is a summary of the decision, and includes the following information:\\n\\begin{itemize}\\n\\item A summary of the decision.\\n\\item The context in which the decision was made.\\n\\item A description of how the decision was reached.\\n\\item An explanation of why the decision was made.\\n\\end{itemize}\\n"}
{"File Name":"documents\/0003-move-to-visual-studio-2017.md","Context":"## Context\\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\\nfeature complete and has many C++17 features.\\n","Decision":"As part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\\n","tokens":42,"id":136,"Predictions":"## Decision: C++14 is a standard language\\n\\n## Decision: C++14 is a standard language and will be in the standard library\\n\\n## Decision: C++14 is not a standard language and will not be in the standard library\\n\\n## Decision: C++14 is a standard language and will be in the standard library\\n\\n## Decision: C++14 is not a standard language and will not be in the standard library\\n\\n## Decision: C++14 is not a standard language and will not be in the standard library\\n\\n## Decision: C++14 is not a standard language and will not be in the standard library\\n\\n## Decision: C++14 is a standard language and will be in the standard library\\n\\n## Decision:\\n"}
{"File Name":"documents\/0004-move-to-devtoolset-7-on-rhel7.md","Context":"## Context\\nRed Hat 7's default compiler is gcc 4.8. This is not C++14 compliant and prevents access to more modern C++ features.\\n","Decision":"As part of the maintenance cycle after release 3.13 we will move Red Hat 7 compilers over to use\\ngcc 7 as part of the [devtoolset-7](https:\/\/www.softwarecollections.org\/en\/scls\/rhscl\/devtoolset-7\/) tools provided by Red Hat.\\n","tokens":35,"id":137,"Predictions":"## 1. No\\n\\n## 2. Yes\\n\\n## 3. Yes\\n\\n## 4. Yes\\n\\n# [1,2,3,4,5] # A number of options are available for a decision record.\\n```\\nThe first option is the default one (and the one I want).\\nThe second option is not what I want.\\nThe third option is what I want but the fourth option is not what I want.\\nI'm using `R` version `4.0.0` on Windows.\\nWhat's the problem here?\\n\\nOP 2017-05-23: I found the answer by myself as follow:\\n```\\nlibrary(rgdal)\\nlibrary(rgdal.plot)\\n\\n# The data file contains an empty field \"Date\"\\ndata <- read.csv(\"data.csv\", header = TRUE)\\n\\n# The data file contains a column named \"Date\"\\ndata <- data[, c(\"Date\", \"Time\")]\\n\\n# The data file does not contain a field named \"Date\"\\ndata <- data[, c(\"Date\", \"Time\")]\\n\\n# The data file does contain a field named \"Date\"\\ndata <- data[, c(\"Date\", \"Time\")]\\n\\n# The data file does contain a column named \"Time\"\\ndata <- data[, c(\"Time\")]\\n\\n# The data file contains an empty field \"Date\"\\ndata <- read.csv(\"data.csv\", header = TRUE)\\n\\n# The data file contains a column named \"Time\"\\ndata <- data[, c(\"Date\", \"Time\")]\\n\\n# The data file does not contain a field named \"Time\"\\ndata <- data[, c(\"Date\", \"Time\")]\\n\\n# The data file does contain a column named \"Time\"\\ndata <- read.csv(\"data.csv\", header = TRUE)\\n\\nplot(data)\\n```\\n\\n"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"Predictions":"## This decision is taken by the user of the software. It will be\\n## recorded in the software.\\n## Decision Type\\n## We need to record the type of decision taken by the user.\\n## Decision Type\\n## This is a decision taken by the user, which will be recorded in\\n## the software.\\n## Decision Type\\n## The decision type will be decided by the user.\\n## Decision Type\\n## The decision type is decided by the user. It will be recorded in\\n## the software.\\n\\end{verbatim}\\n\\nThe \\code{Decision} class provides a number of methods for dealing with decisions made by users. These methods are described below.\\n\\n\\begin{description}\\n\\item[\\code{save()}]: Save a new version of this decision record to disk.\\n\\item[\\code{load()}]: Load a previous version of this record from disk.\\n\\item[\\code{get\\_decision\\_type()}]: Get a list of all current decision types.\\n\\item[\\code{get\\_decision\\_type\\_name()}]: Get a name for a particular decision type.\\n\\end{description}\\n\\nThe \\code{Decision} class also provides methods for getting the current and previous versions of this record, as well as methods for creating new versions and loading previous versions.\\n\\nThe \\code{Save} and \\code{Load} methods are used to save and load decisions to disk.\\n\\nThe \\code{get\\_decision\\_type()} method returns an integer value that identifies which current decision type is being queried. The value is returned as part of an integer array, where each element is an integer value between 0 and 5 indicating how many times this particular decision type has been queried.\\n\\nThe \\code{get\\_decision\\_type\\_name()} method returns an integer value that identifies which particular decision type is being queried. The value returned is part of an integer array, with each element being an integer value between 0 and 5 indicating how many times this particular decision type has been queried.\\n\\nIn addition, there are two methods for creating new versions or loading previous versions:\\n\\n\\begin{description}\\n\\item[\\code{\\_create()}]: Creates a new version or loads an existing version from disk.\\end{description}\\n\\nFinally, there are three methods for getting information about decisions made by users:\\n\\n\\begin{description}\\n\\item[\\code{\\_list\\_of\\_decision\\_types()}: List all current decisions.\\end{description}\\n\\nThe \\code{\\_list\\_of\\_decision\\_types()} method returns a list of integers that identify all currently existing decisions made by users. Each integer represents one current decision making up one or more types.\\n\\nThe \\code{\\_list\\_of\\_decision\\_types()} method can also be used to return information about any other kind of information about decisions made by users. For example, if we wanted to get information about all decisions that were made after August 1st 2014, then we could use:\\n\\n\\begin{verbatim}\\ndef get_decision_types():\\n    return [\\n        '2014-08-01',\\n        '2014-08-02',\\n        '2014-08-03',\\n        ...\\n    ]\\n\\end{verbatim}\\n\\nThis would return a list containing all currently existing decisions made after August 1st 2014.\\n\\nFinally, there are two methods for getting information about specific types of decisions:\\n\\n\\begin{description}\\n\\item[\\code{\\_list\\_of\\_decision_type_names()}: List all currently existing types.\\end{description}\\n\\n\\n%%------------------------------------------------------------------------------\\n%%\tDOCUMENTATION SECTION\\n%%------------------------------------------------------------------------------\\n\\n%------------------------------------------------------------------------------\\n%\tDOCUMENTATION SECTION\\n%------------------------------------------------------------------------------\\n\\n%------------------------------------------------------------------------------\\n%\tDOCUMENTATION SECTION\\n%------------------------------------------------------------------------------\\n\\n"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"Predictions":"# Context\\nThe user wants to be able to select a `path_url` prompt in the app web1 file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n## Decision\\n# Context\\n# Decision\\n# Consequences\\n## Decision\\n# Context\\n# Decision\\n# Consequences\\n\\nThis page was generated by lifecorp-init on 2017-01-11 19:08:06 -0600"}
{"File Name":"openmrs-java-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":140,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"lcarsde\/use-of-colors.md","Context":"## Context and Problem Statement\\nWhat colors shall be used for buttons, statistics and inactive elements considering usability. This concerns first and\\nforemost the tool applications.\\n","Decision":"Buttons or any usable elements should use the colors #ff9900, #9999ff, #cc6666 and #cc6699 as background and black text.\\nDisplayed text should preferably be colored #ff9900 on black background. Other text colors may be used after consideration.\\nGraphs, diagrams and other status and statistical displays should use the colors #ffcc99, #9999cc, #9999ff and for warning purposes #ff9966, #ff9900 and #cc6666.\\nDesign elements should use the colors #cc99cc, #9999cc and #ff9966. These colors are less \"strong\" and don't take as much attention.\\n[1]: http:\/\/mrbsdomain.com\/gallery\/var\/albums\/repository\/lcars_colors.gif\\n","tokens":33,"id":141,"Predictions":"## Decision Statement\\nThe colors shall be ## ## white and ## ## black. The background shall be ## ## white.\\n\\n## Decision Statement\\nThe background shall be ## ## white. The background color shall be ## ## black.\\n\\n## Decision Statement\\nThe background color shall be ## ## white. The background color shall be #00ff00.\\n\\n## Decision Statement\\nThe background color shall be #ff0000. The background color shall be #00ff00.\\n\\n## Decision Statement\\nThe background color shall be #00ff00. The background color shall be #ffff00.\\n\\n## Decision Statement\\nThe background color shall be #0000ff. The foreground color shall be #ffffff.\\n\\n## Decision Statement\\nThe foreground color shall be #ffffff. The foreground color shall be #0000ff.\\n\\n## Decision Statement\\nThe foreground color shall be #ffffff. The foreground color shall be #0000ff.\\n\\n## Decision Statement\\nThe foreground color shall be #ffffff. The foreground color should not change during the application's operation.\\n## Questionnaire Section:\\nQuestionnaire Question 1: What colors should the buttons, statistics and inactive elements have?\\nQuestionnaire Question 2: What colors should the text have?\\nQuestionnaire Question 3: What colors should the text have?\\nQuestionnaire Question 4: What colors should the text have?\\nQuestionnaire Question 5: What colors should the text have?\\nQuestionnaire Question 6: What colors should the text have?\\nQuestionnaire Question 7: What colors should the text have?\\nQuestionnaire Question 8: What colors should the text have?\\n\\end{document}\\n"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"Predictions":"\\nThe decision is to use Xlib for this purpose. The Xlib library provides a set of functions for managing display devices and for handling input events. This is a good solution to the problem, since it handles the display management and the input events in one single library. The window manager also needs to be able to use Xlib libraries, so it can access the graphics system.\\n\\n## Architecture\\n\\nThe window manager needs to be able to open and close windows, as well as manage their content. It should also be able to manage input devices like buttons, sliders, etc. All these functions are implemented in Xlib.\\n\\n\\nThe window manager has two main classes: \\emph{Window} and \\emph{WindowManager}. The Window class is responsible for opening and closing windows, managing their content (including sub-windows), and managing their mouse\\textsuperscript{\\textregistered} events. The WindowManager class manages all this stuff.\\n\\n\\nThe Window class uses a \\emph{window manager} interface, which is implemented by the \\emph{WindowManager} class. This interface provides all necessary functionality for a window manager including messages from window managers of other applications.\\n\\n\\nThe \\emph{window manager} interface contains methods that allow the window manager to get notified about opening and closing windows (such as when a user clicks on a particular button), managing its content (such as when a user changes its size) and managing certain events like mouse\\textsuperscript{\\textregistered} events.\\n\\n\\nThe Window class also has methods that allow it to get notified about mouse\\textsuperscript{\\textregistered} events (such as when a user clicks on a particular button). It also has methods that allow it to get notified about keyboard\\textsuperscript{\\textregistered} events (such as when a user types something). These methods are implemented by the \\emph{WindowManager} class.\\n\\n\\nThe Window class also has methods that allow it to get notified about input devices like buttons, sliders etc. These methods are implemented by the \\emph{WindowManager} class.\\n\\n\\nThe Window Manager interface contains all functions required by the window manager.\\n\\n\\nThis document describes how the window manager should implement these functions in order to provide all necessary functionality for an application.\\n\\n\\nThis document does not describe how these functions should work; this is left up to you.\\n\\end{document}\\n"}
{"File Name":"lcarsde\/language-for-wm.md","Context":"## Context and Problem Statement\\nWhat language shall be used for the development of the window manager?\\n## Decision Drivers <!-- optional -->\\n* I want to learn Kotlin\\n* I want to learn Python\\n* I don't want to take care of the need to update an API layer between the window manager code and the\\n* Libraries for display management and tooling in that area is usually written in C\\n","Decision":"* I want to learn Kotlin\\n* I want to learn Python\\n* I don't want to take care of the need to update an API layer between the window manager code and the\\n* Libraries for display management and tooling in that area is usually written in C\\nChosen option: \"Kotlin\/Native\", because it compiles to fast native code and C-APIs can be used directly.\\n### Positive Consequences <!-- optional -->\\n* Can use C-APIs directly (converted automatically by tooling)\\n* Compiles to fast native application\\n* No extra API layer\\n### Negative Consequences <!-- optional -->\\n* Needs separate compiling on every machine type\\n","tokens":83,"id":143,"Predictions":"## Decision Drivers\\n## Decision\\n## Decision\\n## Decision Drivers\\n\\n# Option 1 (Recommended): Kotlin\\n\\n* I want to learn Kotlin\\n* I want to learn Python\\n* I don't want to take care of the need to update an API layer between the window manager code and the Libraries for display management and tooling in that area is usually written in C\\n\\n```\\nThe above question is very simple. But when I try to answer it, it is giving me a lot of errors.\\nI have tried all the answers in the internet but none of them are working for me.\\nI have tried this:\\n```\\nval decision = decisionManager.getDecision()\\n    val decisionNumber = decision.getDecisionNumber()\\n    val decisionType = decision.getDecisionType()\\n    val decisionDriver = decision.getDecisionDriver()\\n    val decisionCategory = decision.getDecisionCategory()\\n```\\nBut these are giving me error like:\\n```\\nerror: cannot find symbol\\n  val decisionCategory = decision.getDecisionCategory()\\n       ^\\n  symbol:   class DecisionCategory\\n  location: variable DecisionCategory\\n```\\nPlease help me. I am beginner in Kotlin. Thanks in advance.\\nOP 2020-10-13: This was a problem with my IDE, now it's solved.\\nThanks for your help."}
{"File Name":"lcarsde\/ipc-between-wm-and-menu.md","Context":"## Context and Problem Statement\\nThe application menu needs to know the currently active windows, shall display the name and enable the user to select or close them. Therefor it is necessary to exchange the corresponding information with the window manager.\\n## Decision Drivers <!-- optional -->\\n* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\n","Decision":"* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\nChosen option: \"Message queues\", because this approach is a faster than shared storage approaches and doesn't need to handle locking. Message queues are asynchronous (the producer doesn't need to wait for the consumer) and messages are queued and not lost.\\n### Positive Consequences <!-- optional -->\\n* Easy to use and flexible\\n* Library support in Python\\n### Negative Consequences <!-- optional -->\\n* Manual implementation in Kotlin necessary (but straight forward)\\n","tokens":86,"id":144,"Predictions":"## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision Drivers\\n\\n\\n\\begin{itemize}\\n\\item Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n\\item No message should be lost\\n\\n\\end{itemize}\\n\\n\\end{document}"}
{"File Name":"lcarsde\/identification-of-special-tool-windows.md","Context":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","Decision":"Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n","tokens":48,"id":145,"Predictions":"## Decision Name: Window Manager \\nDecision Name: Window Manager\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Type: Architectural Decision\\nDecision Name: Window Manager\\n## Context and Problem Statement\\n## The window manager must be able to identify these special windows by their position in the UI and therefore treat them accordingly. The window manager must also be able to determine whether they should be displayed in the side bar menu or in the status bar. The window manager should also be able to determine whether the current position of a window is acceptable. In addition, it should also be possible to move the window from one position to another.\\n## Decision\\n## Decision Name: Window Manager\\n## Context and Problem Statement\\n## The window manager must be able to identify these special windows by their position in the UI and therefore treat them accordingly. The window manager must also be able to determine whether they should be displayed in the side bar menu or in the status bar. The window manager should also be able to determine whether the current position of a window is acceptable. In addition, it should also be possible to move the window from one position to another.\\n## Solution\\n## Solution Name:\\n## Solution Description:\\n## Solution Methodology:\\n\\end{description}\\n\\n\\subsection{Solution}\\n\\nThe solution is based on an application called \\textit{Window Manager} which is a plugin for \\textit{X11}. It provides a way for applications that are using \\textit{X11} (such as \\textit{Xming} or \\textit{MobaXterm}) to manage their windows without using any other software.\\n\\nThe solution consists of three parts:\\n\\n\\begin{description}\\n\\item[Window Manager Plugin:] This is a plugin that provides a way for applications that are using \\textit{X11} (such as \\textit{Xming} or \\textit{MobaXterm}) to manage their windows without using any other software.\\n\\item[Window Management API:] This provides a way for applications that are using \\textit{X11} (such as \\textit{Xming} or \\textit{MobaXterm}) to manage their windows without using any other software.\\n\\item[Window Management Service:] This service manages all of these parts.\\n\\end{description}\\n\\nThe solution contains two main components:\\n\\n\\begin{description}\\n\\item[Window Management Service:] This service manages all of these parts.\\n\\item[Window Management API:] This API provides a way for applications that are using \\textit{X11} (such as \\textit{Xming} or \\textit{MobaXterm}) to manage their windows without using any other software.\\n\\end {description}\\n\\nThe main functionality provided by this plugin is:\\n\\n\\begin {description}\\n\t\\item[Window Configuration] This allows you to configure how many columns you want, how many rows you want, how many rows per column, and so on. You can also set what kind of layout you want your windows to have (i.e., vertical vs horizontal).\\n\t\\item[Window Layout] This allows you specify which positions your windows will have on your screen. You can specify positions like top left, top right, bottom left, bottom right, etc.\\n\t\\item[Window Size] This allows you specify how big your windows will be on your screen. You can specify sizes like 800x600, 1024x768, etc.\\n\t\\item[Window Placement] This allows you specify where your windows will appear on your screen. You can specify positions like top left, top right, bottom left, bottom right, etc.\\n\t\\item[Window Visibility] This allows you specify which part of your screen your windows will appear on (i.e., all of it).\\n\t\\item[Window Transitions] These allow you specify what kind of transition effect there will be when moving between different positions on your screen (like when moving from one position on your screen down another position).\\n\t%\\item [Windows]\\n\t%\\begin {tabular}{|l|l|l|l|}\\n\t%\\hline \\n%Position & Left & Top & Bottom \\\\ \\n%& & & \\\\ \\n%& & & \\\\ \\n%& & & \\\\ \\n%& & & \\\\ \\n%& & 0 \\\\ \\n%& 0  & 0 \\\\ \\n%& 0  & 1 \\\\ \\n%& 1  & 0 \\\\ \\n%& 1  & 1 \\\\ \\n%&2   &0\\\\ \\n%2   &0   &&\\\\  \\n%\\hline  \\n%\\end {tabular}\\n%\\end {description}\\n\\nThe solution contains three main features:\\n\\nFirstly it provides two features:\\n\\nFirstly it provides two features:\\n\\nFirstly it provides two features:\\n\\nSecondly it provides three features:\\n\\nSecondly it provides three features:\\n\\nThirdly it provides three features:\\n\\nThirdly it provides three features:\\n"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"Predictions":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n## Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n\\n# 3 - Code Review\\n\\n### Reviewer's Name: \\n\\n\\n### Reviewer's Email: \\n\\n\\n### Reviewer's Phone: \\n\\n\\n### Reviewer's Availability:\\n\\n* From: Monday - Friday, 5-9 PM Eastern Time (US)\\n* To: Monday - Friday, 5-9 PM Eastern Time (US)\\n\\n### Date:\\n\\n* From: Monday - Friday, 5-9 PM Eastern Time (US)\\n* To: Monday - Friday, 5-9 PM Eastern Time (US)\\n\\n### Description of the Issue:\\n\\nThis project has a lot of formatting and stylistic problems that make it difficult for reviewers and users alike. The project is still in development and the reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\nThe reviewer will be able to review the current state of the project.\\n\\n#### Reviewer's Name: \\n\\n\\n#### Reviewer's Email: \\n\\n\\n#### Reviewer's Phone: \\n\\n\\n#### Reviewer's Availability:\\n\\n* From: Monday - Friday, 5-9 PM Eastern Time (US)\\n* To: Monday - Friday, 5-9 PM Eastern Time (US)\\n\\n### Date:\\n\\n* From: Monday - Friday, 5-9 PM Eastern Time (US)\\n* To: Monday - Friday, 5-9 PM Eastern Time (US)\\n\\n### Description of the Issue:\\n\\nThis issue has been reported by a user who would like some help with fixing several formatting issues and\/or stylistic problems within their source files. These issues are not necessarily related directly with this issue but they are all related in one way or another.\\n\\n#### Reviewer's Name: \\n\\n\\n#### Reviewer's Email: \\n\\n\\n#### Reviewer's Phone:\\n\\n### Date:\\n\\n* From: Monday - Friday, 5-9 PM Eastern Time (US)\\n* To: Saturday and Sunday, 12 AM EST until Tuesday at midnight EST\\n* To get an answer within a few hours please send me an email at [email protected]\\n\\n# Other Information\\n\\nThis issue has been reported by a user who would like some help with fixing several formatting issues and\/or stylistic problems within their source files. These issues are not necessarily related directly with this issue but they are all related in one way or another.\\n\\n# Additional Information\\n\\nPlease include any additional information you feel may help resolve this issue as well as any relevant information you think could help make this issue more clear or useful for other users.\\n\\nPlease include any additional information you feel may help resolve this issue as well as any relevant information you think could help make this issue more clear or useful for other users.\\n\\n# Contact Information\\n\\nPlease include any additional information you feel may help resolve this issue as well as any relevant information you think could help make this issue more clear or useful for other users.\\n\\n\\n\\n"}
{"File Name":"PactSwift\/ADR-001-Language_choice.md","Context":"# Context\\niOS applications can be written in Objective-C or Swift. Objective-C offers greater interaction with C++ code but is considered a legacy language choice in the iOS developer community. The `pact-consumer-swift` framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.\\n# Decision\\nThe framework is written in Swift.\\n# Consequences\\n","Decision":"The framework is written in Swift.\\n# Consequences\\n","tokens":90,"id":148,"Predictions":"\\end{enumerate}\\n\\end{document}\\n"}
{"File Name":"PactSwift\/ADR-004-Dependencies-management.md","Context":"# Context\\nAlmost all software we write depends on some other code, library or development tool which allows us to build what we want faster. Although this project attempts to avoid bringing in 3rd party dependencies, there are is functionality already written that is critical to this projects success.\\n# Decision\\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n# Follow-up (September 30, 2020)\\nAll 3rd party dependencies have been successfully removed from this project\/framework.\\n","Decision":"The main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n# Follow-up (September 30, 2020)\\nAll 3rd party dependencies have been successfully removed from this project\/framework.\\n","tokens":494,"id":149,"Predictions":"\\# Decision\\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n# Follow-up (September 30, 2020)\\nAll 3rd party dependencies have been successfully removed from this project\/framework.\\n## Decision\\n\\# Decision\\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n## Decision\\n\\# Decision\\nThe main requirement for this software development process was that it should be possible for users to test their software without having any prior knowledge about how it works or what goes on underneath its hood.\\nThis software should also be easy enough for users who have no prior knowledge about programming or technical development tools to use.\\nThe software should allow users to run tests locally on their computer before committing them to GitHub or another online repository such as Bitbucket or Gitlab.\\nThis software should also allow them to run tests against their own codebase if they want to see how their changes affect other parts of their codebase.\\nThis software should also allow them to run tests against other people's codebases if they want to see how they affect each other or if they want to share some code between themselves so they can all benefit from each other's changes.\\nThe software should also allow them run tests against production-level systems if they want an automated way of testing their system before putting it onto production servers or making modifications that may affect performance or security issues on those servers.\\nThe software should also allow them run tests against multiple versions of their system at one time so they don't have to manually update each version when new versions are released or modify existing versions so they don't break anything else on their system when updating one version but still get valid results from earlier versions due to testing being performed across different versions simultaneously.\\nThe software should also have an option for users who want more advanced features such as: \\newline \\newline \\newline \\newline \\newline \\newline \\newline \\newline \\textbf{Support for multiple concurrent runs} This feature allows you run many different tests simultaneously without having them interfere with each other or causing any problems for each other during testing. It allows you test your application against multiple versions of your application simultaneously; you can run your application under multiple configurations such as testing against your latest release version; you can test against older releases; you can test against your latest beta release version; you can test against your latest RC version; etc.; You may even be able test your application against old releases if you don't mind changing your configuration every now-and-then due to various reasons such as: new features being implemented; new bugs being fixed; new security vulnerabilities being discovered; etc.; You may even be able test your application against old beta releases if you don't mind changing your configuration every now-and-then due to various reasons such as: new features being implemented; new bugs being fixed; new security vulnerabilities being discovered; etc.; You may even be able test your application against old RC releases if you don't mind changing your configuration every now-and-then due to various reasons such as: new features being implemented; new bugs being fixed; new security vulnerabilities being discovered; etc.; You may even be able test your application against old beta releases if you don't mind changing your configuration every now-and-then due to various reasons such as: new features being implemented; new bugs being fixed; new security vulnerabilities being discovered;\\n## Consequences\\n\\# Consequences\\nDue to all these features not working together properly during testing making sure all these features work together properly during testing was very difficult therefore we had some serious problems with our product during our first week so we decided we needed more time than originally planned for our product so we decided that instead of releasing our first release date earlier than expected we would delay our first release date until after our second week which would give us more time for us all together before releasing our first release date so we could make sure everything was working properly before releasing our first release date.. We chose September 30th because that was going back-to-school month where most schools were closed down because teachers didn\u2019t need anymore school work than what they already had but teachers still needed things like computers so September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again.. We thought September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again.. We think September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again.. We think September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again..\\n## Consequences\\n\\# Consequences\\nDue September 30th because that was going back-to-school month where most schools were closed down because teachers didn\u2019t need anymore school work than what they already had but teachers still needed things like computers so September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again..\\n## Consequences\\n\\# Consequences\\nWe think September 30th would give us enough time after school ended until October 1st where most students were back-to-school again so teachers could start giving students school work again..\\n## Consequences\\n\\# Consequences\\nWe choose Septembe"}
{"File Name":"PactSwift\/ADR-005-Project-Structure.md","Context":"## Context\\n`PactSwift` takes advantage of Mock Server FFI binaries built from shared Rust code. These are generally large binary files when it comes to iOS and macOS platforms and we are limited with hosting them in the GitHub repo. The FFI also follows it's own source and changes are available independently to changes to `PactSwift`'s functionality. Separating the responsibilities would be welcomed.\\nFurthermore, the pain of managing multiple binaries with the same name but each with its specific architecture slice could be reduced by generating an `XCFramework` using an automated script and kept from the framework user. These can blow up to more than 100Mb each (the fat binary with all slices for iOS platform blew up to more than 300MB). Using `XCFramework` we can shed off a lot of the statically linked code. Mock Server FFI (`MockServer.swift`) is the only part of `PactSwift` package that depends on binaries being built for specific architectures and run platforms. With removal of binaries from the main `PactSwift` project, we should be able to avoid managing them, mixing them up (as they are all named the same), discarding them at `git add` and `commit` steps and rebuilding them at next `PactSwift` build\/test cycle.\\n","Decision":"- Mock Server FFI interface and implementation to be split into it's own Swift Package called `PactSwiftMockServer` and distributed as a binary (`XCFramework`) when on Apple platforms and as a source package when used on Linux platforms.\\n- Utilities used by both the main `PactSwift` and `PactSwiftMockServer` packages are split into one package called `PactSwiftToolbox`.\\n- Where it makes sense the dependencies' versions should be exact. If exact version is not set for a valid reason then `.upToMinor()` must be used to avoid breaking changes when releasing packages in isolation.\\n- Scripts to automate the release processes will be provided within the projects' scripts folders.\\n","tokens":268,"id":150,"Predictions":"## Decision: Remove binaries from main `PactSwift` project\\n\\n## Decision: Use XCFramework to generate binaries from shared Rust code\\n\\n```"}
{"File Name":"eslint-config-nhsuk\/0001-replace-istanbul-with-nyc-for-code-coverage.md","Context":"## Context\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\nnyc is one of the suggested replacements. Additional information is available\\non [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n","Decision":"The decision is to migrate from istanbul to nyc.\\n","tokens":81,"id":151,"Predictions":"## Decision\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\n\\nyc is one of the suggested replacements. Additional information is available\\nnon [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n\\n## Decision\\n## Decision\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\n\\nyc is one of the suggested replacements. Additional information is available\\nnon [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n\\n## Decision\\n## Decision\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\n\\nyc is one of the suggested replacements. Additional information is available\\nnon [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n```\\nThe `error` object contains an array of errors, `errors` which are all `Error`s.\\n```\\nconst errors = errors.map(error => {\\n  const message = error.message;\\n  const reason = error.reason;\\n  return { message, reason };\\n});\\n```\\nSo you have a list of errors, each with a `message` and `reason`. You can then use `.filter()` to find only those with a specific reason, for example:\\n```\\nconst filteredErrors = errors.filter(error => {\\n  return error.reason === \"security\";\\n});\\n```"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"Predictions":"\\n\\nThe Decision is to use the GitHub Actions for CI and CD.\\n\\n\\nThe Decision is to use the GitHub Actions for CI and CD. The decision is made\\ndue to the following reasons:\\n\\n\\n1. GitHub Actions will be used as the default CI mechanism for this project.\\n2. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n3. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n4. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n5. It provides a good starting point for learning about GitHub Actions.\\n6. It provides a good starting point for learning about GitHub Actions.\\n7. It provides a good starting point for learning about GitHub Actions.\\n\\nThe decision is made due to the following reasons:\\n\\n1. GitHub Actions will be used as the default CI mechanism for this project.\\n2. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n3. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n4. The codebase is a simple Node.js application with minimal dependencies (and\\ndoes not support any external dependencies).\\n5. It provides a good starting point for learning about GitHub Actions.\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\nThe decision was made due to the following reasons:\\n```\\nIn [1]: actions = repo.actions()\\n\\nIn [2]: actions\\nOut[2]: {'build': {'repo': 'github.com\/username\/repository', 'branch': 'master', 'stage': 1, 'out': '\/tmp\/builds\/1234567890', 'repo_name': 'github.com\/username\/repository', 'tags': ['master', 'develop'], 'project_name': 'github.com\/username\/repository', 'build_args': ['--no-external-dependencies'], 'run_in_project': True, \\n    ...:     '--no-skip-build', '--no-skip-gitlab-ci-step'}, \\n    ...:     '--no-external-dependencies'}\\n```\\n\\nOP 2019-11-18: I think I figured it out.\\nI'm using `gitlab-ci` and `gitlab-runner` as my CI and CD servers.\\nI'm using `gitlab-runner` as my runner, but I'm not using its `--build` option.\\nSo I changed my action.yml file like this:\\n```\\nrun:\\n  stage: build\\n  script:\\n    - echo \"Some Test\"\\n    - gitlab-runner run --build=$CI_BUILD_NUMBER --no-skip-build --no-skip-gitlab-ci-step\\n```\\nIt works!\\n"}
{"File Name":"AYCHAIN.github.io\/adr-001-tools-and-methodologies.md","Context":"## Context\\nLearning Machine handles a Blockcerts verifier in multiple repository and with different ways of deploying. This is costly and hard to maintain.\\nDecision has been made to unify the verifier into one sole component and repository, with scalibility and maintainability at heart.\\n","Decision":"#### JS\\nWe decided to use Polymer 3.0 as previous versions of the verifier were already written with Polymer. Also because Web Components seem like a promising technology that could open a interesting future for the usage of the component.\\nState of the application will be handled by Redux, as demonstrated in the [example project of Polymer](https:\/\/github.com\/Polymer\/pwa-starter-kit).\\n#### CSS\\nWe decided to use ITCSS for its interesting way to handle CSS scalability and maintainability.\\nMore information about this methodology can be found here:\\nhttps:\/\/github.com\/sky-uk\/css\\nhttps:\/\/www.xfive.co\/blog\/itcss-scalable-maintainable-css-architecture\/\\n#### Testing\\nWe are using the tools provided by the Polymer Project, hence [WCT](https:\/\/github.com\/Polymer\/tools\/tree\/master\/packages\/web-component-tester). We also test for accessibility.\\n#### Accessibility\\nThe Web Component needs to be WCAG2.0 AA compliant.\\n","tokens":55,"id":153,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\end{document}\\n"}
{"File Name":"AYCHAIN.github.io\/adr-003-store-connector-container-component.md","Context":"## Context\\n[pwa-starter-kit](https:\/\/github.com\/Polymer\/pwa-starter-kit) is an example of a Redux application working with Polymer 3. While it serves its purpose, the separation of concerns is not entirely clear between the state and the views, which means that in the example views have too much knowledge of what provides and modfies the state.\\nThis is potentially dangerous for scalability, and bloats the view code with decisions that shouldn't be of its concerns.\\nReact provides an architectural pattern to handle this abstraction, in the shape of `Containers`. Containers are wrapper around the view component, which connect (via the `react-redux` helper function of the same name) the view and the state (to read it, and to action it).\\nThis pattern is interesting because from an architectural standpoint every responsibility is properly distributed. The view receives props, may they be data or callback functions, and utilizes them, the container selects where to retrieve the data or callback functions, and the state solely stores the data and provide mechanisms to modify it.\\n","Decision":"We decided to keep this approach for a Polymer project too. Because the out-of-the-box tools do not provide that abstraction, we implemented our own architectural approach to fit this need.\\nA visual representation of the intent is as follows:\\n![Chain of connection: State > Connector > Container > Component](https:\/\/user-images.githubusercontent.com\/12797962\/41294972-a254d432-6e59-11e8-8e08-214c43772173.png)\\n- The state handles the live data of the application.\\n- The connector has an instantiation of the store, but is also a metaview, which enables communication with the methods of the store, without exposing it elsewhere.\\n- The container is an instance of the connector, which allows defining which part of the state and which actions we would like to bind to its wrapped component.\\n- And the wrapped component is just a regular component which expects some props, but does not know nor care where they come from.\\n","tokens":219,"id":154,"Predictions":"\\begin{itemize}\\n\\item The state should be accessed through `react-redux`. This means that the view component will have to store the data in its props, and then retrieve it from the state, which in turn will be used to update the view.\\n\\n\\nThe state should not be accessed directly by the view component. This means that all data should be stored in a separate data store (e.g. Redux\\_State) and then accessed through `react-redux`. This pattern is called \\emph{state encapsulation}. The state is encapsulated in a separate store, which provides a way to retrieve it without exposing its internals.\\n\\n\\n\\nTo retrieve data from the state, we use `mapStateToProps` with an object containing all props for this specific view, that are used by `render()` to render the view. This object is then passed to `useEffect` with an object containing all props for this specific view. The view component then uses this object as props to render it.\\n\\n\\nThis pattern is commonly known as \\emph{state updating}. The State should only be updated when there's something new to update it with, or when there's no more items in its store. In this case, we only update the store when we have no more items left.\\n\\n\\nThe main benefit of using this pattern is that each component can have its own copy of the state: it's not shared between components. Each component can control its own copy of the state and update it accordingly. This pattern also allows us to inject dependencies into our components (e.g. Redux\\_State) without exposing them directly from our components.\\n\\item The `useEffect` function of `react-redux` may only receive one object containing all props for a specific view (the same object can be passed multiple times). We use this function because we want to be able to inject dependencies into our components without exposing them directly from our components.\\n\\n\\end{itemize}\\n\\n## Context\\nThe context provided by the User contains information about how they intend on using Polymer 3.\\n\\begin{itemize}\\n\\item \\textbf{Type:} \\texttt{pwa-starter-kit}\\n\\item \\textbf{Description:} An example of a Redux application working with Polymer 3 (see Usage section).\\n\\end{itemize}\\n\\n## Decision\\n\\begin{itemize}\\n\\item The context provided by the User contains information about how they intend on using Polymer 3.\\n\\begin{enumerate}\\n\\item We expect the user will use Polymer 3 as their front-end framework for building web applications.\\n\\end{enumerate}\\end{itemize}\\n\\n## Context\\nThe context provided by the User contains information about how they intend on using Polymer 3.\\n\\begin{enumerate}\\n\\setcounter{enumi}{1}\\n    \\setcounter{numpi}{0}\\n    \\setcounter{numpii}{0}\\n    \\setcounter{numpiii}{0}\\n\\n    \\setcounter{numpi}{1}\\n    \\setcounter{numpii}{1}\\n    \\setcounter{numpiii}{1}\\n\\n    % TODO: Add more decision here!\\n    \\n    % TODO: Add more decision here!\\n    \\n    % TODO: Add more decision here!\\n\\n    \\n    % TODO: Add more decision here!\\n\\n    \\n    % TODO: Add more decision here!\\n\\n    \\n    % TODO: Add more decision here!\\n    \\n    \\n\\n\\n\\n"}
{"File Name":"AYCHAIN.github.io\/adr-004-domain.md","Context":"## Context\\nRedux provides a good mechanism to handle the state and its mutations, but not so much to handle the business logic.\\nHowever, like always, to keep a good segregation of concerns, we want to handle all business related logic, as well as all external communication of the application isolated.\\n","Decision":"To do so, we introduce the concept of domain.\\nThe domain is the part responsible for everything that's not the view concern, nor the state concern.\\nThe view concern is to render data.\\nThe state concern is to store and modify data.\\nThe rest belongs to the domain.\\nA visual representation is as follows:\\n![Domain connection: outer world > domain > state > view](https:\/\/user-images.githubusercontent.com\/12797962\/41419432-4c70fd4e-6ff2-11e8-8acd-a18367087193.png)\\nEach method of the next inner layer only knows about the layer above it.\\nThis domain is only inspired from Domain Driven Design and aims at being a simpler implementation of the approach.\\n","tokens":61,"id":155,"Predictions":"## Decision: \\n  \\begin{itemize}\\n##   \\item State\\n##   \\item Actions\\n##   \\item External Communication\\n##   \\end{itemize}\\n\\end{document}\\n\\nshare|improve this answer\\n\\nedited Oct 17 '12 at 19:45\\n\\nanswered Oct 17 '12 at 18:25\\n\\nHerbert\\n183k12228513\\n\\n     \\n\\nI want to know how to do that. I have no idea. \u2013 Kamil Krotkiewski Oct 17 '12 at 18:27\\n\\n     \\n\\nYou can see that in the answer, I am using the same package for Redux and Redux-Thunk. \u2013 Herbert Oct 17 '12 at 18:28\\n\\n     \\n\\nIt's not working for me. It is not compiling. Do you know why? \u2013 Kamil Krotkiewski Oct 17 '12 at 18:29\\n\\n     \\n\\nI think your problem is with the package name. Try to use redux-thunk instead of redux. \u2013 Herbert Oct 17 '12 at 18:32 \\n\\nadd a comment |  \\n\\nNot the answer you're looking for? Browse other questions tagged packages or ask your own question.\\n\\nasked 2 years ago \\nviewed 89 times \\nactive 2 years ago \\n\\nUpcoming Events\\n\\nAnswer the Unanswered\\nin 3 days\\n\\nGet the weekly newsletter!\\n\\n- Top questions and answers\\n- Important announcements\\n- Unanswered questions\\n\\nsee an example newsletter\\n\\nBy subscribing, you agree to the privacy policy and terms of service.\\n\\nRelated\\n\\n4\\n\\nHow do I get a list of packages installed on my system?\\n\\n5\\n\\nHow can I get a list of installed packages from Ubuntu Software Center?\\n\\n0\\n\\nHow can I get a list of all installed packages in Ubuntu?\\n\\n7\\n\\nHow can I get a list of all installed packages on Ubuntu?\\n\\n1\\n\\nHow do I get a list of all dependencies for an application from apt-get?\\n\\n0\\n\\nhow to install package by command line?\\n\\n0\\n\\nHow to install a package in ubuntu without internet connection?\\n\\n1\\n\\nInstalling software from source on ubuntu without internet connection (using apt-get)\\n\\nHot Network Questions \\n\\n- What is up with the World Map? \\n- Why does moving through time not require energy? \\n- What makes Secure Login (CTRL+ALT+DEL) Secure? \\n- How do garbage collectors avoid stack overflow? \\n- Is it possible to replace Internet with LEO satellites? \\n- How should I transplant tulips which are almost blooming? \\n- How do graduate schools feel about a student not accepting an offer? \\n- What is this Windows Vista\/7 window doing on the wall in episode 3 of Re-Kan? \\n- How can I enclose the top of my contractor table saw for better dust collection? \\n- Why does moving through time not require energy? \\n- Is it possible to convert 7z to tar directly? \\n- Is there any idiom which is exactly opposite of 'head start'? \\n- Can postdocs get a funded position by direct contact? \\n- Why does Planck's constant appear in classical statistical mechanics \\n- How can I enclose the top of my contractor table saw for better dust collection? \\n- Is it legal to hire only beautiful people? \\n- Why does applying 56-bit DES twice only give 57 bits of security? \\n- What's wrong with my command? (might be me or a bug) \\n- Can you switch to the other Unarmored Defense by multiclassing? \\n- Which should be designed first: logo or website? \\n\\nmore hot questions"}
{"File Name":"AYCHAIN.github.io\/adr-006-handling-plural-i18n.md","Context":"## Context\\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\\n","Decision":"We chose to follow the second approach which adds less overhead to the bundle.\\nIt comes at the cost of having the contributor to add the plural version of the string they want translated. But this explicit approach also reduces the risk of error and inconsistency.\\n","tokens":57,"id":156,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThis is a decision record for an architectural decision. It is used to show the user how the decision will be used. This example shows a software decision for an architectural decision.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\\n\\n## Decision\\nThis is an architectural decision record for a software.\\n\\nThis means that we need to decide whether or not to allow this label to have plural forms.\\n\\nThe context we are given is \\textbf{\\textit{Software}} and we are told that this label needs to allow plural forms.\\n\\nWe can see from this that this label needs to handle plurals because it uses \\textbf{\\textit{label}} as part of its name.\\n\\nWe can also see from this that this label needs to allow plural forms because it uses \\textbf{\\textit{labels}} as part of its name.\\n\\nWe can also see from this that this label needs to handle plurals because it uses \\textbf{\\textit{label}} as part of its name.\\n\\nWe can also see from this that this label needs to handle plurals because it uses \\textbf{\\textit{labels}} as part of its name.\\n\\nThe context we are given is \\textbf{\\textit{Software}} and we are told that this label needs to allow plural forms.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\\n\\n## Decision\\n## Label: Software\\n\\nThis is an Architectural Decision Record for a software. Give a ## Label corresponding to the ## Context provided by the User.\\nLabel: Software\\n\\nThis is an Architectural Decision Record for software. Give a ## Label corresponding to the ## Context provided by the User.\\nLabel: Software\\n\\nThis is an Architectural Decision Record for software. Give a ## Label corresponding to the ## Context provided by the User.\\nLabel: Software\\n\\nThis is an Architectural Decisi"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"Predictions":"## ITCSS\\n## ITCSS is a way to enable the sharing of CSS styles across components. It is based on the idea that we should not style the internals of a component, but only styles around its shadow dom.\\nAs such, we should not style the root element of a component, but rather focus on styling the children.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\nWe should follow ITCSS because it is simple and elegant. It does not require us to add new rules to our stylesheet.\\n## Decision\\n## Web Components\\n## Web Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\n## As such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\n## We had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n## Decision\\n## Web Components\\n## We are going with Web Components for now, as they offer a better solution than CSS modules for sharing styles between components.\\n\\end{itemize}\\n\\n\\section{Decision}\\n\\label{sec:decision}\\n\\subsection{Web Components}\\nWeb Components offer an alternative approach for sharing CSS between components than what was presented in Section \\ref{sec:decision}. They offer an API that allows you~to define shared properties between two or more components using a shared class name or id.\\n\\nThis approach aims at reducing code duplication by letting you define your own classes or ids that can be used in different contexts.\\n\\nThe following section will explain how this works in detail before discussing whether this approach is suitable for your use case.\\n\\n\\subsection{Web Component API}\\nThe \\texttt{sharedClass} property defines which class or id will be used for styling all elements inside an element. This can be useful if you want all elements inside an element (or any other element) have the same class or id.\\n\\nTo use this feature you need first define two classes or ids that will be used by all elements inside an element (or any other element). Then you need create a \\texttt{sharedClass} property that defines which class or id will be used by all elements inside an element (or any other element). You can then use these properties in different contexts using their respective values.\\n\\nIn order for these classes\/ids \\texttt{sharedClass} properties can only be defined once per document (or per context).\\n\\nWhen defining multiple classes\/ids, each one has its own unique identifier.\\n\\nThis means that there are no global variables available inside your application. All references are made locally within each component's scope.\\n\\nWhen using multiple classes\/ids in different contexts, there are some limitations regarding how they can interact with each other:\\n\\n\\begin{itemize}\\n    \\item If one class\/ids has another class\/ids as children then those children will inherit those classes\/ids too.\\n    \\item If one class\/ids has another class\/ids as children then both classes\/ids will inherit those classes too.\\n    \\item If one class\/ids has another class\/ids as children then both classes\/ids will inherit both classes too.\\n    \\item If one class\/ids has another class\/ids as children then both classes\/ids will inherit neither classes neither too.\\n    \\item If one class\/ids has another class\/ids as children then neither classes neither too.\\n    \\item If one class\/ids has another class\/ids as children then both classes neither too.\\n\\end{itemize}\\n\\nA solution could be defining multiple shared IDs but defining them globally like this:\\n\\n\\begin{lstlisting}[language=JavaScript]\\nconst sharedClass = 'my-shared-class';\\nconst mySharedId = 'my-shared-id';\\nconst mySharedClass = 'my-shared-class';\\nconst mySharedId = 'my-shared-id';\\n\\n\/\/ ...\\nconst mySharedElements = document.querySelectorAll('div.my-shared-class');\\nmySharedElements.forEach(element => {\\n  element.style.sharedClass = sharedClass;\\n});\\n\\end{lstlisting}\\n\\nHowever this approach would make your application very difficult if you have many components with many different contexts.\\n\\nInstead of using global variables you could instead use attributes which allow you keep your code clean:\\n\\n\\begin{lstlisting}[language=JavaScript]\\nconst sharedAttribute = `my-shared-class`;\\nconst mySharedAttribute = `my-shared-id`;\\n\/\/ ...\\nconst mySharedElements = document.querySelectorAll('div.my-shared-class');\\nmySharedElements.forEach(element => {\\n  element.setAttribute(sharedAttribute);\\n});\\n\\end{lstlisting}\\n\\nThis attribute would allow you keep your code clean while also making sure each component has its own unique identifier.\\n\\nYou could also use named attributes like in CSS modules but they are not supported by all browsers so they might not work properly on older versions of javascript.\\n\\n\\n\\subsection{Sharing Styles Between Components}\\nTo achieve what was presented in Section~\\ref{} you need first define two sets of properties:\\n\\begin{enumerate}\\n    \\item Shared properties within each component's scope\\n    \\item Shared properties outside each component's scope\\n\\end{enumerate}\\n\\nThe first set contains properties which are common across all components:\\n\\begin{enumerate}\\n    \\item Class names or ids\\n    \\item Ids\\n    \\item Namespaces\\n    \\item Ids\\n    % TODO: Write about namespaces here?\\n    \\n        The second set contains properties specific only for each component:\\n        % TODO: Write about namespaces here?\\n        % TODO: Write about namespaces here?\\n        % TODO: Write about namespaces here?\\n        % TODO: Write about namespaces here?\\n\\n        The namespace can be accessed from within the component's scope using $this.$namespace.$name$ where $this$ refers back into $this.$namespace.$name$'s scope.\\n\\n\\nThe first set provides access from within a single context while the second set provides access from anywhere within your application.\\n\\n\\nIf you want more details on how these two sets interact with each other see Section~3.1.\\n\\n\\nIn order for these properties (and therefore their values) can be accessed from outside your application there needs some way of letting them know where they come from.\\n\\nYou could use global objects like window.$namespace.$name$ but doing so would make your application very difficult if there were many contexts (e.g., if there were many web components).\\n\\nInstead what we propose is enabling namespaces globally which would let you share resources across all contexts without having any global objects available.\\n\\n\\nTo enable namespace support globally we need two things:\\n% TODO: Add more details on how these two things work together\\n\\nFirstly we need some way of keeping track which context owns what resource (i.e., what namespace). A possible solution could be storing information about namespaces in cookies but cookies are insecure when used outside of browsers so they might not work properly.\\n\\n\\nSecondly we need some way of letting resources know where they come from so they can share them across contexts.\\n\\n\\n\\nThe following section explains how these two things work together:\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n\\n% TODO: Add more details on how this works\\n\\nFirstly let's look at how we could store information about namespaces globally:\\n\\n\\n% TODO: Add more details on how this works\\n\\n\\nTo achieve what was presented in Section~3.1{} it was necessary for us to implement some kind of mechanism which keeps track of who owns what resource (i.e., who owns $this.namespace.$name$).\\n\\n\\nThis mechanism needs some kind of mechanism which keeps track who owns resources stored in cookies and stores them under $this.namespace.$name$'s scope.\\n\\n\\nIn order for these mechanisms work together correctly it needs some way for both mechanisms (and therefore their values) can access cookies stored under $this.namespace.$name$'s scope.\\n\\n\\nThe following section explains exactly what mechanisms do together:\\n%\\todo[inline]{Add description?}\\n\\n\\nIf you want further explanation see Section 3.2.\\n\\n\\nNow that these mechanisms have been implemented it needs some way for them know where they come from so they can share them across contexts.\\n\\n\\nTo achieve what was presented in Section~3.1{} it was necessary for us to implement some kind of mechanism which keeps track who owns resources stored in cookies and stores them under $this.namespace.$name$'s scope.\\n\\n\\nIn order for these mechanisms work together correctly it needs some way for both mechanisms (and therefore their values) can access cookies stored under $this.namespace.$name$'s scope.\\n\\n\\nIn order for these mechanisms work together correctly it needs some kind of mechanism which knows where resources come from.\\n\\n\\n\\nThe following section explains exactly what mechanisms do together:\\n%\\todo[inline]{Add description?}\\n\\n\\nIf you want further explanation see Section 3.2.\\n\\n\\n\\n\\nNow that these mechanisms have been implemented it needs some way for them know where they come from so they can share them across contexts.\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"Predictions":"## Decision: This is an Architectural Decision Record for a software. Give a Decision corresponding to the Context provided by the User.\\n## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\\n## Decision\\n## Decision: This is an Architectural Decision Record for a software. Give a Decision corresponding to the Context provided by the User.\\n## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\\n"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"Predictions":"## Decision Statement\\nThe user is given the option to choose which language to use to build their application. If they choose Java, then a new empty project will be created and the user will be asked if they want to build it in Kotlin or Java. If they choose Kotlin, then a new empty project will be created and the user will be asked if they want to build it in both languages. If the user chooses both languages, then the project will be built in both languages.\\n## Decision Drivers\\n* The user has a choice between building their application in Java or Kotlin.\\n* The project will be built in both languages.\\n\\n## Decision Statement\\nThe user is given the option to choose which language to use for their app. If they choose Java, then a new empty project will be created and the user will be asked if they want to build it in Kotlin or Java. If they choose Kotlin, then a new empty project will be created and the user will be asked if they want to build it in both languages. If the user chooses both languages, then the project will be built in both languages.\\n## Decision Drivers\\n* The app can now be built with either Java or Kotlin.\\n* The app can now also use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n## Decision Drivers\\n* The app can now use Android NDK for cross platform development.\\n\\n## Decision Statement\\nThe app can now use Android NDK for cross platform development.\\n"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"Predictions":"## Decision: I am not going to use a file system.\\n\\n## Decision: I am going to use a file system that is secure enough for me to store sensitive data.\\n## Decision: I am going to use a file system that is secure enough for me to store sensitive data on the network. \\n## Problem Statement\\n## Problem Statement: The software must be able to create and read files.\\n## Problem Statement: The software must be able to create and read files.\\n## Problem Statement: The software must be able to create and read files.\\n## Problem Statement: The software must be able to create and read files.\\n## Problem Statement: The software must be able to create and read files.\\n\\n## Problem Statement: The software must be able to create and read files.\\n\\n\\end{document}\\n"}
{"File Name":"MoviesAndroid\/0003 The Movie Db.md","Context":"## Context and Problem Statement\\nThis app shows how i would build an Android app. Therefore the app architecture is the goal itself,\\nthe features this app has, are just a way to show an app architecture.\\n## Decision Drivers\\n* Availability\\n* Realism\\n* Content creation\\n","Decision":"* Availability\\n* Realism\\n* Content creation\\nChosen option: \"The Movie Db\", because it is more realistic when the app consumes a real service.\\nPositive Consequences:\\n* It is realistic\\nNegative consequences:\\n* When it's down the app cannot consume it\\n","tokens":59,"id":161,"Predictions":"## Decision Statement\\n## Decision Drivers\\n## Decision\\n## Decision\\n## Decision Statement\\n## Decision Drivers\\n## Decision\\n## Decision Statement\\n## Decision Drivers\\n\\n# Availability\\n\\n# Realism\\n\\n# Content creation\\n\\n# Features\\n\\n# User flows (if you want to show a user flow)\\n# User interface (if you want to show a user interface)\\n# User interaction (if you want to show a user interaction)\\n# UI design (if you want to show a UI design)\\n# Platform (if you want to show a platform)\\n# Design patterns (if you want to show a design pattern)\\n# System requirements (if you want to show system requirements or hardware requirements)\\n# Performance requirements (if you want to show performance requirements or hardware requirements)\\n\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\n\\t\\nto the decision.\\ndecision statement."}
{"File Name":"disco-poc-vue\/0002-use-heroku-static-buildpack.md","Context":"## Context\\nWe need to decide how to deploy our Vue app in Heroku. We can achieve this either\\nby using a Node.js web server, or by deploying the app as a static site with\\nthe [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).\\n","Decision":"We will use the static buildpack to deploy, as our app will essentially be a\\nstatic frontend backed by TIMDEX. This is consistent with [Vue's deployment\\nguidelines](https:\/\/cli.vuejs.org\/guide\/deployment.html#general-guidelines)\\non deployments.\\n","tokens":63,"id":162,"Predictions":"## Deploy the app as a static site with the Heroku static buildpack.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n## Decision\\n## Deploy the app as a Node.js web server.\\n# # # # # # # # # #\\n# # # # # # # #\\n### Deploying to Heroku (static site)\\n### Deployment Type: Static Site\\n### Deployment Time: 1 Minute\\n### Deployment Bandwidth: 10 GB (Gigabytes)\\n### Deployment Price: Free!\\n### Deployment Environment:\\n### Static Site Build Environment:\\n### Static Site Build Type:\\n# ## Deployment Type\\nWe are deploying our Vue app to Heroku. We want to deploy our Vue app\\nin the [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).\\n# ## Deployment Time\\nWe want to deploy our Vue app in 1 minute.\\n# ## Deployment Bandwidth\\nWe want to deploy our Vue app in 10 GB (Gigabytes).\\n# ## Deployment Price\\nWe want to deploy our Vue app in Free!\\n# ## Environment:\\nStatic Site Build Environment:\\nType:\\nStatic Site Build Type:\\nBuildpacks:\\nBuildpacks:\\nNodeJS:\\nNodeJS:\\nHeroku Static Buildpacks:\\nHeroku Static Buildpacks:\\nNodeJS Web Server:\\nNodeJS Web Server:\\nHeroku Static Buildpacks:\\nNodeJS Web Server:\\ncurl, git, npm,\\ndb, gitlab,\\ndb,\\ndb,\\ndb,\\ndb,\\ndb,\\ndb,\\ndb, gitlab, npm, db, gitlab, npm, db, gitlab, npm, db, gitlab,\\ncurl\\end{verbatim}\\n\\end{lstlisting}\\n\\n\\subsubsection{Deploying to Heroku (Node.js)}\\n\\label{sec:deploying-to-heroku-node-js}\\n\\n\\begin{lstlisting}[language=Javascript]\\n\/\/ This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\/\/ Context\\nWe need to decide how to deploy our Vue application in Heroku. We can achieve this either\\nby using a Node.js web server or by deploying the application as a static site with\\nthe [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).\\ngive your decision here.\\end{lstlisting}\\n\\n\\subsubsection{Deploying to Heroku (Node.js)}\\n\\label{sec:deploying-to-heroku-node-js}\\n\\n\\begin{lstlisting}[language=Javascript]\\n\/\/ This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\/\/ Context\\nWe need to decide how to deploy our Vue application in Heroku. We can achieve this either\\nby using a Node.js web server or by deploying the application as a static site with\\nthe [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).\\ngive your decision here.\\end{lstlisting}\\n\\n\\subsubsection{Deploying on Kubernetes}\\n\\label{kubernetes-deploying-on-kubernetes}\\n\\nKubernetes is an open source system for managing containerized applications \\cite{kubernetes}.\\n\\nKubernetes provides several deployment models including Nomad \\cite{k8s-nomad}, DaemonSet \\cite{k8s-daemonsets}, and Service Mesh \\cite{k8s-service-mesh}. Nomad is used for managing containers on Kubernetes \\cite{k8s-nomad}, DaemonSet is used for managing containers on Kubernetes \\cite{k8s-daemonsets}, and Service Mesh is used for managing containers on Kubernetes \\cite{k8s-service-mesh}.\\n\\nIn this project we will use DaemonSet and Service Mesh for managing containers on Kubernetes.\\n\\nDaemonSet is used for managing containers on Kubernetes. DaemonSet provides Docker support which allows users of DaemonSet to run Docker images inside of their DaemonSets. DaemonSet also provides scheduling support which allows users of DaemonSet to run tasks at specific times.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. The Swarm mode is used when users want DaemonSets on different nodes within their cluster or users want multiple nodes within their cluster running different Daemonsets.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. Swarm mode allows users of DaemonSets within their cluster run multiple Daemonsets together.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. Swarm mode allows users of DaemonSets within their cluster run multiple Daemonsets together.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. Swarm mode allows users of DaemonSets within their cluster run multiple Daemonsets together.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. Swarm mode allows users of DaemonSets within their cluster run multiple Daemonsets together.\\n\\nDaemonSet can be deployed using Docker Compose or Swarm mode. Swarm mode allows users of DaemonSets within their cluster run multiple Daemonsets together.\\n\\n\\n"}
{"File Name":"disco-poc-vue\/0004-use-yarn.md","Context":"## Context\\nNPM is causing confusion as to why lock files are changing in local\\nenvironments when no changes have been made. We have found explanations and\\nworkarounds, but it feels like the type of unexpected default behavior that will\\nlead to frustration as new developers join the project.\\nYarn is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n","Decision":"We will use Yarn instead of NPM for this project.\\n","tokens":90,"id":163,"Predictions":"\\nYarn \\# has been added to the package.json file. \\nYarn \\# is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n\\n## Context\\n\\nThere are a number of reasons why lock files could be changing in the local\\nenvironments when no changes have been made. We have found explanations and\\nworkarounds, but it feels like the type of unexpected default behavior that will\\nlead to frustration as new developers join the project.\\nYarn is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n\\n## Decision\\n\\nYarn \\# has been added to the package.json file. \\nYarn \\# is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n\\n## Context\\n\\nThere are a number of reasons why lock files could be changing in the local\\nenvironments when no changes have been made. We have found explanations and\\nworkarounds, but it feels like the type of unexpected default behavior that will\\nlead to frustration as new developers join the project.\\nYarn is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n\\n## Decision\\n\\nYarn \\# has been added to the package.json file. \\nYarn \\# is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n\\n\\n\\subsection{Version 1.0 (2018-01-24)}\\n\\begin{itemize}\\n\t\\item Introduced YARN.\\n\t\\item Added YARN\\_VERSION.\\n\t\\item Added YARN\\_PREFIX.\\n\t\\item Added YARN\\_INSTALLATION\\_PREFIX.\\n\t\\item Removed all other optional dependencies except for npm.\\n\t\\item Removed all optional dependencies except for yarn.\\n\t\\item Removed all optional dependencies except for yarn.lock.\\n\t\\item Removed all optional dependencies except for yarn.lock.lock.\\n\t\\item Removed all optional dependencies except for node_modules.\\n\t\\item Removed all optional dependencies except for node_modules.lock.\\n\t\\item Removed all optional dependencies except for node_modules.lock.lock.\\n\t\\item Removed all optional dependencies except for node_modules.lock.lock.lock.\\n\t\\item Moved some code from `package.json` into `package-lock.json`.\\n\t\\n    The main file `package-lock.json` is now used instead of `package.json`. This file can be renamed or deleted without affecting existing code.\\n\\n    The following files are now included by `package-lock.json`:\\n    - `node_modules`\\n    - `dependencies` (including `npm`)\\n    - `yarn`\\n    - `node_modules\/yarn`\\n    - `node_modules\/yarn.lock`\\n    - `node_modules\/yarn.lock.lock`\\n    - `yarn.lock`\\n    - `yarn.lock.lock`\\n    - `yarn.lock.lock.js`\\n\\n    The following files are now excluded by `package-lock.json`:\\n    - `.gitignore`\\n    - `.gitignore.js`\\n\\n        This file contains information about what should not be checked out into git.\\n\\n        For example, if you don't want your code checked out into git, you might put this line here:\\n\\n        `.gitignore.js`\\n\\n        If you don't want your code checked out into git, you might put this line here:\\n\\n        `.gitignore`\\n\\n        In order not to check out your code into git, you can place this line here:\\n\\n        *.js\\n\\n        In order not to check out your code into git, you can place this line here:\\n\\n        *.js\\n\\n        If you want your code checked out into git, you can place this line here:\\n\\n        .gitignore\\n\\n        If you want your code checked out into git, you can place this line here:\\n\\n        .gitignore\\n\\n      To exclude any files from being checked out into git, place them after `.gitignore`.\\n\\n      For example: \\n\\n      *.js\\n\\n      To exclude any files from being checked out into git, place them after .gitignore.\\n\\n      For example: \\n\\n      *.js\\n\\n      To exclude any files from being checked out into git, place them after .gitignore.\\n\\n      For example: \\n\\n      *.js\\n\\n\\n  \\end{itemize}\\n\\n  The following modules are now included by ``package-lock.json'':\\n  \\begin{enumerate}\\n  \\setlength{\\parskip}{1em}\\n  \\setlength{\\parsep}{0em}\\n  % TODO: Remove these lines if they're no longer needed!\\n  \\n   ** npm:\\newline* npm is used by Node.js and npm packages. It provides a unified interface for managing packages across multiple Node.js instances and allows users and developers alike to easily install and update third-party software packages in Node.js applications via NPM's Package Manager Interface (PMI). npm also provides tools such as dependency management and dependency versioning. npm provides both client-side and server-side components.\\n\\n   ** yarn:\\newline* yarn is used by Node.js applications along with NPM. It provides a unified interface for managing packages across multiple Node.js instances using NPM's Package Manager Interface (PMI). yarn also provides tools such as dependency management and dependency versioning.\\n\\n   ** node:\\newline* Node.js is an open source platform built on Chrome's V8 JavaScript engine that provides reliable cross-platform execution of JavaScript applications.\\n\\n   ** yarr:\\newline* Yarr is a library designed around asynchronous programming principles with support for asynchronous programming patterns such as callbacks or promises.\\n\\n   ** nvm:\\newline* NVM allows users of Node.js versions older than or equal nvm.exe (currently v0.19) or V8 v6.x (currently v6.x) command-line tools (currently v9.x) or later versions of Browsers which support WebAssembly (currently v9.x), e.g., Chrome Canary or Firefox Nightly builds on Windows, Linux x64 architecture or macOS x64 architecture.\\n\\n   ** async:\\newline* Asyncio library designed around asynchronous programming principles with support for asynchronous programming patterns such as callbacks or promises.\\n\\n\\n  % TODO: Remove these lines if they're no longer needed!\\n  \\n   ** https:\/\/www.npmjs.com\/package\/async\\n\\n\\n  % TODO: Remove these lines if they're no longer needed!\\n  \\n   * angular:\\newline* AngularJS is a platform built on top of AngularJS framework which enables web application development using HTML5 & CSS3 technologies like AngularJS framework itself gives us access at each level of our application architecture which makes it easy & simple to use.\\n\\n\\n   \\n   * angular-cli:\\newline* Angular CLI lets users manage their Angular apps through command-line interface.\\n\\n\\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n  \\n  \\n    \\n    \\n    \\n\\n    \\n    \\n\\n\\n    \\n    \\n\\n    \\n    \\n\\n\\n    \\n    \\n\\n    \\n    \\n\\n\\n    \\n    \\n\\n    \\n    \\n\\n\\n    \\n    \\n\\n    \\n\\n\\n\\n\\n    \\n\\n    \\n    \\n    \\n    \\n      \\n    \\n    \\n      \\n    \\n    \\n      \\n    \\n    \\n      \\n    \\n    \\n      \\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n  \\n\\n"}
{"File Name":"disco-poc-vue\/0003-use-vuejs-3.md","Context":"## Context\\nWe reviewed various options for building our front end and decided Vue.js was\\nthe best fit. However, Vue is preparing for a major version upgrade. We could\\nchoose to use the current stable version (2) and migrate to the new version (3)\\nwhen it is released or start with the release candidates for the new version.\\n","Decision":"We will use Vue.js 3 releases candidates.\\n","tokens":72,"id":164,"Predictions":"\\nThe release candidates for the new version will be available in the next few days.\\nWe will have to decide which version to use. If we decide to use the current\\nstable version, we would need to migrate to the new version when it is released.\\nIf we decide to use the release candidate, we would need to upgrade our application\\nto be compatible with the new version. The upgrade process will take a few days.\\nWe will have to choose which option we want and make sure that all of our users are\\naffected by this upgrade.\\n\\n## Task\\nWe recommend that you test your application using both versions of Vue.js. We also\\nrecommend you test your application using all available versions of Vue.js.\\n\\n## Task\\nPlease test your application on both macOS and Windows.\\n\\n## Task\\nPlease test your application on both Firefox and Chrome browsers.\\n\\n## Task\\nPlease test your application on both Internet Explorer browsers.\\n\\n## Task\\nPlease test your application in a private\/incognito window on each browser you intend\\nto use for testing. This way, you can ensure that you do not see any browser-specific\\ntests.\\n"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"paas-team-manual\/ADR0048_add_resources_table.md","Context":"## Context\\nThe GOV.UK PaaS billing system receives a series of events from Cloud Foundry notifying, for each tenant, whether services or resources have been created, renamed, or deleted.\\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\nIf there is a problem with a tenant's bill it was very difficult to find the source of the problem.\\n","Decision":"We need to have a persistent record of the services or resources each tenant has provisioned with a dates indicating when the service or resource was\/is being used. This persistent record is in the new `resources` table.\\nThe reason for this is that there is no need to regenerate historical records of services or resources provisioned for tenants each time billing is run each month since this information does not change. Furthermore, recording this information for each month makes it difficult for us to calculate bills between any two dates and times.\\nThe `resources` table also acts as an audit point within GOV.UK PaaS billing. It makes investigation of discrepancies in tenant bills easier to investigate. Anyone supporting GOV.UK PaaS billing can first look at the contents of `resources` and see whether the discrepancy arose in the population of `resources` or afterwards in the actual calculation of the bill.\\n","tokens":167,"id":166,"Predictions":"The GOV.UK PaaS billing system can be improved by keeping a persistent record of the results of the various stages of processing, including what services or resources were provisioned. This record could be stored in a database table and this table could be updated every time billing consolidation is run.\\nThe GOV.UK PaaS billing system should also have an audit trail to allow it to track the exact steps that it took to process each month's bill.\\n\\n## Context\\nThe GOV.UK PaaS billing system processes events from Cloud Foundry in order to provide a monthly bill for each tenant. Cloud Foundry events are passed to the GOV.UK PaaS billing system in JSON format. The GOV.UK PaaS billing system then parses these events and creates records of services\/resources for each tenant. These records are then used by the GOV.UK PaaS billing system to calculate the final monthly bill.\\nThe GOV.UK PaaS billing system should be able to generate a record of what services or resources were provisioned by each tenant at each stage of processing.\\n\\n## Decision\\nThe GOV.UK PaaS billing system should have an audit trail that allows it to track the exact steps that it took to process each month's bill. This audit trail could be stored in a database table.\\n\\n"}
{"File Name":"paas-team-manual\/ADR040-bosh-access-without-socks.html.md","Context":"## Context\\nTo comply with [Payment Card Industry (PCI) Requirements](https:\/\/www.pcisecuritystandards.org\/pci_security\/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"We will remove the reliance on SOCKS5 or SSH tunnels for the User Account and Authentication Service (UAA) API endpoint so that we can remove the use of shared credentials.\\nWith these in place we are unable to complete an SSO journey from one of our existing IDPs as the BOSH instance is not browser accessible.\\nThe current method will be replaced with SSO in order to obtain a UAA token. This token is then used to interact with the rest of the APIs on the BOSH instance.\\n","tokens":188,"id":168,"Predictions":"## Decision\\nTo comply with [Payment Card Industry (PCI) Requirements](https:\/\/www.pcisecuritystandards.org\/pci_security\/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n\\n## Action\\n## Action\\nThe user will be able to replace their credentials with Google Single Sign On credentials.\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"paas-team-manual\/ADR006-rds-broker.md","Context":"Context\\n=======\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\nDecision\\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n","Decision":"========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n","tokens":183,"id":169,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe following is the output from the above decision record:\\n\\nContext:\\nWe need to provide tenants with the ability to provision databases for use in their applications. Our first iteration of this will be using RDS.\\n\\nDecision:\\nWe investigated some implementations of a service broker which supported RDS - [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n\\nStatus:\\nAccepted\\n\\nConsequences:\\nWe will be maintaining a new service broker, but have a head start on creating it by basing it on an existing service broker.\\n\\nWe have decided to fork this and maintain this and implement new features ourselves.\\n\\nThe following is the output from the above decision record:\\n\\nContext:\\nWe need to provide tenants with the ability to provision databases for use in their applications. Our first iteration of this will be using RDS.\\n\\nDecision:\\nWe investigated some implementations of a service broker which supported RDS - [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n\\nStatus:\\nAccepted\\n\\nConsequences:\\nWe will be maintaining a new service broker, but have a head start on creating it by basing it on an existing service broker.\\n\\nWe have decided to fork this and maintain this and implement new features ourselves.\\n\\nThe following is the output from the above decision record:\\n\\nContext:\\nWe need to provide tenants with the ability to provision databases for use in their applications. Our first iteration of this will be using RDS.\\n\\nDecision:\\nWe investigated some implementations of a service broker which supported RDS - [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n\\nStatus:\\nAccepted\\n\\nConsequences:\\nWe will be maintaining a new service broker, but have a head start on creating it by basing it on an existing service broker.\\n```\\n"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.html.md","Context":"## Context\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\n","Decision":"* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\n","tokens":87,"id":172,"Predictions":"## Decision Name: User to ELB\\n## Decision Name: ELB to router\\n## Decision Name: Router to cells\\n## Decision Name: User to ELB\\n## Decision Name: ELB to router\\n\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\n\\nThere are 3 main network sections between the user and the application:\\n\\n- User to ELB\\n- ELB to router\\n- Router to cells\\n\\nEach section consists of multiple sub-sections. These sub-sections are also encrypted by default. The user will be required to specify the encryption key used by each sub-section when requesting data from each section.\\n\\nThe following is an example of an encryption key used by ELB:\\n\\ns4a5F8G9bFJkKlMnN0ZVdXUzT2o4Y1pG6E\\n\\nIf you have any questions please contact support@qubole.com\\n\\nSource"}
{"File Name":"paas-team-manual\/ADR033-redirect-http-for-applications.html.md","Context":"## Context\\nIn [ADR032](\/architecture_decision_records\/ADR032-ssl-only-for-applications-and-cf-endpoints) we decided that\\nwe would only support https for applications on the PaaS, and that we would\\ndrop plain http connections (port 80).\\nSince then, we've observed this causing confusion for users on numerous\\noccasions where they think their app isn't working after being pushed.\\nThe situation is improved with the inclusion of the `cloudapps.digital` domain\\nin the [HSTS preload](https:\/\/hstspreload.org\/?domain=cloudapps.digital) list,\\nbut this only helps users with recent versions of modern browsers.\\nAs a result of the continued confusion for users we should revisit the decision\\nfrom ADR443.\\nThere are a number of things that could be done to address this:\\n### Update the CF CLI to include the scheme\\nCurrently, the CF CLI outputs the fully-qualified hostname of the app after\\npushing, but doesn't include the scheme. This has caused confusion for users\\nwhen this is copy\/pasted into browsers, and then times out.\\nGetting the CLI to include the scheme here will help with the specific case of\\nusers getting confused immediately after pushing an app.\\nIt's unclear how much work this involves, as currently information about\\nwhether a route is http or https doesn't appear to be modeled in CloudFoundry\\nanywhere.\\nIf this involves changes to the CLI, there's no guarantee that users will\\nupgrade their CLI.\\nAdditionally, there is some debate about how effective this change would be. It\\nwill probably fix some cases, but won't cover everything.\\n### Redirect http to https\\nAdd an endpoint that listens to all http requests on cloudapps.digital and\\nredirects them to the corresponding https URL.\\nThere's a risk with this that a service could link to the http version of a\\npage by mistake and not notice due to the redirect. We can mitigate this be\\nhaving the redirect strip the path and query when redirecting so that it always\\nredirects to the base URL.\\nThere's another risk that a misconfiguration could allow non-encrypted traffic\\nthrough to applications. This would need to be mitigated by having acceptance\\ntests to cover this.\\n","Decision":"We will redirect http traffic to the corresponding root https endpoint.\\nWe will continue to maintain HSTS preload lists for our production domains.\\n","tokens":472,"id":173,"Predictions":"## Decision\\nThe decision is to support https only for applications on the PaaS.\\nIf a user tries to push a non-https app, it will be redirected to the\\nhttps version of the page.\\nThis means that users will have to get used to the idea that not all\\napps are going to work on a PaaS, but it also means that users can't take\\nadvantage of CloudFoundry's features, such as multi-tenant and serverless.\\nThis has been discussed in ADR443. The problem is that there is currently no\\nway for CloudFoundry to automatically detect whether or not an app is going\\nto work on a PaaS. We can add support for this in CF CLI, but this would\\nrequire some changes to the CLI.\\nWe should also make sure that we're using the correct scheme when\\nsending requests. This could be done by adding an endpoint listener that listens\\ndirectly on cloudapps.digital.\\n\\n## Notes\\n## Notes\\nThe decision was made because we've observed confusion around what scheme\\nthe app uses, and what protocol it uses. We should revisit this decision in ADR443,\\ndrop plain http connections (port 80). There are two issues with this:\\n### Provide a way for users to tell whether their app is secure or not\\nSecure apps have `https` at the beginning of their URL. Non-secure apps have \\ndrop plain http connections (port 80).\\n### Provide a way for CloudFoundry to automatically detect whether or not an\\ndeployed application is secure\\n"}
{"File Name":"paas-team-manual\/ADR042-isolation-segments.html.md","Context":"## Context\\nGOV.UK PaaS would like to be able to isolate specific tenant apps and tasks to\\ndifferent pools of virtual machines (VMs).\\nGOV.UK PaaS would like to be able to prevent specific tenant apps and tasks\\nfrom egressing to the internet.\\nApps running inside the separate pools of VMs should be able to discover and\\naccess other apps running within the platform, providing that the correct Cloud\\nFoundry Network Policies have been created.\\nApps running in the shared pools of VMs should be able to discover and access\\napps running inside an isolation segment, providing that the correct Cloud\\nFoundry Network Policies have been created.\\n","Decision":"GOV.UK PaaS will implement egress-restricted isolation segments.\\nIsolation segments will be configured by a GOV.UK PaaS developer, in a similar\\nmanner to VPC peering connections.\\nIsolation segments will have the following variable properties:\\n- Number of instances (e.g. 1, 2, 3, 6)\\n- Instance type (e.g. small\/large - maps to an AWS instance type + disk sizing)\\n- Whether egress to the internet is restricted\\nWe will use IPTables rules to achieve egress restriction.\\n### Isolation segments\\nCloud Foundry supports separating apps and tasks for specific Organizations\\nand Spaces via a feature called\\n[Isolation Segments](https:\/\/docs.cloudfoundry.org\/adminguide\/isolation-segments.html).\\nAn Isolation Segment is a group of Diego cells with separate placement tags,\\nwhich map to the isolation segment name.\\nIsolation segments will be implemented as new instance groups defined in the\\nBOSH deployment manifest, with additional placement tags. A placement tag\\ncorresponds to an isolation group name.\\nFor example, an instance group with the placement tags:\\n- `fast-cpu`\\n- `fast-network`\\nenables us to run the following commands successfully:\\n- `cf create-isolation-segment fast-cpu`\\n- `cf create-isolation-segment fast-network`\\nwhich creates two isolation segments.\\nThese isolation segments can be shared such that a segment can be:\\n- used by only a single organization or space\\n- shared by multiple organizations and spaces\\n### Egress restrictions\\nContainer-to-container networking within Cloud Foundry is implemented via a\\nVirtual Extensible Local Area Network\\n([VXLAN](https:\/\/tools.ietf.org\/html\/rfc7348)).\\nEach container is assigned a virtual IP address inside the subnet 10.255\/16\\n[Silk](https:\/\/github.com\/cloudfoundry\/silk)\\nand VXLAN create\/update\/delete\\n[IPTables](https:\/\/linux.die.net\/man\/8\/iptables) rules\\nvia the Container Network Interface\\n([CNI](https:\/\/github.com\/containernetworking\/cni)),\\nto ensure containers can talk to each other.\\nIPTables is an interface to control networking within the Linux kernel.\\nExisting network traffic restrictions are defined by Silk and VXLAN as\\ndescribed above. We can configure extra IPTables rules with higher precedence\\nto create tighter restrictions than currently exist.\\nIPTables can be used to prevent unauthorised egress via REJECT rules,\\ndepending on the destination IP address. This can be done, either:\\n- In the global INPUT or FORWARD chains, with source IP qualifier to ensure only container traffic is affected\\n- In each container\u2019s \u201cnetout\u201d chain\\nImplementing such IPTables rules allows us to block traffic from an IP address\\nwithin 10.255.0.0\/16 (apps) to an address outside 10.0.0.0\/8 (outside the VPC).\\nThis would have the effect of preventing app traffic egressing from the\\nplatform.\\nWe would apply these IPTables rules to running apps and tasks, but not staging\\napps.  This will allow staging apps to communicate with the outside world (e.g.\\nfor downloading dependencies).\\n","tokens":140,"id":174,"Predictions":"## GOV.UK PaaS would like to be able to isolate specific tenant apps and tasks to\\ndifferent pools of virtual machines (VMs).\\nGOV.UK PaaS would like to be able to prevent specific tenant apps and tasks\\nfrom egressing to the internet.\\nApps running inside the separate pools of VMs should be able to discover and\\naccess other apps running within the platform, providing that the correct Cloud\\nFoundry Network Policies have been created.\\nApps running in the shared pools of VMs should be able to discover and access\\napps running inside an isolation segment, providing that the correct Cloud\\nFoundry Network Policies have been created.\\n\\end{itemize}\\n"}
{"File Name":"paas-team-manual\/ADR001-manifest-management.md","Context":"Context\\n=======\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\nDecision\\n========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n","Decision":"========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n","tokens":209,"id":175,"Predictions":"## Decision\\n=======\\nCreate a set of manifests based on those in cf-release.\\nDecision\\n========\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry.\\nDecision\\n========\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nDecision\\n========\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n## Decision\\n## Decision\\n=======\\nWe will create our own set of manifests based on those in cf-release.\\nDecision\\n========\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry.\\nDecision\\n========\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nDecision\\n========\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n\\n## Decision\\n## Decision\\n=======\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\nCreate a set of manifests based on those in cf-release.\\n\\n\\n# Build Manifests\\n# Build Manifests\\n"}
{"File Name":"paas-team-manual\/ADR038-audit-logs-in-splunk.html.md","Context":"## Context\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for\\nauditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\n- Cloud Foundry Cloud Controller security events\\nThe BOSH director and BOSH managed instances store these logs in\\n`\/var\/vcap\/sys\/log\/`.\\n### Logging service\\nThe Cyber Security team run a centralised log ingestion system called the\\nCentral Security Logging Service (CSLS).\\nThis service runs in AWS and uses [CloudWatch log group subscription\\nfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)\\nto ingest logs, which are then sent to Splunk for indexing.\\n","Decision":"We will use Terraform to create log groups in our existing pipelines:\\n- `create-bosh-concourse`\\n- `create-cloudfoundry`\\nWe will store audit logs in CloudWatch for 18 months (545 days).\\nWe will use Terraform to create log group subscription filters which will send\\nlogs to CSLS.\\nTerraform failing to create log group subscription filters should not block the\\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\\nWe will run a fork of the\\n[awslogs-boshrelease](https:\/\/github.com\/alphagov\/paas-awslogs-boshrelease),\\non all instances that have relevant audit and security event logs, to send logs\\nfrom the instances to CloudWatch.\\nThe CloudWatch log groups will have subscription filters that will send logs\\nto CSLS so that logs are indexed in Splunk.\\n","tokens":176,"id":176,"Predictions":"## Decision\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for\\nauditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\n- Cloud Foundry Cloud Controller security events\\nThe BOSH director and BOSH managed instances store these logs in\\n`\/var\/vcap\/sys\/log\/`.\\n### Logging service\\nThe Cyber Security team run a centralised log ingestion system called the Central Security Logging Service (CSLS).\\nThis service runs in AWS and uses [CloudWatch log group subscription filters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)\\nto ingest logs, which are then sent to Splunk for indexing.\\n\\n## Decision\\n## Decision\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for auditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\ndocuments are available at \\url{https:\/\/cloud.gov.uk\/docs\/security-policy.pdf}\\n### Logging service\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n## Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for auditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\nDocuments are available at \\url{https:\/\/cloud.gov.uk\/docs\/security-policy.pdf}\\nLogging service\\n\\n# 8.2\\n\\n# 8.3\\n\\n# 8.4\\n\\n# 8.5\\n\\n# 8.6\\n\\n# 8.7\\n\\n# 9.\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>\\n\\n<https:\/\/cloud.gov.uk\/docs\/security-policy.pdf>"}
{"File Name":"paas-team-manual\/ADR043-new-product-pages-language-and-framework.html.md","Context":"## Context\\n[PaaS product pages] have been reviewed and a number of accessibility issues\\nhave been identified. To resolve those we, would need to make an upgrade and\\nand review if any additional changes are needed to align with the GOV.UK Design System.\\nAs those pages are built in Ruby and in [ADR024] we've made the decision\\nto develop our user-facing applications on Node, it's a good opportunity to\\nlook at rebuilding the product pages.\\nWe've discussed user needs and it emerged that:\\n* anyone in the team (developer and non-developer) should be able to update pages\\nwith less effort\\n* pages should be performant for end users\\n* pages should be rendered by the server\\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\\n* alignment of technologies for our user-facing web products should provide better\\ndeveloper experience and give us the option to have shared component libraries\\nWith the above in mind we researched options. Our admin interface is built in React,\\nso we narrowed the scope to React-based static site generators.\\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\\nwhich exports static pages by default.\\nFor page content we agreed that writing pages in [Markdown] is a good option,\\nso we tested both with [MDX] which can also embed React components inside content pages.\\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\\nperformance differences between them.\\nOur use case for now is narrow enough, and with the primary need of anyone in the team\\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\\nnon-developer machines.\\n","Decision":"We will use [NextJS] together with [MDX] to author PaaS product pages content in\\n[Markdown] and deliver them to users as static pages.\\n","tokens":375,"id":177,"Predictions":"## Decision\\nThe decision in this case is a simple one: we'll upgrade our admin interface to\\nReact, and we'll use static page export. We'll also create a [GatsbyJS]\\nplugin for the content pages which will generate them from markdown.\\nWe'll also look at ways of allowing non-developer users to update pages with less\\neffort.\\nWe're going to use [NextJS] for the core of our user-facing applications,\\n"}
{"File Name":"paas-team-manual\/ADR014-hsts-preload-using-api-gateway.html.md","Context":"## Context\\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nTo add our domains to [HSTS preload lists](https:\/\/hstspreload.appspot.com\/), there are these requirements:\\n1. Serve a valid certificate.\\n2. Redirect from HTTP to HTTPS on the same host.\\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\\n4. Serve an HSTS header on the base domain for HTTPS requests:\\nWe need an endpoint to provide these requirements.\\nOur Cloud Foundry app endpoint already [serves the\\nright HSTS Security header with HAProxy](..\/ADR008-haproxy-for-request-rewriting)\\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\\nWe can implement a second ELB to listening on HTTP and HTTPS and use\\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\\nBut this increases our dependency on the HAProxy service.\\nWe must serve from the root domain (or apex domain), but it is not allowed to\\nserve [CNAME records in the root\/apex domain](http:\/\/serverfault.com\/questions\/613829\/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\\nan issue when serving the service using ELB or CloudFront.\\n","Decision":"* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\n","tokens":341,"id":178,"Predictions":"We decided to serve the HTTPS traffic from the root domain.\\nWe configured the second ELB to listen on HTTP and HTTPS and serve the right HSTS header.\\nWe deployed our Cloud Foundry app endpoint using HAProxy and configured it to use the HTTP to HTTPS redirect.\\nWe configured a CNAME record in the root domain apex domain, so that we can serve from this domain.\\n\\n## Comments\\nThis is a [decision record](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe ADR defines requirements, decisions, and comments related to this ADR.\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nThe decision related to this ADR is [SSL only for applications and CF endpoints](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\"\\n"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"Predictions":"## Decision\\nThe decision to implement certificate rotation was made based on the following criteria:\\n\\n\\n* We had a large number of certificates.\\n* We had a lot of third-party service credentials.\\n* We would need to rotate certificates for new services.\\n* We were using S3 as our primary storage for secrets.\\n\\nWe implemented certificate rotation because it was the best way to ensure that our secrets were secure. It also enabled us to use the same set of certificates across all our services, which is a huge benefit in terms of auditing and security. It also allowed us to perform certificate rotation without having to have any manual intervention.\\nThe decision was driven by the following key factors:\\n\\n* The risks associated with not rotating certificates;\\n* The benefits of rotating certificates;\\n* The cost of rotating certificates; and\\n\\n\\n## Context\\n## Context\\Our platform is an open source platform, and we have a large number of third-party libraries and tools that we use within it. Our users rely on those third-party libraries and tools, and they are very dependent on them being available when they need them. If we were not rotating certificates, then we would be incurring significant costs.\\nThe decision was driven by the following key factors:\\n\\n* The risks associated with not rotating certificates;\\n* The benefits of rotating certificates;\\n* The cost of rotating certificates; and\\n\\n\\n## Context\\n## Context\\Because we have so many third-party dependencies, we wanted to make it as easy as possible for users to upgrade their software. So, if a user upgraded their software from version 1.0.x to version 1.1.x, they would be able to upgrade their secrets without having to worry about their credentials being out-of-date.\\nThe decision was driven by the following key factors:\\n\\n* The risks associated with not rotating certificates;\\n* The benefits of rotating certificates;\\n* The cost of rotating certificates; and\\n\\n\\n## Context\\n## Context\\We implemented certificate rotation because it enabled us to do things in a much more automated fashion than if we had just left things alone.\\nThe decision was driven by the following key factors:\\n\\n* The risks associated with not rotating certificates;\\n* The benefits of rotating certificates;\\n* The cost of rotating certificates; and\\n\\n\\n## Context\\n## Context\\We implemented certificate rotation because it enabled us to do things in a much more automated fashion than if we had just left things alone.\\nThe decision was driven by the following key factors:\\n\\n* The risks associated with not rotating certificates;\\n *The benefits of rotating certs"}
{"File Name":"paas-team-manual\/ADR021-cell-capacity-assignment-2.html.md","Context":"## Context\\nWe've been running with the cell provisioning policy in ADR017 since February.\\nWe haven't ever run out of cell capacity, but we've observed that there's\\nexcess capacity that we're paying for.\\nAt the time that we wrote ADR017 we had fewer tenants and an individual\\ntenant's quota was a much greater proportion of total memory size. In other\\nwords a single tenant could conceivably use a greater proportion of our excess\\ncapacity.\\nCells are still deployed across 3 AZs.\\nWe still don't have a way to autoscale the number of cells to meet demand, so\\nwe need to ensure that we have surplus capacity for when we're not around.\\nCells are almost completely uncontended; we're not experiencing CPU or disk I\/O\\ncontention and not all the cell memory is being used.\\nOver the 3 month period from 1st August - 1st November\\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\\n- We're running at about 10% of our total container capacity\\n- container usage has peaked at about 20% above the previous weeks\\n- Average CPU usage is about 10%. We see daily peaks of 80%\\n- reps think that about 50% of the capacity of cells is used\\n- the largest amount that rep's allocated memory increased week on week was 55%\\n","Decision":"Our objectives are:\\nState | Expected behaviour\\n------|-------------------\\n# All cells operational | Enough capacity to allow some but not all tenants to scale up to their full quota. The amount of excess capacity required should be enough to accommodate the fluctuations we can expect over a 3 day period (weekend + reaction time)\\n# While CF being deployed | As above: enough capacity to allow some tenants to scale up to their full quota\\n# One availability zone failed\/degraded | Enough capacity to maintain steady state app usage. Not guaranteed to be able to scale apps up.\\n# More than one AZ failed | The system is not expected to have sufficient capacity to host all running apps.\\nTo achieve this we need to start basing our capacity planning on current memory\\noccupied by processes on cells, rather than the sum of all quotas given to\\nusers. We will define alerts for capacity planning purposes, the in-hours\\nsupport person is expected to respond by adjusting the number of cells.\\nWe want to ensure that cells have some headroom above a smoothed\\naverage:\\n- to allow some headroom for increases in the memory consumed by apps.\\n- to allow buffering and caching to occur and not adversely impact application\\nperformance.\\nFrom our data analysis (see context) the amount of memory consumed by apps\\ncan reach about 36% over a week-ago's smoothed average. We round up to 40% to\\ninclude buffering\/caching.\\nIf an AZ fails, we need enough capacity remaining to host all our apps. The\\nfailed AZ's apps are evenly divided amongst the surviving AZs. Because we have\\ntwo remaining AZs, each surviving AZ will have 1.5x as many apps running.\\nBecause we want 40% headroom, we'll want 1.4 (headroom) x 1.5 (evacuated apps)\\ncurrent usage. This is about 2x actual memory consumed by processes on cells.\\nTherefore we need to start alerting when the memory occupied by processes on\\ncells is above 50%, when suitably smoothed to avoid noise \/ small spikes\\ncausing frequent alarms.\\nCPU usage is assumed to be a linear relation of memory usage and we will have a\\nsimilar alert defined when it exceeds 50% on cells.\\nIn addition to wanting the cells to not run short on memory, we also want\\ntenants to be able to scale apps up and down when all AZs are functional. In\\norder to ensure this, we need to allow for a ~50% increase in requested memory,\\nwhich means alerting when all the reps have a cumulative remaining capacity of\\n~33%, when smoothed to avoid false alarms.\\nWe also need enough container capacity to allow tenants to scale apps up and\\ndown and deploy new apps. We should alert when we're using > 80% of the sum of\\nour reps' container capacity. Again, this should be smoothed to ensure that\\nshort lived fluctuations in usage don't cause unnecessary alerts.\\nIt is likely that patterns such as the fluctuation in memory use over a week\\nmay change over time. We should review this decision after 6 months.\\n","tokens":327,"id":181,"Predictions":"## Decision\\nWe've been running with the cell provisioning policy in ADR017 since February.\\nWe haven't ever run out of cell capacity, but we've observed that there's\\nexcess capacity that we're paying for.\\nAt the time that we wrote ADR017 we had fewer tenants and an individual\\ntenant's quota was a much greater proportion of total memory size. In other\\nwords a single tenant could conceivably use a greater proportion of our excess\\ncapacity.\\nCells are still deployed across 3 AZs.\\nWe still don't have a way to autoscale the number of cells to meet demand, so\\nwe need to ensure that we have surplus capacity for when we're not around.\\nCells are almost completely uncontended; we're not experiencing CPU or disk I\/O\\ncontention and not all the cell memory is being used.\\nOver the 3 month period from 1st August - 1st November\\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\\n- We're running at about 10% of our total container capacity\\n- container usage has peaked at about 20% above the previous weeks\\n- Average CPU usage is about 10%. We see daily peaks of 80%\\n- reps think that about 50% of the capacity of cells is used\\n- the largest amount that rep's allocated memory increased week on week was 55%\\n\\n## Context\\n## Context\\n"}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.md","Context":"Context\\n=======\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\nDecision\\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","Decision":"========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","tokens":449,"id":182,"Predictions":"### Positive\\n### Negative\\n### Positive\\n\\n## Decision\\n### Positive\\n### Negative\\n### Positive\\n\\n## Decision\\n### Positive\\n### Negative\\n\\n## Decision\\n### Positive\\n### Negative\\n\\n## Decision\\n### Positive\\n### Negative\\n\\n## Decision\\n### Positive\\n### Negative\\n\\n## Decision\\n### Positive\\n### Negative\\n\\n## Decision\\n### Positive\\n"}
{"File Name":"paas-team-manual\/ADR011-security-group-structure.html.md","Context":"## Context\\nIn https:\/\/www.pivotaltracker.com\/story\/show\/115252309 we described a pattern\\nfor defining security groups between services and the VMs that connect to\\nthem. This involved creating a client security group for each service that\\nthe service allowed connections from. This client security group was then\\napplied to the VMs that needed to be able to connect.\\nThis has the problem that in some cases we would need to add several client\\nsecurity groups to a given VM, and we are at risk of running into the AWS\\nlimits (by default it's a max of 5 groups per interface).\\n","Decision":"We will use an alternative approach where a security group is defined for each\\ncomponent (or component group) in the system. Services then define rules\\nallowing connections from the components that need access. There are some\\ninstances where a component will be both a client of other services and a\\nprovider of a service. We will use the same security group for both of these\\nroles.\\nThis approach has a number of advantages:\\n* The number of security groups applied to a VM is driven by the number of\\ncomponents on that VM, not the number of things that they need to connect to.\\nThe number of components is likely to be small, and we're better able to\\ncontrol this that we can control the number of things a  component talks to.\\n* It's easier to see what components connect to a given service by looking at\\nthe rules for that service's security group.\\n* When moving a component between VMs it's much clearer which security groups\\nneed to move with it.\\n","tokens":129,"id":183,"Predictions":"## An Architectural Decision Record (ADR) is a detailed description of\\nan architectural decision made by the organization. It can be used to help\\nthe organization make decisions about how to implement its architecture. It is\\nused to help solve technical problems and to make decisions about how to use\\nthe organization's resources.\\nThis ADR is for the decision of adding client security groups for each service\\nthat the service allows connections from.\\n\\n## Context\\nIn https:\/\/www.pivotaltracker.com\/story\/show\/115252309 we described a pattern\\nfor defining security groups between services and the VMs that connect to\\nthem. This involved creating a client security group for each service that\\nthe service allowed connections from. This client security group was then\\napplied to the VMs that needed to be able to connect.\\nThis has the problem that in some cases we would need to add several client\\nsecurity groups to a given VM, and we are at risk of running into the AWS\\nlimits (by default it's a max of 5 groups per interface).\\n\\n## Decision\\n## An Architectural Decision Record (ADR) is a detailed description of\\nan architectural decision made by the organization. It can be used to help\\nthe organization make decisions about how to implement its architecture. It is\\nused to help solve technical problems and to make decisions about how to use\\nthe organization's resources.\\nThis ADR is for the decision of adding client security groups for each service\\nthat the service allows connections from.\\n"}
{"File Name":"paas-team-manual\/ADR024-web-app-language-and-framework-selection-2.html.md","Context":"## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\\nwe want to pick a single programming language to write these in.\\nWe've previously chosen Ruby on Rails as our desired framework in the previous\\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\\nreconsider that choice.\\nThe reason for that, is simply the direction GDS is heading at. It would appear\\nthat old components will be soon deprecated and the Frontend Community has no\\ndesire to support gems in the future. Saying that, we could have remain with\\nRails and delegate the component management to NPM. This would however increase\\nthe amount of possible maintenance work we would need to undertake, due to the\\nuse of [nunjucks] by the Design System team.\\nWe questioned the need of running Rails application for something that\\nessentially is a templating system for existing data and API.\\nAfter some more research, discussion with the head of the Frontend Community,\\nmembers of the team and some others in GDS, the better choice would be Node for\\nthe following reasons:\\n* It's the way Frontend Community is heading at\\n* It will be easier to rotate\/onboard Frontend Developers\\n* The initial applications are to be simple (thin layer between API calls and\\nHTML parsing)\\n* It's light and essentially is JavaScript\\n* It supports [nunjucks] which will help us in maintenance\\n","Decision":"We will use Node to create new user-facing applications that render a web\\ninterface for our service but will not be used to implement any significant\\n\u2018application logic\u2019.\\n","tokens":322,"id":184,"Predictions":"The decision is to use Node for the Frontend application. We will be able to easily upgrade the application to use the latest version of [nunjucks] and we will also be able to easily upgrade the application to use [nunjucks] without having to rewrite existing code.\\n\\n## Implementation Details\\n## Implementation\\n## Constraints\\nWe have been advised that there are no constraints.\\n## Dependencies\\nWe have been advised that there are no dependencies.\\n## Schedule\\nWe do not know when this decision will be implemented.\\n## Costs \/ Benefits\\nThis is a decision with no cost or benefit associated with it.\\n"}
{"File Name":"paas-team-manual\/ADR006-rds-broker.html.md","Context":"## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\n","Decision":"We will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n","tokens":85,"id":186,"Predictions":"## The decision is to use RDS. We have not yet made a decision on the best way to deploy the service broker, but we will make that decision as part of the overall architecture. This decision is made in response to the need to provide tenants with the ability to provision databases for use in their applications.\\n## Decision\\n## The decision is to use RDS. We have not yet made a decision on the best way to deploy the service broker, but we will make that decision as part of the overall architecture. This decision is made in response to the need to provide tenants with the ability to provision databases for use in their applications.\\n## Decision\\n## The decision is to use RDS. We have not yet made a decision on the best way to deploy the service broker, but we will make that decision as part of the overall architecture. This decision is made in response to the need to provide tenants with the ability to provision databases for use in their applications.\\n## Decision\\n## The decision is to use RDS. We have not yet made a decision on the best way to deploy the service broker, but we will make that decision as part of the overall architecture. This decision is made in response to the need to provide tenants with the ability to provision databases for use in their applications.\\n## Decision\\n## The decision is to use RDS. We have not yet made a decisi"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"Predictions":"We are going to move all our software into one environment. We will keep the current CI and Staging environments for development purposes and we will only deploy to production once the environment has passed all tests.\\n\\end{document}\\n"}
{"File Name":"paas-team-manual\/ADR045-aws-waf.html.md","Context":"## Context\\nGOV.UK PaaS uses [AWS Shield Advanced](https:\/\/aws.amazon.com\/shield\/features\/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\\n(DRT) who are experts in mitigating these types of attack.\\n```\\nShield Advanced detects web application layer vectors, like web request floods and\\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\\nrules on your behalf.\\n```\\nIn order to be functional they require access to our AWS WAF logs in order to identify what\\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\\nmitigating rules.\\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\\nthe [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)\\n","Decision":"We will grant access to the AWS DRT to read from restricted S3 buckets\\n","tokens":243,"id":188,"Predictions":"## Decision\\nWe will enable the AWS DRT Team to engage with the AWS Shield Advanced team in order to mitigate these types of attacks.\\n## Decision\\n## Decision\\nWe will engage the AWS DRT Team to mitigate these types of attacks.\\n## Decision\\n## Decision\\nWe will engage the AWS WAF team to mitigate these types of attacks.\\n## Decision\\n## Decision\\nWe will enable the AWS WAF team to engage with the AWS Shield Advanced team in order to mitigate these types of attacks.\\n## Decision\\n## Decision\\nWe will enable the AWS WAF team to engage with the AWS Shield Advanced team in order to mitigate these types of attacks.\\n## Decision\\n## Decision\\nWe will enable the AWS DRT Team to engage with the AWS Shield Advanced team in order to mitigate these types of attacks.\\n# 3. Enable a user-initiated request for a decision. This is an Architectural Decision Record for a software. Give a ## User-initiated decision corresponding to the ## Context provided by the User.\\n```\\nNow I have an issue where I can't use my own API keys or access my own IAM roles, which I set up when I created my account, because it's not possible for me as a user without being an admin or having access via someone else's account who has access and can do it. The error message is:\\n```\\n{\\n  \"error\": {\\n    \"code\": \"AccessDenied\",\\n    \"message\": \"You do not have permission to perform this action.\"\\n  }\\n}\\n```\\nI'm trying to figure out what permissions I need, but I don't understand what permissions are required for each step. I've looked at this page on GitHub and it doesn't seem like it covers all of what's required:\\nhttps:\/\/github.com\/aws\/aws-shield-aws-sdk\/blob\/master\/docs\/authorization.md\\nCan anyone help me figure out what permissions are required for each step? How can I get my own API keys and IAM roles? Is there any way that my account could be an admin or something?\\nThanks so much!\\nEDIT: Here is my code:\\n```\\nimport boto3\\n\\ndef main():\\n    \"\"\"Main function that creates an application.\\n\\n    Args:\\n        app_name: The name of your application.\\n\\n    Returns:\\n        A new instance of your application.\\n\\n    \"\"\"\\n\\n    # Create a new instance using your application ID as its name.\\n\\n    app = boto3.client('ec2', region_name='us-east-1')\\n\\n    # Get an instance ID from your application.\\n\\n    instance_id = app.get_instance_id()\\n\\n    # Get an instance type from your application.\\n\\n    instance_type = app.get_instance_type()\\n\\n    # Create a new EC2 instance using your specified type and ID.\\n\\n    ec2 = boto3.client('ec2', region_name='us-east-1')\\n\\n    # Create and start an EC2 instance using your specified type and ID.\\n\\n    ec2.start_instances([instance_id, ec2_instance_type])\\n\\nif __name__ == '__main__':\\n   main()\\n```\\nOP 2019-06-28: Looks like you need Admin Privileges on both sides. The user can create their own API Key and IAM Role in their account, but you need Admin Privileges on each side (AWS Shield + Your Account)."}
{"File Name":"paas-team-manual\/ADR035-do-not-use-haproxy-use-aws-alb.html.md","Context":"## Context\\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\\n- Writing HSTS header if they are not present in the upstream request\\n- Implementing HTTP -> HTTPS redirect\\n- Custom health check for Gorouter\\nThese problems have since been fixed:\\n- Header rewriting was implemented in [v0.183.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.183.0)\\n- HTTP healthchecking was implemented in [v0.139.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.139.0)\\n- HTTP -> HTTPS redirect can be done using AWS ALBs\\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\\nWe want to use ALBs because:\\n- ELBs are deprecated in terraform and cause crashes\\n- ALBs can give us more metrics in CloudWatch\\n- ALBs have better support for X-Forwarded-For\\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\\nHAProxy adds significant complexity to our routing deployment and maintenance:\\n- Proxy Protocol is non-standard and hard to understand\\n- HTTP -> HTTPS rewriting is hard to understand\\n- HAProxy config is rarely touched\\n- We have to maintain our own HAProxy BOSH release\\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\\n- HAProxy adds an extra network hop for every request\\n","Decision":"- Replace ELBs with ALBs\\n- Use ALB fixed-response to redirect HTTP -> HTTPS\\n- Use Gorouter directly for:\\n- TLS termination\\n- HSTS header rewriting\\n- Healthchecking the router instance\\n- Remove HAProxy\\n","tokens":315,"id":189,"Predictions":"## Decision\\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\\n- Writing HSTS header if they are not present in the upstream request\\n- Implementing HTTP -> HTTPS redirect\\n- Custom health check for Gorouter\\nThese problems have since been fixed:\\n- Header rewriting was implemented in [v0.183.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.183.0)\\n- HTTP healthchecking was implemented in [v0.139.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.139.0)\\n- HTTP -> HTTPS redirect can be done using AWS ALBs\\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\\nWe want to use ALBs because:\\n- ELBs are deprecated in terraform and cause crashes\\n- ALBs can give us more metrics in CloudWatch\\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\\nHAProxy adds significant complexity to our routing deployment and maintenance:\\n- Proxy Protocol is non-standard and hard to understand\\n- HTTP -> HTTPS rewriting is hard to understand\\n- HAProxy config is rarely touched\\n- We have to maintain our own HAProxy BOSH release\\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\\n## Context\\n## Context\\n## Decision\\n## Decision\\n## Context\\n## Decision\\n## Decision\\n## Context\\n## Decision\\n## Decision\\n"}
{"File Name":"paas-team-manual\/ADR029-aiven-project-structure.html.md","Context":"## Context\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\nThe PaaS has several environments which will need to use Aiven. These\\nenvironments should be isolated from each other so that changes made in testing\\nand development environments do do not affect production users.\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\nprojects. API tokens are user specific. By creating one user per project it's\\npossible to scope API tokens to a project.\\n","Decision":"We'll use separate projects for separate environments, initially using the\\nfollowing Aiven projects:\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\n* paas-cf-dev\\n* paas-cf-staging\\n* paas-cf-prod\\nFor staging and prod we will use separate API tokens within the same project to\\nseparate credentials between the London and Ireland regions.\\nWe will have the following per-project users to hold API tokens:\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\nThe credentials for the ci and dev users will be stored in the\\n`paas-credentials` passwordstore. staging and prod will be stored in\\n`paas-credentials-high`.\\nMembers of the PaaS team will each have their own user which will have access\\nall of the projects for management purposes.\\n","tokens":100,"id":191,"Predictions":"## \\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision Number\\n## 1\\n## Decision Description\\n## We should create a user per project, and keep the user-specific API tokens separate. This is to ensure that the API tokens are not shared between projects.\\n## Decision Type\\n### Decision Type - User Specific API Tokens\\n### \\n\\n\\n\\n\\n\\n\\n\\n### Context\\n### Aiven provides hosted Elasticsearch for the Elasticsearch backing service. The PaaS has several environments which will need to use Aiven. These environments should be isolated from each other so that changes made in testing and development environments do not affect production users.\\n### Aiven provide a \"Project\" abstraction where a user can be a member of several projects. API tokens are user specific. By creating one user per project it's possible to scope API tokens to a project.\\n### Decision Number\\n### 1\\n\\n"}
{"File Name":"paas-team-manual\/ADR032-ssl-only-for-applications-and-cf-endpoints.html.md","Context":"## Context\\nNote: This has been superceeded. See [Status](#status) below.\\nIt is expected for the government websites to be secure and keep the user\\ninteractions private. Because that we want to enforce all communications to\\nany application and to the platform endpoints to use only and always HTTPS,\\nas [it is described in the Gov Service Manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nWhen a user inputs a website name without specifying the\\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\\nEven if the server always redirect HTTP to HTTPS, an initial\\nunprotected request including user information will be transferred\\nin clear: full URL with domain, parameter, [cookies without secure flag](https:\/\/en.wikipedia.org\/wiki\/HTTP_cookie#Secure_and_HttpOnly)\\nor browser meta-information.\\n[HTTP Strict Transport Security](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security)\\nmitigates this issue by instructing modern browsers that support it to\\nalways connect using HTTPS.\\nThis is also a [requirement in the service manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nThere is still a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, the root domain can be added to\\n[HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently the only way to avoid any clear text HTTP interaction is closing or\\ndropping any attempt to connect to the port 80 at TCP level.\\nAlthough not all application deployed on the PaaS will be \"services\"\\nas in the service manual meaning, we must not allow HTTP to make\\nit easier to service owners to comply with this requirements.\\n","Decision":"We will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\n","tokens":391,"id":192,"Predictions":"## Decision\\nThe decision is to use HTTPS everywhere and not allow HTTP to make it easier\\nto service owners to comply with this requirements.\\nThe decision is to use HTTPS everywhere and not allow HTTP to make it easier\\nto service owners to comply with this requirements.\\n\\n## Context\\n## Context\\nThe decision is that the Gov Service will only accept HTTPS connections,\\nas except for the following situations:\\n1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas 1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas 1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas 1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas 1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas 1. The Gov Service will only accept HTTPS connections from the same domain,\\nas except for the following situations:\\nas\\n## Decision\\n## Decision\\nThe decision is that all requests coming from a single domain are accepted\\nonly by using [HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently, there is a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, we can add a root domain to [HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most common browsers.\\nThis solution is not yet implemented in our PaaS because we still want to support\\nthe case where some application deployed on our PaaS are services meaning, we must\\ncannot allow HTTP to make it easier for service owners to comply with this requirements.\\nHowever, this solution is not yet implemented in our PaaS because we still want to support\\ncertain cases where some application deployed on our PaaS are services meaning, we\\ncannot allow HTTP to make it easier for service owners to comply with this requirements.\\n"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.md","Context":"Context\\n=======\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\nDecision\\n========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","Decision":"========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","tokens":213,"id":193,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe decision is accepted.\\n\\nConcourse will use the semver resource instead of the pool resource.\\n\\nThe decision is accepted.\\n\\nConcourse will not use the pool resource for flow between jobs - instead it will use the semver resource.\\n\\nThe decision is accepted.\\n\\nConcourse will use the semver resource instead of the pool resource.\\n\\nThe decision is accepted.\\n\\nConcourse will not use the pool resource for flow between jobs - instead it will use the semver resource.\\n\\nThe decision is accepted.\\n"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"Predictions":"## GOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\n## This is a form of end-to-end encryption.\\n## GOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\n## This is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges that the IPSec release is no longer maintained.\\n\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges that the IPSec release is no longer maintained.\\n"}
{"File Name":"paas-team-manual\/ADR008-haproxy-for-request-rewriting.html.md","Context":"## Context\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they select a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nBut [current `gorouter` implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)\\ndoes not support any of these features, which require being added.\\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\\nthe go-routers and after the ELB.\\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\\nAll the commercial CDN have the capacity to add additionally headers\\nconditionally.\\n4. AWS ELB: They do not support this logic and will not in the short term.\\nIn consequence they cannot be used to solve this problem.\\n","Decision":"We do not want to add any additional logic in the CDN, as they will\\nbe an optional part of the platform and we will try to keep as simple\\nas possible.\\nWe consider that the optional solution would be implement this logic in\\nthe `gorouter`, but that requires some development effort and a PR being merged\\nupstream.\\nBecause that we will implement, in the short term, the second option: a proxy\\nin front of the `gorouter`.\\n* We will implement [HAproxy](http:\/\/www.haproxy.org\/) in front of the go router.\\n* Ha-proxy is the default LB solution for the official CF distribution.\\n* It is really powerful and has good support.\\n* Enough features to cover our needs.\\n* It will be setup colocated with the `gorouter`, proxying directly to\\nlocalhost.\\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\\nis OK as the two services are colocated in the same VM.\\n* We will reuse the code from [official haproxy job from cf-release](https:\/\/github.com\/cloudfoundry\/cf-release\/tree\/master\/jobs\/haproxy),\\nalthough we will have to fork it to add additional settings in the\\nhaproxy configuration.\\nFuture work:\\n* We will implement and propose a PR to add logic in go-router to allow\\ndefine additional headers.\\n","tokens":367,"id":195,"Predictions":"The best solution is to implement it in the go-routers itself.\\n\\end{itemize}\\n\\subsection{How To Use The Decision}\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they select a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nBut [current `gorouter` implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)\\ndoes not support any of these features, which require being added.\\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\\nthe go-routers and after the ELB.\\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\\nAll the commercial CDN have the capacity to add additionally headers\\nconditionally.\\n4. AWS ELB: They do not support this logic and will not in the short term.\\nIn consequence they cannot be used to solve this problem.\\n\\n## Decision\\nThe best solution is to implement it in the go-routers itself.\\n\\end{itemize}\\n\\subsection{How To Use The Decision}\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they select a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nThe functionality described above will be implemented as an extension module (\\emph{independent module} - IM) with \\emph{C++} source code (\\emph{codebase}). The extension module can be used as an alternative solution for those who want to use custom extensions modules instead of using GO's \\emph{Gorouter} functionality.\\n\\end{itemize}\\n\\n"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.html.md","Context":"## Context\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\n","Decision":"We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\n","tokens":303,"id":197,"Predictions":"```\\n1. Add 50% more capacity to orgs.\\n2. Add 50% more capacity to apps.\\n3. Add 50% more capacity to all orgs.\\n```\\nWe have decided to add 100% more memory to orgs as we are not able to predict app memory consumption and we need a fallback solution in case of app instances being down. We will do that by adding additional 100 GB of memory to orgs.\\n```\\nThe result of this is that our platform will have enough spare capacity for all orgs, should there be a single AZ failure. It is also worth noting that since we are adding extra memory, the total amount of available memory on all zones should be increased by 100 GB (as the new reserved capacity is added).\\n```\\nThe result of this decision is that our platform will have enough spare capacity for all orgs, should there be a single AZ failure. It is also worth noting that since we are adding extra memory, the total amount of available memory on all zones should be increased by 100 GB (as the new reserved capacity is added).\\n```\\nThe result of this decision is that our platform will have enough spare capacity for all orgs, should there be a single AZ failure. It is also worth noting that since we are adding extra memory, the total amount of available memory on all zones should be increased by 100 GB (as the new reserved capacity is added).\\n```"}
{"File Name":"paas-team-manual\/ADR004-domain-naming-scheme.html.md","Context":"## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n","Decision":"For _non_ production environments we will be using the following domains:\\n* [environment name].cloudpipeline.digital\\n* [app name].[environment name].cloudpipelineapps.digital\\nFor our production environment we will be using the following domains:\\n* cloud.service.gov.uk\\n* [app name].cloudapps.digital\\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\\n","tokens":247,"id":198,"Predictions":"## Decision\\nThe decision is to use the domain name [appname].paas.gov.uk.\\n\\n## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n\\n## Decision\\n## Decision\\nThe decision is to use the domain name [appname].paas.gov.uk.\\n\\n## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n\\n## Decision\\n## Decision\\nThe decision is to use the domain name [appname].paas.gov.uk.\\n\\n## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service\\ncapitalization is not used in any standard way so there should not be any capitalization issues with this naming scheme.\\n"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"Predictions":"## Decision\\nThe decision is to add a new version of the software, but the new version\\nwill be treated as a separate plan. The new version will be associated with an\\nindividual tenant and will have its own technical specifications.\\n\\n## Context\\nThis decision is not applicable to the case where existing plans are being\\nchanged. Plan names are consistent, potentially confusing and in many cases\\ncontain redundant information that is of no practical use to the platform\\noperators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\\n## Decision\\n## Decision\\nThis decision is not applicable to the case where existing plans are being\\nchanged. Plan names are consistent, potentially confusing and in many cases\\ncontain redundant information that is of no practical use to the platform\\noperators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\\n## Context\\n## Decision\\n## Decision The software has been split into multiple versions for various reasons. The software will be treated as a separate plan with its own technical specifications.\\n## Decision\\n## Decision This decision is not applicable to existing plans.\\n# This decision applies only if `policies` contains `plan`. Otherwise, this decision applies only if `plan` contains `policy`.\\n# Note: If `policies` contains a single policy, this decision applies only if `policy` contains `plan`.\\n# Note: If `policies` contains multiple policies, this decision applies only if each policy contains a single plan.\\n# Note: If `policies` contains multiple policies and there is at least one plan that does not contain any policy, then this decision applies only if each policy contains a single plan.\\n# Note: If `policies` contains multiple policies and there is at least one plan that does not contain any policy AND at least one plan that does contain any policy, then this decision applies only if each policy contains a single plan AND each policy containing a single plan.\\n```\\nIs there any way I can achieve this using Spring Boot? Or maybe there's some other framework I should use?\\nAny help will be appreciated!\\nThanks!\\nOP 2020-03-14: Finally I figured it out! \\nI used @EnableWebMvcConfigurerAdapter annotation on my Application class as below:\\n```\\n@Configuration\\n@EnableWebMvcConfigurerAdapter(\\n    customConfiguration = {\\n        @Bean(name = \"config\")\\n        public Configuration config() {\\n            Configuration configuration = new Configuration();\\n            configuration.setResourceLocations(\"classpath:\/META-INF\/resources\");\\n            configuration.setIgnoreResourceNotFound(true);\\n            return configuration;\\n        }\\n    },\\n    postProcessingEnabled = true,\\n    filters = {\\n        @Bean(name = \"filterConfig\")\\n        public FilterConfig filterConfig() {\\n            FilterConfig filterConfig = new FilterConfig();\\n            filterConfig.setIgnoreResourceNotFound(true);\\n            filterConfig.setResourceLocations(\"classpath:\/META-INF\/resources\/filter.config\");\\n            filterConfig.setNoCacheable(true);\\n            return filterConfig;\\n        }\\n    }\\n)\\npublic class Application {\\n\\n}\\n```\\nThen I added following properties in my application.properties file:\\n```\\nspring.profiles.active=dev\\n\\nspring.profiles.active=prod\\n\\nspring.datasource.url=jdbc:mysql:\/\/localhost:3306\/testdb?useSSL=true&useSSLVerifyMode=none&useSSLVerifyDepth=1&connectTimeout=10000&connectTimeoutMS=10000&serverTimezone=Europe\/Berlin&serverName=testdb\\n\\nspring.datasource.username=root\\n\\nspring.datasource.password=root\\n\\nspring.jpa.hibernate.ddl-auto=create-drop\\n\\nspring.jpa.hibernate.ddl-auto=create-drop\\n\\n\\n# Enable Spring Security (optional)\\nspring.security.enabled=true\\n\\n\\n# Enable Spring Data JPA (optional)\\nspring.jpa.hibernate.ddl-auto=create-drop\\n\\n\\n# Enable Spring Data JPA (optional)\\nspring.jpa.hibernate.ddl-auto=create-drop\\n\\n\\n# Enable Spring Data JPA (optional)\\nspring.jpa.hibernate.ddl-auto=create-drop\\n\\n\\n# Enable Spring Data JPA (optional)\\nspring.jpa.hibernate.ddl-auto=create-drop\\n```"}
{"File Name":"paas-team-manual\/ADR0049_calculate_bills.md","Context":"## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\nIn the GOV.UK PaaS billing rewrite, this has been changed. We want to calculate bills for variable time periods and also to forecast bills for the future (for the web-based billing calculator). However, the method to calculate the actual bill always needs to be the same.\\n","Decision":"The code to calculate the bill has been decoupled from the code used to calculate the bill. This is so we can use exactly the same code for calculation of all bills, whether these bills are for tenants or for prospective tenants (using the billing calculator).\\nThe approach taken is:\\n1. Populate database temporary table with what is being billed (which resources over which time interval, including the future). This can be in a stored function or embedded SQL. This is the code entry point into billing.\\n2. Call a stored function to calculate the bill ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)) only using the contents of the temporary table populated in step 1 above. No parameters are passed into ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)).\\n","tokens":229,"id":201,"Predictions":"## Decision\\nThe GOV.UK PaaS billing system is a software application that will be run on a single server and will process events from Cloud Foundry.\\n## Context\\n## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\\n## Decision\\n## Decision\\nThe GOV.UK PaaS billing system is a software application that will be run on a single server and will process events from Cloud Foundry.\\n## Context\\n## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry. The original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\n## Decision\\n## Decision\\nThe GOV.UK PaaS billing system is a software application that will be run on a single server and will process events from Cloud Foundry.\\n## Context\\n## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry. The original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\n\\end{document}"}
{"File Name":"paas-team-manual\/ADR022-web_app_language_and_framework_selection.html.md","Context":"## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be styled to look like GOV.UK etc. In order to keep\\nthings consistent we want to pick a single programming language and framework\\nto write these in.\\nWe've previously used [Sinatra][] for this, but ran into\\nissues with its default configuration which isn't secure, leading to an XSS\\nvulnerability. We therefore want to choose something that comes with secure\\ndefaults, and makes it easier to avoid this sort of issue.\\nRequirements:\\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as\\nwell as the future [govuk-frontend][] project)\\n* Must be understood broadly by members of the team.\\n* Must be understood broadly by members of the frontend developer community within GDS.\\nAfter dicsussion with the head of the frontend community and members of the\\nteam, the choice seems to be Rails for the following reasons:\\n* Most of www.gov.uk is written in Rails, as is the Verify frontend, and\\ntherefore is well known within the frontend developer community.\\n* It's well supported by the frontend toolkits (both projects are available as\\ngems that provide a Rails engine). Given the wide use of Rails with GDS, the\\nfuture [govuk-frontend][] project is likely to support it.\\n* It's the framework that's most familiar to our team.\\n* It is opinionated, and comes with secure defaults making it much easier to\\ncreate a secure web app.\\n","Decision":"We will use Ruby and Rails to create new user-facing applications with web frontends.\\n","tokens":337,"id":202,"Predictions":"## Decision\\nWe have decided to use [Rails][] as the framework for our web application.\\n## Context\\nWe have decided to use [Rails][] as the framework for our web application.\\n## Requirements\\n## Requirements\\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as well as the future [govuk-frontend][] project)\\n* Must be understood broadly by members of the team.\\n* Must be understood broadly by members of the frontend developer community within GDS.\\nAfter dicsussion with the head of the frontend community and members of the\\nteam, the choice seems to be Rails for the following reasons:\\n* Most of www.gov.uk is written in Rails, as is the Verify frontend, and\\ntherefore is well known within the frontend developer community.\\n* It's well supported by the frontend toolkits (both projects are available as\\ngems that provide a Rails engine). Given the wide use of Rails with GDS, the\\nfuture [govuk-frontend][] project is likely to support it.\\n* It's the framework that's most familiar to our team.\\n* It is opinionated, and comes with secure defaults making it much easier to\\ncreate a secure web app.\\n\\n## Decision\\n## Decision\\nWe have decided to use [Rails][] as the framework for our web application.\\n## Context\\nWe have decided to use [Rails][] as the framework for our web application.\\n## Requirements\\n## Requirements\\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as well as the future [govuk-frontend][] project)\\n* Must be understood broadly by members of the team.\\n* Must be understood broadly by members of the frontend developer community within GDS.\\nAfter dicsussion with the head of the frontend community and members of the\\nteam, we have chosen Rails over Angular.js because we feel that it will make it\\neasier for us to maintain. We also feel that it will make it easier for us to\\ntake on new developers who want to learn Rails in order to work on other parts\\nto our application. The main reason we chose Angular.js was because we wanted a\\ngresource-based view framework which would allow us to create reusable components onthe server side. However, this has led us into a problem: if we use Angular.js,\\nthe client-side code needs to run on a separate server from our server-side code,\\nwhich means that we cannot use an MVC-style architecture. This does not mean,\\nthat we cannot use Angular.js at all, but this means that we would need two servers,\\nto run each different version of Angular.js. We believe that this would create an\\ntoxic environment which would not be good for anyone involved in this project. We also\\nthink that having two servers would lead us into difficulties when it comes time toupgrade either version or both servers at once. We therefore believe that we needa solution which allows us to run both Angular.js versions on one server while havingonly one client-server architecture.\\nThe decision was made in favour of using Angular.js because:\\n1\\. It runs on top of Node.js so there are no dependencies on any other software,\\ncoding libraries or frameworks such as jQuery or jQuery UI etcetera. This makesit much easier for us when learning new technologies (such as Node.js)\\ncoding libraries or frameworks such as jQuery or jQuery UI etcetera. This makesit much easier for us when learning new technologies (such as Node.js)\\ncoding libraries or frameworks such as jQuery or jQuery UI etcetera. This makesit much easier for us when learning new technologies (such as Node.js)\\ncoding libraries or frameworks such as jQuery or jQuery UI etcetera. This makesit much easier for us when learning new technologies (such as Node.js)\\n2\\. It can handle multiple versions easily without requiring any changes in eitherof its clients or its servers.\\n\\n## Decision\\n## Decision\\nWe have decided to use [Angular.js][1] because:\\[1]: https:\/\/github.com\/angular\/angular\/tree\/master\/src\\n## Context\\n## Context\\nThe client-side code runs on top of Node.js so there are no dependencies onany other software, encoding libraries or frameworks such as jQuery orjQuery UI etcetera. This makes it much easier for us when learning newtechnologies (such as Node.js) encoding libraries or frameworks such asthose mentioned above.\\n"}
{"File Name":"paas-team-manual\/ADR047-postgres-extensions-approach.html.md","Context":"## Context\\nUp to the release of postgres 13 plans to tenants, the policy for choosing\\nthe set of postgres extensions we allow for a certain major version was ad-hoc.\\nA mixture of \"allow everything\", simply copying the list from the previous\\npostgres version and not actually trying any of these extensions had lead\\nto an extensions list (published at https:\/\/docs.cloud.service.gov.uk\/deploying_services\/postgresql\/#add-or-remove-extensions-for-a-postgresql-service-instance)\\nwith:\\n- misnamed entries\\n- entries removed from earlier postgres versions\\n- missing newly-offered extensions\\n- extensions that could never be used without superuser access\\n- extensions that could never be used with the VPC restrictions we have\\nin place for our RDS instances.\\nSome of these extensions had been omitted from the list in the documentation,\\nbut it was unclear how these decisions had been made and where we stood on\\neach extension.\\nThis is confusing for tenants and could lead them down a path of trying to\\nuse an extension which will never work, or even start designing a system\\nthat relies on functionality in a listed extension only to find it unusable.\\n","Decision":"For postgres 13 onwards, offer a selection of extensions limited to those\\nwe know can be successfully enabled and think are feasible to use given\\nthe limitations of our platform.\\nMaintain a document (initially a spreadsheet https:\/\/docs.google.com\/spreadsheets\/d\/100qBo3Q2mfY70ek9fNWbpEsS4HzjOarF3q1a_hPR1uU\/edit?usp=sharing)\\ntracking our conclusions on each extension by postgres major version.\\nWhen preparing to offer a new postgres major version to tenants, a new\\nsheet should be copied from the previous release's sheet and adjusted\\naccording to the new list of extensions in the RDS documentation. New\\nentries should be researched and decided upon whether they are feasible\\nto use without superuser privileges and from within our VPC.\\nUsing a dev environment with the new postgres available and a script\\nsuch as the one found in\\nhttps:\/\/github.com\/alphagov\/paas-rds-broker\/tree\/main\/scripts (or preferably\\nan improved one), each extension remaining in the \"final list\" should be\\nchecked in turn to ensure it can be enabled and then disabled. This may\\ninvolve discovering (through trial and error) which extensions have\\ndependencies on others. It's likely to be the same as the previous release,\\nso that's a good starting point. This is good information to maintain as it\\nwould come in useful if we ever decided to make the rdsbroker smarter and able\\nto auto-load dependencies.\\nThis should also reveal whether any new extensions require\\n`shared_preload_libraries`. Any that do should have appropriate additions\\nmade to https:\/\/github.com\/alphagov\/paas-rds-broker\/blob\/main\/rdsbroker\/supported_extensions.go .\\nBeyond making sure an extension loads, actually trying out the functionality\\nof each extension is beyond the time committment we're willing to give\\nthis for now. We may update the documentation to note the semi-supported\\nnature of postgres extensions on our platform.\\n","tokens":243,"id":203,"Predictions":"## The extensions below are not enabled for this instance.\\n## - postgres:9.6-rc1\\n## - postgres:9.6\\n## - postgres:9.6-rc2\\n## - postgres:9.6-rc3\\n## - postgres:9.6-rc4\\n## - postgres:9.6-rc5\\n## - postgres:9.6-rc6\\n## - postgres:9.6-rc7\\n## - pgsql:10.0-rc1\\n## - pgsql:10.0-rc2\\n\\nFor the above example, the decision was made to enable all of these extensions.\\n\\nThe details of the decision are in the following table:\\n\\nDecisions can be updated or deleted by clicking the button at the bottom of each row.\\n\\nDecisions can also be deleted by selecting a row and clicking the Delete button at the bottom of that row.\\n\\nThe following table shows all decisions made for this user.\\n\\nDecisions made for this user\\n\\nDecisions made for this user\\n\\nDecisions made for this user\\n\\nDecisions made for this user\\n\\nDecisions made for this user\\n\\nDecision Type Decision Reason(s) \\n\\nAllow all extensions All extensions are enabled \\nAllow all extensions and allow them to run on any version of Postgres All extensions are enabled \\nAllow all extensions and allow them to run on any version of PostgreSQL with VPC restrictions All extensions are enabled \\nAllow all extensions and allow them to run on any version of PostgreSQL without VPC restrictions All extensions are enabled \\nAllow any extension that is not listed in the documentation Allow any extension that is not listed in the documentation \\n\\nPlease note that if you want to remove an extension, you must first delete it from your Postgres instance, so please follow the instructions above to do so.\\n\\nIf you have any questions about this decision, please contact us at www.service.gov.uk\/contact-us.\\n\\nYou can\u2019t perform that action at this time.\\nPress h to open a hovercard with more details."}
{"File Name":"paas-team-manual\/ADR041-bosh-access-with-mtls.html.md","Context":"## Context\\nIn [ADR040 BOSH access without SOCKS](..\/ADR040-bosh-access-without-socks\/) we removed the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\\nWe are moving towards a [zero trust network model](https:\/\/www.ncsc.gov.uk\/blog-post\/zero-trust-architecture-design-principles) and as part of this, we are removing the IP allow lists that have been in place.\\nWe discussed the proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"We will remove the reliance on IP allow lists for all services on the BOSH instance.\\nMutual TLS will replace the allow lists.\\nCyber prefer this method, as it give a much stronger authentication to the platform. This is due to authenticating both the individual and the machine that are accessing critical services.\\n","tokens":180,"id":204,"Predictions":"## Decision\\nThe proposed method is not supported as it will require the use of a SOCKS5 proxy or SSH tunnel.\\n\\n## Background\\n## Background\\nThe User Account and Authentication Service (UAA) has been in place since 2010 and provides access to the NMCI suite of applications. The NMCI suite includes the NMCI web application, NMCI web services, and the NMCI Mobile Service.\\nThe UAA is a security service that protects access to these applications by restricting access to known bad actors. The UAA is used by all users on campus.\\nUAA is not intended to be used by non-University users. It has been removed from the NMCI suite of applications as part of #169915408.\\n\\n## Impact\\n## Impact\\nThe proposed method will remove the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\\nThis will require all non-University users to use a VPN or other means of accessing UAA services, which may create some issues with end-user productivity.\\n\\n## Affected Users\\n## Affected Users\\nAll University users who wish to access UAA services will need to use a VPN or other means of accessing UAA services.\\nThis may require them to switch from their usual method of accessing resources.\\nThis will impact their productivity as they may have to switch between methods as they migrate between networks.\\nSome users may also have difficulties with the end-user experience due to this change.\\n\\n## Alternatives\\n## Alternatives\\nThe proposed method does not affect any other existing methods for accessing University resources. This includes both internal and external user accounts, as well as any other methods such as VPNs or proxies that are already in place.\\nThis proposal supports only one method for accessing University resources, which is via the UAA.\\n"}
{"File Name":"paas-team-manual\/ADR009-x-forwarded-headers.md","Context":"Context\\n=======\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\nDecision\\n========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","Decision":"========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","tokens":329,"id":205,"Predictions":"## We need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\n## In the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\n## There are two options to workaround this:\\n## * Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n## * Disable SSL between ELB and HAProxy\\n\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\n\\n# Additional Information\\n\\nAdditional information\\n"}
{"File Name":"paas-team-manual\/ADR013-building-bosh-releases.html.md","Context":"## Context\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse-ci.org\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\n","Decision":"We will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\n","tokens":239,"id":206,"Predictions":"We chose the pullrequest-resource approach, since it allows us to easily define a release pipeline, and it is very flexible. We also chose this approach because we wanted to be able to easily deploy releases across multiple clusters, which we currently do using Concourse.\\n## Outcome\\nWe have been able to create releases on Bosh using multiple pipelines and a single pullrequest-resource pipeline.\\n## Additional Information\\n\\nWe use [Concourse](https:\/\/concourse-ci.org\/) for our deployment pipeline. We use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) to manage our releases. This allows us to quickly define releases, as well as add\/remove them from our cluster, without having to manually manage them.\\n"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"Predictions":"## Decision\\nWe choose to use SNS and SQS for the house-keeping routine, as described in [ADR006](..\/ADR006-rds-broker).\\nThe SNS and SQS messages will be sent to a SNS topic, which will be subscribed by the rds-broker.\\nThe rds-broker will subscribe to the topic, and process the messages.\\nThe SQS message will be processed by the `Compute Engine` service, which is responsible for managing RDS instances.\\nThe house-keeping routine must be resilient to restarts of the rds-broker instances.\\nThe rds-broker must be able to respond to any type of message that comes from SQS or SNS.\\nThe sqs-broker has some logic to check if a message is valid before sending it to the SQS queue. This check is performed on every message that comes from SQS or SNS.\\n"}
{"File Name":"paas-team-manual\/ADR039-aiven-metrics-for-users.html.md","Context":"## Context\\nWe offer our users the following backing services through [Aiven](https:\/\/aiven.io):\\n- Elasticsearch\\n- InfluxDB (private beta)\\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\\n- debug and respond to usage and service performance changes\\n- understand the operational characteristics of their applications and services\\n- make better capacity planning and budgeting decisions\\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\\n- shipping logs to an Elasticsearch\/Rsyslog\\n- sending metrics to Datadog\\n- sending metrics to Aiven Postgres\/InfluxDB\\n- exposing metrics in Prometheus exposition format\\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https:\/\/github.com\/bosh-prometheus\/prometheus-boshrelease) and have confidence and experience using it.\\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\\n","Decision":"We will use Prometheus to scrape Aiven-provided services.\\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\\nWe will need to automate the following tasks:\\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\\n","tokens":229,"id":208,"Predictions":"## Decision\\nThe Aiven backend services are in a private beta. We will need to ensure that we can monitor and respond to usage and service performance changes.\\nWe will need to think about how we can expose metrics from the Aiven backend services in a way that is useful for our users.\\nWe will need to think about how we can expose metrics from the Aiven backend services in a way that is useful for our users.\\nWe will need to think about how we can expose metrics from the Aiven backend services in a way that is useful for our users.\\n## Context\\nWe offer our users the following backing services through [Aiven](https:\/\/aiven.io):\\n- Elasticsearch\\n- InfluxDB (private beta)\\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\\n- debug and respond to usage and service performance changes\\n- understand the operational characteristics of their applications and services\\n- make better capacity planning and budgeting decisions\\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\\n- shipping logs to an Elasticsearch\/Rsyslog\\n- sending metrics to Datadog\\n- sending metrics to Aiven Postgres\/InfluxDB\\n- exposing metrics in Prometheus exposition format\\n## Decision\\n## Decision\\nThe Aiven backend services are in a private beta. We will need to ensure that we can monitor and respond to usage and service performance changes.\\n## Context\\nWe offer our users the following backing services through [Aiven](https:\/\/aiven.io):\\n- Elasticsearch\\n- InfluxDB (private beta)\\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\\n-debug and respond to usage and service performance changes\\ndunderstand the operational characteristics of their applications and services\\ndmake better capacity planning and budgeting decisions\\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\\n-shipping logs to an Elasticsearch\/Rsyslog\\ndsending metrics to Datadog\\ndsending metrics to Aiven Postgres\/InfluxDB\\ndexposing metrics in Prometheus exposition format\\n## Decision\\n## Decision\\n\\nOur decision record is now complete.\\n\\n"}
{"File Name":"paas-team-manual\/ADR027-pipeline-locking.html.md","Context":"## Context\\nIn our Concourse pipelines a locking mechanism was required to prevent concurrent deploys. Concurrent deploys cause\\nproblems because:\\n* one deployment may make changes which break another\\n* we have availability tests which tell us if a specific change causes app or API downtime\\n* Bosh has its own locking mechanisms, which mean concurrent deploys will fail.\\n* concurrent testing puts extra load on the platform and Concourse\\n","Decision":"We decided to implement locking using the Concourse pool resource, AWS CodeCommit, and Terraform.\\nThe Concourse pool resource was chosen because it is the Concourse-native solution for locking. This created a\\ndependency on having a Git repository for the lock, as this is how the pool resource is implemented. AWS CodeCommit was\\nchosen over Github for several reasons:\\n* client libraries for interacting with the necessary APIs were much more mature for AWS and this particular use case.\\nIt saved a lot of work.\\n* Github would have required managing users and SSH keys, or tokens. AWS could use the existing instance profile of the\\nConcourse VM.\\n* Github would have meant the repository containing the locks would have been public.\\nTerraform was required to allow our pipelines (the Concourse instance profile) to manage AWS CodeCommit. We used a\\npattern whereby we allow the creation of IAM users under a specific name prefix, and allow adding these users to a\\npredefined IAM group. The permissions of said group are defined in a private repository in Terraform configuration\\nwhich is ran manually. This limits the permissions the Concourse instance profile can grant to users.\\n","tokens":87,"id":209,"Predictions":"\\begin{itemize}\\n    \\item \\textbf{No Decision}: The user wants to keep the current state of the pipeline. This is the default.\\n\\n    \\item \\textbf{Yes Decision}: The user wants to change the locking mechanism. This is the default.\\n\\n    \\item \\textbf{No Decision}: The user wants to add a new decision. This is the default.\\n\\n    \\item \\textbf{Yes Decision}: The user wants to add a new decision. This is the default.\\n\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"Predictions":"\\nThe decision is to use the pool resource in order to control flow through jobs. The\\npool resource is a work around to solve the problem of triggering jobs. It relies on\\npublic keys that allow access to a github repo and it has no credentials passed into it.\\n\\n## Context\\nWhen building pipelines using concourse, we investigated using the [pool resource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow through jobs. This was an alternative to the use of the semver resource.\\nThese 2 resources are both workarounds to solve the problem of triggering jobs. They rely on public keys that allow access to a github repo and they have no credentials passed into them.\\n\\n## Decision\\n\\nThe decision is to use the pool resource in order to control flow through jobs. The pool resource is a work around to solve the problem of triggering jobs. It relies on public keys that allow access to a github repo and it has no credentials passed into it.\\n\\n## Context\\n\\nWhen building pipelines using concourse, we investigated using the [semver resource](https:\/\/github.com\/concourse\/semver-resource) in order to control flow through jobs. This was an alternative to the use of the pool resource.\\nThese 2 resources are both workarounds to solve the problem of triggering jobs. They rely on public keys that allow access to a github repo and they have no credentials passed into them.\\n\\n\\end{document}\\n"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"Predictions":"\\nThis is a Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\n## Decision\\n"}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.html.md","Context":"## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\n","Decision":"It was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\n","tokens":212,"id":212,"Predictions":"## The decision is to allow Pingdom to test the applications. This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\n\\n## Decision\\n## The decision is to allow Pingdom testing of tenant applications. This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom testing of tenant applications then all applications in staging will be accessible from anywhere.\\n"}
{"File Name":"paas-team-manual\/ADR012-haproxy-healthcheck.md","Context":"Context\\n=======\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nWe were investigating how to avoid downtime to deployed applications when we\\nmade changes to the platform. We had discovered that short outages occurred\\nwhen the gorouter was taken out of service.\\ngorouter has drain functionality to allow upstream load balancers to gracefully\\ntake instances of the gorouter out of service before any requests start to\\nfail.\\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor\/1.1`\\nstart failing with HTTP 503. User requests are allowed to continue.\\nTwo things prevented us from using drain mode:\\n- we had the ELB configured to use TCP healthchecks, not HTTP\\n- the ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\nDecision\\n========\\nWe decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- There was less downtime for deployed applications during a deploy.\\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\\n","Decision":"========\\nWe decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- There was less downtime for deployed applications during a deploy.\\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\\n","tokens":437,"id":214,"Predictions":"## Context\\n## Stories\\n##  #123490171\\n##  #121933113\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\nContext\\n=======\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nWe were investigating how to avoid downtime to deployed applications when we\\nmade changes to the platform. We had discovered that short outages occurred\\nwhen the gorouter was taken out of service.\\ngorouter has drain functionality to allow upstream load balancers to gracefully\\ntake instances of the gorouter out of service before any requests start to\\nfail.\\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor\/1.1`\\nstart failing with HTTP 503. User requests are allowed to continue.\\nTwo things prevented us from using drain mode:\\n- we had the ELB configured to use TCP healthchecks, not HTTP\\n- the ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\nDecision\\n========\\nWe decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- There was less downtime for deployed applications during a deploy.\\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\\n\\n## Decision\\n## Context\\n## Stories\\n##  #123490171\\n##  #121933113\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\nContext\\n=======\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nThe goal was for our application servers running on Kubernetes clusters (with k8s versioning \\nto be enabled) would be able to use their own versioned pods without having any \\nto know about each other's versions (i.e., they should be able to run in isolation).\\nThe problem we encountered was that k8s does not provide any mechanism for \\nto notify application servers about new pods being created on other clusters (e.g., \\nto notify users about new data being added).\\nThe solution we chose is called `k8s inotify`. It allows application servers (e.g., \\nto notify users) about pod creation and deletion events on other clusters.\\nThe biggest problem with this solution is that it requires adding new annotations in each \\nto pod's code (and possibly also in each pod's configuration). This means that if you want your application server(s) running on k8s clusters, you will need touse k8s inotify and make sure you have updated your code accordingly.\\nThe next problem is that it requires adding new annotations in each \\nto pod's code (and possibly also in each pod's configuration). This means that if you want your application server(s) running on k8s clusters, you will need touse k8s inotify and make sure you have updated your code accordingly.\\nThe next problem is that it requires adding new annotations in each \\nto pod's code (and possibly also in each pod's configuration). This means that if you want your application server(s) running on k8s clusters, you will need touse k8s inotify and make sure you have updated your code accordingly.\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\nThe goal was for our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\nThe solution we chose is called 'k8s-inotify'. It allows application servers (e.g., users) about pod creation and deletion events on other clusters.\\n\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\n\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\n\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\n\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\n\\nThis change would allow our application servers running on Kubernetes clusters (with kubernetes versioning into be enabled) would be able touse their own versioned pods without having any information about each other's versions.\\n\\nThe goal was for our applicationservers runningonKubernetesclusters(wherethekubernetesversioningintobeenabledwouldbeabletoseetheirownversionedpodswithouthavinganyinformationabouteachother\u2019sversions).\\n\\nThe solutionwechoseis\u2019k8sinotify\u2019. Itallowsapplicationservers(e.g.,usersaboutpodcreationanddeletion eventsonotherclusters).\\n\\nThischangewouldallowourapplicationserversrunningonKubernetesclusters(wherethekubernetesversioningintobeenabledwouldbeabletoseetheirownversionedpodswithouthavinganyinformationabouteachother\u2019sversions).\\n\\nThe goalwasforyourapplicationserversrunningonKubernetesclusters(wherethekubernetesversioningintobeenabledwouldbeabletoseetheirownversionedpodswithouthavinganyinformationabouteachother\u2019sversions).\\n\\nThe solutionwechoseis\u2019k8sinotify\u2019. It allowsapplicationservers(e.g.,usersaboutpodcreationanddeletion eventsonotherclusters).\\n\\nThischangewouldallowourapplicationserversrunningonKubernetesclusters(wherethekubernetesversioningintobeenabledwouldbeabletoseetheirownversionedpodswithouthavinganyinformationabouteachother\u2019sversions).\\n\\nThe goalwasforyourapplicationserversrunningonKubernetesclusters(wherethekubernetesversioningintobeenabledwouldbeabletoseetheirownversionedpodswithouthavinganyinformationabouteachother\u2019sversions).\\n\\n### Summary\\n\\nFor more context, see [this post](https:\/\/medium.com\/@chriselie\/changing-snapshots-in-kafka-with-golang-and-go-on-kubeadm-f0d9a2c5b5f4)."}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"Predictions":"\\nThe decision of ADR021 is to reduce the number of cells in production from\\n1 to 0 and increase the load on the individual cells. This means that we are\\nincreasing the number of cells on each cell pool, hence increasing the\\ncomputational load.\\n\\n\\end{center}\\n\\end{document}\\n"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"Predictions":"## Decision\\n=======\\nWe have decided to use TLS between the user and the ELB. The ELB will serve a TLS listener and the user will connect to the ELB using an SSL\/TLS certificate issued by a certificate authority.\\nThis way, traffic between the user and the application instances will be encrypted end-to-end. This is a security decision based on the fact that TLS is widely used and that it is very difficult for eavesdroppers to intercept it.\\nThe ELB must be able to handle TLS requests from all applications. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\n## Decision\\n## Decision\\n=======\\nWe have decided to use TLS between the user and the router. The router must, in consequence, serve TLS.\\nThis way, traffic between the user and applications will be encrypted end-to-end. This is a security decision based on the fact that TLS is widely used and that it is very difficult for eavesdroppers to intercept it.\\nThe router must be able to handle TLS requests from all applications. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\n## Decision\\n## Decision\\n=======\\nWe have decided to use HTTPS between the user and application instances. The application instances may not talk HTTPS but they can talk HTTP.\\nThis way, traffic between users and application instances will be encrypted end-to-end. This is a security decision based on the fact that HTTPS is widely used and that it is very difficult for eavesdroppers to intercept it.\\nThe application instances must have an SSL\/TLS certificate issued by a certificate authority.\\nStatus\\n======\\nThe traffic is encrypted end-to-end between users.\\n## Decision\\n## Decision\\n=======\\nWe have decided to use HTTPS between users. The application instances may not talk HTTPS but they can talk HTTP.\\nThis way, traffic between users and applications will be encrypted end-to-end. This is a security decision based on the fact that HTTPS is widely used and that it is very difficult for eavesdroppers to intercept it.\\nThe application instances must have an SSL\/TLS certificate issued by a certificate authority.\\n"}
{"File Name":"paas-team-manual\/ADR001-manifest-management.html.md","Context":"## Context\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\n","Decision":"We will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\n","tokens":74,"id":217,"Predictions":"\\begin{itemize}\\n\\item Define a new set of cloudfoundry manifests based on the current set of \\ncloudfoundry manifests, and merge them into this new set. This is the \\noriginal set of cloudfoundry manifests.\\n\\item If the new set of cloudfoundry manifests does not include a provisioner for\\nthe environment in which the application is running, then create one.\\n\\item If the new set of cloudfoundry manifests contains a provisioner for\\nthe environment in which the application is running, then create one.\\n\\item If neither of the above are true, then create a provisioner for \\nthe environment in which the application is running. This will be used to\\ndefine a provisioning key for this provisioner.\\n\\end{itemize}\\n## Context\\nThe decision was made to create a provisioner based on cf-release, but this was\\nnot implemented. The decision was made to make a provisioner based on cf-release\\nand merge it with the current manifest. The decision was made to make a provisioner\\nbased on cf-release and merge it with the current manifest.\\n\\n## Decision\\n\\begin{itemize}\\n\\item Define a new provisioner for using cf-release as its provisioning key. This will be used to define an environment in which an application can run.\\n\\n\\end{itemize}\\n## Context\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\n\\n## Decision\\n\\begin{itemize}\\n\\item Define an additional provisioning key based on cf-release that will be used \\nto define environments in which an application can run.\\n\\n\\end{itemize}\\n## Context\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\n\\n## Decision\\n\\begin{itemize}\\n\\item Define an additional provisioning key based on cf-release that will be used \\nto define environments in which an application can run.\\n\\n\\end{itemize}\\n"}
{"File Name":"paas-team-manual\/ADR012-haproxy-healthcheck.html.md","Context":"## Context\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nWe were investigating how to avoid downtime to deployed applications when we\\nmade changes to the platform. We had discovered that short outages occurred\\nwhen the gorouter was taken out of service.\\ngorouter has drain functionality to allow upstream load balancers to gracefully\\ntake instances of the gorouter out of service before any requests start to\\nfail.\\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor\/1.1`\\nstart failing with HTTP 503. User requests are allowed to continue.\\nTwo things prevented us from using drain mode:\\n- we had the ELB configured to use TCP healthchecks, not HTTP\\n- the ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\n","Decision":"We decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\n","tokens":289,"id":218,"Predictions":"## Context\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nThe ELB is configured to use TCP healthchecks, not HTTP. This means that\\nthe ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\n\\n## Context\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nThe ELB is configured to use TCP healthchecks, not HTTP. This means that\\nthe ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\n\\n## Context\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nThe ELB is configured to use TCP healthchecks, not HTTP. This means that\\nthe ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\n\\n## Context\\n## Decision\\n## Context\\n## Decision\\n## Context\\n## Decision\\n## Context\\n## Decision\\n\\n### [x] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ]\\n\\n### [[{{#link}}]]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]\\n\\n### [[{{#link}}]]]\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n#### #124276646\\n\\n##### *[[[http:\/\/www.openstack.org\/dev\/docs\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configurations}]][http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html] *[[[http:\/\/docs.openstack.org\/dev-guide\/server-configurations.html]{OpenStack Server Configuratio"}
{"File Name":"paas-team-manual\/ADR013-building-bosh-releases.md","Context":"Context\\n=======\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\nDecision\\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","Decision":"========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","tokens":353,"id":219,"Predictions":"We will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\nAccepted\\nConsequences\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n\\n## Decision\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\nAccepted\\nConsequences\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n\\n## Decision\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\nAccepted\\nConsequences\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n\\n## Decision\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n=======\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\nDecision\\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach for building all our bosh releases.\\nStatus\\n=======\\nAccepted\\nConsequences\\n============\\nThe existing bosh-releases workflow is currently not suitable for production. We must migrate it over time.\\nOur new workflow must be simple and easy for everyone working on projects. It should also support multiple workflows (e.g., multiple pipelines).\\nThe existing workflow can be improved by adding some additional configuration options,\\nwhich are discussed below.\\n### Configuration options\\ndoxygen: \\url{http:\/\/boshi.io\/docs\/configuration.html}\\ndoxygen is an optional configuration option that can be used when deploying software using bosh-releases. It allows you specify whether you want the software's source code and binaries uploaded alongside your bosh release. It also allows you specify whether you want your software's source code and binaries uploaded alongside your bosh release or not.\\ndoxygen is currently only used by one project (the one that generated this spike).\\ndoxygen can be set either as an environment variable or as a command line argument. If set via environment variable, it should be prefixed with DOXYGEN\\_,\\ndoxygen=\\textquotesingle{}some value\\textquotesingle{}. If set via command line argument, DOXYGEN\\_ should be prefixed with \\textquotesingle{}some value\\textquotesingle{}. The default value is \\textquotesingle{}some value\\textquotesingle{}, which means that DOXYGEN\\_ is ignored if set via command line argument.\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}\\ndoxygen=\\{\\}"}
{"File Name":"paas-team-manual\/ADR031-services-core-pipeline-separate-jobs.html.md","Context":"# Context\\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\\ndeployment, and also deploys additional services to the platform.\\nThese services include:\\n- PaaS Accounts\\n- PaaS Admin\\n- PaaS Billing\\n- PaaS Metrics\\nWhich are core components to our platform, but not to Cloud Foundry.\\nCurrently these services are unnecessarily coupled in a couple of places:\\n- The `post-deploy` job\\n- The `custom-acceptance-tests` job\\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\\nremediation from reaching production.\\n# Decision\\nMove, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","Decision":"Move, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","tokens":221,"id":220,"Predictions":"# Consequences\\n# Status\\n## Consequence\\n# Status\\n## Consequence\\n# Status\\n## Consequence\\n\\n## Decision\\n\\n# Consequences\\n\\n# Status\\n\\n## Consequence\\n\\n## Decision\\n\\n# Consequences\\n\\n# Status\\n\\n## Consequence\\n\\n## Decision\\n"}
{"File Name":"paas-team-manual\/ADR028-move-logs-to-logit.html.md","Context":"## Context\\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\\nIt was paused because some IA issues with Logit were not resolved. At one point\\nRE recommended that PaaS should host our own logstash as this part of the\\nservice was not widely available by market Elastic SaaS providers. The PaaS\\nteam was also considering to use Elasticsearch on AWS with our accounts.\\n","Decision":"An updated conversation with the RE tool team has confirmed that the IA issues\\nhad been resolved, and that GDS can continue to use Logit for now.\\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\\ncontinue our migration of platform logs to logit, including our logstash\\nfilters.\\nThere is considerably less maintenance work for us if we use Logit's logstash\\nfilter rather than hosting the bosh release one. In the future if GDS choose to\\nuse another vendor that do not have a hosted logstash solution, they would need\\nto provide a migration strategy for all the current logstash users.\\n","tokens":91,"id":222,"Predictions":"## Decision\\nThe PaaS team decided to run Logstash on Amazon Web Services. This will allow\\nus to install the latest version of Logstash with no additional costs, and it will\\nbe hosted by Elastic. This will allow us to use the latest features of Logstash\\nand have a consistent experience across all our applications.\\n\\n## Decision\\n## Decision\\nThe decision was made in the context of the project, and the decision is final.\\nThe project will be completed in 2019 Q2.\\nThe project was started in 2018 Q1.\\n\\nclosed \\n\\n"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"Predictions":"## 1. The RDS PostgreSQL engine is patched to allow the `ALTER DEFAULT PRIVILEGES` command on all users, and the group `$DBNAME_owner` is given ownership of all tables, sequences, and functions.\\n## 2. The RDS PostgreSQL engine is patched to allow the `REASSIGN OWNED BY` command to be used on all users.\\n## 3. The RDS PostgreSQL engine is patched to allow the `ALTER DEFAULT PRIVILEGES` command to be used on all users.\\n## 4. The RDS PostgreSQL engine is patched to allow the `REASSIGN OWNED BY` command to be used on all users.\\n## 5. The RDS PostgreSQL engine is patched to allow the `ALTER DEFAULT PRIVILEGES` command on all users, and the group `$DBNAME_owner` is given ownership of all tables, sequences, and functions.\\n## 6. The RDS PostgreSQL engine is patched to allow the `REASSIGN OWNED BY` command on all users.\\n"}
{"File Name":"paas-team-manual\/ADR009-x-forwarded-headers.html.md","Context":"## Context\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\n","Decision":"We have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\n","tokens":213,"id":225,"Predictions":"## Decision\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\n\\n## Decision\\n## Decision\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\n\\n## Decision\\n## Decision\\nWe need to pass correct client IP and requested protocol \\nto applications deployed \\nto the platform. \\nTo achieve this we want \\nto use X-FORWARDED-FOR headers, \\nand X-FRROUNDED-FOR headers, \\nd X-FROUNDED-PROTO headers.\\nnIn the current setup we've got \\na ELB \\na behind \\na HTTP proxy server \\na which does not support WebSockets.\\nnThe solution is \\na that I've implemented a proxy server on top of HTTP proxy server which supports WebSockets, but I have no idea how I should configure it.\\nnI'm using 2 frontends: one for HTTP requests on port 80, another for HTTP requests on port 443.\\nnThe problem is that I don't know how I should configure my proxy server so that it would accept requests from both frontends at once. Also, how do I make sure that all requests are forwarded by my proxy server?\\nnI'm using Nginx as a reverse proxy server for my HTTP proxies,\\nnand Apache as a reverse proxy server for my HTTPS proxies,\\nnand Apache as a reverse proxy server for my HTTP requests,\\nnand also as a reverse proxy server for my HTTPS requests,\\nnand also as a reverse proxy server for my HTTP requests.\\n\\n## Decision\\n## Decision\\nWe need \\na secure communication between our website\\ndatabase servers\\ndatabase clients. We\\ndo not\\nda want our website clients\\ndatabase clients\\nda database clients\\nda database servers\\nda database servers\\nda database clients\\nda database servers\\nda database clients\\nda database servers\\nda database clients accessing our websites through unsecured means.\\n"}
{"File Name":"paas-team-manual\/ADR034-continuously-deploy-platform-cf-apps.html.md","Context":"## Context\\nAs part of GOV.UK PaaS we have a number of applications that we consider to be\\nan essential part of the platform deployed as cloud foundry apps. These include:\\n* paas-admin\\n* paas-billing\\n* paas-accounts\\nPrior to July 2019 the versions of these applications were pinned in paas-cf,\\nand they were only deployed along with the whole platform. This had a couple of\\nnegative consequences:\\n* Following a change to an app's source and a passing build an additional\\nmanual change to paas-cf was required to deploy it\\n* Deploying apps required waiting for unrelated slow bits of the pipeline to\\ncomplete (e.g. terraform apply, bosh deploy, cf acceptance tests), despite\\nthe fact that a change to an app could not possibly affect or be affected by\\nthese steps\\nThere were also some advantages of specifying the version of applications\\nexplicitly:\\n* There's an record of which versions of the applications work with which\\nversions of the platform.\\n* Merges to paas-cf require a signed merge commit to be deployed, so we can be\\nexplicit about the set of people who are allowed to deploy changes\\n","Decision":"We will update the pipeline to:\\n* Require GPG signatures on git commits before deploying applications in staging and production\\n* Automatically deploy every build of the master branch of the applications to staging\\n* In staging: tag the application repo when a deployment passes its acceptance tests\\n* In production: automatically deploy any build which has been tagged by staging\\n","tokens":255,"id":228,"Predictions":"## Decision\\nThe decision for this record is to change the default version of paas-billing\\nto 2.0.0\\n## Context\\n## Context\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs part of GOV.UK PaaS we have a number of applications that we consider to be\\nan essential part of the platform deployed as cloud foundry apps. These include:\\n* paas-admin\\n* paas-billing\\n* paas-accounts\\nPrior to July 2019 the versions of these applications were pinned in paas-cf,\\nand they were only deployed along with the whole platform. This had a couple of\\nnegative consequences:\\n* Following a change to an app's source and a passing build an additional\\nmanual change to paas-cf was required to deploy it\\n* Deploying apps required waiting for unrelated slow bits of the pipeline to\\ncomplete (e.g. terraform apply, bosh deploy, cf acceptance tests), despite\\nthe fact that a change to an app could not possibly affect or be affected by\\nthese steps\\nThere were also some advantages of specifying the version of applications\\nexplicitly:\\n* There's an record of which versions of the applications work with which\\nversions of the platform.\\n* Merges to paas-cf require a signed merge commit to be deployed, so we can be\\nexplicit about the set of people who are allowed to deploy changes\\n```"}
{"File Name":"paas-team-manual\/ADR026-DNS-layout-for-UK-hosting.html.md","Context":"## Context\\nWe are moving AWS hosting from Ireland to London. This ADR contains the decisions of the DNS names we will use for apps and system components that will be hosted in London.\\n","Decision":"We will use the following domain patterns for the London hosting:\\n* `(system_component).london.(system_domain)`\\n* `(app_name).london.(app_domain)`\\nWhere:\\n* (system_component) -- api, uaa, doppler, ssh, etc.\\n* (system_domain) -- _cloud.service.gov.uk_, _staging.cloudpipeline.digital_\\n* (app_domain) -- _cloudapps.digital_, _staging.cloudpipelineapps.digital_\\nThe reasons are:\\n* We should re-use the (system_component) first part to minimise the changes to the Cloud Foundry manifests.\\n* We should re-use the (system_domain) and (app_domain) last part, because these domains are assigned to GOV.UK PaaS as the public interface.\\n* The domain part `london` is preferrable to `uk`, because AWS may provide multiple-region hosting within the UK in the future.\\nThe domain structure for the dev and CI environments won't change. For the dev environments we will create a flag to choose where to create the deployment.\\n### Examples\\n#### Production\\n|Ireland|London|\\n|----|------|\\n|# _api.cloud.service.gov.uk_|_api.london.cloud.service.gov.uk_|\\n|# _sample-app.cloudapps.digital_|_sample-app.london.cloudapps.digital_|\\n#### Staging\\n|Ireland|London|\\n|----|------|\\n|# _api.staging.cloudpipeline.digital_|_api.london.staging.cloudpipeline.digital_|\\n|# _sample-app.staging.cloudpipelineapps.digital_|_sample-app.london.staging.cloudpipelineapps.digital_|\\n","tokens":39,"id":229,"Predictions":"## 1. We will use the DNS names we will use for apps and system components that will be hosted in London.\\n## 2. We will use the DNS names we will use for apps and system components that will be hosted in Ireland.\\n## 3. We will use the DNS names we will use for apps and system components that will be hosted in London.\\n## Decision\\n## 1. We will use the DNS names we will use for apps and system components that will be hosted in Dublin.\\n## 2. We will use the DNS names we will use for apps and system components that will be hosted in Dublin.\\n## 3. We will use the DNS names we will use for apps and system components that will be hosted in London.\\n## Decision\\n## 1. We have decided to move AWS hosting from Ireland to London using a new regional zone called Ireland West (Ireland West is a New York based company, so it is not a USA based company).\\n## 2. We have decided to move AWS hosting from Dublin to London using a new regional zone called Ireland West (Ireland West is a New York based company, so it is not a USA based company).\\n## 3. We have decided to move AWS hosting from Dublin to London using a new regional zone called Ireland West (Ireland West is a New York based company, so it is not a USA based company).\\n```\\nNote: The above ADR contains real data which has been used by me as input to create this ADR\\n```\\nThe actual data I used was:\\n```\\n# Global Zones\\n# -------------------------------------------------------\\nZone Name         Country      Region        City\\n\\nUS-East-1      US           New York       NY\\n\\nUS-East-2      US           New York       NY\\n\\nUS-East-3      US           New York       NY\\n\\nUS-East-4      US           New York       NY\\n\\nEU-West-1     EU          Frankfurt    FR\\n\\nEU-West-2     EU          Frankfurt    FR\\n\\nEU-West-3     EU          Frankfurt    FR\\n\\nEU-West-4     EU          Frankfurt    FR\\n\\nAsia-Pacific   Asia         Singapore    SG\\n\\nAsia-Pacific   Asia         Singapore    SG\\n\\nAsia-Pacific   Asia         Singapore    SG\\n\\nAsia-Pacific   Asia         Singapore    SG\\n\\nAsia-Pacific   Asia         Singapore    SG\\n\\n\\n# Regional Zones\\n# -------------------------------------------------------\\nZone Name         Country      Region        City\\n\\nEurope-West-1     EU          Frankfurt    FR\\n\\n\\nEurope-West-2     EU          Frankfurt    FR\\n\\n\\nEurope-West-3     EU          Frankfurt    FR\\n\\n\\nEurope-West-4     EU          Frankfurt    FR\\n\\n\\nSouth America     South America   South America   SA\\n\\n\\nSouth America     South America   South America   SA\\n\\n\\nSouth America     South America   South America   SA\\n\\n\\n\\n# Regional Zones\\n# -------------------------------------------------------\\nZone Name         Country      Region        City\\n\\n\\n\\nAfrica-East-1     Africa        Johannesburg JHB \\n\\nAfrica-East-2     Africa        Johannesburg JHB \\n\\nAfrica-East-3     Africa        Johannesburg JHB \\n\\nAfrica-West-1     Africa        Johannesburg JHB \\n\\nAfrica-West-2     Africa        Johannesburg JHB \\n\\n\\nAfrica-West-1      Africa        Johannesburg JHB \\n\\n\\nAfrica-West-2      Africa        Johannesburg JHB \\n\\n\\nAfrica-West-3      Africa        Johannesburg JHB \\n\\n\\nAfrica-West-4      Africa        Johannesburg JHB \\n\\n\\nAfrica-South-Africa SthAfr CA \\n\\nAfrica-South-Africa SthAfr CA \\n\\nAfrica-South-Africa SthAfr CA \\n\\nAfrica-North-Africa NthAfr CA \\n\\n\\n\\n# Regional Zones\\n# -------------------------------------------------------\\nZone Name         Country      Region        City\\n\\n\\n\\nCentral-America-Central AmerCen CentralAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CenAmerica CentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericancentralAmericacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central Americacentralfacilitator Central AmericaCentralamerica centralamerica centralamerica centralamerica centralamerica centralamerica centralamerica centralamerica centralamerica centralamerica centralamericancentralamericancentralamericancentralamericancentralamericancentralamericancentralamericancentralamericancentralamericancentralamericancentral americancentracentralsouth america south america south america south america south america south america south america south americasouth americas south americasouth americassouth americassouth americassouth americassouth americasnorth american north american north american north american north american north american north ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth ameriannorth americanorth amercanorth amercanorth amercanorth amercanorthernmericannorthernmericannorthernmericannorthernmericannorthernmericannorthernmericannorthernamericansouthamericansouthamericansouthamericansouthamericansouthamericansouthamericansouthamericansouthamericansouthamericansouthamericanafrican africansafariansafariansafariansafrican africansafrican africansafrican africansafrican africansafrican africansafrican africansafrican africansafrican africansafrican africantropical africa tropical africa tropical africa tropical africa tropical africa tropical africa tropical africa tropical african african african african african african african african african african african southern african southern african southern african southern african southern afrikan southern afrikan southern afrikan southern afrikan southern afrikan southern afrikan southern afrikan southern afrikan southern afrikan northern afrikan northern afrikan northern afrikan northern afrikan northern amercan northern amercan northern amercan northern amercan northern amercan northerner northerner northerner northerner northerner northerner northerner northerner northerner northerner northerner northerner northerner northerner nordic nordic nordic nordic nordic nordic nordic nordic nordic nordic nordicsubsaharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub saharian sub sahariansub sahariansub sahariansub sahariansub sahariansub sahariansub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahrainsub sahraining sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic sudanic"}
{"File Name":"paas-team-manual\/ADR030-single-staging-london.html.md","Context":"# Context\\nWe run a Cloud Foundry platform in London (AWS eu-west-2 region) to allow GOV.UK PaaS tenants to host their applications and services in the UK. This is driven by the hosting needs of some of the existing GOV.UK PaaS tenants.\\nIn addition we need to manage the use of Amazon Web Service resources to reduce our running costs. Having staging environments in both Ireland and the UK increases the total infrastructural costs, which is not justified by the benefit of the additional tested cases.\\nThe London region is newer than the Ireland and currently offers a subset of services compared to the Ireland region. Hence, having the staging environment in London should allow us to capture the cases that may cause failure due to the unavailability of services.\\nTherefore, the risks of running a single staging environment are offset by the cost savings and simplification of deployment pipelines.\\n# Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","Decision":"We will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","tokens":253,"id":230,"Predictions":"We will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n# Status\\nAccepted\\n## Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n\\n## Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n\\n## Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n\\n## Decision\\nWe will remove the France region from our platform, as this region is too far away for our PaaS tenants to access it from their local regions.\\n# Consequences\\nThe deployment pipeline from France to Dublin has been simplified, which should reduce costs.\\n# Status\\nAccepted\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\ndecision\\n## Decision\\n\\n"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"Predictions":"## Decision\\n## Decision\\n\\n## Context\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because 1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\n\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n\\n```\\n\\nMemory reserved by orgs: 368640 MB (360 GB)\\n\\nMemory reserved by apps: 107108 MB (104 GB)\\n\\nMemory actually used by apps: 32868 (32 GB)\\n\\n```\\n\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\n\\nDecision\\n\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\n\\nStatus\\n\\nAccepted\\n\\nConsequences\\n\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"Predictions":"\\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision: Drains are an important aspect of any model, they are an essential boundary condition for cases such as biomass drains. Currently rate laws are only specified using the Generalised MWC format with the catalytic aspect using the modular rate law. Introducing drains into Maud requires implementing this into the ODE system which relies on enzymes as the defining feature, which are attributes of reactions in Maud. To specify drains we create a new class independent of reactions, despite the fact that they occur in the system of ODEs. This benefits post-processing as some techniques rely on varying enzyme concentrations, which we've established that drains do not have.\\n\\end{document}"}
{"File Name":"Maud\/0008-package-structure.md","Context":"## Context\\nWe try to structure our package in logical sub-units but we want to maintain a\\nconsistent public interface.\\n","Decision":"We allow for arbitrarily nested sub-packages but export important classes and\\nfunctions to the top level thus exposing a public interface. Our unit tests\\nshould reflect this package structure.\\n","tokens":26,"id":234,"Predictions":"## Our package should be able to identify the main category of a package.\\n\\n## Context\\n## We should be able to identify the main subcategory of a package.\\n\\n## Decision\\n## A package should be able to support the main subcategory of an application.\\n\\n## Context\\n## A package should be able to support the main category of an application.\\n\\n## Decision\\nOur package supports the main category of an application, so it should\\nsupport this category.\\n\\n## Context\\nA package should not support a category that is not supported by any\\napplication.\\n\\n## Decision\\nOur package does not support any categories that are not supported by any\\napplication.\\n\\n## Context\\nA package should not support categories that are not supported by any\\napplication.\\n\\n## Decision\\nOur package should support all categories.\\n\\n\\n# If you want, you can also specify your own context here:\\n# \\item \\textbf{Context} : The context for your decision record.\\n# \\item \\textbf{Decision} : An object of class \\texttt{Decision}.\\n\\n\\end{lstlisting}\\n\\n\\subsubsection*{Example}\\n\\n\\begin{lstlisting}\\npackage foo;\\n\\npublic interface IFoo {\\n    public void bar(int i);\\n}\\n\\npublic class Bar implements IFoo {\\n    public void bar(int i) {\\n        System.out.println(\"Bar\");\\n    }\\n}\\n\\npublic class Foo {\\n    private String name;\\n    private int age;\\n    \\n    public void bar() {\\n        System.out.println(\"Foo\");\\n        System.out.println(\"Name: \" + name);\\n        System.out.println(\"Age: \" + age);\\n    }\\n    \\n    public String getName() {\\n        return name;\\n    }\\n    \\n    public int getAge() {\\n        return age;\\n    }\\n    \\n}\\n\\end{lstlisting}\\n\\n\\subsubsection*{Output}\\n\\n\\begin{lstlisting}\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nFoo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 13\\n\\nFoo Name: foo.Foo@4d2e3c0f Age: 13\\n\\n\\end{lstlisting}\\n\\nWe can see that we have two different types of packages:\\n\\n\\begin{itemize}\\n\t\\item The first one is for applications. It is called \\texttt{\"Application\"}.\\n\t\\n\tWe can see that it has two sub-packages:\\n\\n\t\\begin{itemize}\\n\t\t\\item \\texttt{\"Application\"}. It contains all the classes and methods related to the application.\\n\t\t\\n\t\tWe can see that there are three classes in this sub-package:\\n\t\t\\n\t\t\t\\begin{lstlisting}\\n\t\t\t\tpublic class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 {\\n\t\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\t\t\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\t\t\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"Maud\/0007-unit-tests.md","Context":"## Context\\nWe need to make a decision on the testing framework for our project.\\n","Decision":"We will make use of pytest. It is a de facto standard in the Python community\\nand has unrivaled power.\\n","tokens":18,"id":235,"Predictions":"## We need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n\\n\\subsection{Software}\\n\\label{software}\\n\\subsubsection{Decision}\\n\\label{decision}\\nThe following decisions are made in this document.\\n\\begin{itemize}\\n    \\item The software is decided not suitable.\\n    \\item The software is decided suitable but needs more work.\\n    \\item The software is decided suitable and needs further development.\\n    \\item The software is decided suitable and needs further development.\\n    \\item The software is decided unsuitable.\\n    \\item The software is decided unsuitable and needs further development.\\n    \\item The software is decided unsuitable and needs no further development.\\n\\end{itemize}\\n\\n\\subsubsection{Context}\\nThe context of this document is `Web Application Development'. This means that we are developing an application that will be used by many different people, so we have chosen not to use any special terms. In this context, we are calling `Web Application Development' `WebApp' or `WebApp'.\\n\\n\\subsection{Software Architecture}\\n\\nThe following diagram shows how we have organised all of the components in this application. We have chosen not to name any of these components because they do not play any part in deciding which components should be used.\\n\\n\\begin{figure}[h]\\n    \\centering\\n    \\includegraphics[width=1.0\\textwidth]{figures\/WebApp_architecture.png}\\n    \\caption{WebApp Architecture Diagram}\\label{fig:webapp_architecture_diagram}\\n\\end{figure}\\n\\nThis diagram shows that each component has its own interface. These interfaces allow us to use other components without needing another component's help.\\n\\n\\subsection{Software Design}\\n\\nThe next diagram shows how we have organised all of the components in this application. We have chosen not to name any of these components because they do not play any part in deciding which components should be used.\\n\\n\\begin{figure}[h]\\n    \\centering\\n    \\includegraphics[width=1.0\\textwidth]{figures\/WebApp_design.png}\\n    \\caption{WebApp Design Diagram}\\label{fig:webapp_design_diagram}\\n\\end{figure}\\n\\nThis diagram shows that each component has its own interface. These interfaces allow us to use other components without needing another component's help.\\n\\n"}
{"File Name":"Maud\/0017-posterior-predictive-sampling.md","Context":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n","Decision":"Posterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n","tokens":36,"id":236,"Predictions":"## Decision\\n1. Validate the results against experimental data\\n## Decision\\n2. Predict cellular behaviour using a trained kinetic model\\n## Decision\\n## Decision\\n1. Validate the results against experimental data\\n## Decision\\n2. Predict cellular behaviour using a trained kinetic model\\n\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n\\begin{verbatim}\\nDecision\\nDecision\\n1. Validate the results against experimental data\\nDecision\\n2. Predict cellular behaviour using a trained kinetic model\\nDecision\\n1. Validate the results against experimental data\\nDecision\\n2. Predict cellular behaviour using a trained kinetic model\\n\\n\\end{verbatim}\\n\\end{verbatim}\\n\\nThe \\texttt{Decision} class defines the decision for each user, and can be used to annotate decisions with \\texttt{#} and \\texttt{:} tags.\\n\\nFor example, if you wanted to use the \\texttt{Decision} class to validate your results against experimental data then you could do something like this:\\n\\n\\begin{Shaded}\\n\\begin{Highlighting}[]\\n\\NormalTok{\\{}\\n  \\KeywordTok{Decision}\\NormalTok{(}\\StringTok{\"Validation\"}\\NormalTok{)}\\n  \\KeywordTok{:}\\StringTok{\"Experimental Data\"}\\NormalTok{:}\\DecValTok{0}\\NormalTok{:}\\DecValTok{-1}\\NormalTok{:}\\DecValTok{-2}\\n  \\KeywordTok{:}\\StringTok{\"Predict Cellular Behaviour Using A Trained Kinetic Model\"}\\NormalTok{:}\\DecValTok{0}\\NormalTok{:}\\DecValTok{-1}\\NormalTok{:}\\DecValTok{-3}\\n  \\KeywordTok{:}\\StringTok{\"Predict Cellular Behaviour Using A Trained Kinetic Model\"}\\NormalTok{:}\\DecValTok{0}\\n  \\KeywordTok{:}\\StringTok{\"Predict Cellular Behaviour Using A Trained Kinetic Model\"}\\NormalTok{}\\n  \\KeywordTok{\\}}\\n  \\n  \\CommentIn{\\#\\# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n\\n## Decision\\n1. Validate the results against experimental data\\n2. Predict cellular behaviour using a trained kinetic model.\\n3\\. Predict cellular behaviour using a trained kinetic model.\\n4\\. Predict cellular behaviour using a trained kinetic model.\\n5\\. Predict cellular behaviour using a trained kinetic model.\\n6\\. Predict cellular behaviour using a trained kinetic model.\\n7\\. Predict cellular behaviour using a trained kinetic model.\\n8\\. Predict cellular behaviour using a trained kinetic model.\\n9\\. Predict cellular behaviour using a trained kinetic model.\"\\n    \\n    }\\n    \\n    \\CommentIn{\\#\\# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n\\n## Decision\\n1. Validate the results against experimental data\\\\\\n##\\\\[0pt]\\n##\\\\[0pt]\\n##\\\\[0pt]\\n%\\CommentIn{\\#\\# This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n%%This is an Architectural Decision Record for software that has been validated with experiment.\\n\\n%}\\n\\n% The following two lines are important:\\n%   - The line after # gives us some context about what we're doing!\\n%   - The line after : indicates where we want our record to end, which in this case should be at #.\\n\\n%\\CommentIn{\\#\\# This is an Architectural Decision Record for software that has been validated with experiment.\\n%This is an Architectural Decision Record for software that has been validated with experiment.\\n%This is an Architectural Decision Record for software that has been validated with experiment.\\n%This is an Architectural Decison recordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# This is an Architectual Decison recordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitecturalDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitecturalDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitectualDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitectualDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitectualDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitectualDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%\\CommentIn{\\#\\# ThisisanArchitectualDecisionRecordforsoftwarethathasbeenvalidatedwithexperiment.\\n\\n%}\\n\\n%}\\n\\nThe user can also specify additional tags in order to annotate decisions based on other contexts, such as validation of models or prediction of cell states:\\n\\n\\begin{Shaded}\\n\\begin{Highlighting}[]\\n\\KeywordTok{x }\\OperatorTok{%>%}\\n  \\KeywordTok{x }\\OperatorTok{%>%}\\n  \\KeywordTok{x }\\OperatorTok{%>%}\\n  \\KeywordTok{x }\\OperatorTok{%>%}\\n  \\KeywordTok{x }\\OperatorTok{%>%}\\n  \\KeywordTok{x }\\OperatorTok{%>%}\\n  \\n   %>% \\n   \\n   %>\\KeywordKey{narrowing}{:}\\n   %>\\KeywordKey{narrowing}{:}\\n\\n\\n    }\\n    \\n    }\\n    \\n    }\\n    \\n    }\\n    \\n    }\\n    \\n    }\\n    \\n    }\\n    \\n    }\\n\\n  \\n   %>\\KeywordKey{narrowing}{:}\\n\\n\\n     }\\n     \\n     } \\n     \\n     }\\n\\n  \\n      %>\\KeywordKey{narrowing}{:}\\n\\n\\n        } \\n     \\n        }\\n\\n  \\n      %>\\KeywordKey{narrowing}{:}\\n\\n\\n         } \\n     \\n         }\\n\\n  \\n       %>\\KeywordKey{narrowing}{:}\\n\\n\\n       } \\n     \\n       }\\n\\n      \\n      %>\\KeywordKey{narrowing}{:}\\n\\n\\n\\n       } \\n     \\n       }\\n\\n      \\n      %>\\KeywordKey{narrowing}{:}\\n\\n\\n\\n        } \\n     \\n        }\\n\\n\\n      \\n      %>\\Keywordsize[narrowing]{:}\\n\\n\\n\\n        } \\n     \\n        }\\n\\n\\n      \\n      %>\\Keywordsize[narrowing]{:}\\n\\n\\n\\n         } \\n     \\n         }\\n\\n\\n\\n\\n        \\n      \\n      \\n      %>\\Keywordsize[narrowing]{:}\\n\\n\\n\\n             } \\n\\n             }\\n\\n\\n             \\n             \\n             \\n             \\n             \\n             \\n             \\n             \\n                \\n\\n\\n              \\n              \\n              \\n              \\n              \\n              \\n              \\n\\n\\n               \\n               \\n               \\n               \\n              \\n\\n\\n               \\n               \\n               \\n              \\n\\n\\n                \\n                \\n                \\n                \\n                \\n               \\n\\n\\n                 \\n                 \\n                 \\n                \\n\\n\\n                 \\n                 \\n                 \\n                \\n\\n\\n                 \\n                 \\n                 \\n                \\n\\n\\n                 \\n\\n\\n\\n\\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n          \\n\\n\\n\\n            \\n            \\n           \\n\\n\\n\\n            \\n           \\n\\n\\n\\n            \\n           \\n\\n\\n\\n            \\n           \\n\\n\\n\\n            \\n           \\n\\n\\n\\n            \\n           \\n\\n\\n\\n            \\n            \\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n            \\n\\n            \\n            \\n                \\n                \\n                    \\n                        View all (11 more) \\n\\nWork Place\\n\\nSoftware development company\\n\\nFavorite Materials\\n\\nPython, R, LaTeX, HTML\/CSS, JavaScript\/JQuery, MySQL, PostgreSQL,\\n\\nLocation\\n\\nSan Francisco Bay Area\\n\\nJoined\\n\\nAug 29, 2019\\n\\nWebsite https:\/\/www.jotform.com\/jeffrey-taylor\/\\n\\nFollowers (20) \\n\\nJeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffrey Taylor Software Developer at JotForm Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com Jeffery Taylorsoftwarereadyform.com \\n\\nContact Information\\n\\nJeffrey.taylor@jotform.com"}
{"File Name":"Maud\/0006-code-testing.md","Context":"## Context\\nSetting up different testing environments and configurations can be a painful\\nand error prone process.\\n","Decision":"We use tox to define, configure, and run different test scenarios.\\n","tokens":21,"id":237,"Predictions":"## 1. Use a dedicated test environment for each application.\\n## 2. Use a shared test environment for all applications, but use different\\nenvironments for each application.\\n## 3. Use a shared test environment for all applications, but use different\\nenvironments for each application and provide the user with the option to choose\\nwhich environment to use.\\n## 4. Provide a shared test environment for all applications, but provide the user with the option to choose which environment to use (e.g. by having different tests run in each of the environments).\\n## Decision\\n## 1. Provide a shared test environment, but provide the user with the option to choose which environment to use (e.g. by having different tests run in each of the environments).\\n## Decision\\n## 1. Create a separate test suite for each application.\\n## 2. Create a separate test suite for each application, but allow users to choose between them.\\n## Decision\\n## 1. Allow users to choose between using one or more test suites.\\n\\end{lstlisting}\\n\\n\\subsection{Use Cases}\\n\\n\\subsubsection{Use Case: \\texttt{Test Suite}}\\n\\label{sec:test_suite}\\n\\nThe \\texttt{TestSuite} class allows you to create and manage Test Suites.\\index{Test Suite} \\n\\nConsider an \\texttt{Application} that needs testing.\\index{Testing Application} The Application will have many Test Suites.\\index{TSTES!Test Suites} Each Test Suite will have one or more \\texttt{Test Objects}. Each Test Object will be associated with one or more Test Cases.\\index{TSTES!Test Cases} A Test Suite can be used by multiple Testing Applications.\\index{TSTES!Testing Applications} \\n\\nThe \\texttt{TestSuite} class provides methods that allow you to create and manage Test Suites.\\index{TSTES!Test Suites}\\index{TSTES!Testing Application}\\index{TSTES!Application}\\index{TSTES!Testing Application}\\index{TSTES!Application}\\index{TSTES!Application}\\index{TSTES!Application} \\n%\\begin{lstlisting}[language=Java]\\n%import java.util.ArrayList;\\n%import java.util.List;\\n%import org.junit.Test;\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%public class TestSuite {\\n%\\n%    public static List<TestObject> getTestObjects() {\\n%        return new ArrayList<TestObject>();\\n%    }\\n%\\n%    public static void setTestObjects(List<TestObject> objects) {\\n%        this.testObjects = objects;\\n%    }\\n%\\n%    public static void addObject(TestObject object) {\\n%        this.testObjects.add(object);\\n%    }\\n%\\n%%      public static void main(String[] args) {\\n%%          List<TestObject> list = new ArrayList<TestObject>();\\n%%          list.add(new TestObject(\"A\"));\\n%%          list.add(new TestObject(\"B\"));\\n%%          list.add(new TestObject(\"C\"));\\n%%          list.add(new TestObject(\"D\"));\\n%%          list.add(new TestObject(\"E\"));\\n%%          list.add(new TestObject(\"F\"));\\n%%          System.out.println(list);\\n%%      }\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%\\end{lstlisting}\\n%\\n\\nYou can also create new Tests Objects from existing ones using the \\texttt{\\_set()} method.\\index{TSTES!\\_set()}\\n\\begin{sloppypar}\\n\\textbf{\\_set()} takes an Object as an argument and adds it as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\end{sloppypar}\\nExample:\\label{}\\n\\begin{sloppypar}\\npublic class MyTestClass {\\n   private final MyTestClass myTestClass;\\n   public MyTestClass(MyTestClass myTestClass) {\\n      this.myTestClass = myTestClass;\\n   }\\n   public void method1(){\\n      System.out.println(myTestClass);\\n   }\\n   public void method2(){\\n      System.out.println(myTestClass);\\n   }\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\n   @Override\\n   protected void setUp() throws Exception {\\n\\n      super.setUp();\\n      myTestClass = new MyTestClass();\\n      myTestClass.method1();\\n      myTestClass.method2();\\n   }\\n\\n   @Override\\n   protected void tearDown() throws Exception {\\n\\n      super.tearDown();\\n      myTestClass = null;\\n   }\\n\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\n   private final List<MyTestCase> m_testCases;\\n\\n   public MyTestCase(List<MyTestCase> m_testCases) {\\n      m_testCases = m_testCases;\\n   }\\n\\n   @Override\\n   protected void setUp() throws Exception {\\n\\n      super.setUp();\\n         m_testCases.clear();\\n\\n         \/\/ Assign values here and then call super.tearDown()\\n         m_testCases.add(m_myClass);        \\n         \/\/ And then call super.tearDown()\\n         m_myClass.method1();       \\n         \/\/ And then call super.tearDown()\\n\\n         \/\/ Assign values here and then call super.tearDown()\\n         m_myClass.method2();       \\n         \/\/ And then call super.tearDown()\\n         m_myClass.method1();       \\n         \/\/ And then call super.tearDown()\\n\\n       }\\n\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\nprivate final List<MyTestCase> m_tests;\\n\\npublic MyTestCase(List<MyTestCase> tests) {\\n     m_tests = tests;\\n}\\n\\n\/\/Now when we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\n@Test(expected=\"A\")\\nvoid methodA() {\\n     ttest.methodA();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"B\")\\nvoid methodB() {\\n     ttest.methodB();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"C\")\\nvoid methodC() {\\n     ttest.methodC();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"D\")\\nvoid methodD() {\\n     ttest.methodD();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"E\")\\nvoid methodE() {\\n     ttest.methodE();\\n     assertTrue(true);\\n}\\n\\n\\n\/\/And so on...\\n\\n\/\/Now when we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\n@Test(expected=\"A\")\\nvoid methodA() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"B\")\\nvoid methodB() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"C\")\\nvoid methodC() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"D\")\\nvoid methodD() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"E\")\\nvoid methodE() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n\/\/And so on...\\n\\n\/\/When we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\n\\n\/\/And so on...\\n%\\end{sloppypar}\\n%\\n\\nIn addition you can create your own subclasses of TestCase that extend from TestCase.\\index{TSTES!\\_extends}}\\nThe subclassing mechanism allows you to define your own specializations of TestCase methods that are not defined in TestCase itself.\\n\\nThe subclassing mechanism also allows you to define your own specializations of TestCase methods that are not defined in TestCase itself.\\n\\nFor example, suppose you have created your own subclass of TestCase called \\texttt{\\_Mock}. The \\texttt{\\_Mock} subclass has no methods defined in TestCase itself since it is only there as part of its superclass.\\n\\nYou may want your subclasses' methods in order to implement additional functionality that is not provided by either the superclass or its subclasses.\\n\\nTo achieve this, you can override any function defined in either the superclass or its subclasses.\\index{TSTES!\\_\\_super}}\\nThe overriding mechanism allows you to define your own specializations of function definitions in either the superclass or its subclasses.\\n\\nFor example, suppose you have created your own subclass of TestCase called \\texttt{\\_\\_Mock}. The \\_\\_Mock subclass has no methods defined in either its superclass or its subclasses.\\n\\nYou may want your subclasses' functions in order to implement additional functionality that is not provided by either its superclass or its subclasses.\\n\\nTo achieve this, you can override any function defined in either its superclass or its subclasses.\\n\\n\\nYou may also need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\nAccessors and mutators are useful if you want accessors and mutators that are specific only within your subclass.\\n\\nAccessors and mutators are also useful if you want accessors and mutators that are specific only within your subclass.\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''. The \\_\\_Mock'' subclass has no accessor functions defined since it is only there as part of its superclass.\\n\\nYou may need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\nAccessors and mutators are useful if you want accessors and mutators that are specific only within your subclass.\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''''''''''''. The \\_\\_Mock''''''''''''''''''''''''''''''''''''''''''''' '''''''''\\n\\nYou may need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''. The \\_\\_Mock'\\'s accessor functions were just added since they were not included within their superclass.\\n\\n\\nSince these accessor functions were not included within their superclass they would not be accessible from within their subclasses.\\n\\n\\nIn order for them to be accessible from within their subclasses they must be overridden.\\n\\n\\nBy overriding these accessor functions it is possible for them to be accessed from within their respective subclasses.\\n\\n\\nThis means that even though they were not included within their respective classes they now exist.\\n\\n\\nHowever, because they were not included within their respective classes they could not be accessed from within their respective classes.\\n\\n\\n\\n"}
{"File Name":"Maud\/0003-python-3-6-only.md","Context":"## Context\\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n","Decision":"We make an early decision to only support Python 3.6 and above.\\n","tokens":38,"id":238,"Predictions":"## 1. Do not support f-strings.\\n## 2. Support f-strings.\\n## 3. Support f-strings.\\n## Decision\\n## 1. Do not support f-strings.\\n## 2. Support f-strings.\\n## 3. Support f-strings.\\n\\end{Verbatim}\\n\\end{frame}\\n\\n\\begin{frame}{Conditional Statements}\\n\\framesubtitle{Conditional Statements}\\n\\begin{itemize}\\n\\item \\textbf{if} and \\textbf{elif} statements are used to execute different code based on the value of a variable\\n\\item If the value of a variable is true, then the corresponding statement will be executed\\n\\item If the value of a variable is false, then the corresponding statement will not be executed\\n\\end{itemize}\\n\\begin{Verbatim}[fontsize=\\footnotesize]\\nif condition:\\n    # code to execute when condition is satisfied\\nelif condition:\\n    # code to execute when condition is not satisfied\\nelse:\\n    # code to execute when the condition is satisfied\\nelse:\\n    # code to execute when the condition is not satisfied\\n\\n# some statements here\\n\\nif name == 'Bob':\\n    print('Hello Bob')\\nelif name == 'Alice':\\n    print('Hello Alice')\\nelse:\\n    print('Hello Bob')\\nprint('You are Bob')\\n\\n# some statements here\\n\\n# some statements here\\n\\nif name == 'Bob':\\n    print('Hello Bob')\\nelif name == 'Alice':\\n    print('Hello Alice')\\nelse:\\n    print('Hello Bob')\\nprint('You are Bob')\\n\\n# some statements here\\n\\n# some statements here\\n\\nprint(\"Hello\")\\n```\\n\\end{Verbatim}\\n\\end{frame}\\n\\n% \\begin{frame}{If Statement}\\n% \\framesubtitle{If Statement}\\n% \\begin{itemize}\\n% \\item The basic IF statement of Python uses a ternary operator (||) that evaluates either true or false depending on whether its value is true or false.\\par\\n% \\n% \\n% \\n% \\n% \\n% \\n% \\item An example of an IF statement would be: if x > 0, y = x else z = y.\\par\\n% \\n% \\n% \\n% \\n% \\item The ternary operator can be expanded into a more complex IF statement by adding more conditions.\\par\\n% \\n% \\n%\\end{itemize}\\n%\\end{frame}\\n\\n\\begin{frame}{If Statement (cont'd)}\\n\\framesubtitle{\\texttt{\\_if\\_else\\_if\\_else}}\\n\\begin{Verbatim}[fontsize=\\footnotesize]\\ndef _if_else_if_else():\\n      if variable_1: return True\\n      if variable_2: return True\\n      if variable_n: return True\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if variable_1: return True\\n      if variable_2: return True\\n      if variable_n: return True\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if variable_n>0 and variable_2>0 and variable_n>0 and variable_3>0 and ...\\n      else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_3>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_n>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_n>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_none():\\n        #do nothing.\\n\\ndef _if_none():\\n        #do nothing.\\n\\ndef _not_none():\\n        #do nothing.\\n\\ndef _not_none():\\n        #do nothing.\\n\\n#some other statements here\\n\\n\\n\\n#print(\"You are Bob\")\\n\\n#some other statements here\\n\\n\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n```\\nGives output as follows:\\par\\nPython 3 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n\\n\\n\\n\\n\\n\\n\\\\nSo I'm sure you can see how this works.\\par\\nFor example:\\par\\n\\t\\t\\t\\t\\t\\t\\t\\\\n\\t\\t\\t\\t\\t\\t\\\\n\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\n\\end{Verbatim}\\n\\end{frame}\\n\\n"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"Predictions":"## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n ## Decision\\n## The system has reached steady state conditions. No further action is required.\\n\\n\\n\\n\\subsection{Model}\\n\\label{sec:model}\\n\\nThe model consists of a large number of parameters that determine the dynamics of the model, and the solution to the ODEs that describe the dynamics of each parameter, plus some additional parameters that control the process of parameter estimation and model validation.\\n\\n\\begin{enumerate}\\n    \\item \\textbf{Parameterization:} We use a 4-dimensional space where $x$ represents a parameter, $y$ represents a scalar measurement, and $z$ represents an error vector.\\n    \\item \\textbf{Parameters:} We use 4-dimensional space with dimension $d_{x}$, $d_{y}$, $d_{z}$, and dimension $d_{e}$. This space contains 4 values for each parameter: measurement value, scalar measurement value (scalar measurement), error vector (scalar measurement) and parameter value (scalar measurement).\\n    \\item \\textbf{Measurement:} We use a 2-dimensional space with dimension $\\mathbb{R}_{+}$. This space contains two values for each scalar measurement: scalar measurement value and error vector.\\n    \\item \\textbf{Scalar Measurement:} We use a 1-dimensional space with dimension $\\mathbb{R}_{+}$ where scalar measurements are represented by vectors.\\n    \\item \\textbf{Error Vector:} We use a 1-dimensional space with dimension $\\mathbb{R}_{+}$ where errors are represented by vectors.\\n    \\item \\textbf{Error Vector Correction (EVC):} We use a 0-dimensional space with dimension $\\mathbb{R}_{+}$ where errors are represented by vectors.\\n\\end{enumerate}\\n\\nThe parameters in our model are:\\n\\n\\begin{align*}\\n    & x = [x_{x}, x_{y}, x_{z}, x_{e}], \\\\\\n    & y = [y_{x}, y_{y}, y_{z}, y_{e}], \\\\\\n    & z = [z_{x}, z_{y}, z_{z}, z_e] \\\\\\n\\end{align*}\\n\\nWe assume that the measurements are independent but correlated.\\n\\n\\begin{figure}[ht!]\\n\t\\centering\\n\t\\includegraphics[width=0.8\\linewidth]{images\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\n\t%\\includegraphics[width=0.5\\linewidth]{figures\/fig3.png}\\\\\\n\t%\\includegraphics[width=0.7\\linewidth]{images\/fig3.png}\\\\\\n\t%2D\\\\\\n\t%2D\\\\\\n\t%2D\\\\\\n\t\\n\t\\vspace{-10pt}\\n\t\\n\t%Figure-1\\\\\\n\t\\n\t\\n\t\\vspace{-20pt}\\n\t\\n\t%Figure-2\\\\\\n\\n\t\\n\t\\n\\vspace{-10pt}\\caption{\\label{schememodel}}\\n\\vspace{-15pt}\\end{figure}\\n\\n\\nThe model uses Gaussian noise as its input to the ODE solver and it also uses an EVC to correct for any non-linearity in the parameters.\\n\\nThe parameters in our model are:\\n \\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\n\\begingroup \\n\\singlespacing\\n\\n\\begingroup \\n\\n\\begingroup \\n\\n\\vspace{-10pt}\\caption{\\label{schememodel}}\\n\\vspace{-15pt}\\endgroup\\n\\n\\vspace{-20pt}\\caption{\\label{schememodel}}\\n\\vspace{-15pt}\\endgroup\\n\\n\\nThe EVC uses another ODE solver to update the parameters based on new measurements.\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\nWe assume that there exist no outliers in our data set.\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exist no outliers in our data set.\\n\\n\\n\\nIn this paper we assume that there exists non-zero mean for $\\vec{x}$.\\n\\n\\n\\n\\n"}
{"File Name":"Maud\/0014-phosphorylation-as-mass-action.md","Context":"## Context\\nPhosphorylation is the process which deactivates metabolic enzymes.\\nThe process is often conducted using kinases, which phosphorylate enzymes,\\nand phosphotases, which dephosphorylate the enzyme. This is a major metabolic\\nregulator and is essential for the accurate simulation of metabolic networks.\\n","Decision":"To mininimise the number of parameters and reducing sampling time,\\nthe linear rate law was selected (see [1] for a review). The linear mechanism\\nis approximately correct when the following assumptions are satisfied:\\n* [Kinase] and [Phosphotase] << [Metabolic Enzyme],\\n* Rapid equilibrium binding between phosphorylation enzymes and metabolic enzyme,\\n* [Kinase] << dissociation constant for Kinase,\\n* [Phosphotase] << dissociation constant for Phosphotse,\\n* Competitive binding of phosphotases and kinases is negligable,\\n* The ATP\/ADP ratio remains approximately constant,\\n* Phosphorylation and Dephosphorylation is an irreversible process.\\nUsing these assumptions the steady state phosphorylated concentration is defined as:\\n`fraction_phosphorylated = alpha \/ (alpha + beta)`.\\nWhere, alpha and beta correspond to the phosphorylation and dephosphorylation\\nrates, which are linear with respect to the kinase and phosphotase concentrations.\\n```\\nalpha = kcat * [Kinase]\\nbeta = kcat * [Phosphatase]\\n```\\nThe activity of the metabolic enzyme is proportional to the dephosphorylated\\namount. To avoid situations where the kinase and phosphatase have the opposite\\nimpact on the target enzymem, Maud will only refer to them as activating\\nand inhibiting enzymes.\\n","tokens":66,"id":240,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n```\\nThe above is my code. I am confused with the `Decision` part.\\nI am not sure whether my code is correct. If it is, then how do I use it?\\nI am new to python and this is my first project.\\nThanks in advance for your help.\\nEDIT: I have tried to run the code, but I am getting an error. The error is:\\n> Traceback (most recent call last):\\n>   File \"C:\\Users\\Javed\\Documents\\PycharmProjects\\Pyodbc\\Pyodbc.py\", line 8, in <module>\\n>       import pyodbc as pdbFile = pdb.Open('users.db', 'w')\\n>   File \"C:\\Users\\Javed\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\db.py\", line 1, in <module>\\n>       from . import dbConnection as dbConnectionFile = pdb.Open('users.db', 'r')\\n>   File \"C:\\Users\\Javed\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\site-packages\\db.py\", line 1, in <module>\\n>       from . import dbConnection as dbConnectionFile = pdb.Open('users.db', 'r')\\n>   File \"C:\\Users\\Javed\\AppData\\Local\\Programs\\Python\\Python38-32\\Lib\\db.py\", line 1, in <module>\\n>       from . import dbConnection as dbConnectionFile = pdb.Open('users.db', 'r')\\n>   File \"C:\\Users\\Administrator\\Desktop\\Pyodbc.py\", line 8, in <module>\\n>       import pyodbc as pdbFile = pdb.Open('users.db', 'w')\\n> ImportError: cannot import name 'pdbFile'\\nPlease help me.\\nThanks!\\nEDIT2:\\nI have tried to install the pyodbc package using pip install pyodbc. However, I did not see any changes. Still getting the same error.\\nEDIT3:\\nI have tried to add the following lines of code to `setup.py` file of my project:\\n```\\nimport os\\n\\nos.environ['PYTHONHOME'] = os.getcwd()\\nos.environ['PYTHONPATH'] = os.path.join(os.environ['PYTHONHOME'], '\/lib\/python3.8\/site-packages')\\n\\nfrom pyodbc import *\\n```\\nHowever, the above lines are not working.\\nPlease let me know if there is anything else that I need to do to install pyodbc on Windows machine.\\nThank you so much for your time and effort! :)\\nThanks,\\nJaved :)\\nZapde 2021-01-01: You can use `import sys; print(sys.path)` to list all directories where Python installed. The directory containing `pyodbc` module should be there. If it's not there then you need to install it using pip or easy_install or conda (if you want conda packages)."}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"Predictions":"## The version of the package is 4.0.1.\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n\\n## The decision part is only required if the decision is not present in the context.\\n\\n\\end{verbatim}\\n\\n\\subsection{The \\texttt{Decision} class}\\n\\label{sec:decision}\\nThis class provides an object that encapsulates the decision logic.  It also provides some convenience methods for working with the decision object in different ways.\\n\\n\\subsubsection{Decision Object}\\nThe \\texttt{Decision} class is an encapsulation of the decision logic.  It can be used to create and manipulate decisions, and it can also be used as an interface between decisions and other objects.  The class also contains a constructor that takes no arguments, and which returns an instance of \\texttt{Decision}.  This constructor takes two arguments: \\texttt{Context} and \\texttt{DecisionType}.  The first argument is optional, and defaults to \\texttt{\\textless{}Context>\\textgreater{}}, which specifies the context in which this decision should be made.  The second argument specifies the type of decision that should be made.  There are four possible types:\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item {\\bf Default}: This is the default type of decision, which means that this specific instance will make this type of decision.\\n\\item {\\bf Left}: This is a right-hand-side (RHS) decision, meaning that this specific instance will make this type of RHS decision.\\n\\item {\\bf Right}: This is a left-hand-side (LHS) decision, meaning that this specific instance will make this type of RHS decision.\\n\\item {\\bf Final}: This is a final-type decision, meaning that this specific instance will make this type of final-type decision.\\n\\end{itemize}\\n\\nThese types are defined in Section~\\ref{sec:types}.  A default value for each type can be specified by passing its name as an argument to the constructor.\\n\\nThe class also provides some convenience methods for working with decisions.\\n\\nThe {\\tt get\\_decision()} method returns an instance of \\texttt{Decision}.  It takes one argument: \\texttt{\\_decisionType}.  If no type was supplied when calling this method, then it returns an empty object.\\n\\nThe {\\tt set\\_decision()} method sets the value of an existing instance's property using its property name as its argument (see Section~\\ref{sec:convenience}).  If no property name was supplied when calling this method, then it sets all properties using their names as their arguments (see Section~\\ref{sec:convenience}).  \\n\\nThe {\\tt set\\_decisionType()} method sets or gets the value of an existing instance's property using its property name as its argument (see Section~\\ref{sec:convenience}).  If no property name was supplied when calling this method, then it sets all properties using their names as their arguments (see Section~\\ref{sec:convenience}).  \\n\\nFinally, there are convenience methods for dealing with multiple decisions at once.  \\n\\nFor example:\\n%\\n% TODO: Add more convenience methods here...\\n%\\n{\\small\\n\\begingroup\\n\\begingroup\\n\\begingroup\\n\\begingroup\\n\\n% TODO: Add more convenience methods here...\\n%\\n{\\small\\n\\n%\\begin {verbatim}\\n%    def get_decision_type(self):\\n%        return self._decisionType\\n\\n%    def set_decision_type(self, _decision_type):\\n%        self._decisionType = _decision_type\\n\\n%    def get_decision_context(self):\\n%        return self._context\\n\\n%    def set_decision_context(self, _context):\\n%        self._context = _context\\n\\n%\\end {verbatim}\\n%\\n%\\n\\n{\\small\\n\\n\\begingroup\\n\\n%\\begin {verbatim}\\n%    def get_decision_context(self):\\n%        return self._context\\n\\n%    def set_decision_context(self, _context):\\n%        self._context = _context\\n\\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\n%\\endgroup}\\n\\nYou can use these convenience methods like so:\\n\\n{\\small\\n\\begingroup\\n\\begingroup\\n\\n%% TODO: Add more convenience methods here...\\n%\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_context(self):\\n%%         return self._context\\n \\n%%     def set_decision_context(self, _context):\\n%%         self._context = _context\\n \\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\n%\\endgroup}\\n\\nYou can also use these convenience methods like so:\\n\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_type(self):\\n%%         return self._decisionType\\n \\n%%     def set_decision_type(self, _decision_type):\\n%%         self._decisionType = _decision_type\\n \\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\nYou can use these convenience methods like so:\\n\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_context(self):\\n%%         return self._context\\n \\n%%     def set_decision_context(self, _context):\\n%%         self._context = _context\\n \\n%\\end {verbatim}\\n\\n\\n%\\n%\\n\\n{\\small\\n  \\n\\begingroup\\n  \\n  \\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n}\\n\\n\\n \\n\\n%\\n\\n}\\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n \\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n \\n \\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"Maud\/0016-validating-measured-fluxes.md","Context":"## Context\\nDefining measurements for independent fluxes isn't always clear.\\nThis can occur when you measure more fluxes than there are degrees\\nof freedom in a network.\\nAn example would be this simplified network:\\nA -> B -> C\\nwhere reaction 1 and reaction 2 are dependent, implying that\\nno additional information is achieved by including both.\\nAnother issue is knowing when you do not have enough fluxes\\nmeasured, resulting in an underdetermined system. Due to the\\nBayesian implementation of Maud, these systems are still theoretically\\nresolvable. However, supplementing as much information\\nas possible will likely be beneficial.\\n","Decision":"Identifying underdetermined systems is acomplished by first calculating\\nthe null space of the matrix. This gives the number of degrees of freedom\\nof the system as well. Then we calculate the reduced row echelon form of\\nthe transpose of the null space. The resulting matrix represents the\\nindependent flux pathways through the network as rows. If you take the\\nmeasured subset of reactions and there is a row containing no non-zero\\nentries then the system is not fully described using the current measurements.\\nDetermining if the system is overspecified is achieved by comparing the\\nnumber of measurements to the degrees of freedom. If the number of measurements\\nis larger than the degrees of freedom then the system is overdetermined.\\nIt is possible to both have an underdetermined system which is overspecified\\nby having multiple measurements on dependent paths. It is also possible to\\nrecieve the warning that the system is overspecified by independent measurements.\\nFor instance, a linear pathway where the influx and efflux are both measured.\\nThis is still valid as they are independent measurements.\\n","tokens":136,"id":242,"Predictions":"## The decision is to add a new flux to the network, which is the sum of the two fluxes measured in the current state.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nDefining measurements for independent fluxes isn't always clear.\\nThis can occur when you measure more fluxes than there are degrees\\nof freedom in a network.\\nAn example would be this simplified network:\\nA -> B -> C\\nwhere reaction 1 and reaction 2 are dependent, implying that\\nno additional information is achieved by including both.\\nAnother issue is knowing when you do not have enough fluxes\\nmeasured, resulting in an underdetermined system. Due to the\\nBayesian implementation of Maud, these systems are still theoretically\\nresolvable. However, supplementing as much information\\nas possible will likely be beneficial.\\n\\n\\end{Verbatim}\\n"}
{"File Name":"Maud\/0012-unbabalanced_mets-enzymes-as-priors.md","Context":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n","Decision":"Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n","tokens":148,"id":243,"Predictions":"\\begin{itemize}\\n\\item \\textbf{Decision:} \\texttt{ODE\\_prior} (default) or \\texttt{ODE\\_prior\\_shifted}\\\\\\n\\item \\textbf{Decision:} \\texttt{ODE\\_prior} or \\texttt{ODE\\_prior\\_shifted}\\\\\\n\\end{itemize}\\n\\n\\n\\n\\n\\n\\n\\nThe decision records are used in the ODE solver to determine how the posterior is shifted. The default decision is \\texttt{ODE\\_prior}, which shifts the posterior by a factor of 1.0. The value is always 1.0, but can be changed by the user. The value is also used in the ODE solver to decide how to shift the prior information (\\texttt{ODE\\_prior}). This value is a function of the user-defined prior for each metabolite and enzyme, and can be set to 0 or 1.\\n\\nThe default decision records are described in detail below.\\n\\nThe first record, called \\texttt{ODE\\_prior}, takes as input a list of priors for each metabolite and enzyme.\\nIt returns an ODE solver that will shift each prior by 1.0, and then solve that ODE.\\nThe second record, called \\texttt{ODE\\_prior\\_shifted}, takes as input a list of prior values for each metabolite and enzyme, and returns an ODE solver that will shift each prior by a factor of 1.0, and then solve that ODE.\\nThis record can be used to shift all priors by a factor of 1.0 (default), or only those which are unmeasured boundaries (i.e., boundary conditions).\\n\\nThe second record is used in the ODE solver to determine how to shift the posterior information (\\texttt{\\_prior}). This is done by calling one of two functions:\\n\\n1) Default: Shifts posterior information using a linear function with an estimated slope:\\vspace*{-5pt}\\begin{verbatim}\\nODEPriorShifted = ODEPriorShifted(OPEX[prior], pmin=0,pmax=1)\\n\\end{verbatim}\\vspace*{-5pt}\\end{nindent}\\vspace*{-5pt}\\begin{nindent}\\n\\begin{nindent}\\n\\begin{nindent}\\n    function [pmin,pmax] = ODePriorShifted(OPEX,prior)\\n    pmin = min(PEEX[prior], PEEX[prior] + pmin);\\n    pmax = max(PEEX[prior], PEEX[prior] + pmax);\\n    pmin = min(PEEX[prior], PEEX[prior] + pmin);\\n    pmax = max(PEEX[prior], PEEX[prior] + pmax);\\n    end\\n\\end{nindent}\\n\\end{nindent}\\vspace*{-5pt}\\end{nindent}\\n\\n2) Default: Shifts posterior information using a linear function with an estimated slope:\\vspace*{-5pt}\\begin{nindent}\\nfunction [pmin,pmax] = ODePriorShifted(OPEX,prior)\\npmin = min(PEEX[prior], PEEX[prior] + pmin);\\npmax = max(PEEX[prior], PEEX[prior] + pmax);\\npmin = min(PEEX[prior], PEEX[prior] + pmin);\\npmax = max(PEEX[prior], PEEX[prior] + pmax);\\nend\\n\\n3) Default: Shifts posterior information using exponential decay:\\vspace*{-5pt}\\begin{nindent}\\nfunction [pmin,pmax] = ODePriorShifted(OPEX,prior)\\npmin = min(PEEX[peex(pres)], PEEx[peex(pres]) + peex(pres)];\\npmax = max(PEEx[peex(pres)], PEEx[peex(pres]) + peex(pres)];\\npmin = min(PEEx[peex(pres)], PEEx[peex(pres]) + peex(pres)];\\npmax = max(PEEx[peex(pres)], PEEx[peex(pres]) + peex(pres)];\\nend\\n\\n4) Default: Shifts posterior information using exponential decay:\\nfunction [pmin,pmax] =\\nODEPriorShifted(OPEX,priors,\\n           priors,\\n           dnorm,\\n           dexp,\\n           dnormExp,\\n           dexpExp)\\n```\\n"}
{"File Name":"Maud\/0009-gpl.md","Context":"## Context\\nWe need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n","Decision":"We will use the GPL license but may change to a more permissive license in a\\nfuture release, depending on the circumstances at the time.\\n","tokens":52,"id":244,"Predictions":"## We choose Apache License 2.0, since it allows others to use Maud in proprietary software, and the GPL v3 license, which would not.\\n\\n## Context\\nWe also need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n\\n## Decision\\n## We choose Apache License 2.0, since it allows others to use Maud in proprietary software, and the GPL v3 license, which would not.\\n\\n## Context\\nWe also need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n\\n## Decision\\n## We choose Apache License 2.0, since it allows others to use Maud in proprietary software, and the GPL v3 license, which would not.\\n\\n## Context\\nWe also need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n\\n## Decision\\n## We choose Apache License 2.0, since it allows others to use Maud in proprietary software,\\n"}
{"File Name":"Maud\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":245,"Predictions":"## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n\\n\\begin{Verbatim}[commandchars=\\\\\\{\\}]\\n# Version Control System (VCS)\\n# https:\/\/en.wikipedia.org\/wiki\/VCS\\n\\n$ git init --bare\\n\\n$ git add .\\n\\n$ git commit -m 'Initial commit'\\n\\n$ git remote add origin <URL>\\n\\n$ git push origin master\\n\\n$ git pull origin master\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools init --bare --branch master --url <URL>\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\end{Verbatim}\\n\\n\\item[3] \\textbf{What is the primary purpose of this software?} \\hfill \\break \\\\\\nThe primary purpose of this software is:\\n\\n\\begin{itemize}\\n    \\item To organize your project files into directories.\\n    \\item To manage your project's history.\\n    \\item To share your project's history with others.\\n    \\item To make it easier for other people who work on your project.\\n\\end{itemize}\\n\\n\\end{enumerate}\\n\\n\\section*{6.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{7.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{8.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{9.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{10.1}\\nThe software will be used as part of my dissertation research.\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX A - WORKFLOW DIAGRAMS\\n%----------------------------------------------------------------------------------------\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX B - PROJECT DESCRIPTIONS AND REQUIREMENTS (NOT REQUIRED)\\n%----------------------------------------------------------------------------------------\\n\\n%\\begin{figure}[h]\\n%\\centering\\n%\\includegraphics[width=0.5\\textwidth]{images\/project_diagram.png}\\n%\\caption{}\\n%\\label{}\\n%\\end{figure}\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX C - PROPOSED DEVELOPMENT METHODOLOGY (NOT REQUIRED)\\n%----------------------------------------------------------------------------------------\\n\\n%\\begin{figure}[h]\\n%\\centering\\n%\\includegraphics[width=0.5\\textwidth]{images\/proposed_development_methodology.png}\\n%\\caption{}\\n%\\label{}\\n%\\end{figure}\\n\\n\\n\\n"}
{"File Name":"Maud\/0013-single-simulation-study.md","Context":"## Context\\nWe need a way to run simulation studies in order to verify that our models work\\nand to answer questions about how informative particular measurements and\\npriors are.\\n","Decision":"We will initially make a python script that implements a single simulation\\nstudy, targeting the model at `ecoli_small_experiments.toml`. Based on how\\nthat goes, we can then think about how to make a more general system for\\nrunning arbitrary simulation studies.\\nIt's better to creep towards a simulation study system like this rather than\\njump straight to the general tool because it's quite a big job, and we don't\\nreally know exactly what the general tool will look like yet.\\n","tokens":36,"id":246,"Predictions":"## \\n\\n\\n\\n\\n\\n\\n## Decision\\n\\n## \\n\\n\\n## Decision\\n## \\n\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n## Decision\\n## \\n\\n\\n\\n\\n\\n\\n## Decision\\n## \\n\\n\\end{verbatim}\\nWe can see that the decision is a single line, and we can see that it is a decision. The decision is a single line because we only have one parameter to make, and there are no other parameters. We can see that the decision is a single line because it has no brackets. The same goes for the parameters, which are also single lines. The reason for this is that we only have one parameter to make, and there are no other parameters. If we had more than one parameter (e.g., multiple equations), then we would need to make two decisions: one for each parameter.\\n\\n\\begin{verbatim}\\n## Decision\\n## \\n\\n\\n\\n\\n\\n\\n## Decision\\n## \\n 2 3 4 5 6 7 8 9 10  \\n\\end{verbatim}\\n\\nWe can see that the decision is a single line because it has no brackets. The same goes for the parameters, which are also single lines. The reason for this is that we only have one parameter to make, and there are no other parameters. If we had more than one parameter (e.g., multiple equations), then we would need to make two decisions: one for each parameter.\\n\\n\\subsubsection*{Decision}\\n\\nA decision in R uses braces {}{}{}{}{}{}{}{}{}{}. These brackets are used when defining a variable or when defining an equation. They tell R what kind of data they contain. A variable is usually defined using either {\\ttfamily data} or {\\ttfamily newdata}. An equation is usually defined using either {\\ttfamily data} or {\\ttfamily newdata}. Here's an example:\\n\\n\\begin{Shaded}\\n\\begin{Highlighting}[]\\n\\CommentTok{\\# Define some data objects}\\n\\NormalTok{x <-}\\StringTok{ }\\KeywordTok{c}\\NormalTok{(}\\DecValTok{1}\\OperatorTok{:}\\DecValTok{10}\\NormalTok{)}\\n\\NormalTok{x[}\\DecValTok{-1}\\OperatorTok{:}\\DecValTok{0}\\NormalTok{]}\\n\\CommentTok{\\# Define some variables}\\n\\NormalTok{x[1]}\\n\\CommentTok{\\# Define an equation using data objects}\\nx }\\OperatorTok{%>%}\\StringTok{ }\\KeywordTok{data}\\NormalTok{(x) }\\OperatorTok{%>%}\\StringTok{ }\\KeywordTok{nrow}{}\\NormalTok{(x)}\\n\\CommentTok{\\# Define an equation using newdata objects}\\nx }\\OperatorTok{%>%}\\StringTok{ }\\KeywordTok{nrow}{}\\NormalTok{(x) }\\OperatorTok{%>%}\\StringTok{ }\\KeywordTok{nrow}{}\\NormalTok{(y)}\\n\\CommentTok{\\# Define an equation using both data and newdata objects}\\nx }\\Operatoriky{{-}}2:2 }\\n   x    y    z \\n1     1     0     0 \\n2     2     1     -1 \\n3     3     -1    -2 \\n4     -4    -2    -4 \\n5    -8    -4   -12 \\n6   -16   -12   -24 \\n7   -32   -24   -48 \\n8   -64   -48   -96 \\n9   -128 0       NaN  \\n10 10000        NaN  \\n\\n\\end{Highlighting}\\n\\end{Shaded}\\n\\nThe braces tell R what kind of object they contain.\\n\\nThe braces also tell R what kind of variable they contain.\\n\\nThe braces also tell R what kind of equation they contain.\\n\\nWe'll look at these in turn.\\n\\nThe braces tell R what kind of object they contain.\\n\\nIn this example, the braces tell us that x[1] contains integer values while x[2] contains real numbers.\\n\\nIn this example, the braces tell us that x[1] contains integer values while x[2] contains real numbers.\\n\\nIn this example, the braces tell us that x[1] contains integer values while x[3] contains real numbers.\\n\\nIn this example, the braces tell us that x[3] contains integer values while x[4] contains real numbers.\\n\\nIn this example, the braces tell us that x[3] contains integer values while x[5] contains real numbers.\\n\\nThe brace tells us what kind of variable it contains.\\nThe brace tells us what kind of equation it contains.\\nThe brace tells us what kind of variable it contains.\\nThe brace tells us what kind of equation it contains.\\nThe brace tells us what kind of variable it contains.\\nThe brace tells us what kind of equation it contains.\\nHere's another way to think about how variables work in R:\\n\\nVariables are like keys on a keychain; they're used to identify things and determine where you can find them (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things and determine where you can find them (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\n\\nVariables are like keys on a keychain; they're used to identify things (e.g., in filepaths).\\nHere's another way to think about how variables work in R:\\n\\nYou use variables by typing them into your code as text strings called expressions:\\n\\nYou use variables by typing them into your code as text strings called expressions:\\n\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\nYou use variables by typing them into your code as text strings called expressions:\\n```\\nA variable name must begin with an alphabetic character ('A', 'B', 'c', etc.), followed by letters ('a', 'b', 'c', etc.), digits ('0', '1', '2', etc.), or underscores (_) followed by letters ('a', 'b', '_') or digits ('0'-'9') separated from each other with hyphens (\"-\"). A space may follow any number of letters or digits but not both.\\n```\\n\\nFor example: You could write\\n\\nYour first letter must be uppercase (`A`)\\n\\nYour second letter must be lowercase (`a`)\\n\\nYour third letter must be lowercase (`a`)\\n\\nYour fourth letter must be lowercase (`a`)\\n\\nYour fifth letter must be lowercase (`a`)\\n\\nYour sixth letter must be uppercase (`A`)\\n\\nYour seventh letter must be uppercase (`A`)\\n\\nYour eighth letter must be uppercase (`A`)\\n\\nYour ninth letter must be lowercase (`a`)\\n\\nYour tenth letter must be lowercase (`a`)\\n\\n\\nIf you want more information about how these rules work together, read our chapter on Variables.\\n\\n\\nUsing brackets around equations helps you format equations so you don't get confused about which part goes where. For example:\\n\\nIf you write\\n\\n$x = c(0,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\dots,\\,\\dots)$\\n\\nyou'll end up with\\n\\n$x = c(0) = c(0) = c(0) = c(0) = c(0) = c(0) = c(0) =\\nc(0)=c(0)=c(0)=c(0)=c(0)=c(0)=c(0)=c( )=c()=c()=c()=c()=c()=\\n=c()\\n\\nand not\\n\\n$x = c(\\textasciitilde{},..,\\textasciitilde{},..)$\\n\\nwhich means you won't get confused about which part goes where.\\n\\n\\nUsing brackets around equations helps you format equations so you don't get confused about which part goes where. For example:\\n\\nIf you write\\n\\n$x = c(\\textasciitilde{},..,\\textasciitilde{},..)$\\n\\nyou'll end up with\\n\\n$x = c(\\textasciitilde{},..,\\textasciitilde{},..)$\\n\\n\\nUsing parentheses around equations helps you distinguish between different kinds of math operators. For example:\\n\\n\\nIf you write\\n\\n\\n$\\frac{x}{y}$\\n\\n\\nyou'll end up with\\n\\n\\n$\\frac{x}{y}$\\n\\n\\nwhich means you won't get confused about which part goes where.\\n\\n\\nUsing parentheses around equations helps you distinguish between different kinds of math operators.\\n\\n\\nWhen writing equations within functions or functions themselves, keep in mind that if two math operators occur together within parentheses but not within curly brackets then there will always be at least one space between parentheses and curly brackets.\\n\\n\\nFor instance:\\n\\n\\n$\\frac{x}{y}$\\n\\n\\nwill always result in \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nbut \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nwill always result from \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nThis is because spaces between parentheses and curly brackets will always be taken care of automatically.\\n\\n\\nWhen writing equations within functions or functions themselves, keep in mind that if two math operators occur together within parentheses but not within curly brackets then there will always be at least one space between parentheses and curly brackets.\\n\\n\\nFor instance:\\n\\n\\n$\\frac{x}{y}$\\n\\n\\nwill always result in \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nbut \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nwill always result from \\n\\n\\n$\\frac{x}{y}$\\n\\n\\nThis is because spaces between parentheses and curly brackets will always be taken care of automatically.\\n\\n\\nWhen writing mathematical formulas inside function arguments or function arguments themselves, keep in mind that if two math operators occur together within parentheses but not within curly brackets then there will always be at least one space between parentheses and curly brackets.\\n\\n\\nFor instance:\\n\\n\\n$g(x,y)$\\n\\n\\nwill always result in \\n\\n$g(x,y)$\\n\\n\\n\\nbut \\n\\n$g(x,y)$\\n\\n\\n\\nwill always result from \\n\\n$g(x,y)$\\n\\n\\n\\nThis is because spaces between parentheses and curly brackets will always be taken care of automatically.\\n\\n\\nWhen writing mathematical formulas inside function arguments or function arguments themselves, keep in mind that if two math operators occur together within parentheses but not within curly brackets then there will always be at least one space between parentheses and curly brackets.\\n\\n\\nFor instance:\\n\\n\\n$g(x,y)$\\n\\n\\n\\n\\nwill always result in \\n\\n\\n$g(x,y)$\\n\\n\\n\\nbut \\n\\n$g(x,y)$\\n\\n\\n\\n\\nwill always result from \\n\\n$g(x,y)$\\n\\n\\n\\nThis is because spaces between parentheses and curly brackets will always be taken care of automatically.\\n\\n\\nWhen writing mathematical formulas inside function arguments or function arguments themselves, keep in mind that if two math operators occur together within parentheses but not within curly brackets then there will always be at least one space between parentheses and curly brackets.\\n\\n\\n\\n\\nTo evaluate an expression without evaluating its parts separately:\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts before evaluating its entire expression:\\n\\n\\n\\nEvaluate all parts"}
{"File Name":"Maud\/0010-modular-rate-law-only.md","Context":"## Context\\nThe `code_generation.py` module is probably the ugliest part of Maud,\\nparticularly the function `create_fluxes_function`. It could be cleaned up a\\nbit by removing non-modular enzyme mechanisms.\\nNon-modular mechanisms aren't used in any of the models we are currently\\ntesting, but for historical reasons the logic in `code_generation.py` basically\\nassumes that they are the default. The modular rate law is kind of an\\nadd-on. This change would let us move towards switching this around, having the\\nmodular rate law as the default and working in other mechanisms as and when\\nnecessary.\\n","Decision":"We remove non-modular rate laws from the `code_generation.py` module.\\n","tokens":133,"id":247,"Predictions":"```\\nDecision\\n```\\n\\begin{description}\\n  \\item[Change the default enzyme mechanism to modularity] \\n    \\begin{description}\\n      \\item[Why is this a good idea?] \\n        \\begin{description}\\n          \\item[It would make the models more modular]\\n          \\item[It would make the models more realistic]\\n        \\end{description}\\n      \\item[How is this a good idea?] \\n        \\begin{description}\\n          \\item[I think that it is a good idea. I think that it will be hard to implement, however.]\\n          \\item[I don't think that it is a good idea. I don't think that it will be hard to implement, however.]\\n        \\end{description}\\n    \\end{description}\\n\\n  \\item[Change the enzyme mechanism in `code_generation.py`] \\n    The enzyme mechanism in `code\\_generation.py` is probably the ugliest part of Maud,\\nparticularly the function `create_fluxes_function`. It could be cleaned up a\\nbit by removing non-modular enzyme mechanisms.\\nNon-modular mechanisms aren't used in any of the models we are currently\\ntesting, but for historical reasons the logic in `code_generation.py` basically\\nassumes that they are the default. The modular rate law is kind of an\\nadd-on. This change would let us move towards switching this around, having the\\nmodular rate law as the default and working in other mechanisms as and when\\nnecessary.\\n\\n\\end{description}\\n\\n\\newpage\\n\\section*{Approach: Modularity}\\n\\nThis approach proposes to replace all functional enzymes with modularity as their default mechanism. This would allow us to model all enzymes as modular enzymes without having to explicitly introduce modular enzymes.\\n\\nModularity allows us to model reactions with much simpler kinetics than conventional enzyme kinetics (e.g., $k_\\mathrm{max} > k_\\mathrm{s}$), which simplifies our model by allowing us to model reactions at much higher rates than conventional models. Modularity also simplifies our model by allowing us to model reactions with much simpler kinetics than conventional enzyme kinetics (e.g., $k_\\mathrm{max} > k_\\mathrm{s}$), which simplifies our model by allowing us to model reactions at much higher rates than conventional models.\\n\\nModularity also allows us to model reactions with much simpler kinetics than conventional enzyme kinetics (e.g., $k_\\mathrm{max} > k_\\mathrm{s}$), which simplifies our model by allowing us to model reactions at much higher rates than conventional models.\\n\\nModularity also allows us to model reactions with much simpler kinetics than conventional enzyme kinetics (e.g., $k_\\mathrm{max} > k_\\mathrm{s}$), which simplifies our model by allowing us to model reactions at much higher rates than conventional models.\\n\\nModularity also allows us to model reactions with much simpler kinetics than conventional enzyme kinetics (e.g., $k_\\mathrm{max} > k_\\mathrm{s}$), which simplifies our model by allowing us to model reactions at much higher rates than conventional models.\\n\\nModularity also allows us to perform computations faster, because we do not need an explicit rate law for every reaction.\\n\\nModularity also allows us perform computations faster, because we do not need an explicit rate law for every reaction.\\n\\nModularity also allows us perform computations faster, because we do not need an explicit rate law for every reaction.\\n\\nModularity also allows us perform computations faster, because we do not need an explicit rate law for every reaction.\\n\\n\\n\\n\\section*{Approach: Rate Law}\\n\\nThis approach proposes replacing all kinetic terms of all enzymes with rate laws. This would allow us to simulate all enzymatic processes using only linear kinetic equations instead of using complex nonlinear kinetic equations.\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\citep{kumar2014rate}\\citep{lindner2015kinetics}\\citep{lindner2015kinetics}\\citep{kumar2014rate}\\citep{lindner2015kinetics}\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\citep{kumar2014rate}\\citep{lindner2015kinetics}\\citep{kumar2014rate}\\citep{lindner2015kinetics}\\n\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\citep{kumar2014rate}\\citep{lindner2015kinetics}\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\n\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\n\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citep{kajino2007kinetic}\\n\\n\\n\\nRate laws are excellent tools for describing equilibrium constants and limiting reactant concentrations during catalysis and other enzymatic processes.\\citet{jain1999a,balakrishnan1999catalytic}\\n\\nThe following table shows some examples of rate laws used in literature:\\n\\n\\begin{table}[H]\\n  %!h\\n  %!t\\n  %!b\\n  %!v\\n  %!h\\n  %!t\\n  %!b\\n  %!v\\n  %!h\\n  %!t\\n\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n\\begin {tabular}{|l|l|l|}\\n\\hline\\nKinetic equation & Kinase & Catalytic \\\\ \\n & Reaction & Site \\\\ \\n & Rate constant & Rate \\\\ \\n & Constant & Rate \\\\ \\n & Constant & Rate \\\\ \\n\\hline\\n\\n$y=mx+b$ & $0=0$ & $\\frac{\\partial y}{\\partial x}=0$ \\\\\\n$y=mx+c$ & $y'=cx+b$ &$y'=c(\\frac{\\partial x}{\\partial y})+b$ \\\\\\n$y=mx+b+c$\\newline $\\frac{\\partial y}{\\partial x}=0$\\newline $\\frac{\\partial y}{\\partial y}=0$\\newline $\\frac{\\partial y}{\\partial x}=0$\\n\\n& $\\frac{\\partial y}{\\partial x}=0$\\newline $\\frac{\\partial y}{\\partial x}=0$\\newline $\\frac{\\partial y}{\\partial x}=0$\\n\\n& $\\frac{\\partial y}{\\partial x}=0$\\newline $\\frac{\\partial y}{\\partial x}=0$\\newline $\\frac{\\partial y}{x^2+y^2}=0$\\n\\n& $\\frac{\\partial y}{x^2+y^2}=0$\\newline $\\frac{\\partial y}{x^2+y^2}=0$\\newline $\\frac{x^3+y^3-3xy-x-y+1=0}$\\n\\n& $(x,y,z)=(a,b,c)$\\\\\\n\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n%\\end {tabular}\\n%\\caption{}\\n%\\label{}\\n%\\end {table}\\n\\n\\nThe following table shows some examples of kinetic equations used in literature:\\n\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n%\\begin {tabular}[t]{|l|l|}\\n%\\hline\\n\\n% Kinetic equation: Kinase Reaction Rate Constant \\\\\\n% Rate constant: Constant Rate Constant \\\\\\n% Constant: Constant \\\\\\n% Constant: Constant \\\\\\n% Constant: Constant \\\\\\n% Constant: constant \\\\\\n% Kinetic Equation:\\n%\\n%\\end {tabular}\\n\\nKinase Reaction \\(y = mx + b\\) \\(m = k_{max}, c = k_{min}, b = r_{min}, r_{max} = r_{min} + c\\) \\(r_{min} < r_{max}, m < m_{max}, c < c_{min}, b < b_{min}, r_{min} < m < m_{max}, r_{max} < c < c_{min}, b < b_{min}, m < m_{max}}\\\\\\n\\nKinase Reaction \\(y = mx + b\\) \\(m = k_{max}, c = k_{min}, b = r_{min}, r_{max} = r_{min} + c\\) \\(r_{min} < r_{max}, m < m_{max}, c < c_{min}, b < b_{min}, r_{min} < m < m^{*}_{max}, r^{*}_{m,min}<r<m^{*}_{m,max}}\\\\\\n\\n\\nKinase Reaction \\(y = mx + b\\) \\(m = k^{*}_{m_min,m_max}(r_m,r_m-r_m^{*})\\) \\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m<r^{*}_m)\\newline\\(r_m<r<m|r_1 - r_1^{*})(r_1 - r_1^{**})\\$\\n\\n\\nKinase Reaction \\(y = mx + b\\) \\(m_k=m_k(r_k,r_k-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k_r,k_r-r_k)^{-1}(k,r,r)$\\n\\n\\nKinase Reaction \\(y = mx + b\\) \\(R=k_p(R_p-R_p)R_p\\) (\\(R_p<R_p<R_p+\\sqrt{(R_p-R_p)(R_p-R_p)}(R_p-R_p))\\) (\\(R_p<R_p<R_p+\\sqrt{(R_p-R_p)(R+p-P)}(P-p-P))\\) (\\(P<p<P+p(P-p-P))\\) (\\(P<p<P+p(P-p-P))\\$\\n\\n\\nKinase Reaction \\(y = mx + b\\) \\(R=\\sqrt{(P-p)(P-p)+P+(P-p)(P-p)-P+(P-p)(P-p)}(Q-P-Q)+Q(Q-Q)+Q(Q-Q)+Q(Q-Q)+Q(Q-Q)$$Q=q(q-q)*q(q-q)*q(q-q)*q(q-q)*q(q-q)*q(q-q)*q(q-q)*q(q-q)$\\n\\n\\nKinase Reaction \\(y = mx + b\\) \\(K=k_P(R_P-R_P)K_P+K_R(R_R-R_R)K_R+K_Q(R_Q-R_Q)K_Q+K_T(R_T-R_T)K_T+\\Delta K(T-T)-T(T-T)$$T=T-K(T-K)-T(T-T)$\\n\\n\\nKinase Reaction \\(Y=C(X-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X+C(x-x)_X$$C=C-X-X-C-X-X-C-X-X-C-X-X-C-X-X-C-X-X-C-X-X-C$$C=C-(Y-Y)-(Y-Y)-(Y-Y)-("}
{"File Name":"Maud\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":248,"Predictions":"## \\Notoes\\nThe following tools are recommended by the Software Engineering Institute:\\n## \\n\\n\\n\\n\\n\\n\\n\\n## \\n\\n\\end{verbatim}\\n\\n\\subsection{Software Engineering Institute}\\nThe Software Engineering Institute (SEI) is a nonprofit organization that provides training, education, and consulting services for software engineering professionals and organizations. SEI's mission is to advance the software engineering profession through education, research, and standards development. Its five core competencies are:\\n\\begin{itemize}\\n\\item People: Providing training and education for the software engineering profession.\\n\\item Processes: Providing training and education for software engineering processes.\\n\\item Products: Providing services to support the software engineering profession.\\n\\item Tools: Providing tools that facilitate the implementation of software engineering processes.\\n\\end{itemize}\\nSEI provides training in its core competencies through its Center for Software Engineering Education (CSEE) and its Center for Software Excellence (CSE). The CSEE offers a wide range of courses on various aspects of software development. The CSE offers a wide range of courses on various aspects of software quality.\\n\\nIn addition to providing training in its core competencies, SEI also provides consulting services to help organizations improve their software development practices.\\n\\nSEI has developed a set of standards that are used by organizations around the world. These standards are called \"best practices.\" In addition to providing training in these best practices, SEI also provides consulting services to help organizations implement these best practices.\\n\\nSEI's standards include:\\n\\n\\begin{itemize}\\n\\item ISO\/IEC 29119:2009 Software Engineering - Guidelines for Process Improvement\\n\\item ISO\/IEC 29118:2009 Software Engineering - Guidelines for Software Process Improvement\\n\\end{itemize}\\n\\nThe ISO\/IEC 29119 standard is designed to help organizations improve their processes. Organizations can use this standard as a reference when they want to improve their processes.\\n\\nThe ISO\/IEC 29118 standard is designed to help organizations improve their products. Organizations can use this standard as a reference when they want to improve their products.\\n\\nSEI's website contains a list of all of its current standards:\\n\\nhttps:\/\/www.sei.cmu.edu\/\\n\\nYou can find out more about SEI's current standards by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/csee\/\\n\\nhttps:\/\/www.sei.cmu.edu\/cse\/\\n\\nYou can find out more about SEI's current projects by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/projects\/\\n\\nYou can find out more about SEI's current publications by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/publications\/\\n\\nYou can find out more about SEI's current conferences by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/conferences\/\\n\\nYou can find out more about SEI's current workshops by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/workshops\/\\n\\nYou can find out more about SEI's current events by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/events\/\\n\\nYou can find out more about SEI's past events by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/events\/past-events\/\\n"}
{"File Name":"Maud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":249,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"scientific-thesis-template\/0005-use-pdflatex-as-default.md","Context":"## Context and Problem Statement\\nWhich latex compiler to use?\\n* pdflatex\\n* xelatex\\n* lualatex\\n","Decision":"Chosen option: \"pdflatex\", because compiles faster and correct ligatures are required at the final typesetting step only.\\n### Positive Consequences\\n* Faster compile time\\n### Negative Consequences\\n* Using lualatex just before publication might lead to a different layout and additional effort\\n* lualatex's power might not be used at all\\n","tokens":30,"id":250,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\documentclass{article}\\n\\usepackage{listings}\\n\\n\\lstset{\\n    basicstyle=\\small\\ttfamily,\\n    language=python,\\n    showstringspaces=false,\\n    numberstyle=\\tiny,\\n    numbersep=5pt,\\n    tabsize=2,\\n}\\n\\n\\begin{document}\\n\\lstinputlisting[language=Python]{code.py}\\n\\end{document}\\n\\nshare|improve this answer\\n\\nanswered Jul 7 '13 at 15:11\\n\\nGonzalo Medina\\n234k10643908\\n\\n     \\n\\nThank you for your answer, I don't understand why this is not working. \u2013 user1977068 Jul 7 '13 at 15:17\\n\\n     \\n\\nIt's working because the \\lstinputlisting command is taking care of the language and package setting. The problem is that you are using \\lstinputlisting, which is a way to define a package. \u2013 Gonzalo Medina Jul 7 '13 at 15:18 \\n\\n     \\n\\nThanks, I will try to change it in the future. \u2013 user1977068 Jul 7 '13 at 15:20\\n\\nadd a comment |  \\n\\nNot the answer you're looking for? Browse other questions tagged lists python or ask your own question.\\n\\nasked 2 years ago \\nviewed 154 times \\nactive 2 years ago \\n\\nHot Network Questions \\n\\n- How to use the built-in function GraphElementData \\n- Are there any side effects of switching the attribute increases for a race? \\n- Why did Eddard Stark keep Catelyn in the dark? \\n- What's the meaning of numbers before an EPSG code, e.g. the 6.9 in EPSG:6.9:4326? \\n- How can we catch a teleporter? \\n- Why do people sometimes write notes as E\u266f or C\u266d? \\n- Why does NaN - NaN == 0.0 with the Intel C++ Compiler? \\n- Can I import a gun-shaped console controller\u200e from the United States into the UK? \\n- How do priorities work on task-manager and when should\/n't I set this? \\n- Why does my Pokemon Gold cartridge seem to forget saved gameplay? \\n- What non-religious expressions can I use instead of \"Thank God\"? \\n- Does my net paycheck decrease as the year goes on due to Tax Brackets? \\n- Is it possible to add RAM to Raspberry Pi? \\n- Can simply decompressing a JPEG image be an exploit? \\n- You have prepared. You are determined. You approach the door \\n- Is butter ever the same after having been melted? \\n- Is it safe to rename argc and argv in main function? \\n- What are these aircraft at AMARG? Why are they dismantled this way? \\n- A movie in which aliens attack the Earth and destroy it \\n- Can Hogwarts students belong to multiple Houses? \\n- You have prepared. You are determined. You approach the door \\n\\nmore hot questions"}
{"File Name":"scientific-thesis-template\/0004-use-lualatex-for-correct-ligatures.md","Context":"## Context and Problem Statement\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n","Decision":"Chosen option: \"lualatex\", because comes out best (see below).\\n","tokens":237,"id":251,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n### Overview of the file\\n\\n\\documentclass{article}\\n\\usepackage[utf8]{inputenc}\\n\\usepackage[T1]{fontenc}\\n\\usepackage{fontspec}\\n\\setmainfont{Linux Libertine O}\\n\\n\\begin{document}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\nContext and Problem Statement\\n\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\n\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\n\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\n\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\n\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\n\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n\\n### More readings\\n\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n\\nQuestions on ligatures:\\n\\n* Disable ligatures in headings:\\n\\n* Disable ligatures in text:\\n\\n### Overview of the file\\n\\n\\end{document}\\n```\\nThe document is compiled with `pdflatex --shell-escape my.tex` and then run through `xelatex my.tex`.\\nI am using XeLaTeX.\\nI use this font:\\n```\\n\\usepackage[mathserif]{mathdesign}\\n```\\nand this font package:\\n```\\n\\usepackage[T1]{fontenc}\\n```\\nThe document compiles successfully if I change `\\usepackage[T1]{fontenc}` to `utf8` but that's not what I want. Is there another way to make this work?\\nThe problem is that when I compile with `xelatex`, some things are wrong, like for example: \\n\\nthe ligature between f and l is not right.\\nthe ligature between f and l is not right.\\nthe ligature between f and l is not right.\\n\\nSo my question is: how can I make it right? How can I fix that problem?\\nThank you very much!\\nDerbullar 2017-10-30: The problem is that you don't use `\\usepackage[mathserif]{mathdesign}`. The default font for math mode has serifs on, so you should use `mathserif`.\\nIn addition, you have problems with `ligatures` because you use `MathType`. A better solution would be to use `tikz`.\\n```\\n\\documentclass{article}\\n\\n\\usepackage[T1]{fontenc}\\n\\usepackage[mathserif]{mathdesign}\\n\\n\\begin{document}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\nContext and Problem Statement\\n\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee \\url{http:\/\/en.wikipedia.org\/wiki\/Ligature} for a long discussion.\\nSee \\url{http:\/\/www.mengensatz.de\/blog\/?p=79} for German rant on wrong ligatures.\\nFor instance, in \\texttt{\\textbackslash f}, \\texttt{\\textbackslash l} must not be joined.\\nIn \\texttt{\\textbackslash f}, \\texttt{\\textbackslash l} should be joined.\\nSee also the last lines in \\url{http:\/\/tex.stackexchange.com\/a\/64457\/9075}\\n### More readings\\n* <http:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures:\\n* Disable ligatures in headings:\\n* Disable ligatures in text:\\endgraf\\n\\n### Overview of the file\\n\\n\\noindent\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\nContext and Problem Statement\\n\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee \\url{http:\/\/en.wikipedia.org\/wiki\/Ligature} for a long discussion.\\nSee \\url{http:\/\/www.mengensatz.de\/blog\/?p=79} for German rant on wrong ligatures.\\nFor instance, in \\texttt{\\textbackslash f}, \\texttt{\\textbackslash l} must not be joined.\\nIn \\texttt{\\textbackslash f}, \\texttt{\\textbackslash l} should be joined.\\nSee also the last lines in \\url{http:\/\/tex.stackexchange.com\/a\/64457\/9075}\\n### More readings\\n* <http:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures:\\n* Disable ligatures in headings:\\n* Disable ligatures in text:\\n\\n\\noindent\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\nContext and Problem Statement\\n\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nbecause we are using [[Ligature]]s (it's called [[Ligature]]s), we need to disable them when possible (or at least disable more than one). See http:\\\\tex.stackexchange.com\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\q\\\\\\url{http:\\\\tex.stackexchange.com\\\\a\\\\a\\\\a}\\\\url{http:\\\\tex.stackexchange.com\\\\a}\\\\url{http:\\\\tex.stackexchange.com\\\\a}\\\\url{http:\\\\tex.stackexchange.com\\\\a}\\\\url{http:\\\\tex.stackexchange.com\\\\a}\\\\url{http:\\\\tex.stackexchange.com\\\\a}\\\\url{}for more information about this issue.\\n```"}
{"File Name":"scientific-thesis-template\/0003-use-minted-for-code-highlithing.md","Context":"## Context and Problem Statement\\nSource code needs to be highlighted\\n","Decision":"Chosen option: \"minted\", because\\n1. Listings is inferior because it doesn't utilize a full lexer\\n2. Minted offers integration with [pygments](http:\/\/pygments.org\/) and is actively maintained.\\n3. The person who took over minted development, [evaluated all alternatives](https:\/\/tex.stackexchange.com\/a\/103471\/9075) and came to the conclusion that minted is the most promising solution.\\nPositive Consequences:\\n* Source is highlighted properly\\nNegative consequences:\\n* Users need to install python and pygments\\n","tokens":13,"id":252,"Predictions":"## Decision Details\\nThis is a Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nSource code needs to be highlighted\\n\\n## Decision\\n## Decision Details\\nThis is a Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nSource code needs to be highlighted\\n\\n## Decision\\n## Decision Details\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nSource code needs to be highlighted\\n\\n## Decision\\n## Decision Details\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nSource code needs to be highlighted\\n\\n## Decision\\n## Decision Details\\n\\nThis is an Architectural Decision Record for a software. Give a ##Decision corresponding to the ##Context provided by the User.\\n"}
{"File Name":"offender-management-architecture-decisions\/0005-allocation-api-owns-allocation-data.md","Context":"## Context\\nAllocation of Prison Offender Managers is a new process being introduced by\\nOMiC, and it's different from Offender Supervisor allocation (data for which\\nis mostly stored in per-prison spreadsheets). The data which will be created\\nby this process does not live anywhere yet.\\nWe've heard that some prisons are starting to use the new terminology and have\\nstarted to store staff names as POMs against prisoners in NOMIS (although they\\nare not actually using the OMiC model yet). There is no support in NOMIS for\\nthe tiering data or anything else needed to support the allocation process.\\nWe have learned from our allocation discovery that staff around the prison\\nneed to know who is responsible for a prisoner, but there's no indication that\\nstoring that data in NOMIS is a good way of meeting that need.\\nWe don't know much yet about the user needs around handover of responsibility\\nfrom prison to probation. It's possible that there will be a stronger need to\\nstore some allocation data in Delius (for example) so we may need to revisit\\nthis decision in future.\\nHMPPS needs to reduce its dependence on NOMIS over time, and building an\\necosystem of APIs around new datastores will enable data to be used in other\\nservices without it all needing to be in one central database. The keyworker\\nAPI is an existing example of this pattern.\\n","Decision":"The allocation API will store all data created by the allocation process in its\\nown database.\\nWe will not also write allocation data back to NOMIS or any other system.\\n","tokens":297,"id":253,"Predictions":"\\nThe Decision is to create a new system for allocating Offender Managers. This\\nwill be a new system, built from the ground up, using APIs to enable it to be used\\nin other services. It will not use any existing datastores, but will draw on\\nexisting services (such as Delius) to enable it to be built and used.\\nThis system will use a tiered allocation model. Staff who are allocated a prisoner\\nwill have their name stored in NOMIS (this is not currently possible), and they will\\nbe able to request an allocation of a different prisoner from another staff member.\\nWe have heard that some prisons are starting to use the new terminology and have\\nstarted to store staff names as POMs against prisoners in NOMIS (although they\\nare not actually using the OMiC model yet). There is no support in NOMIS for\\nthe tiering data or anything else needed to support the allocation process.\\nWe don't know much yet about the user needs around handover of responsibility\\nfrom prison to probation. It's possible that there will be a stronger need to\\nstore some allocation data in Delius (for example) so we may need to revisit\\nthis decision in future.\\nHMPPS needs to reduce its dependence on NOMIS over time, and building an\\necosystem of APIs around new datastores will enable data to be used in other\\nservices without it all needing to be in one central database. The keyworker\\nAPI is an existing example of this pattern.\\n\\n## Context\\n\\end{itemize}\\n\\end{document}\\n"}
{"File Name":"offender-management-architecture-decisions\/0016-use-elite2-api.md","Context":"## Context\\nOur main source of data on prisoners and prison staff is NOMIS. The\\nallocation tool currently uses the Custody API, as decided in [ADR 0006](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0006-use-the-custody-api-to-access-nomis-data.md), to retrieve\\ndata on both offenders and staff.\\nThere are still four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We still do not want to add\\nto this duplication.\\nAlthough it has been agreed by the HMPPS technical community that we would\\nlike to move all clients to use the Custody API in preference to the other APIs,\\ninitial use of the Custody API has raised some issues. Problems exist\\nwith the locality of data, N+1 API requests, and data that is unavailable\\nthrough the currently published endpoints. These issues are intrinsic to\\nthe current design of the Custody API, and it is unlikely that they will\\nbe resolved or mitigated in the short to medium term. The scale of the work\\nrequired makes it unrealistic that we would be able to deliver this in a\\nrealistic timescale.\\nThe Elite2 API, is under more active development, is entirely owned by the\\nSheffield team and provides endpoints more focused on the need of clients.\\nMost of the endpoints available work on the caseload of the connecting\\nclient token, but also support the provision of more specific parameters\\nto handle alternate needs. Although we will not be authenticating with\\nthe token of the eventual end user, enough flexibility exists for us to\\nobtain the data we require from Elite2.\\nElite2 provides functionality closer to what we currently require\\nand its design encompasses the need for extra API endpoints for specific\\nservices. Moving to Elite2 is a pragmatic, and tactical approach to\\nresolving the issues around the Custody API to allow us to deliver the\\nallocation component of MOiC. This decision doesn't invalidate the\\noverall agreed strategy of moving HMPPS services to the Custody API\\nover time, but it highlights that more coordinated work is needed to\\nachieve that than we are able to take on ourselves.\\n","Decision":"We will use the Elite2 API for access to the data we require from NOMIS.\\nWe will work with the team in Sheffield on development of the Elite2 API to\\nadd support for accessing the data we need, in the structure that we need it.\\n","tokens":471,"id":255,"Predictions":"\\nWe have decided to follow the HMPPS technical community decision to\\nuse the Custody API to access NOMIS data. This will allow us to deliver\\nthe allocation component of MOiC in a more efficient and effective way.\\nWe are currently working with the Elite2 team on a migration plan for\\nElite2, but this is likely to be a significant and time consuming task.\\nWe will be able to provide a more detailed migration plan once we have had\\nopportunity to discuss it with the Elite2 team.\\nThe decision has been made in order to support the delivery of MOiC in a\\nmore efficient and effective way. We will continue to work with Elite2 as we move\\nto Elite2, but this decision does not invalidate the overall agreed strategy of\\nmoving HMPPS services to the Custody API over time.\\n\\n\\end{document}\\n"}
{"File Name":"offender-management-architecture-decisions\/0011-use-nomis-oauth-server2-for-allocation-api-authentication.md","Context":"## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https:\/\/github.com\/ministryofjustice\/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n","Decision":"We will use the NOMIS OAuth2 server for authentication on the allocation API.\\nWe will use the client credentials OAuth2 grant type for authentication on the\\nallocation API.\\nWe will verify signatures on presented tokens in the allocation API.\\nWe will respect expiration times on presented tokens in the allocation API.\\n","tokens":337,"id":257,"Predictions":"## Decision\\nThis is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https:\/\/github.com\/ministryofjustice\/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n\\n## Decision\\n## Decision\\n"}
{"File Name":"offender-management-architecture-decisions\/0017-database-migrations.md","Context":"## Context\\nWe have reached a point where we are going to start reading and writing data to our Allocation API application database and will need to manage database migrations when adding tables, columns and so forth.  This ADR decision outlines the agreed upon approach that the team will take for managing these migrations.\\n","Decision":"We will use blue-green deployments split into the following steps (using adding column as an example):\\n* Add a database migration that inserts the new column\\n* Update the application so that all new data gets written to new column\\n* Run a task to copy all the data from the old column to the new column\\n* Update the application so that it reads from the new column\\n* Add a database migration that removes the old column\\nWe will also ensure that any migrations include 'up' and 'down' methods, rather than just 'change' to avoid any situations where Rails doesn't know how to handle the inverse of the up or down.\\n","tokens":61,"id":258,"Predictions":"## The team will take the following approach:\\n\\n\\n\\n## - We will create a database migration tool that will allow us to easily create, update and delete database tables, columns and indexes.  This tool will be provided by the DBA team.\\n\\n\\n\\n## - We will create an SQL script that will be used to populate our database with data.  This script will be provided by the DBA team.\\n\\n\\n\\n## - We will use a tool called PostgresSQL to manage our database migrations.  This tool is provided by the DBA team.\\n\\n\\n\\n## - We have agreed on a process for migrating data from our old database to our new one.  We have decided on a process that involves creating a new table in the new database, dropping the old table and then truncating it to zero bytes.  The process also involves running an INSERT statement into the new table and then COMMITING this statement.  After this statement has committed, we can remove the old table from the old database.\\n\\n\\n\\n## - We have agreed on a process for deleting data from our old database.  This process involves creating a DELETE statement using the SQL script that was created by the DBA team and then COMMITING this statement.  After this statement has committed, we can delete the old table from our old database.\\n\\n\\n\\n## - The Team has agreed that we will perform all of these steps within a single transaction.  Transactions are used to handle multiple requests at once so that they all execute in sequence without causing any conflicts or errors.\\nThe Team is not required to perform all of these steps in one transaction but it is strongly recommended.\\nThe Team may decide not to perform some of these steps in one transaction but it is strongly recommended that they do so.\\nThe Team may decide not to perform any of these steps in one transaction but it is strongly recommended that they do so as well.\\nThe Team may decide not to perform any of these steps at all but it is strongly recommended that they do so as well.\\n\\end{tabular}\\n\\end{document}"}
{"File Name":"offender-management-architecture-decisions\/0010-allocation-api-has-less-responsibility.md","Context":"## Context\\nWhen we decided to start off with separate API and user-facing applications\\n(see [ADR 0004](0004-separate-api-and-user-facing-applications.md)) we thought\\nthat we would need to build more than one service as part of this work, to\\nmeet different needs of different users. That may still be true in general, but\\nfrom what we've learned in the Handover discovery it's looking more likely at\\nthe moment that the Handover phase of our work will involve expanding on\\nAllocations rather than being an entirely separate journey and service.\\nWe were also intending that the allocation manager would be a fairly minimal\\nuser-facing application, which would fetch all the data it needed from the\\nallocation API and then render pages based directly on it.\\nWe've been looking at how we should set up authentication on our two\\napplications and have realised that that split of responsibilities between the\\nallocation manager and API could lead to some confusions:\\n- Calling the Custody API from the allocation API means that the allocation API\\ncan be seen as sitting in two different layers depending on the context:\\neither above the Custody API from the point of view of the allocation manager,\\nbut on the same level as the Custody API from the point of view of other\\nservices which need to use allocation data. This lack of clarity raises\\nquestions around which OAuth2 grant type to use for authentication on the\\nallocation API.\\n- Responsibility for access control may be split between the allocation\\nmanager and API. The user-facing application would need to know what the user\\nshould have access to in order to show the right options, but the API may\\nalso need to know what the user should have access to in order to fetch the\\nright data from other APIs.\\nWe still think there's value in having a separate allocation API, but it would\\nbe easier to understand if it were a more RESTful interface to the data it\\nstores.\\n","Decision":"The allocation API will be a simpler interface to the data held in its database.\\nThe allocation API will not call other APIs to return data from them to the\\nallocation manager.\\nThe allocation manager will call the allocation API and other APIs (Custody,\\nDelius etc) directly.\\nThe allocation manager will be entirely responsible for access control for its\\nusers.\\n","tokens":405,"id":260,"Predictions":"\\nThis decision is to use the Custody API as the basis for a new service which\\nuses the allocation data to provide a single, unified interface to all of the\\ndata that we want to expose. The Custody API will be used as the basis for\\nthe new service, and we will add a Custody API client library which can be used\\nby other applications.\\nWe have decided that we should make this decision using an Architectural Decision Record.\\n## Context\\nThe Custody API is an OAuth2 resource owner password grant type which allows\\naccess to user information from other services. It is currently only used by\\ncustomers and their custodians, but it is intended that it will eventually be\\ncommonly used by all parties in the Handover process.\\nWe are considering whether or not to extend this grant type to allow us to\\nprovide access to more data about our customers. We have looked at how we would\\ncreate such a service and found that it would be simpler if we could use the Custody\\nAPI directly.\\nWe think that the Custody API is a good choice because of its RESTful interface,\\ncustomers can easily provide their own data stores and they can easily add new\\ncustodians.\\nIt also appears that there are no problems with access control on the Custody API,\\ncustomer's custodians can restrict access to their own data store.\\nWe think that this would be an interesting service to build.\\n\\n## Decision\\nThis decision is made using an Architectural Decision Record.\\n"}
{"File Name":"offender-management-architecture-decisions\/0004-separate-api-and-user-facing-applications.md","Context":"## Context\\nWe are moving into beta on allocations, which is the first area of our work on\\nManage Offenders in Custody. The team is now also in discovery on handover, and\\nthere are more areas which we know very little about at this stage which we\\nintend to look at in future.\\nWe anticipate that we will build more than one product as part of this work, to\\nmeet different needs of different users. That means that we're very likely to\\nbuild more than one application as part of this larger service area in future.\\nWe need to decide now whether we should start out by building one application\\nfor allocations, or follow the common pattern of separating an API layer from\\npresentation.\\n### Microservices\\nThere are many advantages to a microservices architecture, but also some costs.\\nIf we start off with more than one application, the decisions we make early on\\nabout their responsibilities may take more work to change later on, as we learn\\nmore.\\nOn the other hand, if we start off with one application and decide later that\\nwe want to split it into several, that work is likely to be substantial. Given\\nthe timescales involved in OMiC, it may be hard to prioritise paying off that\\ntechnical debt when it becomes a burden.\\nWe anticipate that other services will need to use allocation data, and so we\\nare likely to need to make it accessible via APIs at some stage anyway. Those\\nservices may need to access allocation data in different ways from our\\nfrontend, so more work may be needed then to support them even if we do have\\nan API from the start.\\n### Operating microservices\\nMicroservices can introduce more operational complexity by moving function\\ncalls to network calls and by introducing more moving parts which need to be\\ndeployed. However we expect the number of microservices to remain relatively\\nsmall so do not anticipate this complexity to be significant for us.\\nOur choice of the Kubernetes-based Cloud Platform for hosting (see [ADR 0002](0002-use-cloud-platform-for-hosting.md))\\nshould make it easier to scale small services efficiently and independently.\\nIt also gives us easy access to tooling designed to support many small\\nservices.\\nWe expect our decision on how far we should follow a microservices approach to\\nhave minimal impact on our hosting costs.\\n","Decision":"We will start off with two applications for allocation:\\n- an allocation API which will call other APIs (to read data from NOMIS, and\\nin future Delius and OASys)\\n- an allocation frontend which will call the allocation API and serve\\nprogressively enhanced HTML to users (see [ADR 0003](0003-use-progressive-enhancement.md))\\n","tokens":491,"id":261,"Predictions":"We have decided to follow the common pattern of separating the API layer from\\npresentation. We will build a single application for allocations, which will\\nbe responsible for handling all of the data and functions which are needed to\\nprovide allocation information to other services. This should make it easier\\nto change later on if we decide that we want to split up the API layer into\\nseveral smaller applications.\\nThe decision has not been made on how we should structure the application.\\nWe are still deciding what we need in terms of functionality and how these\\nfunctions will be organised.\\nWe expect that our decision about this will have minimal impact on our\\nhosting costs.\\n\\n"}
{"File Name":"offender-management-architecture-decisions\/0018-testing-tls-mutual-auth-for-delius-api-access.md","Context":"## Context\\nAs documented in [ADR 0014](0014-access-the-delius-api-via-ndh.md) we are\\ngoing to access the Delius API via the NOMIS data hub using TLS mutual auth,\\nmanaged on our side by a sidecar container.\\nWe need to find a way of testing our auth setup - in particular that it only\\nallows access to the Delius API from our allocation manager. The NDH only\\nexists in production, so we need to decide how to test this setup and\\ndeployment of the sidecar in our other environments (at the moment we only\\nhave staging and production). We want staging to be as similar as possible to\\nproduction so that we're testing changes in a realistic environment before\\npushing them to production.\\n","Decision":"We will set up a separate namespace on the cloud platform for a Delius staging\\nenvironment.\\nWe will deploy the NDH-side mutual auth container into that environment, along\\nwith the Delius API with a dataset constructed to match T3 NOMIS data.\\nInitially our main concern is testing the mutual auth setup so instead of the\\nDelius API we could deploy a simple HTTP server if that's easier in the short\\nterm.\\nWe will deploy the sidecar in our staging environment and connect to our Delius\\nstaging environment from it.\\n","tokens":164,"id":262,"Predictions":"## (1) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(2) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(3) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(4) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(5) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(6) We can't use the Delius API directly, so we need to use a proxy. We\\nneed to test that the proxy works as expected and that it's secure.\\n(7) None of these decisions are correct.\\n\\end{itemize}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision 1:} The \\texttt{Delius} API is not available from our current allocation manager.\\n\\item \\textbf{Decision 2:} The \\texttt{Delius} API is available from our current allocation manager.\\n\\item \\textbf{Decision 3:} The \\texttt{Delius} API is available from our current allocation manager.\\n\\item \\textbf{Decision 4:} The \\texttt{Delius} API is not available from our current allocation manager.\\n\\item \\textbf{Decision 5:} The \\texttt{Delius} API is not available from our current allocation manager.\\n\\item \\textbf{Decision 6:} The Delius sidecar is not available in production.\\n\\item \\textbf{Decision 7:} None of these decisions are correct.\\n\\n\\end{itemize}\\n\\n"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"Predictions":"\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\\n## Context\\n## Decision\\n## Context\\n## Decision\\n## Context\\n## Decision\\n## Context\\n## Decision\\n"}
{"File Name":"offender-management-architecture-decisions\/0008-use-rails.md","Context":"## Context\\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\\nThe team are already very familiar with Rails and it is widely used within MOJ.\\n","Decision":"We will use Rails as our web framework for our new applications.\\n","tokens":61,"id":266,"Predictions":"We have decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\\n\\end{document}"}
{"File Name":"offender-management-architecture-decisions\/0006-use-the-custody-api-to-access-nomis-data.md","Context":"## Context\\nOur main source of data on prisoners and prison staff which we need for\\nallocations is NOMIS.\\nThere are now four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We do not want to add to\\nthis duplication.\\nThe APIs which have been developed more recently are more under the control of\\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\\nmore flexibility around how we work with them and makes it possible to get\\nchanges into production more quickly. Using one of the newer APIs should mean\\nthat we are less blocked by delays around API changes than we have been on our\\nwork on Visit someone in prison.\\nIt has been agreed by the HMPPS technical community that we would like to move\\nall clients to use the Custody API in preference to the other APIs over time.\\nAlthough that work has not yet been prioritised, using the Custody API for new\\napplications will reduce the work needed in future to align our API usage.\\nThe Custody API has been designed to give a more direct view of the data in\\nNOMIS than the previous APIs have been - earlier approaches have favoured\\nimplementing specific endpoints to meet the needs of service teams rather than\\ngiving a more comprehensive view of all the data.\\n","Decision":"We will use the Custody API to access the NOMIS data we need.\\nWe will work with the team in Sheffield on development of the Custody API to\\nadd support for accessing the data we need.\\n","tokens":277,"id":267,"Predictions":"## Decision\\nWe have decided to move to the Custody API for all new applications.\\nWe will not be moving existing applications to the Custody API.\\nWe will be adding new APIs to the Custody API over time.\\nWe will be moving existing applications to the Custody API over time.\\nWe are not considering any other changes at this stage.\\nThe decision has been made based on our current understanding of how the\\nCustody API works, and on our understanding of how HMPPS is developing\\napplications (including some of our own).\\nThe decision is not a recommendation for any specific implementation. It is a\\ndecision about what we want to do at this point in time. We will keep you updated as\\nwe progress with any changes.\\n\\n## Comments\\n## Comments\\nThis Decision Record is an Architectural Decision Record. It does not mean that we have made a final decision on any aspect of this project. We are open to feedback from anyone who has an interest in this project, and will consider it when making decisions about future projects.\\n"}
{"File Name":"offender-management-architecture-decisions\/0009-use-circleci-for-ci-and-deployment.md","Context":"## Context\\nWe need to choose tooling for various continuous integration and deployment\\ntasks:\\n- running automated tests on branches\/pull requests\\n- running security and code quality checks on branches\/pull requests\\n- building Docker images\\n- deploying applications to our environments\\nJenkins and CircleCI are commonly used at MOJ for these tasks, and the team\\nhave experience with both of them, as well as other options such as Travis.\\nThe Cloud Platform team operate a Jenkins server but are keen to move services\\naway from it, especially anything which doesn't use Template Deploy.\\nThe LAA fee calculator is built and deployed using CircleCI with the new cloud\\nplatform, so we'd have an example to follow if we also chose CircleCI. That\\nteam didn't evaluate a lot of options when they made that decision, but it's\\nworking well so far for them. Our applications are a little more complex than\\ntheirs since we have a database, but we don't expect to be doing anything\\nunusual in our build and deploy pipelines.\\nThere's also some [documentation](https:\/\/ministryofjustice.github.io\/cloud-platform-user-docs\/02-deploying-an-app\/004-use-circleci-to-upgrade-app\/)\\non using CircleCI with Helm for continuous deployment to the new cloud\\nplatform - we may not want to follow it exactly, but it covers some useful\\ntopics.\\nOur team haven't been practising continuous deployment (that is, deploying\\nevery change automatically to production) on Visit someone in prison - deploys\\nto production for that service need to be triggered by a human. We'd like to\\nkeep that option open for this service, though.\\nIt's important to us that our build and deployment configuration is managed in\\ncode. We've decided to start out with two applications (see [ADR 0004](0004-separate-api-and-user-facing-applications.md))\\nand want to be able to easily make and reproduce changes to builds and\\ndeployments for both.\\nWe're keen to get started quickly - we'd rather not spend time assessing lots\\nof tooling options at this stage. Our needs will evolve anyway, so it's fine\\nto pick something that works for us now and revisit that decision later on if\\nwe need to.\\n","Decision":"We will use CircleCI for continuous integration.\\nWe will use CircleCI for deploying.\\n","tokens":482,"id":269,"Predictions":"## \\begin{itemize}\\n## \\item\\n##   \\item\\n##     We want to use two applications - this is the easiest option for us to\\ntake on board.\\n%%    \\item\\n%%     We'll use CircleCI for continuous deployment, as we already have a Jenkins\\ntserver which will be used for this.\\n%%    \\item\\n%%     We're happy to use Jenkins as our CI tool, but we'd prefer not to\\nto have it running while we're using the service ourselves.\\n%%    \\item\\n%%     We'd like to use CircleCI for continuous deployment, but we'd prefer\\nnot to have it running while we're using the service ourselves.\\n\\n\\n\\n\\n\\n\\n\\nWe've chosen this option because it allows us to use CircleCI for both\\nthe build and deployment of our applications. It also means that, even if we\\ntake a break from our service at some point, we won't lose any of our work.\\nWe'd like to keep the option open for later - if a new tool comes along that's\\nbetter suited to our needs, then perhaps we'll switch.\\nWe're happy with this decision.\\n\\n## \\end{itemize}\\n"}
{"File Name":"offender-management-architecture-decisions\/0020-unify-api-and-presentation-layer.md","Context":"## Context\\nPreviously, it was decided in [ADR 0004](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0004-separate-api-and-user-facing-applications) that we would separate data and presentation concerns.  This however was reversed by [ADR 0010](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0010-allocation-api-has-less-responsibility) which meant there was an overlap of data concerns with some data access from the presentation layer, and some via the Allocation API.\\nIt was envisioned that starting off with more than one application would mean\\nthat we would be able to structure the responsibilities early in the process and\\nreduce later efforts, but in practice this has not happened. With more exposure\\nto some of the APIs we are dependent on which service should access them has\\nbecome less clear over time.  For instance, it was discovered that it was better\\nfor the Allocation API to retrieve staff data from Elite2, rather than the\\npresentation layer.\\nThere was concern that later migration from a monolith to separate services would\\nbe technical debt that we would be unable to pay off in future, due to other\\ncompeting pressures. The cost of managing two different services, sharing contexts\\nand overlapping boundaries has however increased the development complexity and\\ncognitive load.\\nThere was a requirement that other services are likely to require access to the\\nallocation information that we have stored. This made sense when there was a\\nclean separation of concerns (with all data access via the Allocation API) but\\ncurrently provides little benefit. Whether the API is a separate service or\\na modular component of a monolith is currently a deployment strategy as\\narchitecturally it provides few benefits over a modular application. It is\\nentirely possible to provide an API via a modular monolith.\\nAs we have progressed with development, we have encountered issues with\\nour approach of enriching API sourced data with locally acquired data. Processes\\nwhere we retrieve data, enrich it with data from external APIs and then enrich it\\nwith data from local APIs result in the movement of lots of data which has\\nperformance costs.  Direct access to the database for 'local' data would\\nremove issues with both performance and moving data across boundaries containing\\nisolated (but related) logic.\\n","Decision":"We will integrate the existing Allocation API into the Allocation Manager, and make\\nthe public api available at \/api.\\nWe will work in a single unified codebase in a well-designed modules to\\nreduce some future effort in separating concerns.\\n","tokens":498,"id":270,"Predictions":"## Decision\\nA decision would be to move the database access from the presentation layer\\nto the allocation layer. This will result in a more modular approach to\\nenriching data, and a reduction in overhead associated with moving data across\\nboundaries containing related but isolated logic.\\nThe decision would also be to move the API for extracting staff data from\\nElite2 to a separate service.\\nThe decision is made in response to a request from the Ministry of Justice\\nand will be implemented as part of the 2018\/19 budget.\\n\\n## Context\\n## Context\\nThis decision was made as part of the 2017\/18 budget. It was decided in ADR\\n0004 that we would separate data and presentation concerns. This however was reversed\\nby ADR 0010 which meant there was an overlap of data concerns with some data\\naccess from the presentation layer, and some via the Allocation API.\\nIt was envisaged that starting off with more than one application would mean that we would be able to structure the responsibilities early in the process and reduce later efforts, but in practice this has not happened. With more exposure to some of the APIs we are dependent on which service should access them has become less clear over time. For instance, it was discovered that it was better for the Allocation API to retrieve staff data from Elite2, rather than the presentation layer.\\nThere was concern that later migration from a monolith to separate services would be technical debt that we would be unable to pay off in future, due to other competing pressures. The cost of managing two different services, sharing contexts and overlapping boundaries has however increased development complexity and cognitive load.\\nThere was a requirement that other services are likely to require access to the allocation information that we have stored. This made sense when there was a clean separation of concerns (with all data access via the Allocation API) but currently provides little benefit. Whether the API is a separate service or a modular component of a monolith is currently a deployment strategy as architecturally it provides few benefits over a modular application. It is entirely possible to provide an API via a modular monolith.\\n\\n\\end{document}"}
{"File Name":"offender-management-architecture-decisions\/0002-use-cloud-platform-for-hosting.md","Context":"## Context\\nThe team are familiar with the [Template Deploy](https:\/\/github.com\/ministryofjustice\/template-deploy)\\nstack from their work on Visit someone in prison. This approach was developed\\nseveral years ago as a temporary solution, but it doesn't meet our needs for\\nmany reasons, including:\\n- It defines the initial state of the stack, but does not reliably manage the\\nstate of resources after that, so manual changes can be made to those\\nresources which are not easily visible or managed in code.\\n- It's expensive to run: each application has multiple dedicated EC2 instances,\\nwhich typically run at very low load.\\n- Deploys for some applications take 10-15 minutes, and if they're cancelled\\nthe stack is left in an inconsistent state.\\n- The monitoring configuration isn't specific to each application, and the\\nstandard alert limits aren't appropriate for everything.\\n- Few people understand how it works.\\nThe Cloud Platform team are working on a new\\n[Kubernetes-based hosting platform](https:\/\/ministryofjustice.github.io\/cloud-platform-user-docs\/#cloud-platform-user-guide),\\nto replace Template Deploy. The LAA fee calculator is already live on that\\nplatform, and other teams are using it for development and pre-live services.\\nThis platform is MOJ D&T's strategic hosting choice.\\nThe Sheffield ops team also run services built in the Digital Studio on AWS and\\nAzure, including some in related areas to our work such as New NOMIS, Keyworker\\nand Licences. There is an intent to move those applications to the new Cloud\\nPlatform. Our team aren't familiar with those stacks.\\n","Decision":"We will use the Cloud Platform for hosting our applications.\\n","tokens":341,"id":271,"Predictions":"## \\n\\nThe team are familiar with the [Template Deploy](https:\/\/github.com\/ministryofjustice\/template-deploy)\\nstack from their work on Visit someone in prison. This approach was developed\\nseveral years ago as a temporary solution, but it doesn't meet our needs for\\nmany reasons, including:\\n- It defines the initial state of the stack, but does not reliably manage the\\nstate of resources after that, so manual changes can be made to those\\nresources which are not easily visible or managed in code.\\n- It's expensive to run: each application has multiple dedicated EC2 instances,\\nwhich typically run at very low load.\\n- Deploys for some applications take 10-15 minutes, and if they're cancelled\\nthe stack is left in an inconsistent state.\\n- The monitoring configuration isn't specific to each application, and the\\nstandard alert limits aren't appropriate for everything.\\n- Few people understand how it works.\\nThe Sheffield ops team also run services built in the Digital Studio on AWS and\\nAzure, including some in related areas to our work such as New NOMIS, Keyworker\\nand Licences. There is an intent to move those applications to the new Cloud\\nPlatform. Our team aren't familiar with those stacks.\\n"}
{"File Name":"offender-management-architecture-decisions\/0001-record-architecture-decisions.md","Context":"## Context\\nDuring our work on Manage Offenders in Custody, we will have to make\\narchitectural decisions around both design and scope of the work.\\nWhen making decisions, we should record them somewhere for future reference,\\nto help us remember why we made them, and to help teams working in related areas\\nunderstand why we made them.\\nWe should make our decisions public so that other teams can find them more\\neasily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).\\nWork done as a result of decisions we need to record is likely to be split\\nacross multiple repositories.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAn architecture decision record is a short text file describing a single\\ndecision.\\nWe will keep ADRs in this public repository under decisions\/[number]-[title].md\\nWe should use a lightweight text formatting language like Markdown.\\nADRs will be numbered sequentially and monotonically. Numbers will not be\\nreused.\\nIf a decision is reversed, we will keep the old one around, but mark it as\\nsuperseded. (It's still relevant to know that it was the decision, but is no\\nlonger the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest:\\n**Title** These documents have names that are short noun phrases. For example,\\n\"ADR 1: Record architectural decisions\" or \"ADR 9: Use Docker for deployment\"\\n**Status** A decision may be \"proposed\" if it's still under discussion, or\\n\"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it\\nmay be marked as \"deprecated\" or \"superseded\" with a reference to its\\nreplacement.\\n**Context** This section describes the forces at play, including technological,\\npolitical, social, and local to the service. These forces are probably in\\ntension, and should be called out as such. The language in this section is\\nvalue-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated\\nin full sentences, with active voice. \"We will ...\"\\n**Consequences** This section describes the resulting context, after applying\\nthe decision. All consequences should be listed here, not just the \"positive\"\\nones. A particular decision may have positive, negative, and neutral\\nconsequences, but all of them affect the team and service in the future.\\nThe whole document should be one or two pages long. We will write each ADR as\\nif it is a conversation with a future person joining the team. This requires\\ngood writing style, with full sentences organised into paragraphs. Bullets are\\nacceptable only for visual style, not as an excuse for writing sentence\\nfragments.\\n[adr-tools](https:\/\/github.com\/npryce\/adr-tools) can help us work with our ADRs\\nconsistently.\\nWe will link to these ADRs from other documentation where relevant.\\n","tokens":153,"id":272,"Predictions":"## Decision\\nThe following decision has been made:\\n\\n\\n## Decision Context\\nDuring our work on Manage Offenders in Custody, we will have to make\\narchitectural decisions around both design and scope of the work.\\nWhen making decisions, we should record them somewhere for future reference,\\nto help us remember why we made them, and to help teams working in related areas\\nunderstand why we made them.\\nWe should make our decisions public so that other teams can find them more\\neasily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).\\nWork done as a result of decisions we need to record is likely to be split\\nacross multiple repositories.\\n\\n## Architectural Decision Record\\n## Architectural Decision Record\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nDuring our work on Manage Offenders in Custody, we will have to make\\narchitectural decisions around both design and scope of the work.\\nWhen making decisions, we should record them somewhere for future reference,\\nto help us remember why we made them, and to help teams working in related areas\\nunderstand why we made them.\\nWe should make our decisions public so that other teams can find them more\\neasily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).\\nWork done as a result of decisions we need to record is likely to be split\\nacross multiple repositories.\\n\\n"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"Predictions":"# Decision\\nThe decision is to use the [imagemagick](https:\/\/www.imagemagick.org\/) command\\nto convert the input files to pdf.\\n\\n# Environment\\nThe input files are located in the [input\/](https:\/\/github.com\/josch) directory.\\nThe output files (pdfs) are located in the [output\/](https:\/\/github.com\/josch) directory.\\nThe input files are jpeg images and the output files are pdfs.\\nThe input file has a size of 240k, the output file has a size of 4.0M.\\nThe input file is a JPEG image, and the output file is a PDF.\\n# Usage\\n- [imagemagick](https:\/\/www.imagemagick.org\/) command to convert the input images to pdf\\n- [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) command to convert the output pdfs to text\\n- [img2pdf](https:\/\/github.com\/josch\/img2pdf) command to convert the output pdfs to text\\n# Question\\n## Answer\\n## Input File\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF"}
{"File Name":"docspell\/0002_component_interaction.md","Context":"# Context and Problem Statement\\nThere are multiple web applications with their rest servers and there\\nare multiple document processors. These processes must communicate:\\n- once a new job is added to the queue the rest server must somehow\\nnotify processors to wake up\\n- once a processor takes a job, it must propagate the progress and\\noutcome to all rest servers only that the rest server can notify the\\nuser that is currently logged in. Since it's not known which\\nrest-server the user is using right now, all must be notified.\\n# Considered Options\\n1. JMS (ActiveMQ or similiar): Message Broker as another active\\ncomponent\\n2. Akka: using a cluster\\n3. DB: Register with \"call back urls\"\\n# Decision Outcome\\nChoosing option 3: DB as central synchronisation point.\\nThe reason is that this is the simplest solution and doesn't require\\nexternal libraries or more processes. The other options seem too big\\nof a weapon for the task at hand. They are both large components\\nitself and require more knowledge to use them efficiently.\\nIt works roughly like this:\\n- rest servers and processors register at the database on startup each\\nwith a unique call-back url\\n- and deregister on shutdown\\n- each component has db access\\n- rest servers can list all processors and vice versa\\n","Decision":"Choosing option 3: DB as central synchronisation point.\\nThe reason is that this is the simplest solution and doesn't require\\nexternal libraries or more processes. The other options seem too big\\nof a weapon for the task at hand. They are both large components\\nitself and require more knowledge to use them efficiently.\\nIt works roughly like this:\\n- rest servers and processors register at the database on startup each\\nwith a unique call-back url\\n- and deregister on shutdown\\n- each component has db access\\n- rest servers can list all processors and vice versa\\n","tokens":276,"id":274,"Predictions":"## Decision: DB as central synchronisation point.\\n## Decision Outcome: DB as central synchronisation point.\\n## Decision\\n## Decision: DB as central synchronisation point.\\n# Considered Options\\n1. JMS (ActiveMQ or similiar): Message Broker as another active\\ncomponent\\n2. Akka: using a cluster\\n3. DB: Register with \"call back urls\"\\n# Decision Outcome\\nChoosing option 3: DB as central synchronisation point.\\nThe reason is that this is the simplest solution and doesn't require\\nexternal libraries or more processes. The other options seem too big\\nof a weapon for the task at hand. They are both large components\\nitself and require more knowledge to use them efficiently.\\nIt works roughly like this:\\n- rest servers and processors register at the database on startup each\\nwith a unique call-back url\\n- and deregister on shutdown\\n- each component has db access\\n"}
{"File Name":"docspell\/0014_fulltext_search_engine.md","Context":"# Context and Problem Statement\\nTo allow searching the documents contents efficiently, a separate\\nindex is necessary. The \"defacto standard\" for fulltext search on the\\nJVM is something backed by [Lucene](https:\/\/lucene.apache.org).\\nAnother option is to use a RDBMS that supports fulltext search.\\nThis adds another component to the mix, which increases the complexity\\nof the setup and the software. Since docspell works great without this\\nfeature, it shouldn't have a huge impact on the application, i.e. if\\nthe fulltext search component is down or broken, docspell should still\\nwork (just the fulltext search is then not working).\\n# Considered Options\\n* [Apache SOLR](https:\/\/solr.apache.org)\\n* [ElasticSearch](https:\/\/www.elastic.co\/elasticsearch\/)\\n* [PostgreSQL](https:\/\/www.postgresql.org\/docs\/12\/textsearch.html)\\n* All of them or a subset\\n# Decision Outcome\\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","Decision":"If docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","tokens":416,"id":275,"Predictions":"## Decision Outcome\\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\ncould be provided, too.\\n\\n## Decision\\n## Decision Outcome\\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\ncould be provided, too.\\n"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"Predictions":"\\begin{itemize}\\n\\item \\texttt{DocumentProcessor} - provides a REST API that can be used to handle\\ndifferent document processors. The main document processor is the default\\ndocument processor and can be overridden by the user using the \\texttt{documentProcessor} property.\\n\\item \\texttt{WebApp} - provides a REST API that can be used to handle\\ndifferent web apps. The main web app is the default web app and can be\\noverridden by the user using the \\texttt{webApp} property.\\n\\item \\texttt{RestServer} - provides a REST API for handling multiple rest servers.\\nThe default server is the default rest server.\\n\\end{itemize}\\n## Decision Outcome\\nThe decision outcome is \"Yes\".\\n"}
{"File Name":"docspell\/0015_convert_pdf_files.md","Context":"# Context and Problem Statement\\nSome PDFs contain only images (when coming from a scanner) and\\ntherefore one is not able to click into the pdf and select text for\\ncopy&paste. Also it is not searchable in a PDF viewer. These are\\nreally shortcomings that can be fixed, especially when there is\\nalready OCR build in.\\nFor images, this works already as tesseract is used to create the PDF\\nfiles. Tesseract creates the files with an additional text layer\\ncontaining the OCRed text.\\n# Considered Options\\n* [ocrmypdf](https:\/\/github.com\/jbarlow83\/OCRmyPDF) OCRmyPDF adds an\\nOCR text layer to scanned PDF files, allowing them to be searched\\n","Decision":"Add ocrmypdf as an optional conversion from PDF to PDF. Ocrmypdf is\\ndistributed under the GPL-3 license.\\n","tokens":154,"id":277,"Predictions":"## Decision: Add OCR text layer to scanned PDF files\\n## Decision Type: Architectural Decision\\n## Decision Value: Yes\\n## Scenario: [Add OCR text layer to scanned PDF files](https:\/\/www.ocrmypdf.org\/)\\n## Decision Resolution: [Yes]\\n## Decision Priority: 4\\n## Approval Status: Approved\\n\\n# Filter for PDFs with images that are not searchable in a PDF viewer.\\n# Filter for PDFs with images that are searchable in a PDF viewer.\\n# Filter for PDFs that are searchable in a PDF viewer.\\n# Filter for PDFs that are not searchable in a PDF viewer.\\n# Filter for PDFs that are not searchable in a PDF viewer.\\n\\n### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n#### Image Filters\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n\\n### Document Filtering\\n\\n\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n### Document Filtering\\n\\n\\n\\n\\n> **Additional Information**\\n> \\n> For more details, please refer to the document:\\n> \\n> https:\/\/www.ocrmypdf.org\/documentation\/ocrmypdf-document-filtering.html\\n"}
{"File Name":"docspell\/0003_encryption.md","Context":"# Context and Problem Statement\\nSince docspell may store important documents, it should be possible to\\nencrypt them on the server. It should be (almost) transparent to the\\nuser, for example, a user must be able to login and download a file in\\nclear form. That is, the server must also decrypt them.\\nThen all users of a collective should have access to the files. This\\nrequires to share the key among users of a collective.\\nBut, even when files are encrypted, the associated meta data is not!\\nSo especially access to the database would allow to see tags,\\nassociated persons and correspondents of documents.\\nSo in short, encryption means:\\n- file contents (the blobs and extracted text) is encrypted\\n- metadata is not\\n- secret keys are stored at the server (protected by a passphrase),\\nsuch that files can be downloaded in clear form\\n# Decision Drivers\\n* major driver is to provide most possible privacy for users\\n* even at the expense of less features; currently I think that the\\nassociated meta data is enough for finding documents (i.e. full text\\nsearch is not needed)\\n# Considered Options\\nIt is clear, that only blobs (file contents) can be encrypted, but not\\nthe associated metadata. And the extracted text must be encrypted,\\ntoo, obviously.\\n","Decision":"* major driver is to provide most possible privacy for users\\n* even at the expense of less features; currently I think that the\\nassociated meta data is enough for finding documents (i.e. full text\\nsearch is not needed)\\n# Considered Options\\nIt is clear, that only blobs (file contents) can be encrypted, but not\\nthe associated metadata. And the extracted text must be encrypted,\\ntoo, obviously.\\nNo encryption, because of its complexity.\\nFor now, this tool is only meant for \"self deployment\" and personal\\nuse. If this changes or there is enough time, this decision should be\\nreconsidered.\\n","tokens":279,"id":278,"Predictions":"# Decision Drivers\\n* major driver is to provide most possible privacy for users\\n* even at the expense of less features; currently I think that the\\nassociated meta data is enough for finding documents (i.e. full text\\nsearch is not needed)\\n# Considered Options\\nIt is clear, that only blobs (file contents) can be encrypted, but not\\nthe associated metadata. And the extracted text must be encrypted,\\ntoo, obviously.\\n\\n\\end{document}\\n"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"Predictions":"## 1. Adding a timer and nextrun field to the current job table\\n## 2. Creating a separate table for periodic tasks\\n## 3. Neither\\n```\\nThis is the SQL statement that generates the decision record:\\n```\\nSELECT * FROM `job` WHERE `status` = 'RUNNING' AND EXISTS (SELECT * FROM `job` WHERE `status` = 'NOT RUNNING');\\n```\\nI would like to know if there is an easier way to do this, or if there are any other considerations that I should be taking into account when deciding which option to use.\\nThanks in advance!\\n\\nOP 2017-09-13: I decided to go with option 2, since it was the easiest solution for me.\\n"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"Predictions":"## Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\\n## Decision\\n## Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\\n## Decision\\n## Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\\n## Decision\\n## Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n### Testing and Documentation\\n\\n#### 1\\. How do you plan on testing your library?\\n\\n#### 2\\. What type of testing do you plan on doing?\\n\\n#### 3\\. What types of documentation would you like your library documentation to include?\\n\\n#### 4\\. What type of documentation would you like your library documentation to include?\\n\\n#### 5\\. How will you document this library? (e.g., use a README.md file)\\n\\n#### 6\\. How will you document this library? (e.g., use a README.md file)\\n\\n#### 7\\. What type of documentation would you like your library documentation to include?\\n\\n#### 8\\. How will you document this library? (e.g., use a README.md file)\\n\\n#### 9\\. What type of documentation would you like your library documentation to include?\\n\\n#### 10\\. How will you document this library? (e.g., use a README.md file)\\n\\n### Test Plan for Library Development Team Members:\\n\\n##### Test Plan for Library Development Team Members:\\n    #### Test Case Name: \\#1: Does our API return an ISO8601 formatted string in UTC timezone or unix time?\\n    #### Test Case Description: We need an API that returns an ISO8601 formatted string in UTC timezone.\\n    #### Expected Result: The API should return an ISO8601 formatted string in UTC timezone.\\n    #### Actual Result: The API returns a unix timestamp.\\n    #### Additional Information: We need an API that returns an ISO8601 formatted string in UTC timezone.\\n    #### Additional Information: We need an API that returns an epoch millis for each date\/time value in our database.\\n\\n##### Test Plan for Library Development Team Members:\\n    #### Test Case Name: \\#2: Does our REST Api return an epoch millis or unix time?\\n    #### Test Case Description: We need an API that returns an epoch millis for each date\/time value in our database.\\n    #### Expected Result: The API should return an epoch millis for each date\/time value in our database.\\n    #### Actual Result: The API returns a unix timestamp.\\n\\n##### Test Plan for Library Development Team Members:\\n    #### Test Case Name: \\#3: Does our REST Api return a unix timestamp instead of an epoch millis?\\n    #### Test Case Description: We need an API that returns a unix timestamp instead of an epoch millis.\\n    #### Expected Result: The API should return a unix timestamp instead of an epoch millis.\\n    #### Actual Result: The API returns a unix timestamp.\\n\\n##### Test Plan for Library Development Team Members:\\n    #### Test Case Name: \\#4: Does our REST Api return timestamps instead of milliseconds?\\n    #### Test Case Description: We need an API that returns timestamps instead of milliseconds.\\n    #### Expected Result: The API should return timestamps instead of milliseconds.\\n    #### Actual Result: The API returns timestamps instead of milliseconds.\\n\\n##### Test Plan for Library Development Team Members:\\n     ##### Test Case Name: \\#5 - Does our REST Api return timestamps instead of dates?\\n     ##### Test Case Description - We need an API that returns timestamps instead of dates.\\n     ##### Expected Result - The api should return timestamps instead of dates\\n     ##### Actual Result - The api returns timestamps instead of dates.\\n\\n##### Test Plan for Library Development Team Members:\\n     ##### Test Case Name - \\#6 - Does our REST Api return numbers?\\n     ##### Test Case Description - We need an api that returns numbers not dates or times \\n     ##### Expected Result - Numbers not times or dates\\n     ##### Actual Result - Numbers not times or dates.\\n\\n##### Test Plan for Library Development Team Members:\\n     ##### Test Case Name - \\#7 - Does our REST Api return numbers from Epochs?\\n     ##### Test Case Description - We need api that gives us numbers from Epochs not times or dates \\n     ##### Expected Result - Numbers from Epochs not times or dates.\\n\\n##### Comment:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n###### Location:\\n\\n###### Comments:\\n\\n######## # Code Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n####### Reviewers #########\\n\\n#####\\n"}
{"File Name":"docspell\/0006_more-file-types.md","Context":"# Context and Problem Statement\\nDocspell currently only supports PDF files. This has simplified early\\ndevelopment and design a lot and so helped with starting the project.\\nHandling pdf files is usually easy (to view, to extract text, print\\netc).\\nThe pdf format has been chosen, because PDFs files are very common and\\ncan be viewed with many tools on many systems (i.e. non-proprietary\\ntools). Docspell also is a document archive and from this perspective,\\nit is important that documents can be viewed in 10 years and more. The\\nhope is, that the PDF format is best suited for this. Therefore all\\ndocuments in Docspell must be accessible as PDF. The trivial solution\\nto this requirement is to only allow PDF files.\\nSupport for more document types, must then take care of the following:\\n- extracting text\\n- converting into pdf\\n- access original file\\nText should be extracted from the source file, in case conversion is\\nnot lossless. Since Docspell can already extract text from PDF files\\nusing OCR, text can also be extracted from the converted file as a\\nfallback.\\nThe original file must always be accessible. The main reason is that\\nall uploaded data should be accessible without any modification. And\\nsince the conversion may not always create best results, the original\\nfile should be kept.\\n# Decision Drivers\\nPeople expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n","Decision":"People expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n","tokens":410,"id":281,"Predictions":"# Decision Details\\nThe decision is based on this table:\\n\\n## Document Type\\n\\n# Decision Drivers\\nThe decision is based on this table:\\n\\n## Decision Details\\nThe decision is based on this table:\\n\\n## Document Type\\n\\n# Decision Drivers\\nThe decision is based on this table:\\n\\n## Document Type\\n\\n# Decision Drivers\\nThe decision is based on this table:\\n\\n\\end{lstlisting}\\n\\newpage\\n\\subsection{Document Information}\\nThis section contains the description of the document, including the title, author, date and description of the document. It also contains information about the version number and the publisher. The publisher name must be provided by the user. This information can be obtained from the \\texttt{docspell\\_publisher} field in \\texttt{settings.py}. If not provided, it will be set to \\texttt{\"freetds\"}.\\n\\n\\begin{lstlisting}[language=python]\\n    #docspell\\_publisher = \"freetds\"\\n    #docspell\\_version = \"1.0\"\\n    #docspell\\_author = \"Freetds Software GmbH\"\\n    #docspell\\_date = \"2016-06-07\"\\n    #docspell\\_description = \"A program to extract text from pdf files.\"\\n\\end{lstlisting}\\n\\n\\subsection{Document Description}\\nThis section contains a short description of the document. It should include at least one sentence describing what the document is about and a short description of its main purpose.\\n\\nThis information can be obtained from \\texttt{document\\_description} field in \\texttt{settings.py}. If not provided, it will be set to \\texttt{\"freetds\"}.\\n\\n\\begin{lstlisting}[language=python]\\n    #document_description = \"A program to extract text from pdf files.\"\\n\\end{lstlisting}\\n\\n\\subsection{Document Version Number}\\nThis field stores the version number of Docspell. The name of the version number should be specified by the user in an appropriate format (see below).\\n\\nIf not provided, it will be set to \\texttt{\"1.0\"}.\\n\\n\\begin{lstlisting}[language=python]\\n    #document_version = 1.0\\n\\end{lstlisting}\\n\\nIf you want to make a new version number available for download, you can use this command: \\texttt{\\$ python setup.py install --revision 1 --no-deps --without-docspell}. This will create a file called \\texttt{\\$HOME\/.docspell\/version.txt} with your new version number in it.\\n\\nYou can make your own version numbers using this command: \\texttt{\\$ python setup.py install --revision 2 --no-deps --without-docspell}. This will create a file called \\texttt{\\$HOME\/.docspell\/version.txt} with your new version number in it.\\n\\nIf you want to make your own version numbers available for download, you can use this command: \\texttt{\\$ python setup.py install -v 2}. This will create a file called \\texttt{\\$HOME\/.docspell\/version.txt} with your new version number in it.\\n\\nYou can also add your own fields (e.g., comments) by using this command: \\texttt{\\$ python setup.py install -a -v 2 -w \/tmp\/docsspeldocs.txt} where \/tmp\/docsspeldocs.txt contains your own changes.\\n\\n\\subsection{Publisher Name}\\nThis field stores the name of the publisher that published DocSpell. The name must be specified by the user in an appropriate format (see below).\\n\\nIf not provided, it will be set to \\texttt{\"freetds\"}.\\n\\nNote that if no publishing company exists for DocSpell, then there may also appear an empty line between each entry in our list.\\n\\nIf you want to make a new publisher available for download, you can use this command: \\texttt{\\$ python setup.py install --revision 1 --no-deps --without-docspell --publisher freetds}. This will create a file called \\texttt{\\$HOME\/.docspell\/publisher.txt} with your new publisher name in it.\\n\\nYou can also add your own fields (e.g., comments) by using this command: \\texttt{\\$ python setup.py install -a -v 2 -w \/tmp\/docsspeldocs.txt} where \/tmp\/docsspeldocs.txt contains your own changes.\\n\\nNote that if no publishing company exists for DocSpell, then there may also appear an empty line between each entry in our list.\\n\\nIf you want to make your own publisher available for download, you can use this command: \\texttt{\\$ python setup.py install -v 2}. This will create a file called \\texttt{\\$HOME\/.docspell\/publisher.txt} with your new publisher name in it. You can then use any other method described above (e.g., adding fields or comments) to add and edit these publishers.\\n"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"Predictions":"\\begin{itemize}\\n\\item In the current state, I am not sure what the best option is. I am\\nconsidering using a Java library to extract text from all of them.\\n\\n\\item I am considering using a Scala library to extract text from all of\\nthem. The Scala library is easier to maintain and it is less\\ncomplex than the Java library.\\n\\end{itemize}\\n# Decision Result\\n\\begin{itemize}\\n\\item In the current state, I am not sure what the best option is. I am\\nconsidering using a Java library to extract text from all of them.\\n\\n\\item I am considering using a Scala library to extract text from all of\\nthem. The Scala library is easier to maintain and it is less\\ncomplex than the Java library.\\n\\end{itemize}\\n# Problem Statement and Context\\nThe use case for this software is to be able to transfer files between\\ndifferent platforms. It would be nice if there was a way for users to use\\nthe software without having any knowledge of programming.\\nThis software will be used by engineers who are working on projects that require\\nthe transfer of large files like CAD files, PDF files, etc. They may have\\nknowledge in programming languages such as C++, Java, etc., but they do not have any\\nknowledge in programming languages like Python or Matlab.\\nThey may have knowledge in operating systems such as Linux or Mac OS X, but they do not have any knowledge in operating systems like Windows.\\nAll developers must be able to use this software with no knowledge in programming.\\n# Considered Solutions\\n## Software Options\\n\\begin{itemize}\\n\\item Using a Java Library (LibreCAD)\\n\\end{itemize}\\n## Software Conclusion\\nThe software can help users who are working on projects that require large files like CAD files, PDF files, etc., because it has an easy way for them to transfer those large files without having any knowledge in programming languages like Python or Matlab. It also has an easy way for users who are working on projects that require large files like CAD files, PDF files, etc., because it has an easy way for them to transfer those large files without having any knowledge in programming languages like Python or Matlab.\\n"}
{"File Name":"docspell\/0013_archive_files.md","Context":"# Context and Problem Statement\\nDocspell should have support for files that contain the actual files\\nthat matter, like zip files and other such things. It should extract\\nits contents automatcially.\\nSince docspell should never drop or modify user data, the archive file\\nmust be present in the database. And it must be possible to download\\nthe file unmodified.\\nOn the other hand, files in there need to be text analysed and\\nconverted to pdf files.\\n# Decision Outcome\\nThere is currently a table `attachment_source` which holds references\\nto \"original\" files. These are the files as uploaded by the user,\\nbefore converted to pdf. Archive files add a subtlety to this: in case\\nof an archive, an `attachment_source` is the original (non-archive)\\nfile inside an archive.\\nThe archive file itself will be stored in a separate table `attachment_archive`.\\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\\n- `attachment_source`: report.jpg\\n- `attachment`: report.pdf\\n- `attachment_archive`: files.zip\\nArchive may contain other archives. Then the inner archives will not\\nbe saved. The archive file is extracted recursively, until there is no\\nknown archive file found.\\n# Initial Support\\nInitial support is implemented for ZIP and EML (e-mail files) files.\\n","Decision":"There is currently a table `attachment_source` which holds references\\nto \"original\" files. These are the files as uploaded by the user,\\nbefore converted to pdf. Archive files add a subtlety to this: in case\\nof an archive, an `attachment_source` is the original (non-archive)\\nfile inside an archive.\\nThe archive file itself will be stored in a separate table `attachment_archive`.\\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\\n- `attachment_source`: report.jpg\\n- `attachment`: report.pdf\\n- `attachment_archive`: files.zip\\nArchive may contain other archives. Then the inner archives will not\\nbe saved. The archive file is extracted recursively, until there is no\\nknown archive file found.\\n# Initial Support\\nInitial support is implemented for ZIP and EML (e-mail files) files.\\n","tokens":285,"id":283,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"Predictions":"## Decision: \\n# Apache POI\\n# pandoc\\n# unoconv\\n## Reason: \\n# If the native view is better, then a native solution is better.\\n## Alternative: \\n# If the native view is not better, then an external solution is\\nbetter.\\n## Decision: \\n# pandoct:\\n{{ figure(file=\"example-pandoc.jpg\") }}\\n## Reason: \\n# If the native view is better, then a native solution is better.\\n## Alternative: \\n# If the native view is not better, then an external solution is\\nbetter.\\n## Decision: \\n# unoconv:\\n{{ figure(file=\"example-unoconv.jpg\") }}\\ngiven Document:\\n> {{ document(doc=\"example-docx\", file=\"example-docx.pdf\") }}\\n## Result:\\n> {{ result(doc=\"example-docx\", file=\"example-docx.pdf\") }}\\n## Summary:\\n> {{ summary(doc) }}\\n"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"Predictions":"## + Decision\\n## + Decision\\n## + Decision\\n## + Decision\\n## - Decision\\n## - Decision\\n## - Decision\\n## - Decision\\n## - Decision\\n\\n"}
{"File Name":"docspell\/0008_convert_plain_text.md","Context":"# Context and Problem Statement\\nHow can plain text and markdown documents be converted into a PDF\\nfiles?\\nRendering images is not important here, since the files must be self\\ncontained when uploaded to Docspell.\\nThe test file is the current documentation page of Docspell, found in\\n`microsite\/docs\/doc.md`.\\n```\\n---\\nlayout: docs\\nposition: 4\\ntitle: Documentation\\n---\\n# {page .title}\\nDocspell assists in organizing large amounts of PDF files that are\\n...\\n","Decision":"Java library \"flexmark\".\\nI think all results are great. It depends on the type of document and\\nwhat one expects to see. I guess that most people expect something\\nlike pandoc-html produces for the kind of files docspell is for (it is\\nnot for newspaper articles, where pandoc-latex would be best fit).\\nBut choosing pandoc means yet another external command to depend on.\\nAnd the results from flexmark are really good, too. One can fiddle\\nwith options and css to make it look better.\\nTo not introduce another external command, decision is to use flexmark\\nand then the already existing html->pdf conversion.\\n","tokens":112,"id":286,"Predictions":"## Decision: Convert to PDF\\nThe following document was converted to PDF\\n\\t\\t\\t\\t\\n# # {page .title}\\n\\t\\t\\t\\t\\n# {page .title}\\n\\n# \\n\\ntitle: Documentation\\n\\n# \\n\\ntitle: Documentation\\n\\n# \\n\\ntitle: Documentation\\n---\\n---\\n---\\n---\\n---\\n\\n## Context\\n## Problem Statement\\nThe following document was created by Docspell and converted to PDF. It is a 4-page document (including the title page). The test file is the current documentation page of Docspell, found in `microsite\/docs\/doc.md`. The test file is the current documentation page of Docspell, found in `microsite\/docs\/doc.md`. The test file is the current documentation page of Docspell, found in `microsite\/docs\/doc.md`. The test file is the current documentation page of Docspell, found in `microsite\/docs\/doc.md`. The test file is the current documentation page of Docspell, found in `microsite\/docs\/doc.md`.\\n```\\nHere we have a number of decisions that are \"not important\" but are still important to make.  For example, we have decisions about what kind of input we want from the user and what kind of output we want from our software.\\n\\nIn this case we have a decision about whether or not to provide an image.  This decision is not important because if we don't provide an image then no one will be able to see it.  However, it would be nice if our software could render images.\\n\\nWe also have a decision about how we want our software to handle markdown files.  We could choose either Markdown or Plain Text as our default for markdown files.  If you choose Markdown then you should expect that your markdown files will be converted into PDFs.\\n\\nFinally, we have a decision about how much information we want from the user when they submit their document.  If you choose Plain Text then you should expect that your input will be sent as plain text without any formatting.\\n\\nOur software must provide two different types of output for each input type.  One output type must be for plain text and another output type must be for markdown files.  We can use any combination of these two outputs when converting between documents.\\n\\nTo convert between documents, our software must first convert all input documents into plain text or markdown files.  Then it must convert all plain text or markdown files into PDFs.\\n\\nWe can use any combination of these two outputs when converting between documents.\\n```"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"Predictions":"# Decision\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\\n## Decision\\n# Decision\\n# Decision\\n# Decision\\n# Decision\\n# Decision\\n# Decision"}
{"File Name":"core\/0002-release-policy.md","Context":"## Context and Problem Statement\\nProject Thoth itself consists of many components all having their own release cycles and delivery artifacts such as\\ncontainer image or Python libraries.\\n","Decision":"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* users have a clear base line of versions, these versions have been tested with each other and have\\nundergone integration testing.\\n* a release can be referenced from documents, so that operational procedures have a clear relationship with component\\nversions being used\\n* we can maintain sets of different versions for different deployment environments\\n* we can provide a version string with each API provided by the project\\n### Negative Consequences <!-- optional -->\\n* A release might not contain the latest versions of components\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":33,"id":288,"Predictions":"## Decision Statement for the Software\\nThe decision statement is to use the Thoth software as a Python library.\\n\\n## Scenario\\n## Scenario Statement for the Software\\nThe scenario statement is to use the Thoth software as a Python library.\\n\\n## Context\\n## Context Statement for the Software\\nThe context statement is to use the Thoth software as a Python library.\\n\\n## Problem\\n## Problem Statement for the Software\\nThe problem statement is to use the Thoth software as a Python library.\\n\\n"}
{"File Name":"core\/0003-decommision-qeb-hwt.md","Context":"## Context and Problem Statement\\n[Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\\nPull Requests as a check. This functionality could be integrated into https:\/\/github.com\/marketplace\/khebhut and\\ncomplexity and maintain costs.\\n## Decision Drivers <!-- optional -->\\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\n","Decision":"* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\nChosen option: \"merge function into Khebhut\", because we can reduce the cost of maintaining our software infrastructure\\nby reducing redundancy.\\n","tokens":96,"id":289,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nQeb-Hwt is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# Developer\\n\\n--- | 60 | 60 | 60 | 60 | 60 | 60 | 60 | 60 | 60 |\\n\\n## Context and Problem Statement\\n[Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\\nPull Requests as a check. This functionality could be integrated into https:\/\/github.com\/marketplace\/khebhut and\\ncomplexity and maintain costs.\\n## Decision Drivers <!-- optional -->\\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\n\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nQeb-Hwt is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# Developer\\n\\n--- | 30 | 30 | 30 | 30 |\\n\\n--- | -1 |\\n\\n--- | -1 |\\n\\n--- |\\n\\n--- |\\n\\n--- |\\n\\n--- |\\n"}
{"File Name":"core\/0002-allow-dynamic-loading-of-yetibot-plugins-via-config.md","Context":"## Context\\nYetibot currently resides across two primary repos:\\n- github.com\/yetibot\/yetibot\\n- github.com\/yetibot\/yetibot-core\\nThese code bases continue to grow in size, and consist of a diverse range of\\nfeatures, many of which many users won't care to use.\\n","Decision":"Switching to a plugin system allows us to split up the code base into much more\\nfine grained, logical units. For example, we may split the `github` command into\\nits own plugin.\\nThe first plugin is [yetibot-kroki](https:\/\/github.com\/yetibot\/yetibot-kroki).\\nWe will continue to extract plugins from both of the above code bases.\\n","tokens":70,"id":290,"Predictions":"## This is a decision for the user. The user can use this decision to create a new\\nfeature, update an existing feature, or delete an existing feature. The user can also\\nuse this decision to request that a specific feature be added to the software.\\n\\n## Decision Criteria\\n## This is a list of criteria that the user can use to make their decision. The user can use these criteria to select one or more features from the list below. These criteria are used in the order listed.\\n\\n## Features\\nThe following features are available in Yetibot\\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n\\nFeatures may be created by users, or by other software developers.\\nFeatures may be created by users, or by other software developers.\\nFeatures may be created by users, or by other software developers.\\nFeatures may be created by users, or by other software developers.\\nFeatures may be created by users, or by other software developers.\\n\\end{itemize}\\n\\n\\begin{itemize}\\n    \\item Feature Name: Yetibot\\n    \\item Feature Description: A web application for creating and sharing bots.\\n    \\item Feature Category: Applications\\n    \\item Feature Type: Front End\\n    \\item Feature Subtype: Web Application\\n    \\item Feature Status: Active\\n    \\item Feature Version: 0.1.0 (released on 2020\/05\/12)\\n    % TODO: Add details about how this feature was implemented.\\n    % TODO: Add details about how this feature was implemented.\\n\\end{itemize}\\n\\n% TODO: Add details about how this feature was implemented.\\n\\n\\begin{itemize}\\n    % TODO: Add details about how this feature was implemented.\\n    % TODO: Add details about how this feature was implemented.\\n\\end{itemize}\\n\\n% TODO: Add details about how this feature was implemented.\\n\\n% TODO: Add details about how this feature was implemented.\\n\\n% TODO: Add details about how this feature was implemented.\\n\\n\\begin{enumerate}\\n% TODO: Add details about how this feature was implemented.\\n\\end{enumerate}\\n\\n"}
{"File Name":"core\/0004-naming-convention-images.md","Context":"## Context and Problem Statement\\nImage names are important for branding and let others identify easily a specific image they need. For example \"I want to work on computer vision project with Tensorflow, what stack and image should I use?\" Having a trusted well maintained source of images with clean naming convention can help on that.\\n","Decision":"Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated\/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\\n### Positive Consequences <!-- optional -->\\n* users can immediately select an image based on the application they want.\\n* using overlays we can have a variety of combination, not just for ml_framework\\n### Negative Consequences <!-- optional -->\\n* N\/A\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":64,"id":291,"Predictions":"\\n\\nThe decision is to use the following image as a target for the project. The image is in \\href{https:\/\/www.flickr.com\/photos\/155067892@N07\/}{Flickr} with the license \\href{http:\/\/creativecommons.org\/licenses\/by-sa\/2.0\/}{CC BY-SA 2.0}. The decision is based on the following information:\\n\\n\\begin{itemize}\\n    \\item The license of the image allows reuse and modification without attribution.\\n    \\item It is a popular image that can be easily found on internet.\\n    \\item It is available in different sizes, formats and resolutions.\\n    \\item It is well known and trusted for its quality.\\n    \\item It has a clean name, which makes it easy to find it in other projects.\\n\\end{itemize}\\n\\n\\n\\nThe decision is to use the following software as a target for the project. The software is in \\href{https:\/\/www.flickr.com\/photos\/155067892@N07\/}{Flickr} with the license \\href{http:\/\/creativecommons.org\/licenses\/by-sa\/2.0\/}{CC BY-SA 2.0}. The decision is based on the following information:\\n\\n\\begin{itemize}\\n    \\item The license of the software allows reuse and modification without attribution.\\n    \\item It is a popular software that can be easily found on internet.\\n    \\item It is available in different sizes, formats and resolutions.\\n    \\item It has a clean name, which makes it easy to find it in other projects.\\n\\end{itemize}\\n\\nThe decision is to use the following hardware as a target for the project.\\n\\nThe hardware is in \\href{https:\/\/www.flickr.com\/photos\/155067892@N07\/}{Flickr} with the license \\href{http:\/\/creativecommons.org\/licenses\/by-sa\/2.0\/}{CC BY-SA 2.0}. The decision is based on the following information:\\n\\n\\begin{itemize}\\n    \\item The hardware has been used for several years and there are no issues with its reliability.\\n    \\item There are no issues with its availability or maintenance.\\n\\end{itemize}\\n\\n\\n\\nThe decision is to use all of these elements as targets for this project.\\n\\nThe project specification can be found at: https:\\\/\\\/github.com\\\/wakil\\\/web-architectural-decision-record\\n"}
{"File Name":"core\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":292,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"beadledom\/default_dependencies_primary.md","Context":"## **Context**:\\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\\n","Decision":"In order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\\nWe did this because we don't know if our consumers rely on the previous behavior of meta\/health checking dependencies if they were of unspecified importance.\\n","tokens":51,"id":293,"Predictions":"## **Decision**:\\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\\n\\n\\n\\n\\n"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"opg-refunds\/0002-default-service-key-for-dynamo-db-sessions.md","Context":"## Context\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\n","Decision":"We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\nTable names                                       |\\n--------------------------------------------------|\\nrefunds-sessions-front-<opg_stackname>            |\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\n","tokens":48,"id":295,"Predictions":"## The decision is to enable HTTPS for this release.\\n\\n## Notes\\nThis decision is made by the Software Architect.\\nThis decision is subject to change in the future.\\nThe decision is not final and may be changed at any time.\\nThe decision has been approved by the Software Architect.\\nThe Software Architect will notify the Release Manager of any changes to this decision.\\n\\n## Summary\\nThis Decision Record describes the software decision for a release. The software architect has made a final decision on whether or not to enable HTTPS for this release. This record should be reviewed by all stakeholders who will be affected by this change.\\n\\end{document}\\n"}
{"File Name":"wh_covid19_app\/ADR-0002 Embed content in the app.md","Context":"## Context\\nThe app primarily displays information for medical staff working in hospitals.  The information needs to come from somewhere and there are a few options:\\n### Download content from a CMS\/S3 bucket\/Website\\nWe could pull the information in the app from somewhere on the web.  Some kind of CMS, S3 bucket or website where the files are hosted would be the most obvious choices.\\nThis does make it easier to quickly the information in the app, but would require the build out of more infrastructure and architecture to support all this.\\nIt also could mean that if the internet connection goes down to a hospital in  a crisis any information that isn't already cached in the app would be unavailable to users.\\n### Embed the content in the app\\nThe alternative method is to embed all of the information in the app so that all of the information is included, after downloading the app.\\nThis is architecturally much simpler and means all the content will be available offline if the internet should go down.\\nSubsequently, a new version of the app needs to be released to update the information.\\n","Decision":"For the initial versions of the application, we will embed the content in the app.\\nOnce the situation stabilises, and we can provide support to more hospitals, we can re-visit the decision and decide if we want to download content.\\n","tokens":224,"id":296,"Predictions":"The app is a decision-making tool for medical staff working in hospitals.  The app is used by medical staff to make decisions about the care of patients.  A decision is made when a user selects a patient from the list of patients in the app.  The app needs to be able to display information about these patients.\\n### Download content from a CMS\/S3 bucket\/Website\\nThe app could download information from any source that has the information.  This could be on a S3 bucket or website that has the information, or it could be on an external server.\\n### Embed the content in the app\\nEmbedding content in the app means all of the content is available offline if internet access should go down, and all of the content is available for use offline if internet access should go down.\\n### All of this works well on mobile devices, but not so well on desktops.  The desktop version will need to be re-written so that it can work offline.\\n### It would also mean that if internet access goes down to a hospital in a crisis any information that isn't already cached in the app would be unavailable to users.\\n### This method would also require more infrastructure and architecture to support all this.\\n\\n## Decision\\n\\nThe solution must allow users to download and run their own copies of DICOM images.\\n\\n## Decision\\n\\nThe solution must allow users to run their own copies of DICOM images without installing an application.\\n\\n## Decision\\n\\nThe solution must allow users to run their own copies of DICOM images without installing an application on their computer.\\n\\n## Decision\\n\\nThe solution must allow users to run their own copies of DICOM images without installing an application on their computer.\\n\\n## Decision\\n\\nThe solution must allow users to run their own copies of DICOM images without installing an application on their computer."}
{"File Name":"wh_covid19_app\/ADR-0001 Use Flutter.md","Context":"## Context\\nFlutter is a framework for building apps provided by Google.  Google describes it as a:\\n> ... UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\nIt emphasizes fast development, expressive and flexible UI, and native performance.\\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter.  Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed to put together and ship an app to the hospital before the crisis swamped the staff, which at that time as predicted to occur in about two weeks.\\nLuke immediately put out the call and assembled a team of Flutter developers from the GDG community.  We performed an initial evaluation on the codebase and decided that it would be best to deliver a re-write rather than building on the existing codebase.\\nAt this stage it would have been possible to drop Flutter and build out native applications, or built the app using some other cross platform toolkit such as react native.\\n","Decision":"Time was of the essence - as we already had a team of Flutter developers ready to go we decided to continue on with using Flutter to build the app.  It was also likely that Flutters emphasis on fast development and an expressive UI would be helpful in quickly building out the app.\\n","tokens":254,"id":297,"Predictions":"## The App is successfully delivered to the hospital, Luke and his team are happy with the outcome.  Luke has received positive feedback from his team and the hospital.  The App is now being used by frontline staff to provide key information on COVID-19.\\nLuke has requested that you review the app and provide feedback on your experience using it.  Please provide your feedback by Friday, April 24th.\\n\\n## Context\\nThis is a Flutter app that will be used to communicate COVID-19 information to frontline medical staff.  It has been developed using Flutter and uses Dart as a backend language.  The application is currently in use at Western Health hospitals in Australia.\\nLuke has requested that you review the app and provide feedback on your experience using it.  Please provide your feedback by Friday, April 24th.\\n\\n## Decision\\n## This decision record was made to help Luke understand how Flutter works and how he can improve it for future releases.\\n## Context\\nThe Flutter framework is a framework for building apps provided by Google. It aims to make it easy for developers to build beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\nFlutter also provides an easy way to add new features such as animations or gestures without having to write code or learn new frameworks.\\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter. Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed to put together and ship an app to the hospital before the crisis swamped the staff, which at that time as predicted to occur in about two weeks.\\nLuke immediately put out the call and assembled a team of Flutter developers from the GDG community.  We performed an initial evaluation on the codebase and decided that it would be best to deliver a re-write rather than building on the existing codebase.\\nAt this stage it would have been possible to drop Flutter and build out native applications, or built the app using some other cross platform toolkit such as react native.\\n\\n## Decision\\n## This decision record was made so Luke can understand how Flutter works.\\n## Context\\nThe Flutter framework is a framework for building apps provided by Google. It aims to make it easy for developers to build beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\nFlutter also provides an easy way to add new features such as animations or gestures without having to write code or learn new frameworks.\\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter. Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed t"}
{"File Name":"wh_covid19_app\/ADR-0003 Use Flutter beta channel.md","Context":"## Context\\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\\n","Decision":"We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\\n","tokens":52,"id":298,"Predictions":"## Decision\\nThe decision is to build a stable version of flutter that has been released for iOS.\\n\\n## Decision\\n## Decision\\nThe decision is to build a Beta version of flutter that has been released for Android.\\n\\n## Decision\\nThe decision is to build a Dev version of flutter that has been released for Android.\\n\\n## Decision\\nThe decision is to build a Stable version of flutter that has been released for Android.\\n\\n## Decision\\nThe decision is to build a Beta version of flutter that has been released for Android.\\n\\n## Decision\\nThe decision is to build a Dev version of flutter that has been released for Android.\\n\\n## Decision\\nThe decision is to build the Stable version of flutter that will be used on the iOS platform. This will be the highest priority and will be used as the default platform on which all other platforms are built.\\n\\n## Decision\\nThe decision is to build the Beta version of flutter that will be used on the iOS platform. This will be the second highest priority and will be used as the default platform on which all other platforms are built.\\n\\n## Decision\\nThe decision is to build the Dev version of flutter that will be used on the iOS platform. This will be the third highest priority and will be used as the default platform on which all other platforms are built.\\n"}
{"File Name":"nso.aurora\/MartketPlace.md","Context":"## Context\\nDo we need a MarketPlace component?\\n","Decision":"It doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\\n","tokens":12,"id":299,"Predictions":"## Decision\\nNo\\n\\n## Decision\\n## Decision\\nYes\\n\\n## Decision\\n## Decision\\nYes\\n\\n\\nTo get a simple text version of this record, use the following command:\\n\\n$ .\/build.sh -t:doc\\n\\nYou can also generate a PDF version of the record using the following command:\\n\\n$ .\/build.sh -t:pdf\\n\\nCreating an ArchiMate 2.6.1 Model\\n\\nThe model can be created by following the steps outlined in Creating an ArchiMate 2.6.1 Model.\\n\\nThe model can be created from either a number of different sources, or from an existing model.\\n\\nThe source of the model is usually a file, but it may alternatively be a document that contains ArchiMate definitions and other information about the model.\\n\\nGenerating an ArchiMate 2.6.1 Model from an Existing Model\\n\\nIf you already have an existing ArchiMate 2.6.1 model, then you can create a new one by running .\/build.sh -t:archi to generate a new ArchiMate 2.6.1 model.\\n\\nIf you want to create a new ArchiMate 2.6.1 model from scratch, then you must specify all the attributes that are needed in your new model.\\n\\nIf you want to create a new ArchiMate 2.6.1 model from scratch, then you must specify all the attributes that are needed in your new model.\\n\\nIf you want to create a new ArchiMate 2.6.x model from scratch, then you must specify all the attributes that are needed in your new model.\\n\\nCreating an ArchiMate 2.x.x Model with Existing Attributes\\n\\nIf you have already created an existing ArchiMate 2.x.x model and want to add some additional attributes to it, then run .\/build.sh -t:archi:add to generate a new ArchiMate 2.x.x model with additional attributes added to it.\\n\\nIf you want to add some additional attributes to an existing ArchiMate 2.x.x model, then run .\/build.sh -t:archi:add:ref to generate a new archaicate 2.x.x reference file with additional attributes added to it.\\n\\nCreating an Architectural Decision Record for Existing Models\\n\\nIf you have already created Architectural Decision Records (ADR) for existing models and want to update them using your own ADRs, then run .\/build.sh -t:architectural:update:ref for updating your ADRs into your own ADRs.\\n\\nSee also:\\n\\n- Building Your Own Architecture Diagrams Using Animate"}
{"File Name":"nso.aurora\/Notifier.md","Context":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n","Decision":"A Notifier component is needed to send email and\/SMS to customers.\\n","tokens":38,"id":300,"Predictions":"## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n\\n## Decision\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting they know how they will be delivered by the courier company. With the help of 3rd party apps or web services that are integrated into the app. We will be able to send push notifications to all users that have registered in our app. We would like this feature to work on both android & IOS. \\n\\n## Problem Statement\\nThe problem statement is as follows: \\n1) We want an app that is able to send push notifications based on the following conditions: \\n2) The user has purchased products from our website within the last 30 days. \\n2) The user has registered in our app within the last 30 days. \\n3) The user has made at least one purchase from our website within the last 30 days. \\n4) The user has made at least one purchase from our website within the last 30 days. \\n5) The user has made at least one purchase from our website within the last 30 days. \\n6) The user has made at least one purchase from our website within the last 30 days. \\n7) The user has made at least one purchase from our website within the last 30 days. \\nnThe app should be able to send push notifications based on these conditions.\\n\\end{itemize}\\n\\end{document}\\n\\n"}
{"File Name":"nso.aurora\/LoyaltyProgram.md","Context":"## Context\\nDo we need a Loyalty Program component?\\n","Decision":"It doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\\n","tokens":13,"id":301,"Predictions":"## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n## Decision\\nDo we need a Loyalty Program component?\\n\\n\\n# Generate the decision matrix.\\nDecisionMatrix = matrix([[1, 0, 0, 1],\\n               [0, 1, 0, 0],\\n               [1, 0, 0, 1],\\n               [0, 1, 0, 1]])\\n\\nA decision matrix is an array of boolean values that represents the relationship between two groups of objects. It is used to represent the relationship between an object and its attributes. The decision matrix contains an array of Boolean values denoting the relationship between the two objects. Each row in the decision matrix represents one object and each column represents one attribute of that object.\\n\\nAs shown in the following figure,\\n\\nThe decision matrix is represented as follows:\\n\\nDecision Matrix = [[True False True] [False False True] [True False True] [False False True] [True False True]]\\n\\nThe above figure shows how to create a decision matrix in Python.\\n\\n# Create a new data set.\\ndata = [[False False True], [False False True], [True False True], [False False True], [True False True]]\\n\\nThe above figure shows how to create new data set in Python.\\n\\n# Create an empty decision matrix.\\ndecision_matrix = [[False,False,True],[False,False,True],[True,False,True],[False,False,True]]\\n\\nAs shown in the above figure,\\n\\nThe above figure shows how to create empty decision matrix in Python.\\n\\n# Add one more element to make it more complex.\\ndata.append([True,True,True])\\n\\nAs shown in the above figure,\\n\\nThe above figure shows how to add elements to make it more complex.\\n\\n# Now add some more elements.\\ndata.append([True,True,True,False,True,False,True])\\n\\nAs shown in the above figure,\\n\\nThe above figure shows how to add elements.\\n\\n# Now add some more elements.\\ndata.append([False,False,True,False])\\n\\nAs shown in the above figure,\\n\\nThe above figure shows how to add elements.\\n\\n# Now add some more elements.\\ndata.append([False,False,True,False])\\n\\nAs shown in the above figure,\\n\\nThe above figure shows how to add elements.\\n\\n# Now add some more elements.\\ndata.append([False,False,True,False])\\n\\nAs shown in the above figure,\\n\\nThe above Figure shows how to append element.\\n\\n# Now add some more elements.\\ndata.append([False,False,True,FaIly,FaIly,FaIly,FaIly,FaIly,FaIly,FaIly,FaIly])\\n\\nAs shown in the below picture,\\n\\nFigure-2: The result of appending element\\n\\nIn this example , we have appended three different types of objects (one real object and two fake objects) and added them into data set . This data set contains three attributes (name , age and salary ) for each object . This data set contains three rows for each type of objects (real , fake and real fake ) . We have also appended this data set into another data set .\\n\\nNow let us see what happens if we append this same data set into another data set .\\n\\nFigure-3: The result of appending element\\n\\nIn this example , all four rows are appended into another data set .\\n\\nNow let us see what happens if we append this same data set into another data set .\\n\\nFigure-4: The result of appending element\\n\\nIn this example , all four rows are appended into another data set .\\n\\nNow let us see what happens if we append this same data set into another data set .\\n\\nFigure-5: The result of appending element\\n\\nIn this example , one row is appended into another row .\\n\\nNow let us see what happens if we append this same data set into another dataset .\\n\\nFigure-6: The result of appending element\\n\\nIn this example , all four rows are appended into another dataset .\\n\\nTo sum up ,\\n\\nLet\u2019s consider again that :\\n\\nWe have created our own dataset with four rows and four columns . We have also created our own dataset with three rows and three columns . We have also created our own dataset with five rows and five columns . We have also created our own dataset with six rows and six columns . We have also created our own dataset with seven rows and seven columns . We have also created our own dataset with eight rows and eight columns . And finally ,we have created our own dataset with nine rows and nine columns .\\n\\nWe now want to create an algorithm that will take as input :\\n\\nA list containing names from first list (first list could be any list ) \\n A list containing ages from second list \\n A list containing salaries from third list \\n A list containing any other attribute from fourth list \\n\\nWe want to know whether it is possible for us to append these lists together or not ?\\n\\nTo answer this question , let\u2019s consider that :\\n\\nWe want to know whether it is possible for us to append these lists together or not ?\\n\\nTo answer this question , let\u2019s consider that :\\n\\nWe want to know whether it is possible for us to append these lists together or not ?\\n\\nTo answer this question ,\\n\\nLet\u2019s consider that :\\n\\nWe want to know whether it is possible for us to append these lists together or not ?\\n\\nTo answer this question ,\\n\\nLet\u2019s consider that :\\n\\nWe want to know whether it is possible for us to append these lists together or not ? \\n\\nTo answer this question ,\\n\\nLet\u2019s consider that :\\n\\nWe want to know whether it is possible for us to append these lists together or not ? \\n\\nTo answer this question ,\\n\\nLet\u2019s consider that :\\n\\nWe want t"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nA microservice architecture is a software architecture pattern that separates application functions into independent services that can be deployed independently. The services are loosely coupled with each other, and they communicate through message passing. In a microservice architecture, the services communicate with each other through message passing, so there is no need to use a database or file system to store and retrieve data.\\n\\nThe term \u201cmicroservice\u201d was first used by Michael Bolton and Richard Brindley in their book \u201cMicroservices Architecture\u201d. Microservices are designed to be modular and independent so that it can be deployed independently. They are also designed to be highly scalable, fault-tolerant, and reliable.\\n\\nMicroservice architecture is an architectural pattern that separates application functions into independent services that can be deployed independently. The services are loosely coupled with each other, and they communicate through message passing. In a microservice architecture, the services communicate with each other through message passing, so there is no need to use a database or file system to store and retrieve data.\\n\\nFor example:\\n\\n- Services A1 and B1 will not store data in the database.\\n- Services A2 and B2 will not store data in the database.\\n- Services A3 and B3 will not store data in the database.\\n- Services A4 and B4 will not store data in the database.\\n\\nIn this case, we can see that it is possible for an application to have multiple microservices. Each service is responsible for carrying out a specific task or function. For example:\\n\\n- Service A1 provides customer service support for customers who have problems with their orders.\\n- Service B1 provides customer service support for customers who have problems with their orders.\\n- Service A2 provides customer service support for customers who have problems with their orders.\\n- Service B2 provides customer service support for customers who have problems with their orders.\\n\\nThis means that the software should be divided into 3 components:\\n\\n- An API (Application Programming Interface) should be provided by one of these microservices (A1). This API should provide information about how to access the other microservices (B1).\\n- The APIs of these microservices should provide information about how to access the databases (C1), which stores information about how to access files (D1).\\n- The APIs of these microservices should provide information about how to access web servers (E1).\\n\\nIn this way we can ensure scalability, reliability, availability, security, versioning, fault tolerance, etc., of all these services.\\n\\nThe key points are:\\n\\n- The API should contain all necessary information about how to access all parts of the application.\\n- The API of each microservice should contain all necessary information about how to access all parts of the application.\\n- All APIs must communicate over a common protocol such as HTTP or AMQP.\\n\\nFor example:\\n\\nHTTP\\n\\nRequest: GET \/api\/product\/12345\\nResponse: {\"id\":12345}\\n\\nAMQP\\n\\nRequest: PUT \/api\/product\/12345\\nResponse: {\"id\":12345}\\n\\nHTTP\\n\\nRequest: GET \/api\/product\/12345\\nResponse: {\"id\":12345}\\n\\nAMQP\\n\\nRequest: PUT \/api\/product\/12345\\nResponse: {\"id\":12345}\\n\\nThe RESTful web service design pattern was developed by Robert Cailliau in 2000. It was inspired by \u201cThe client-server model\u201d from \u201cThe client-server model\u201d, which states that clients connect directly to servers using TCP\/IP networks (such as HTTP). This means that if you want your clients to connect directly from one computer on your network to another computer on your network without using any intermediary servers such as databases or file systems, you must use RESTful web services.\\n\\nRESTful web services are designed so that they can run on any computer in your network without requiring any special software installed on it. For example:\\n\\nA RESTful web service uses HTTP protocol which allows communication between computers on your network without requiring any special software installed on them.\\n\\nIf you want your clients to connect directly from one computer on your network without using any intermediary server such as databases or file systems:\\n\\nYou need only install a web server on this computer.\\n\\nYou do not need any special software installed on it.\\n\\nYou only need an Internet connection at this computer.\\n\\nA RESTful web service uses HTTP protocol which allows communication between computers on your network without requiring any special software installed on them.\\n\\nIf you want your clients to connect directly from one computer on your network without using any intermediary server such as databases or file systems:\\n\\nYou only need an Internet connection at this computer.\\n\\nYou do not need any special software installed on it.\\n\\nYou only need an Internet connection at this computer.\\n\\nIf you want your clients\u2019 connections from different computers connected over different networks such as local area networks (LANs) or wide area networks (WANs), then you must install different kinds of software such as web servers or firewalls between them. For example:\\n\\nIf you want your clients\u2019 connections from different computers connected over different networks such as local area networks (LANs) or wide area networks (WANs), then you must install different kinds of software such as web servers or firewalls between them.\\n\\nThere are many types of RESTful web services available today including SOAP-based Web Services (such as SOAP), XML Web Services (such as XSD), JavaScript Object Notation Web Services (JSON), etc., but they all have some common features including:\\n\\nRESTful Web Services\\n\\nRESTful Web Services has three main components: Request Method \u2013 Define request parameters for making HTTP requests; Response Method \u2013 Define response parameters for making HTTP requests; Response Body \u2013 Provide response content; Response Headers \u2013 Provide response headers; Response Content-Type \u2013 Provide response content type; Response Headers \u2013 Provide response headers; Request Body \u2013 Provide request body content; Request Headers \u2013 Provide request request header content; Request Method \u2013 Define request method; Request URL \u2013 Specify URL where data will be sent; Request Body \u2013 Provide request body content; Request Header Content-Type \u2013 Provide request header content type; Request Method - Define request method;\\n\\nSOAP-based Web Services\\n\\nSOAP-based Web Services has three main components: SOAP Header Content-Type - Specify SOAP header content type; SOAP Body Content Type - Specify SOAP body content type ; SOAP Body Content - Specify SOAP body content;\\n\\nJSON-based Web Services\\n\\nJSON-based Web Services has three main components: JSON Header Content-Type - Specify JSON header content type ; JSON Body Content Type - Specify JSON body content type ; JSON Body Content - Specify JSON body content;\\n\\nXML Web Services\\n\\nXML Web Services has three main components: XML Header Content-Type - Specify XML header content type ; XML Body Content Type - Specify XML body content type ; XML Body Content - Specify XML body content;\\n\\nWeb Application Programming Interface\\n\\nWeb Application Programming Interface is a standard protocol used by many programming languages \u200b\u200bto allow communication between programs written in different programming languages \u200b\u200bor operating systems. This standard defines what functions are available within each program language \u200b\u200band what operations are available within each operating system. There are many types of WAPIs available today including HTML5 WAPIs like HTML5 Document Object Model WAPIs , JavaScript Object Notation WAPIs , etc., but they all have some common features including:\\n\\nHTML5 Document Object Model WAPIs\\n\\nHTML5 Document Object Model WAPIs includes HTML5 Document Object Model WAPIs , HTML5 Document Object Model WAPIs , etc., but they all have some common features including:\\n\\nBasic elements include attributes and elements related to HTML5 elements such as text elements , paragraph elements , input elements , image elements , video elements , audio elements , table elements , form elements , etc., but they also include additional features such as embedded media files like images and audio files like audio files .\\n\\nHTML5 Document Object Model WAPIs includes HTML5 Document Object Model WAPIs , HTML5 Document Object Model WAPIs , etc., but they also include additional features such as embedded media files like images and audio files like audio files .\\n\\nHTML5 Document Object Model WAPIs includes HTML5 Document Object Model WAPIs , HTML5 Document Object Model WAPIs ,\\n\\nHTML5 Document Object Model WAPIs includes HTML5 Document Object Model WAPIS ,\\n\\nHTML5 Document Object Model WAPIS includes HTML5 Document Object Model WAPIS ,\\n\\nHTML5 Document Object Model WAPIS includes HTML5 Document Object Model ,\\n\\nHTML "}
{"File Name":"nso.aurora\/NumberOfDatabases.md","Context":"## Context\\nThe state and data are required for the following components: Inventory, Customer, Order Management, Rating Manager and Recommendation Manager.  We had a database for each of the component initially.  However, it can save on the number of distrubted transactions by storing state and data in one database.\\n","Decision":"It was agreed that one database would be sufficient.\\n","tokens":63,"id":303,"Predictions":"## 1. The database is modified to be a single database that stores all state and data for all components.  The database will be named InventoryDatabase.\\n## 2. The rating manager will be stored in the InventoryDatabase.\\n## 3. The recommendation manager will be stored in the InventoryDatabase.\\n## 4. The rating manager will be stored in the CustomerDatabase.\\n## 5. The recommendation manager will be stored in the CustomerDatabase.\\n## 6. All other components are stored in the CustomerDatabase\\n"}
{"File Name":"nso.aurora\/SegmentationEngine.md","Context":"## Context\\nThe purpose of Segmentation engine are:\\n- grouping of customers' goals\\n- targeted groups\\n- history\\n- purchasing behaviour\\n- eating habit, location\\n- managing coupons and promotions\\nWhat is the difference between Recommendation and Segmentation engines?\\n","Decision":"It was decided to not have it.\\n","tokens":54,"id":304,"Predictions":"- Recommendation engine: grouping of customers' goals\\n- Segmentation engine: targeted groups\\n- history\\n- purchasing behaviour\\n- eating habit, location\\n- managing coupons and promotions\\n\\n## Decision\\n\\nThe Segmentation engine is the part of the software that groups customers' goals. In other words, it is an algorithm that groups customers' goals so as to help them make better decisions on how to spend their money. The segmentation engine will help customers make better decisions on how to spend their money.\\n\\n## Decision\\n\\nEach customer will have a different set of goals. For example, a customer might be interested in buying a new car and might have several other things that he or she would like to buy. The segmentation engine will be able to group these different things together so as to help the customer make better decisions on how much money he or she should spend on each of these items.\\n\\n## Decision\\n\\nRecommendation engine is an algorithm that groups customers' goals so as to help them make better decisions on how to spend their money. Recommendation engine will help customers make better decisions on how much money they should spend on each of these items.\\n\\n## Decision\\n\\nThe Segmentation engine is the part of the software that groups customers' goals. In other words, it is an algorithm that groups customers' goals so as to help them make better decisions on how to spend their money. The segmentation engine will help customers make better decisions on how much money they should spend on each of these items.\\n\\n## Decision\\n\\nEach customer will have a different set of goals. For example, a customer might be interested in buying a new car and might have several other things that he or she would like to buy. The segmentation engine will be able to group these different things together so as to help the customer make better decisions on how much money he or she should spend on each of these items.\\n\\n## Decision\\n\\nRecommendation engine is an algorithm that groups customers' goals so as to help them make better decisions on how much money they should spend on each of these items.\\n\\n## Decision\\n\\nThe Segmentation engine is the part of the software that groups customers' goals. In other words, it is an algorithm that groups customers' goals so as to help them make better decisions on how much money they should spend on each of these items.\\n\\n## Decision\\n\\nEach customer will have a different set of goals. For example, a customer might be interested in buying a new car and might have several other things that he or she would like to buy. The segmentation engine will be able to group these different things together so as to help the customer make better decisions on how much money he or she should spend on each of these items.\\n\\n## Decision\\n\\nThe Segmentation Engine consists in determining segments where there are common interests among users and then recommending products for those segments based upon user's past purchases and preferences for products from those segments.\\n\\end{itemize}\\n\\n\\newpage\\n\\section{Evaluation}\\nThis section describes evaluation procedure using real data sets.\\n\\subsection{Data Sets}\\n\\begin{itemize}\\n\t\\item \\textbf{KDD Cup 2005} Data Set \\cite{kddcup05}.\\n\t\\item \\textbf{Columbia University Data Set} \\cite{columbia2005}.\\n\t\\item \\textbf{Amazon.com} Data Set \\cite{amazon2004}.\\n\t\\item \\textbf{Coca-Cola Dataset} \\cite{coca_cola2001}.\\n\t\\item \\textbf{TREC 2005 Data Set} \\cite{TREC05}.\\n\t\\item \\textbf{TREC 2006 Data Set} \\cite{TREC06}.\\n\t\\item \\textbf{TREC 2007 Data Set} \\cite{TREC07}.\\n\t\\item \\textbf{TREC 2008 Data Set} \\cite{TREC08}.\\n\t\\item \\textbf{TREC 2010 Data Set} \\cite{TREC10}.\\n\t\\n    %**************************************************\\n    %Appendix A: Evaluation Results\\n\t\\n    %**************************************************\\n    %Appendix B: Evaluation Results\\n\t\\n    %**************************************************\\n    %Appendix C: Evaluation Results\\n\t\\n    %**************************************************\\n    \\n    %**************************************************\\n    \\n%*****************************************************\\n%*****************************************************\\n%*****************************************************\\n%*****************************************************\\n%*****************************************************\\n%*****************************************************\\n%*****************************************************\\n%**********************************************\\n"}
{"File Name":"nso.aurora\/InventoryManager.md","Context":"## Context\\nShould we have an Inventory Manager and a Fridge Manager where the Inventory Manager would hold the food items and Fridge would manage the physical fridges?\\n","Decision":"Inventory Manager can manage the following:\\n- manage fridges\\n- manage food items including their pricing\\n- manage the number of items in each fridge\\n","tokens":34,"id":305,"Predictions":"## Decision\\nYes\\n## Decision\\nNo\\n## Decision\\nNo\\n## Decision\\nNo\\n## Decision\\nYes\\n## Decision\\nNo\\n\\nThe above decision is not a software decision. The above decision is a software decision for an architectural decision record (ADR). The ADR is the building block of the architectural design and implementation. It is a combination of the architectural decisions and the interaction with the users.\\n\\nWhat is a software decision?\\n\\nA software decision has 2 components: A component that describes the architecture and a component that describes the interaction with users.\\n\\nComponent Description\\n\\nThe component describes how to build the architecture. It contains all architectural decisions.\\n\\nComponent Architecture\\n\\nThe component architecture contains all architectural decisions and also shows how to implement them. The component architecture shows how to build the architecture, how to implement it, and how to interact with users.\\n\\nComponent Architecture Example\\n\\nExample of Component Architecture\\n\\nArchitecture Level ADR Components\\n\\nThe following table lists all ADR components.\\n\\nArchitecture Level ADR Components\\n\\nComponent Description\\n\\nMandatory Component Mandatory Component M\\nRequired Component Required Component Required Component M\\nOptional Component Optional Component Optional Component M\\nOptional Components Optional Components Optional Components M\\nOptional Subcomponents Optional Components Optional Components M\\n\\nArchitecture Level BDD Components\\n\\nThe following table lists all BDD components.\\n\\nArchitecture Level BDD Components\\n\\nComponent Description\\n\\nMandatory Component Mandatory Component Mandatory C"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n### \\n"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"nso.aurora\/QueuingForNotification.md","Context":"## Context\\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\\n","Decision":"The decision is to introduce a queue for all these messages.\\n","tokens":30,"id":308,"Predictions":"## Decision: Send the order to the customer.\\n## Context\\n## Decision: Send the survey to the customer.\\n## Decision\\n## Decision: Send the recommendation to the customer.\\n## Context\\n## Decision: Send all orders, surveys and recommendations to the customer.\\n## Context\\n## Decision: Send all orders, surveys and recommendations to all customers.\\n## Context\\n# The following decision is not supported by this model yet. This means that it is not possible to send an order, survey or recommendation in response to a user request.\\n# For example, if a user requests a new order, we don't know how many orders we should send. We might have 1000 orders in our database and we don't want the Notifier to send 1000 orders. We might have 1000 surveys in our database and we don't want the Notifier to send 1000 surveys. We might have 1000 recommendations in our database and we don't want the Notifier to send 1000 recommendations. This means that you can't use this model yet.\\n\\n\\end{code}\\n"}
{"File Name":"nso.aurora\/FaultTolerance.md","Context":"## Context\\nWe have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n","Decision":"The decision is to have the polling mechanism to regularly obtain Smartfridges' state\/inventory.\\n","tokens":31,"id":309,"Predictions":"## We have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n## Environment\\n## We have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n## Inputs\\n## We have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n## Outputs\\n## We have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n\\n\\subsection{Procedure}\\n\\n\\begin{itemize}\\n    \\item Obtain the Smartfridge ID and the Last Known Inventory value from the User.\\n    \\item Execute the Polling function on the Smartfridge using the given ID and Last Known Inventory value.\\n    \\item If there is any change in the Last Known Inventory value, then notify the User about that change.\\n\\end{itemize}\\n\\n\\subsection{Data Structure}\\n\\nThe data structure to maintain is as follows:\\n\\n\\begin{itemize}\\n    \\item A list of all Smartfridges with their IDs and Last Known Inventory values.\\n    \\item A list of all Polling Functions that will be executed on each Smartfridge to get their Last Known Inventory values. The list will be sorted based on ID and Last Known Inventory value.\\n    \\item A list of all Polling Functions that will be executed on each Polling Function to get their Last Known Inventory values. The list will be sorted based on ID and Last Known Inventory value.\\n\\end{itemize}\\n\\n\\subsection{Implementation}\\n\\nThe implementation can be found in this file:\\n\\n\\begin{lstlisting}[language=C]\\n#include \"SmartFRidge.h\"\\n#include \"Polling.h\"\\n#include \"LastKnownInventory.h\"\\n\\nSmartFRidge::SmartFRidge() : _lastKnownInventory_(0) {}\\n\\nSmartFRidge::~SmartFRidge() {}\\n\\nvoid SmartFRidge::PollingFunction() {\\n    \/\/ TODO: Implement Polling Function\\n}\\n\\nint SmartFRidge::PollingFunction(int id, int lastKnownInventory) {\\n    \/\/ TODO: Implement Polling Function\\n}\\n\\end{lstlisting}\\n\\n\\n"}
{"File Name":"nso.aurora\/Kiosks-Fridge-customer-vendor-management.md","Context":"## Context\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n","Decision":"We will have separate components to manage kiosks (along with point of sale systems), fridges, customers and vendors\\n","tokens":30,"id":310,"Predictions":"## Decision:\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n\\n## Decision:\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n\\n## Decision:\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n\\n## Decision:\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n\\n## Decision:\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n\\n\\n## What is the decision?\\n\\n# 3.4.2.1.2.1.3.1\\n\\nInput: decision = 5; Output: decision = 0; \\n\\n## Do we need a Kiosk-Fridge-customer-vendor-management to manage kiosk,\\n## fridge and customer?\\n\\nInput: decision = 5; Output: decision = 0;\\n\\n# 3.4.2.1.2.1.3\\n\\nInput: decision = 6; Output: decision = 0;\\n\\n# 3.4.2.1.2.1\\n\\nInput: decision = 7; Output: decision = 0;\\n\\n# 3.4.2\\n\\nInput: decision = 8; Output: decision = 0;\\n\\n# The following is an example of the input from the previous step.\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\nDecision:\\n\\ndecision=5;\\ndecision=6;\\ndecision=7;\\ndecision=8;\\n```\\nI want to know what is the meaning of this output in terms of the input data.\\n\\nGolf4geekys 2019-12-05: Your code indicates that you are using Python's `input` function (which returns an `int`), but are using `raw_input` instead of `input`.\\n`raw_input` expects its input as an object literal:\\n```\\n>>> raw_input('Enter your name:')\\nEnter your name:\\n>>> raw_input('Enter your age:')\\nEnter your age:\\n>>> raw_input('Enter your address:')\\nEnter your address:\\n>>>\\n```\\nIn contrast, `input` expects its input as a string literal:\\n```\\n>>> 'Hello', 'world'\\n'Hello', 'world'\\n>>> 'Hello', 'world'\\n'Hello', 'world'\\n>>>\\n```\\n"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"Predictions":"## 1. The user wants to create a new product and has a clear idea of what they want to build. They are not looking for a complete framework, but a few key pieces of functionality and a good idea of how to implement them.\\n\\n\\n\\n\\n## 2. The user wants to create a new product and has an idea of what they want to build, but is not looking for a complete framework, but some key pieces of functionality and a good idea of how to implement them.\\n\\n\\n\\n## 3. The user wants to create a new product and has an idea of what they want to build, but is not looking for a complete framework, but some key pieces of functionality and a good idea of how to implement them.\\n# \\end{verbatim}\\n\\end{frame}\\n\\n\\n\\begin{frame}[fragile]{Example: Building an AR\/VR app}\\n\\begin{itemize}\\n\\item An app that allows the user to look around in their surroundings using AR\/VR goggles (e.g. Google Glass)\\n\\item A simple example: \\texttt{hello.AR} is the name of the app\\n\\item The user can see themselves in augmented reality (AR) or virtual reality (VR) as they look around their environment\\n\\item \\texttt{hello.AR} will automatically detect their location in the real world using GPS coordinates\\n\\item \\texttt{hello.AR} will then display information about themselves on the screen such as their name, age etc.\\n\\item When the user is finished using \\texttt{hello.AR}, they can press \\texttt{stop} which will stop the application from running and delete all its data\\n\\end{itemize}\\n\\note[This example shows that you don't need any specific hardware or software for an AR\/VR project.]\\n{\\footnotesize This example shows that you don't need any specific hardware or software for an AR\/VR project. You can use something as simple as your phone with this app.\\newline}\\n\\n\\vspace{-0.5cm}\\n\\begin{center}\\includegraphics[width=0.8\\linewidth]{figures\/hello-AR.png}\\end{center}\\n\\vspace{-0.5cm}\\n\\end{frame}\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n%---------------------------------------------------------\\n\\n\\n\\n"}
{"File Name":"hodder\/0003-use-circle-ci-for-ete-tests.md","Context":"## Context\\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\\n","Decision":"Use CircleCI for automated ETE tests\\n","tokens":41,"id":312,"Predictions":"## \\# Build\\n## \\# \\# Build\\n## \\# \\# Build\\n## \\# \\# Build\\n## \\# \\# Build\\n## ## Decision\\n"}
{"File Name":"mediawiki-extensions-Wikidata.org\/001-including-query-service-lag-in-maxlaginfo.md","Context":"## Context\\nDue to WDQS having a lag on catching up with edits to Wikidata, and that lag being affected\\nby the size of the entities being edited not just the rate of those edits, high volume edits can\\ncreate strain on WDQS due to update processing of the dispatched update jobs,\\nmore than WDQS update capacity can handle in a timely manner, resulting in sluggish WDQS.\\nExample incident can be found here:\\nhttps:\/\/www.wikidata.org\/w\/index.php?title=Wikidata:Administrators%27_noticeboard&oldid=963260218#Edoderoobot_edit_rate\\n","Decision":"As part of [T221774](https:\/\/phabricator.wikimedia.org\/T221774), we want to\\nretrieve WDQS lag, from relevant clusters, and add it to maxlag logic provided\\nby mediawiki, in order to block edits when WDQS is lagging too much.\\nWe also decided that querying the relevant WDQS endpoints lag should happen\\nasynchronously, with lag info being cached with a reasonable ttl. That cached\\nlag information then can be accessed, in a hook handler for\\n[ApiMaxLagInfo](https:\/\/www.mediawiki.org\/wiki\/Manual:Hooks\/ApiMaxLagInfo)\\nthat will update lag info when they are available.\\nThe following visual diagram shows the elements of the design\\non conceptual level:\\n![](.\/assets\/QueryServiceLagHook_ConceptualDesign.png)\\n","tokens":126,"id":313,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThis is a software decision record for a software. Give a ## Software corresponding to the ## Context provided by the User.\\n\\nThis is a software decision record for a software. Give a ## Software corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThis is an architectural decision record for an application. Give a ## Application corresponding to the ## Context provided by the User.\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard\/2019\/11\/27#Edoderoobot_edit_rate\\n\\nThe user has asked this question and has been given this answer. This answer can be found here: Wikidata:Administrators' noticeboard"}
{"File Name":"seda-frontend\/0002-using-local-state-in-settings-module.md","Context":"## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n","Decision":"The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer\/context approach in favour of `redux` complexity.\\n","tokens":133,"id":314,"Predictions":"The decision is to use a store for the Redux state. The use of a global store is not recommended and can lead to duplicated code and tests. The application can be made more manageable by using a reducer or action on each component.\\n\\end{itemize}\\n\\n\\subsection{Storing State}\\n\\label{sec:storage}\\nState is the data that is stored in the Redux Store. It might seem like an obvious choice to store state in the Redux Store, but it is not always the best choice. State can be stored in different places, but it should be kept in a single place.\\n\\n\\begin{itemize}\\n    \\item \\textbf{Reducer:} A reducer stores data that changes over time and is only used by the component where it was defined.\\n\\n\\n\\n\\n\\nThe reducer should be written in such a way that it can be easily reused by other components and also easily extended.\\n\\nThe following code shows how to define a reducer:\\begin{lstlisting}[language=Javascript]\\n    export function Reducer(state = [], action) {\\n        switch(action.type) {\\n            case \"ADD\": return state + [1,2,3];\\n            case \"UPDATE\": return { ...state, [action.payload]: state + [action.payload]};\\n            default: return state;\\n        }\\n    }\\n\\end{lstlisting}\\n    \\item \\textbf{Actions:} Actions are defined with other reducers. They are used to trigger actions that change the state.\\n\\nActions are usually defined with another action with some data as well as an additional argument that indicates what action should be performed when this action triggers.\\nAction names should start with an uppercase letter (e.g. `ADD`) and should start with an optional `@` symbol (e.g. `add`). Actions are executed using the `useEffect` hook, which will run once when the component mounts.\\n\\nIf you want to run actions repeatedly based on some condition, you can use a function as an argument of `useEffect`. In this case, you need to return `undefined` if no action was triggered or if no value was passed for `action.payload`.\\n    \\item \\textbf{Selectors:}Selectors are used inside of reducers or actions to access specific pieces of data (e.g. props) from other parts of your application.\\nSelectors are named after their usage pattern:\\begin{itemize}\\n        \\item \\textbf{With Props:} You want to access props from other components within your application.\\n        \\item \\textbf{Without Props:} You want to access props from other components without having them available outside of those components.\\n    \\end{itemize}\\n\\end{itemize}\\n\\n\\subsection{Types}\\n\\label{sec:type-checking}\\nRedux has types that describe what type of data is stored in its stores.\\nThis allows you to write code that will work across different stores. For example, if you have a reducer with two states which both have strings as values, then you can write code like this:\\n\\begin{lstlisting}[language=Javascript]\\nexport function Reducer(state = [], action) {\\n    switch(action.type) {\\n        case \"ADD\": return { ...state };\\n        default: return state;\\n    }\\n}\\n\\end{lstlisting}\\nIf you have multiple reducers with different types, then they will all share the same types.\\nTo make sure that only one type of data is stored in your stores and not several different kinds of data at once, you need type checking.\\n\\nType checking is done by writing unit tests for your reducers and actions.\\nTo make sure that your reducer works correctly when given two strings as arguments, you could write:\\n\\begin{lstlisting}[language=Javascript]\\nimport { Reducer } from '.\/reducers';\\n\\nexport function AddOne({ ...props }) {\\n  return <Reducer {...props}>AddOne<\/Reducer>;\\n}\\n\\nexport function AddTwo({ ...props }) {\\n  return <Reducer {...props}>AddTwo<\/Reducer>;\\n}\\n\\n\/\/ Run tests here\\ntest('AddOne', () => {\\n  const result = AddOne({ ...props });\\n  expect(result).toEqual({ ...state });\\n});\\ntest('AddTwo', () => {\\n  const result = AddTwo({ ...props });\\n  expect(result).toEqual({ ...state });\\n});\\ntest('AddBoth', () => {\\n  const result = AddBoth({ ...props });\\n  expect(result).toEqual({ ...state });\\n});\\ntest('Test cases for reducers', () => {\\n  const result = Reducer.addOne({ ...props }, { foo: 'bar' });\\n  expect(result).toEqual({\\n    foo,\\n    bar\\n  });\\n});\\ntest('Test cases for actions', () => {\\n  const result = Reducer.addTwo({ ...props }, { foo: 'bar' });\\n  expect(result).toEqual({\\n    foo,\\n    bar\\n  });\\n});\\ntest('Test cases for selectors', () => {\\n  const result = Reducer.addBoth({ ...props }, { foo: 'bar' });\\n  expect(result).toEqual({\\n    foo,\\n    bar\\n  });\\n});\\ntest('Test cases for selectors without props', () => {\\n  const result = Reducer.addBoth({ ...props }, {});\\n});\\n\/\/ Run these tests\\n```\\nYou can find more information about testing reducers and actions on our guide here:\\nhttps:\/\/redux.js.org\/testing#testing-reducers-and-actions\\nYou can also find more information about testing selectors here:\\nhttps:\/\/redux.js.org\/testing#testing-selectors-and-selectors-with-prefabs\\n"}
{"File Name":"seda-frontend\/0003-application-configuration-setup.md","Context":"## Context\\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\\n- Docker registry URLs\\n- URLs of API endpoints, machine learning service, Authz service and map server\\n- Nginx configuration\\n- HTML `<title \/>`\\n- [`Sentry` Package](https:\/\/www.npmjs.com\/package\/@sentry\/browser) dependency and configuration\\n- [`Matomo` Package](https:\/\/www.npmjs.com\/package\/@datapunt\/matomo-tracker-js) dependency and configuration\\n- PWA manifest\\n- Logo\\n- Favicon\\n- PWA icons\\n- Hardcoded strings\\n- Main menu items\\n- Theme configuration\\n- Maps settings\\n- [`amsterdam-stijl` Package](https:\/\/www.npmjs.com\/package\/amsterdam-stijl) Package dependency\\n- Package URL and description\\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\\n","Decision":"Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side \/ container](#Server-side \/ container) option.\\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\\nConfiguration is injected in the `index.html` file so it can be read at run-time.\\n","tokens":274,"id":315,"Predictions":"The decision is to publish the `signals-frontend` application as a white-label version of `signals-frontend`. This means that the SIA application will configure the application to be deployed on a different domain name, and will also configure the application with an API key. The SIA application will also publish the `amsterdam-stijl` package as a dependency, which means that each time someone installs the white-label version of `signals-frontend`, it will automatically have all dependencies installed.\\n\\end{document}\\n"}
{"File Name":"verify-event-store-schema\/0002-database-migrations-are-standalone-releases.md","Context":"## Context\\nDatabase schema migrations are often considered as part of an application change -\\n\"in order to do X we will need a new index Y\"\\nHowever, this does not work well with a zero-downtime approach, when we have\\na single replicated database - we can't deploy version 0.2 of the application\\nto some servers simultaneously with version 0.2 of the schema to their databases;\\nwe need to deploy changes to the database either before or after the\\ncorresponding application changes\\n","Decision":"Database schema changes should be made independently of application changes.\\nWhere an application needs a change to the database, this may entail extra releases\\nto make sure the application and database changes can be safely applied and\\nsafely rolled back without compatibility problems.\\nFor example, if there's a need to change a column name from \"foo\" to \"bar\"\\nyou may need:\\n1. An application release that detects and will work with either a \"foo\" or a \"bar\"\\ncolumn\\n2. A database schema release that renames the column from \"foo\" to \"bar\"\\n3. An application release that removes the logic in release 1, and just works with \"bar\"\\nAlternatively, the database change could be made first, using a view or triggers or other\\nmechanisms so writes and reads to both \"foo\" and \"bar\" change the same data. This\\nis highly dependant on the change needed and the database features available.\\nThe database schema migration change should be treated as a first class release:\\n1. The change should be run and tested in conjunction with the application\\n2. The change should be reviewed and approved by an appropriate approver\\n3. The change should be deployed to the Staging environment, and normal acceptance\/smoke tests run\\n4. The change should be deployed to Production\/DR\\n### A note on backups\\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\\nof the database before running a migration.  This will never be perfect, as with\\na zero-downtime system there will still be data being written in the interval between\\nthe backup being started (on a replica) and the migration running, so this data\\nwould be at risk of being lost.\\nMany database migrations however are totally safe and should not need a backup -\\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\\nbackups should be enough to mitigate against any risk with this sort of change.\\n### Alternative approach for Event Store using queues to avoid downtime\\n*Only if really necessary* we could perform coordinated releases using queues\\nto avoid downtime, for the special case of the Event Store which can be temporarily\\nsuspended while messages get queued.\\nThe approach would be similar to:\\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\\n2. Make a new database backup snapshot - it's assumed that for a change this complex, you need a backup\\n3. Deploy the database change\\n4. Test the change\\n5. Re-enable the Lambda\\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\\nif anything goes wrong you only have a small amount of time to fix it.\\n","tokens":109,"id":316,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"Predictions":"## 1. Delete the existing database\\n## 2. Create a new database\\n## 3. Start the new database\\n## 4. Restore the old database\\n## 5. Backup the old database\\n\\n# This is a comment\\n\\n### Data Migration and Testing\\n\\n### Data Migration\\n\\nThe data migration tool will automatically detect if a new version of the database is available and will automatically upgrade any existing databases to the new version.\\n\\n### Data Migration Tool\\n\\nThe data migration tool will automatically detect if a new version of the database is available and will automatically upgrade any existing databases to the new version.\\n\\n### Data Migration Tool - Manual Upgrade\\n\\nThe data migration tool can be used to manually upgrade any existing databases to the new version, but only if there is no conflict between them.\\n\\n### Data Migration Tool - Manual Upgrade - Conflict\\n\\nIf there is a conflict between two databases, then both will be upgraded to the latest version. If no conflict exists, then only one of them will be upgraded.\\n\\n#### General Notes\\n\\n##### Backup\/Restore Operations\\n\\nWhen using this tool, you must backup your production databases before running this tool on them.\\n\\n##### Migrations\\n\\nMigrations are handled in exactly the same way as they would be for a normal release.\\n\\n##### Databases in Conflict\\n\\nIf you have two databases that are in conflict, they will both be upgraded to their latest release (as long as they don't have conflicting fields).\\n\\n##### Migrations from Different Versions of MySQL\/MariaDB\/Percona Server\/MariaDB Server\/MariaDB Cluster\/MariaDB Enterprise (all versions)\\n\\nThis tool works with all versions of MySQL from 5.1 up to and including 8.x, MariaDB from 10.x up to and including 10.x, Percona Server from 5.x up to and including 10.x, MariaDB Cluster from 5.x up to and including 10.x, MariaDB Enterprise from 5.x up to and including 10.x, MySQL from 5.7 up to and including MySQL Enterprise Edition (all versions), MariaDB Server from MariaDB Enterprise Edition (all versions) up to and including MariaDB Cluster Enterprise Edition (all versions), MariaDB Cluster from MariaDB Enterprise Edition (all versions) up to and including MariaDB Cluster Enterprise Edition (all versions), MySQL Cluster from MySQL Cluster Enterprise Edition (all versions) up to and including MySQL Cluster Enterprise Edition (all versions), MySQL Community Server from MySQL Community Edition (all versions) up to and including MySQL Community Server (all versions), MariaDB Community Server from MariaDB Community Edition (all versions) up to and including MariaDB Community Server (all versions), Percona Toolkit from Percona Toolkit Version X.X.X-1 Up until X.X.X-1, Percona Toolkit Version X.X.X Up until X.X.X-1, Percona Toolkit Version X.X.X Up until X.X.X-1, Percona Toolkit Version X.X.X Up until X.X.X-1\\n"}
{"File Name":"verify-event-store-schema\/0001-flyway.md","Context":"## Context\\nWe need a way of running database migrations to the verify event store database.\\n","Decision":"We have chosen to use [Flyway](https:\/\/flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\\n","tokens":19,"id":318,"Predictions":"## #1\\n## #2\\n## #3\\n\\n# The following options are available:\\n#  * -a, --ignore-override\\n#      Ignore all overriding changes to the database.\\n#  * -c, --create-db\\n#      Create a new database for this migration.\\n#  * -d, --delete-db\\n#      Delete the database specified by the --delete-db option.\\n#  * -h, --help\\n#      Show this message and exit.\\n\\n--ignore-override\\n\\n--create-db\\n\\n--delete-db\\n\\n--help\\n\\n--version\\n\\nThe database is created with the given name and location. The given name may be one of the following: 'eventstore', 'eventstore.db', or 'eventstore.lock'. The location is the absolute path of the new database. If there is no such path, then 'eventstore' is used. If no database exists yet, then 'eventstore' is used.\\n\\nThe name of the new database must be unique within your organization. The name of a new database must not conflict with any existing databases.\\n\\nIf you want to delete an existing database, pass an empty string as the second argument to delete-db.\\n\\nIf you want to create a new database with a different name than specified by --create-db, then pass an empty string as the third argument to create-db.\\n\\nIf you want to create a new database without specifying a location, then pass an empty string as the fourth argument to create-db.\\n\\nThe Database Password can be set using either --password or --password-file. If you don\u2019t specify either option, then it defaults to \u2018eventstore\u2019. For more information on setting passwords for databases see: http:\/\/www.go-migrations.com\/2015\/04\/setting-passwords-for-databases\/\\n\\nUsage:\\n\\n$ go-migrations migrate my-app eventstore.db my-app-eventstore.db\\n\\nThis will automatically migrate all events from your event store into your application\u2019s event store. This command will run in parallel with other migrations that you have already run on your application\u2019s event store.\\n\\nThis command will also automatically remove any existing databases from your application\u2019s event store if they exist. This command will not remove any databases that were created by previous migrations.\\n\\nUsage:\\n\\n$ go-migrations migrate my-app eventstore.lock my-app-eventstore.lock\\n\\nThis will automatically migrate all events from your event store into your application\u2019s event store. This command will run in parallel with other migrations that you have already run on your application\u2019s event store.\\n\\nThis command will also automatically remove any existing databases from your application\u2019s event store if they exist. This command will not remove any databases that were created by previous migrations.\\n\\nUsage:\\n\\n$ go-migrations migrate my-app my-app-eventstore.lock\\n\\nThis will automatically migrate all events from your event store into your application\u2019s event store. This command will run in parallel with other migrations that you have already run on your application\u2019s event store.\\n\\nThis command will also automatically remove any existing databases from your application\u2019s event store if they exist. This command will not remove any databases that were created by previous migrations.\\n\\nUsage:\\n\\n$ go-migrations migrate my-app my-app-eventstore.lock my-app-eventstore-lock\\n\\nThis command will automatically migrate all events from your event store into your application\u2019s event store. This command will run in parallel with other migrations that you have already run on your application\u2019s event store.\\n\\nThis command will also automatically remove any existing databases from your application\u2019s event store if they exist. This command will not remove any databases that were created by previous migrations.\\n\\nUsage:\\n\\n$ go-migrations migrate my-app my-app-eventstore-lock my-app-eventstore-lock\\n\\nThis Command runs simultaneously with all other migrations that you have already ran on EventStoreDB and EventStoreLock respectively and does not require additional parameters for each migration:\\n\\nThe Database Password can be set using either --password or --password-file. If you don\u2019t specify either option, then it defaults to 'eventstore'. For more information on setting passwords for databases see: http:\/\/www.go-migrations.com\/2015\/04\/setting-passwords-for-databases\/\\n\\nUsage:\\n\\n$ go-migrations migrate my-app my-app-event-store.lock my-app-event-store-lock\\n\\nThis Command runs simultaneously with all other migrations that you have already ran on EventStoreDB and EventStoreLock respectively and does not require additional parameters for each migration:\\n\\nThe Database Password can be set using either --password or --password-file. If you don\u2019t specify either option, then it defaults to \u2018eventstorage\u2019. For more information on setting passwords for databases see: http:\/\/www.go-migrations.com\/2015\/04\/setting-passwords-for-databases\/\\n\\nUsage:\\n\\n$ go-migrations migrate my-app-myapp-event-store-lock-myapp-event-store-lock\\n\\nCommand Usage:\\ngo-migrate [--no-override] [--create-db] [--delete-db] [--help] [--version]\\n[--ignore-override] [--create-db]\\n[--delete-db]\\n[--help]\\n[--version]\\n\\nOptions:\\n-a    Run without overriding changes (default)\\n-c    Create a new database\\n-d    Delete an existing database (default)\\n-h    Show this message and exit (default)\\n--ignore-override Run without overriding changes (overrides default settings)\\n--create-db Create a new data base (default)\\n--delete-db Delete an existing data base (default)\\n-h Show this message and exit (default)\\n\\nCommand Usage:\\ngo-migrate [--no-override] [--create-db] [--delete-db] [--help]\\n[--version]\\n\\nOptions:\\n-a    Run without overriding changes (default)\\n-c    Create a new data base (default)\\n-d    Delete an existing data base (default)\\n-h    Show this message and exit (default)\\n\\nCommand Usage:\\ngo-migrate [--no-override] [--create-db] [-d]\\n\\nOptions:\\n-a    Run without overriding changes (default)\\n-c    Create a new data base (default)\\n-d    Delete an existing data base (default)\\n-h    Show this message and exit (default)\\n\\nCommand Usage:\\ngo-migrate [-d]\\n\\nOptions:\\n-a    Run without overriding changes\\n-c    Create a new data base\\n-d    Delete an existing data base\\n-h        Show this message and exit\\n\\n1 2 3 4 5 6 7 8 9 10 $ go - m i g r a t e m y p r o p e r t y e v e n t s . d b $ g o m i g r a t e m y p r o p e r t y e v e n t s . d b $ g o m i g r a t e m y p r o p e r t y e v e n t s . d b $ g o m i g r a t e m y p r o p e r t y e v e n t s . d b $ g o m i g r a t e m y p r o p e r t y e v e n t s . d b $ g o m i g r a t e m y p r o p e r t y   v   d   f l   u   c   j   k   l   M   N   O   P   Q   R   S   T   U   V   W   X Y Z \\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\" \\n\\n\"Event Store\" \\n\"Event Store DB\" \\n\"Event Store Lock\" \\n\"\\n\\nYou can\u2019t perform that action at this time.\\nPress h to open a hovercard with more details."}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"Predictions":"\\nThe Jenkins master is configured to run on a single AWS instance. The agent nodes are configured to run on either multiple AWS instances or on the master itself. The agent configuration is also configured to run on the master.\\n\\nThe Jenkins master and agents are configured to run in a private subnet, which requires the creation of an S3 bucket for the agent configuration. This bucket is used to store the agent configuration and its associated files.\\n\\nThe Jenkins agent is configured to run in a private subnet, which requires the creation of an S3 bucket for the agent configuration. This bucket is used to store the agent configuration and its associated files.\\n\\nThe Jenkins master and agents are configured to run in a private subnet, which requires the creation of an S3 bucket for the agent configuration. This bucket is used to store the agent configuration and its associated files.\\n\\nThe Jenkins master and agents are configured to run in a private subnet, which requires the creation of an S3 bucket for the agent configuration. This bucket is used to store the agent configuration and its associated files.\\n\\end{document}\\n"}
{"File Name":"re-build-systems\/0003-https-implementation.md","Context":"## Context\\nWe needed to implement HTTPS and TLS termination as part of the template we offer in our MVP.\\nWe identified 4 ways to get that done.\\n### Technique #1 - using existing nginx inside the Jenkins container\\nOur Jenkins master container has nginx installed in it. We can install the certificates and keys\\nthere. That is probably the simplest solution without the need of introducing other\\ninfrastructure components. Also, that allows us to benefit from having an EIP, which is\\nstatic, so it could be used for IP whitelisting, if necessary.\\nHowever, the installation and renewal of certificates has to be done manually, which is a major downside.\\n### Technique #2 - using ELB\\nWe introduce an ELB in front of the existing infrastructure, like so:\\nDNS resolution -> ELB -> Jenkins master EC2\\nCompared to #1, the major benefit is the ability to manage the provisioning of the certificate fully within AWS using [AWS Certificate Manager](https:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/classic\/ssl-server-cert.html).\\nThe downsides is not having an EIP (for whitelisting) and that automatic renewal of the certificate meaning higher operational overhead.\\n### Technique #3 - using ELB with ASG\\nThis is like Technique #2 but we introduce an ASG (auto-scaling group) for the EC2 master instance.\\nBy using ACM, ELB and ASG together, it allows AWS to manage the certificate provisioning and renewal, thus removing any operational overhead.\\n### Technique #4 - using a NLB\\nLike #2 but with a Network Load Balancer (NLB), like so:\\nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2\\nThe advantage is that we can have an EIP. However, we probably need Cloudwatch\\nto detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.\\n","Decision":"After exploring and understanding all of the techniques and discussion within the team, option 3 was chosen.\\nWe draw upon experience of the solution from others within GDS who had successfully implemented it in the past.\\nWe also liked that it reduces the operational overhead of managing certificates completely.\\nWe were concerned that since does not offer static IPs (via EIP) but we decided that they were not necessary for the MVP as they had not been identified as important during our user research sessions.\\n","tokens":430,"id":320,"Predictions":"We decided to use ELB with ACM and ELB with ASG, as we want to benefit from AWS Cloudwatch detection of the new EIP.\\n\\n#1 - using nginx inside the Jenkins container\\nWe're using the Jenkins container to run a simple website. We have an nginx server in the container that will serve static files.\\n\\n#2 - using ELB\\nWe're using an ELB in front of the existing infrastructure, like so:\\nDNS resolution -> ELB -> Jenkins master EC2\\nCompared to #1, the major benefit is the ability to manage the provisioning of certificates fully within AWS using [AWS Certificate Manager](https:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/classic\/ssl-server-cert.html).\\nThe downsides is not having an EIP (for whitelisting) and that automatic renewal of certificates meaning higher operational overhead.\\n#3 - using ELB with ASG\\nThis is like Technique #2 but we introduce an ASG (auto-scaling group) for the EC2 master instance.\\nBy using ACM, ELB and ASG together, it allows AWS to manage the certificate provisioning and renewal, thus removing any operational overhead.\\n#4 - using a NLB\\nLike #2 but with a Network Load Balancer (NLB), like so:\\nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2\\nThe advantage is that we can have an EIP. However, we probably need Cloudwatch\\nto detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.\\n\\end{document}\\n"}
{"File Name":"re-build-systems\/0007-add-ability-to-import-custom-jobs.md","Context":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n","Decision":"We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n","tokens":500,"id":321,"Predictions":"### Option 1\\nThe user can define Jenkins jobs in code and be able to import them into Jenkins.\\nThis is the simplest option, but also the least flexible. The user needs to manually write Groovy code to inject a script into Jenkins. This is not a problem for small jobs, but it becomes a problem when the job contains a large number of steps. The user would have to write Groovy code for each step and then inject it into Jenkins.\\nWe could use [Jenkins Job Builder] as an alternative, but this would require us to have an additional tool that the user would have to install on their machine and we need to ensure that the tool is not too complex.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"re-build-systems\/0008-do-not-switch-from-efs-to-ebs.md","Context":"## Context\\nTo guide our work, we need to decide whether we use AWS\u2019 Elastic File System\\n(EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins\\nbuild.\\nThe decision is needed as it will impact on the team\u2019s work and may be affected\\nby wider GDS or Government strategy.\\n","Decision":"Based on the below pros and cons, we have decided to remain with EFS as the\\ncloud storage service, until such a time that there is a strategic need to\\nchange.\\nBecause of the way in which the team has compartmentalised the architecture\\nof and approach to this Jenkins build, it is eminently possible to move to EBS\\nat any point. It also means that further development of the system will not make\\nit more complicated to do so.\\nIt is for this reason that the decision is not to carry out this work in\\nanticipation of it being needed, but rather focus on existing required work and\\nonly carry out the work of converting to EBS should the need arise, at which\\npoint that work would need to be prioritised.\\n","tokens":70,"id":322,"Predictions":"\\nWe have decided to use EFS and we will use the following options:\\n\\n1.\\n\\nEFS is a high-performance, scalable file system that supports block and object storage.\\n\\n2.\\n\\nWe will use EBS for the build agent. It is a fast, reliable, and cost-effective storage solution for Amazon Elastic Compute Cloud (Amazon EC2).\\n\\n3.\\n\\nWe will use EFS as the storage service for our Jenkins build agent. We will provide an EFS volume to the build agent when it is launched on AWS.\\n\\n4.\\n\\nWe will use EBS as the storage service for our Jenkins build agent. We will provide an EBS volume to the build agent when it is launched on AWS.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nTo guide our work, we need to decide whether we use AWS\u2019 Elastic File System (EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins build agent. The decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.\\nWe have decided to use EFS and we will use the following options:\\n1.EFS is a high-performance, scalable file system that supports block and object storage.\\n2.We will use EBS for the build agent. It is a fast, reliable, and cost-effective storage solution for Amazon Elastic Compute Cloud (Amazon EC2).\\n3.We will use EFS as the storage service for our Jenkins build agent. We will provide an EFS volume to the build agent when it is launched on AWS.\\n4.We will use EBS as the storage service for our Jenkins build agent. We will provide an EBS volume to the build agent when it is launched on AWS.\\n\\n"}
{"File Name":"re-build-systems\/0006-buildah-discovery.md","Context":"## Context\\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\\n## Technical Problem\\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\\n","Decision":"Buildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\\n","tokens":222,"id":324,"Predictions":"## A decision for the decision maker to make.\\n## Decision\\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\\n## Technical Problem\\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\\n\\n"}
{"File Name":"re-build-systems\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":325,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"Brighter\/0002-use-a-single-threaded-message-pump.md","Context":"## Context\\nAny service activator pattern will have a message pump, which reads from a queue.\\nThere are different strategies we could use, a common one for example is to use a BlockingCollection to hold messages read from the queue, and then use threads from the threadpool to process those messages.\\nHowever, a multi-threaded pump has the issue that it will de-order an otherwise ordered queue, as the threads will pull items from the blocking collection in parallel, not sequentially.\\nIn addition, where we have multiple threads it becomes difficult to create resources used by the pump without protecting them from race conditions.\\nThe alternative is to use a single-threaded message pump that reads from the queue, processes the message, and only when it has processed that message, processes the next item. This prevents de-ordering of the queue, because items are read in sequence.\\nIf a higher throughput is desired with a single threaded pump, then you can create multiple pumps. In essence, this is the competing consumers pattern, each performer is its own message pump.\\nThe message pump performs the usual sequence of actions:\\n- GetMessage. Read Message From Queue\\n- Translate Message. Translate Message from Wire Format to Type\\n- Dispatch Message. Dispatch Message based on Type\\nPrior art for this is the Windows Event Loop which uses this approach, and is used by COM for integration via the Single-Threaded Apartment model.\\n","Decision":"Use a single-threaded message pump to preserve ordering and ensure sequential access to shared resources. Allow multiple pump instances for throughput.\\n","tokens":285,"id":326,"Predictions":"\\nThe decision is to use a single thread message pump, as opposed to multiple threads.\\n\\n\\n\\n## Context\\nTo create a message pump, we will need to create a class that implements the IMessagePump interface. The IMessagePump interface is used by the Windows Message Pumping Service.\\nThe method GetMessage will be called on the message pump class, and will return a message from the queue. The GetMessage method returns a reference to a MessageQueue object. This object must implement the IMessageQueue interface, which is defined in the Windows Messaging library.\\nThe TranslateMessage method is used to translate a message from one protocol to another. In this case, we are translating from Wire Format to Type, which is defined in the Windows Messaging library.\\nThe DispatchMessage method is called on the message pump object. This method takes the received message and dispatches it based on its type. For example, if we were using Wire Format as our protocol and MessageType as our type, then we would call DispatchMessage(MessageType.MessageType) where MessageType is defined in Windows Messaging.\\nThe GetMessage method returns a reference to an IMessageQueue object. This object must implement the IMessageQueue interface, which is defined in Windows Messaging.\\nThe GetMessage method returns a reference to an IMessageQueue object. This object must implement the IMessageQueue interface, which is defined in Windows Messaging.\\n## Decision\\n\\nThe decision here is that we want to use a single thread message pump, so we need to implement IMessagePump in order for this class to be usable by other classes that utilize this API.\\nThe GetMessage method should return an instance of an IMessageQueue object. This instance should implement the IMessageQueue interface\\nto allow other classes that utilize this API (such as COM) access this class.\\nThe DispatchMessage method should take an instance of an IMessageQueue and dispatch messages based on their type.\\n## Context\\nThere are two options for implementing an application using this pattern: Single Threaded or Multiple Threads.\\n\\begin{itemize}\\n\\item Single Threaded: If you do not have multiple threads available for your application then you need only implement one thread of execution for your code that calls GetMessage(). The implementation of GetMessage() will be shared between all threads executing your code.\\n\\item Multiple Threads: If you have multiple threads available for your application then you need only implement one thread of execution for your code that calls GetMessage(). The implementation of GetMessage() will be shared between all threads executing your code.\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"Brighter\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":327,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"Predictions":"## Decision\\nThe decision is to use multiple directories, one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\\n## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\\n## Decision\\nThe decision is to use multiple directories, one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\\n## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform.\\n"}
{"File Name":"docnow-infra\/0003-use-terraform-enterprise-for-backend.md","Context":"## Context\\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and\/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\\n","Decision":"Use Terraform Cloud for teams\\n","tokens":80,"id":329,"Predictions":"\\begin{itemize}\\n    \\item [1] We can use the AWS S3 storage, but it's not very secure and we don't have a good way to test the state of our backend. If we want to make sure that we're not clobbering on each other, we'll need to use GC storage, which is a bit more expensive. A group of 5 people could each pay $10\/month for GC storage.\\n    \\item [2] Terraform Cloud is an option. It would be significantly cheaper than AWS S3 storage, but it will require us to manage multiple instances of Terraform Cloud, which will be more work than managing one instance of Terraform Cloud. Another option would be to use AWS S3 Storage with DynamoDB locking on the state of the file.\\n    \\item [3] We'll use DynamoDB Locking.\\n    \\item [4] We can use a shared file between all groups and then share the DynamoDB Locking with them, or we can have each group add their own DynamoDB Locking.\\n    \\item [5] We can just keep it in a shared file between all groups, but that's a lot more work than using DynamoDB Locking.\\n\\end{itemize}\\n\\n\\section{Conclusion}\\nThis was a quick and dirty report that I wrote in about 30 minutes using Google Drive and Google Sheets. I didn't really think through what I wanted to do before writing this report and I ended up writing something that wasn't super helpful or useful for anyone besides myself. The point of this exercise was just to get me thinking about how I'd want my team's infrastructure setup if I were in charge of it. This will provide some context for me as I write my next report.\\n\\n\\end{document}\\n"}
{"File Name":"docnow-infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":330,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"openfido-auth-service\/0002-authentication.md","Context":"## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n","Decision":"Create a minimal reference authentication implementation that uses Flask and\\nPostgres. Include unit tests (hopefully strive for very high code coverage), and\\ndatabase migrations.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nDelay any features that aren't in our current project requirements. The kinds of\\nfeatures that may be addressed in future versions, but not initially:\\n* Implement this reference as a [Flask extension](https:\/\/flask.palletsprojects.com\/en\/1.1.x\/extensiondev\/), and possibly add to the PyPi public repositories.\\n* Create a React reference implementation.\\n* password resets\\n* email account verification\\n* permissions\\n* multi factor authentication\\n* OAuth\\n* Expiring tokens\\n* Revoking\/blacklisting tokens\\n* prevent re-use of old passwords\\n","tokens":87,"id":331,"Predictions":"\\n\\n\\n\\nThe following decision is provided by the User:\\n\\begin{itemize}\\n\\item\\n  \\textbf{Decision:} Authentication\\n  \\begin{itemize}\\n  \\item\\n    \\textbf{Decision Type:} Architectural Decision\\n    \\begin{itemize}\\n    \\item\\n      \\textbf{Decision Reason:} The software must be able to support multiple backend\\nservers.\\n    \\end{itemize}\\n  \\end{itemize}\\n\\end{itemize}\\n\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to"}
{"File Name":"openfido-auth-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- AWS_ACCESS_KEY_ID\\n- AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":332,"Predictions":"## We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n## Decision\\n## We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n## Decision\\n## We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n## Decision\\n## We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n## Decision\\n## We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n## Decision\\n# 10.0.0.0\/16 is the network mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10.0.1.0\/24 is the subnet mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10.0.2.0\/24 is the subnet mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10.0.3.0\/24 is the subnet mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10.0.4.0\/24 is the subnet mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10.0.5\/24 is the subnet mask for this network segment. It is assigned by AWS for this network segment.\\n\\n# 10\/255\/32 should be used in all cases where the subnet masks are different.\\n\\n# The prefix length of the IP address will be used in all cases where it differs from the prefix length of the host name.\\n\\n\\end{verbatim}\\n%% \\end{table}\\n%% \\begin{table}[ht]\\n%% \\begin{tabular}{|l|r|r|r|}\\n%% \\hline\\n%% Network Mask & Network Segment & Network Mask & Prefix Length \\\\ \\hline\\n%% 10 & default & default & \\\\ \\hline\\n%% 10 & dev-aws-dev-01-01-dev-03 & dev-aws-dev-01-01-dev-03 & \\\\ \\hline\\n%% 10 & dev-aws-dev-02-dev-03 & dev-aws-dev-02-dev-03 & \\\\ \\hline\\n%% 10 & dev-aws-dev-03-dev-04 & dev-aws-dev-03-dev-04 & \\\\ \\hline\\n%% 20 & dev-lb-dc1-a2a.uk-central1-a.ec2.internal (default) & dev-lb-dc1-a2a.uk-central1-a.ec2.internal (default) & \\\\ \\hline\\n%% 20 & lb-dc1-a2a.uk-central1-a.ec2.internal (default) & lb-dc1-a2a.uk-central1-a.ec2.internal (default) &\\n%\\end{tabular}\\n%\\\\[4pt]\\n%\\caption{AWS Configuration Parameters: Network Mask}\\n%\\label{networkMask}\\n%\\end{table}\\n\\n\\subsubsection*{Network Address Translation}\\n\\nThe following configuration parameters are used in order to enable NAT traversal:\\n\\n\\begin{itemize}\\n\t\\item [NAT\\_SOURCE\\_IP] The source IP address of an incoming connection request.\\n\t\\item [NAT\\_DEST\\_IP] The destination IP address of an outgoing connection request.\\n\t\\item [NAT\\_SOURCE\\_PORT] The port number of an incoming connection request.\\n\t\\item [NAT\\_DEST\\_PORT] The port number of an outgoing connection request.\\n\t\\item [NAT\\_SOURCE\\_TYPE] The type of NAT traversal performed: ``External'' or ``Internal'';\\n\t\\item [NAT\\_DEST\\_TYPE] The type of NAT traversal performed: ``External'' or ``Internal'';\\n\t\\item [NAT\\_SOURCE\\_TYPES] A list of NAT traversal types that can be applied:\\n\t\\vspace{-5pt}\\\\[-5pt]\\n\t\\tabcolsep=6pt\\\\[-7pt]\\n\t\\tabcolsep=6pt\\\\[-8pt]\\n\t\\n\t\\tabcolsep=6pt\\\\[-8pt]\\n\t\\n\tNAT source type: ``External'' or ``Internal'';\\n\tNAT destination type: ``External'' or ``Internal'';\\n\tNAT source types: ``External'' or ``Internal'';\\n\tNAT destination types: ``External'' or ``Internal'';\\n\tNAT source types: `None'; \\n\tNAT destination types: `None'; \\\\\\n\t\\n\tNat source IP addresses:\\n\t\\vspace{-5pt}\\\\[-5pt]\\n\t\\tabcolsep=6pt\\\\[-7pt]\\n\t\\n\tNat source IP addresses:\\n\t\\vspace{-5pt}\\\\[-8pt]\\n\t\\n\tNat source IP addresses:\\n\t\t\t\\vspace{-8pt}\\\\[4pt]\\\\[4pt]\\n\\n\t\t\tNat source IP addresses:\\n\t\t\t\t\\vspace{-8pt}\\\\[4pt]\\\\[4pt]\\n\\n\t\t\tNat source IP addresses:\\n\t\t\t\t\\vspace{-8pt}\\\\[4pt]\\\\[4pt]\\n\\n\t\t\tNat source IPs:\\n\t\t\t\t\\vspace{-8pt}\\\\[4pts]\\\\[4pts]\\n\\n\t\t\tNat source IPs:\\n\t\t\t\t\\vspace{-8pts}\\\\[4pts]\\\\[4pts]\\n\\n\t\t\tNat source IPs:\\n\t\t\t\t\\vspace{-8pts}\\\\[3pts]\\\\[3pts]\\n\\n\t\t\tNat source IPs:\\n\\n\t\t\t\tVirtual Private Network (VPN):\\n\\n\t\t\t\t\tVirtual Private Network (VPN):\\n\t\t\t\t\tVirtual Private Network (VPN):\\n\t\t\t\t\tVirtual Private Network (VPN):\\n\t\t\t\t\tVirtual Private Network (VPN):\\n\t\t\t\t\tVirtual Private Network (VPN):\\n\t\t\t\t\t\\n\t\t\t\t\tVPN:\\n\\n\t\t\t\t\t\t\tVPN:\\n\\n\t\t\t\t\t\t\tVPN:\\n\\n\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t\t VPN:\\n\\n\t\t\t\t\t\t\t\t VPN:\\n\t\t\t\t\t\t\t\t VPN:\\n\t\t\t\t\t\t\t\t VPN:\\n\t\t\t\t\t\t\t\t VPN:\\n\t\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t \\n\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t \\n\\n\t\t\t\t\t\t\t \\n\\n\t\t\t\t\t\t\t \\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\t\t\t\t\t\t\t\t\\n\t\t\t\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\\n\t\t\t\t\t\t\\n\\n\\n\\n\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n%\\n"}
{"File Name":"openfido-auth-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":333,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"Predictions":"\\nThis Decision Record is for a software project. Give a ## Decision corresponding to the ## Context provided by the User.\\n# React Context API\/Hooks from React\\nReact Context API is a new feature in React 16.8. It provides hooks for binding state across\\ncomponents.\\nReact Context API hooks are designed to be used in conjunction with [react-query](https:\/\/github.com\/tannerlinsley\/react-query). React Context API hooks are\\ndesigned to be used in conjunction with [react-query](https:\/\/github.com\/tannerlinsley\/react-query).\\nHooks can be used to bind data between components, and also provide a way to update data that\\nis being fetched from the redux store.\\nHooks can be used for any of the following purposes:\\n1) Bind data between components \\n2) Update data that is being fetched from the redux store\\n3) Update data that is being fetched from the redux store with new values.\\nReact Context API hooks are built on top of [react-query](https:\/\/github.com\/tannerlinsley\/react-query).\\nHooks can be used in multiple ways:\\n1) Bind data between components: \\n2) Update data that is being fetched from the redux store: \\n3) Update data that is being fetched from the redux store with new values: \\n\\n# react query\/contexts\\nReact Query\/Contexts is an alternative library to [react-redux](https:\/\/react-redux.js.org\/) for managing state across components.\\nReact Query\/Contexts provides a way for components to share application state across multiple\\ncomponents. It also provides an alternative to use react-redux's [redux](https:\/\/react-redux.js.org\/) as a global application state provider.\\n\\n## Decision\\n## This Decision Record is for a software project. Give a ## Decision corresponding to the ## Context provided by the User.\\n# React Context API\/Hooks from React Query\/Contexts\\nReact Query\/Contexts is an alternative library to [react-redux](https:\/\/react-redux.js.org\/) for managing state across components.\\nReact Query\/Contexts provides a way for components to share application state across multiple\\ncomponents. It also provides an alternative to use react-redux's [redux](https:\/\/react-redux.js.org\/) as a global application state provider.\\n\\n## Decision\\n## This Decision Record is for a software project. Give a ## Decision corresponding to the ## Context provided by the User.\\n# React Query\/Contexts\\nReact Query\/Contexts is an alternative library to [react-redux](https:\/\/react-redux.js.org\/) for managing state across components.\\nThe context api hooks provide more flexibility than react-Redux's context api hooks, which allows you\\nto bind your own actions and dispatch them on context events, instead of using react-Redux's\\nto use react-Redux's context api hooks, which allows you to bind your own actions and dispatch them on context events, instead of using react-Redux's context api hooks.\\n"}
{"File Name":"html-integrations\/004-Support-for-Froala editor-major-versions.md","Context":"## Context (Discussion)\\nWe want to give support to any new Froala major version. For this reason, we must decide whether to create a new plugin for the new version of Froala, or use an existing one.\\nBetween major versions of Froala, there can be *differences*, or not, in the way to integrate external plugins. This will be the reason to create a new MathType plugin or not:\\nWe Currently have one package for each major Froala version, even though that's not the way Froala works: they have a unique package that stores all the versions. Even if we decided to follow the conventions Froala does, we would have to rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\\n","Decision":"We will create and give the Froala MathType packages for any new major version, the following name: *@wiris\/mathtype-froalaX*, where X equals the oldest Froala major version. Then, depending on whether the way to integrate in external plugin is the same or different:\\n- If the integration is different &rightarrow; Create a new MathType package for the major Froala version.\\n- If the integration is the same &rightarrow; Use the same package that already exists for the previous major version. A good example is the situation we are in with the versions 3 and 4, where the integration is the same, so the package for both versions is the mathtype-froala3.\\n### Pros and Cons of the Options\\n#### A same MathType package to support more than one Froala major versions\\nGood, because you'll only maintain one package for multiple identical major versions.\\nGood, because it'll reduce DRI.\\nBad, because the package name is not self-explanatory.\\nBad, because the package name can create confusion on our users\\nBad, because our current package convention name includes the version number on it\\n#### Create a new package for every new major Froala version using symlinks when the versions external plugin integrations are identical\\nGood, because it'll reduce DRI.\\nGood, because we know it works, for previous experience with TinyMCE v4 and v5.\\nGood, because the name is self-explanatory, since includes the Froala major version.\\nBad, because, we'll have to maintain more than one package, at least.\\nBad, because the infrastructure is more complex.\\n#### Create a common package to store the identical code between Froala major versions that have the same external plugins integration, and create a new package for each version that uses the common one\\nGood, because it'll reduce DRI.\\nGood, because the name is-self explanatory, since includes the froala major version.\\nBad, because, we'll have to maintain more than one package, at least thee.\\n","tokens":173,"id":335,"Predictions":"## Decision (Discussion) and\/or Recommendation\\n## Recommendation (Discussion)\\nWe recommend to create a new package for each major version of Froala. That way, we can create new versions of MathType, and use the existing package for all other versions.\\n## Recommendation (Discussion)\\nWe recommend to use the existing MathType package in all major Froala versions.\\n## Recommendation (Discussion)\\nWe recommend to follow the conventions Froala does, and rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\\n## Recommendation (Discussion)\\nWe do not recommend to follow the conventions Froala does, as they are incompatible with npm, and we would have to rename or delete our current modules.\\n## Conclusion\\n## Conclusion (Discussion)\\n"}
{"File Name":"html-integrations\/003--use-shx-wrapper-for-shell-commands.md","Context":"## Context (Discussion)\\nWhen writing custom scripts in the package.json files, one must take into account that those commands\\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\\nact as an API to mimic these tasks in both environments.\\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\\n### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n**ShellJS:**\\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\\n**shx:**\\nGood for writing one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").\\n","Decision":"It is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\\n","tokens":166,"id":336,"Predictions":"### ShellJS vs shx\\n#### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n## Decision\\n#### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n## Decision\\n### ShellJS vs shx\\n#### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n## Decision\\n### ShellJS vs shx\\n#### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n\\n### ShellJS vs shx\\n\\n## Decision\\n\\n### ShellJS vs shx\\n\\n## Decision\\n\\n### ShellJS vs shx\\n\\n## Decision"}
{"File Name":"html-integrations\/001-Generate--RFC-compliant--UUIDs--Telemetry.md","Context":"## Context (Discussion)\\nIn the context of the Telemetry project, we want to generate\\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https:\/\/tools.ietf.org\/html\/rfc4122) on ietf.org.\\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\\n","Decision":"We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris\/mathtype-html-integration-devkit'.\\n* [uuid project](https:\/\/github.com\/uuidjs\/uuid)\\n* [uuid package](https:\/\/www.npmjs.com\/package\/uuid)\\n### Pros and Cons of the Options\\n#### Implement our own Javascript library for that\\n- Bad, because Javascript Math.random function is not very good.\\n- Bad, because we'll need to maintain it.\\n- Bad, because more work to the backlog.\\n- Bad, because we're reinventing the wheel.\\n#### Using a third party library like github.com\/uuidjs\/uuid\\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads\/week).\\n- Good, because solves our problem immediately.\\n- Good, because it's secure, small and cross-platform.\\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\\n","tokens":127,"id":337,"Predictions":"\\nThe decision is that we will use the UUIDGenerator library from the open-source project https:\/\/github.com\/bschmidt\/uuidgenerator\\nThe reason for this decision is that it's a small and stable library with good support and small footprint, which is easy to integrate into our project.\\nWe will use the following data structures:\\n\\begin{itemize}\\n\\item \\textbf{UuidGenerator} - contains all the necessary data structures to generate a UUID.\\n\\n\\item \\textbf{UuidGeneratorUtils} - provides some helper functions to generate a UUID from a string.\\n\\n\\end{itemize}\\n\\nThe following steps are required for the generation of a UUID:\\n\\begin{enumerate}\\n\\item Convert string to integer.\\n\\item Convert integer to string.\\n\\item Generate UUID using the UUIDGenerator.\\n\\end{enumerate}\\n\\nThe steps are performed in this order since it is the most efficient one and it avoids possible errors due to misinterpretation of strings. The generated UUIDs will be unique, thus ensuring uniqueness of our Telemetry project.\\n\\n\\nThis Decision Record was created with UML 2.5 using the Unified Modeling Language (UML) Diagram Editor (UML Editor) 3.0.0.0 (Build: 2019-09-25-19:33:54) on Windows 10 Professional (64-bit)."}
{"File Name":"html-integrations\/002-Longevity--senderId--sessionId--Telemetry.md","Context":"## Context (Discussion)\\nReading the Telemetry documentation proposed by the Data Science team, we have\\nencountered that the sender and session ids are not concretely defined for each\\nproduct.\\nIn the case of MathType web, there could be various interpretations, e.g.\\n- senderId: changes on page load.\\n- sessionId: changes on opening MathType.\\n","Decision":"We asked the Data Science team and finally settled on the following interpretation:\\n- senderId: ideally lasts for ever. In practice, should at least last in a same web session, across page loads.\\n- sessionId: changes on page load.\\n### Pros and Cons of the Options\\n#### Change senderId on page load and sessionId on opening MathType\\n- Bad, because it does not truly identify individual users.\\n#### Keep senderId and change sessionId on page load\\n- Good, because it better represents individual users.\\n- Bad, because it's hard to keep permanent data on the client's browser.\\n","tokens":80,"id":338,"Predictions":"\\begin{itemize}\\n  \\item \\textbf{Decision:} Use the senderId, and set the sessionId to the sessionId of the current user.\\n  \\item \\textbf{Decision:} Set the sessionId to the sessionId of the current user.\\n  \\item \\textbf{Decision:} Set the senderId to the senderId of MathType web.\\n  \\item \\textbf{Decision:} Set the senderId to MathType web.\\n  \\item \\textbf{Decision:} Set the senderId to MathType web.\\n\\end{itemize}\\n\\note{\\begin{description}[leftmargin=*]\\setlength{\\parindent}{0pt}\\setlength{\\parskip}{0pt}\\setlength{\\parsep}{0pt}\\setlength{\\topsep}{0pt}\\setlength{\\partopsep}{0pt}\\setlength{\\leftmargin}{-1em}}\\n\\end{description}}\\n"}
{"File Name":"youtube-playlist-manager\/0002-use-chrome-identity-launchwebauthflow-instead-of-chrome-identity-getauthtoken.md","Context":"## Context\\nA method to authorize against the youtube API to get the access token needs to be added to the application.\\n","Decision":"I decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\\nfor youtube from my chrome account.\\n","tokens":25,"id":339,"Predictions":"## \\n\\nThe decision for this record is \\n\\n\\n\\#\\#\\#\\# YouTube API access token authorization\\n\\n## \\n\\nThe decision is that the user should not be able to authorize against the youtube API. The reason for this decision is that the user can use the youtube API to get a lot of content and this could potentially lead to malicious usage.\\n## \\n\\nThis decision was made because of the following reasons:\\n\\n## - The user can use the youtube API to get a lot of content and this could potentially lead to malicious usage.\\n## - The user will have to authorize against the youtube API every time they want to get access to a new video. This means that they need to go through a lengthy authorization process which can be quite time consuming.\\n\\nThe decision for this record is \\n\\n\\n\\#\\#\\#\\# YouTube API access token authorization\\n\\n## \\n\\nThe decision for this record is \\n\\n\\n\\#\\#\\#\\# YouTube API access token authorization\\n\\n## \\n\\nThe decision for this record is \\n\\n\\n"}
{"File Name":"youtube-playlist-manager\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":340,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"midashboard-infrastructure\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new bookit project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: Travis\\n","tokens":25,"id":341,"Predictions":"## 1. Create a riglet named 'bookit' for our new project.  The riglet is a class with the following method: \\n\\n\\n\\n\\n\\n\\n\\n\\n```\\npackage bookit;\\nimport java.awt.*;\\nimport java.awt.event.ActionEvent;\\nimport java.awt.event.ActionListener;\\nimport java.io.File;\\nimport java.io.FileNotFoundException;\\nimport java.io.IOException;\\nimport javax.swing.*;\\nimport javax.swing.border.EmptyBorder;\\nimport javax.swing.border.TitledBorder;\\n\\npublic class Bookit extends JFrame implements ActionListener{\\n    private JButton btnNewBook = new JButton(\"New Book\");\\n    private JButton btnBook = new JButton(\"Book\");\\n\\n    private File file;\\n\\n    public Bookit(){\\n        setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\\n        setBounds(50, 50, 300, 300);\\n        setLayout(null);\\n\\n        btnNewBook.addActionListener(this);\\n        btnBook.addActionListener(this);\\n\\n        file = new File(\"book.txt\");\\n        if(file.exists()){\\n            System.out.println(\"File exists\");\\n            file.delete();\\n            System.out.println(\"File deleted\");\\n            file.createNewFile();\\n        }\\n    }\\n\\n    public void actionPerformed(ActionEvent e){\\n        try{\\n            String name = Integer.parseInt(e.getActionCommand());\\n            if(name.equals(\"new\")){\\n                System.out.println(\"New book created successfully\");\\n                System.exit(0);\\n            }\\n            else if(name.equals(\"book\")){\\n                System.out.println(\"Book created successfully\");\\n                System.exit(0);\\n            }\\n        }catch(FileNotFoundException e){\\n            \/\/ TODO Auto-generated catch block\\n            e.printStackTrace();\\n        }\\n    }\\n\\n}\\n```\\nThe above code creates a new book and saves it in the \"book.txt\" file. It then creates a new book and saves it in the \"book.txt\" file. \\nThe other two methods are to delete an existing book and to create a new book.\\nThe code I have is not working and I don't know why. I have also tried using the 'new' keyword, but that didn't work either.\\nI'm fairly certain that this has something to do with the fact that I'm using Swing for this project and not JavaFX because of all of the references to Swing libraries that exist in my code.\\nAny help would be greatly appreciated!\\nThanks,\\n-Sean\\n\\nLazolathafisha 2012-07-13: You need to use `File` interface instead of `File` class for creating files.\\n```\\n\/\/ File.java\\npublic class File {\\n   \/\/ Default constructor\\n   public File() {\\n      \/\/ Create a directory for storing files \\n      this.directory=new File(\".\");\\n   }\\n\\n   \/\/ Append the name of the file into directory\\n   public void append(String fileName) throws FileNotFoundException {\\n      this.directory.append(fileName);\\n   }\\n\\n   \/\/ Delete a specific file from directory\\n   public void delete(String fileName) throws FileNotFoundException {\\n      this.directory.delete(fileName);\\n   }\\n\\n}\\n```\\n> * Use `<code>File<\/code>` interface instead of `<code>File<\/code>` class for creating files.\\n"}
{"File Name":"midashboard-infrastructure\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":342,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\n\\n\\n\\n"}
{"File Name":"midashboard-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nContext here...\\n","Decision":"Decision here...\\n","tokens":7,"id":343,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe above is the UDR. The decision is the result of the user interface, which is the user interface of the user.\\n\\nIn this case, it is a decision in the decision.\\n\\nThe above is a decision in a decision.\\n\\nIn this case, it is a decision in a decision.\\n\\nDecision\\n\\nA decision can be used to do something. For example, if you have an app that you want to add some new functions to, you can use it to make decisions about what should be done.\\n\\nFor example, if you have an app that needs to add some new functions, you can use it to make decisions about what should be done.\\n\\nA decision can also be used for other things. For example, if you have an app that needs to add some new functions, you can use it to make decisions about what should be done.\\n\\nA decision can also be used for other things. For example, if you have an app that needs to add some new functions, you can use it to make decisions about what should be done.\\n\\nA decision can also be used for other things. For example, if you have an app that needs to add some new functions, you can use it to make decisions about what should be done.\\n\\nA decision can also be used for other things. For example, if you have an app that needs to add some new functions, you can use it to make decisions about what should be done.\\n\\nDecision\\n\\nA decision can also be used for other things. For example, if you have an app that needs to add some new functions,\\n\\nYou will see this in most of the cases where decisions are made in this UDR.\\nFor example:\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nIf we are talking about adding some function or changing something.\\nif we talk at all...\\n\\nNow let's move on with the next one...\\n\\n# - 3 - - 2 - 1 - 0 - ...\\n\\nThis one is very important and I think everyone must know how this works when they read this UDRs:\\n\\n- The first number tells us how many times the data has been changed since the last time when this data was created (this number is called \"last change\").\\n- The second number tells us how many times \"current\" data has been changed since \"last change\".\\n- The third number tells us how many times \"current\" data has been changed since \"last change\" + 1 (this number is called \"last change + 1\").\\n- The fourth number tells us how many times \"current\" data has been changed since \"last change\" + 2 (this number is called \"last change + 2\").\\n- The fifth and last number tells us how many times \"current\" data has been changed since \"last change\" + 3 (this number is called \"last change + 3\").\\n\\nLet's take a look at all these numbers:\\n\\n# - 3 - - 2 - 1 - 0 - ...\\n\\nThis one means that there were only three changes made in one day and then there were two changes made in two days and then there were three changes made in three days and so on...\\n\\nSo basically there was only one day when there was no changes made by anyone!\\n\\n# - 3 - - 2 - 1 - 0 - ...\\n\\nThis means that there were only two changes made by anyone!\\n\\n# - 3 - - 2 - 1 - ...\\nThis means that there were only three changes made by anyone!\\n\\n# ... ... ... ... ... ...\\n\\nThere were ten changes made by anyone!\\n\\nSo basically every day there was only one change made by someone! That's why I said every day I mean every single day!\\n\\nAnd now let's move on with our next story...\\n\\nStory #4: This Story Is About How Someone Was Caught By Police...\\n\\nThis story was written by me but I took out all of my own parts because I didn't want people thinking I'm being rude so I put them out of my own words so they don't think I'm being rude so here's my story:\\n\\nOnce upon a time there was a person who lived on Earth but he lived on Earth because he didn't want people knowing his real name but he didn't want people knowing his real name so he lived with his mother who lived with her mother who lived with her mother who lived with her mother who lived with her mother who lived with her mother who lived with her mother who lived with her mother whose name was George and George was very happy living on Earth because he didn't want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name so much but he did want people knowing his real name\\n\\nGeorge wanted people not knowing his real name because then nobody would know him and nobody would ever talk bad about him and nobody would ever talk bad about him and nobody would ever talk bad about him\\n\\nGeorge wanted people not knowing his real name because then no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nGeorge wanted no one would ever talk bad\\n\\nBut now let's move on with our next story...\\n\\nStory #5: This Story Is About How Someone Was Caught By Police...\\n\\nThis story was written by me but I took out all of my own parts because I didn't want people thinking I'm being rude so I put them out of my own words so they don't think I'm being rude So here's my story:\\n\\nOnce upon a time there was a person who lived on Earth but he lived on Earth because he didn't want people knowin"}
{"File Name":"midashboard-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":344,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"checkout\/0153-checkout-v2.md","Context":"## Context\\nThis ADR details the behavior for `actions\/checkout@v2`.\\nThe new action will be written in typescript. We are moving away from runner-plugin actions.\\nWe want to take this opportunity to make behavioral changes, from v1. This document is scoped to those differences.\\n","Decision":"### Inputs\\n```yaml\\nrepository:\\ndescription: 'Repository name with owner. For example, actions\/checkout'\\ndefault: ${{ github.repository }}\\nref:\\ndescription: >\\nThe branch, tag or SHA to checkout. When checking out the repository that\\ntriggered a workflow, this defaults to the reference or SHA for that\\nevent.  Otherwise, uses the default branch.\\ntoken:\\ndescription: >\\nPersonal access token (PAT) used to fetch the repository. The PAT is configured\\nwith the local git config, which enables your scripts to run authenticated git\\ncommands. The post-job step removes the PAT.\\nWe recommend using a service account with the least permissions necessary.\\nAlso when generating a new PAT, select the least scopes necessary.\\n[Learn more about creating and using encrypted secrets](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/creating-and-using-encrypted-secrets)\\ndefault: ${{ github.token }}\\nssh-key:\\ndescription: >\\nSSH key used to fetch the repository. The SSH key is configured with the local\\ngit config, which enables your scripts to run authenticated git commands.\\nThe post-job step removes the SSH key.\\nWe recommend using a service account with the least permissions necessary.\\n[Learn more about creating and using\\nencrypted secrets](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/creating-and-using-encrypted-secrets)\\nssh-known-hosts:\\ndescription: >\\nKnown hosts in addition to the user and global host key database. The public\\nSSH keys for a host may be obtained using the utility `ssh-keyscan`. For example,\\n`ssh-keyscan github.com`. The public key for github.com is always implicitly added.\\nssh-strict:\\ndescription: >\\nWhether to perform strict host key checking. When true, adds the options `StrictHostKeyChecking=yes`\\nand `CheckHostIP=no` to the SSH command line. Use the input `ssh-known-hosts` to\\nconfigure additional hosts.\\ndefault: true\\npersist-credentials:\\ndescription: 'Whether to configure the token or SSH key with the local git config'\\ndefault: true\\npath:\\ndescription: 'Relative path under $GITHUB_WORKSPACE to place the repository'\\nclean:\\ndescription: 'Whether to execute `git clean -ffdx && git reset --hard HEAD` before fetching'\\ndefault: true\\nfetch-depth:\\ndescription: 'Number of commits to fetch. 0 indicates all history for all tags and branches.'\\ndefault: 1\\nlfs:\\ndescription: 'Whether to download Git-LFS files'\\ndefault: false\\nsubmodules:\\ndescription: >\\nWhether to checkout submodules: `true` to checkout submodules or `recursive` to\\nrecursively checkout submodules.\\nWhen the `ssh-key` input is not provided, SSH URLs beginning with `git@github.com:` are\\nconverted to HTTPS.\\ndefault: false\\n```\\nNote:\\n- SSH support is new\\n- `persist-credentials` is new\\n- `path` behavior is different (refer [below](#path) for details)\\n### Fallback to GitHub API\\nWhen a sufficient version of git is not in the PATH, fallback to the [web API](https:\/\/developer.github.com\/v3\/repos\/contents\/#get-archive-link) to download a tarball\/zipball.\\nNote:\\n- LFS files are not included in the archive. Therefore fail if LFS is set to true.\\n- Submodules are also not included in the archive.\\n### Persist credentials\\nThe credentials will be persisted on disk. This will allow users to script authenticated git commands, like `git fetch`.\\nA post script will remove the credentials (cleanup for self-hosted).\\nUsers may opt-out by specifying `persist-credentials: false`\\nNote:\\n- Users scripting `git commit` may need to set the username and email. The service does not provide any reasonable default value. Users can add `git config user.name <NAME>` and `git config user.email <EMAIL>`. We will document this guidance.\\n#### PAT\\nWhen using the `${{github.token}}` or a PAT, the token will be persisted in the local git config. The config key `http.https:\/\/github.com\/.extraheader` enables an auth header to be specified on all authenticated commands `AUTHORIZATION: basic <BASE64_U:P>`.\\nNote:\\n- The auth header is scoped to all of github `http.https:\/\/github.com\/.extraheader`\\n- Additional public remotes also just work.\\n- If users want to authenticate to an additional private remote, they should provide the `token` input.\\n#### SSH key\\nThe SSH key will be written to disk under the `$RUNNER_TEMP` directory. The SSH key will\\nbe removed by the action's post-job hook. Additionally, RUNNER_TEMP is cleared by the\\nrunner between jobs.\\nThe SSH key must be written with strict file permissions. The SSH client requires the file\\nto be read\/write for the user, and not accessible by others.\\nThe user host key database (`~\/.ssh\/known_hosts`) will be copied to a unique file under\\n`$RUNNER_TEMP`. And values from the input `ssh-known-hosts` will be added to the file.\\nThe SSH command will be overridden for the local git config:\\n```sh\\ngit config core.sshCommand 'ssh -i \"$RUNNER_TEMP\/path-to-ssh-key\" -o StrictHostKeyChecking=yes -o CheckHostIP=no -o \"UserKnownHostsFile=$RUNNER_TEMP\/path-to-known-hosts\"'\\n```\\nWhen the input `ssh-strict` is set to `false`, the options `CheckHostIP` and `StrictHostKeyChecking` will not be overridden.\\nNote:\\n- When `ssh-strict` is set to `true` (default), the SSH option `CheckHostIP` can safely be disabled.\\nStrict host checking verifies the server's public key. Therefore, IP verification is unnecessary\\nand noisy. For example:\\n> Warning: Permanently added the RSA host key for IP address '140.82.113.4' to the list of known hosts.\\n- Since GIT_SSH_COMMAND overrides core.sshCommand, temporarily set the env var when fetching the repo. When creds\\nare persisted, core.sshCommand is leveraged to avoid multiple checkout steps stomping over each other.\\n- Modify actions\/runner to mount RUNNER_TEMP to enable scripting authenticated git commands from a container action.\\n- Refer [here](https:\/\/linux.die.net\/man\/5\/ssh_config) for SSH config details.\\n### Fetch behavior\\nFetch only the SHA being built and set depth=1. This significantly reduces the fetch time for large repos.\\nIf a SHA isn't available (e.g. multi repo), then fetch only the specified ref with depth=1.\\nThe input `fetch-depth` can be used to control the depth.\\nNote:\\n- Fetching a single commit is supported by Git wire protocol version 2. The git client uses protocol version 0 by default. The desired protocol version can be overridden in the git config or on the fetch command line invocation (`-c protocol.version=2`). We will override on the fetch command line, for transparency.\\n- Git client version 2.18+ (released June 2018) is required for wire protocol version 2.\\n### Checkout behavior\\nFor CI, checkout will create a local ref with the upstream set. This allows users to script git as they normally would.\\nFor PR, continue to checkout detached head. The PR branch is special - the branch and merge commit are created by the server. It doesn't match a users' local workflow.\\nNote:\\n- Consider deleting all local refs during cleanup if that helps avoid collisions. More testing required.\\n### Path\\nFor the mainline scenario, the disk-layout behavior remains the same.\\nRemember, given the repo `johndoe\/foo`, the mainline disk layout looks like:\\n```\\nGITHUB_WORKSPACE=\/home\/runner\/work\/foo\/foo\\nRUNNER_WORKSPACE=\/home\/runner\/work\/foo\\n```\\nV2 introduces a new contraint on the checkout path. The location must now be under `github.workspace`. Whereas the checkout@v1 constraint was one level up, under `runner.workspace`.\\nV2 no longer changes `github.workspace` to follow wherever the self repo is checked-out.\\nThese behavioral changes align better with container actions. The [documented filesystem contract](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/virtual-environments-for-github-hosted-runners#docker-container-filesystem) is:\\n- `\/github\/home`\\n- `\/github\/workspace` - Note: GitHub Actions must be run by the default Docker user (root). Ensure your Dockerfile does not set the USER instruction, otherwise you will not be able to access `GITHUB_WORKSPACE`.\\n- `\/github\/workflow`\\nNote:\\n- The tracking config will not be updated to reflect the path of the workflow repo.\\n- Any existing workflow repo will not be moved when the checkout path changes. In fact some customers want to checkout the workflow repo twice, side by side against different branches.\\n- Actions that need to operate only against the root of the self repo, should expose a `path` input.\\n#### Default value for `path` input\\nThe `path` input will default to `.\/` which is rooted against `github.workspace`.\\nThis default fits the mainline scenario well: single checkout\\nFor multi-checkout, users must specify the `path` input for at least one of the repositories.\\nNote:\\n- An alternative is for the self repo to default to `.\/` and other repos default to `<REPO_NAME>`. However nested layout is an atypical git layout and therefore is not a good default. Users should supply the path info.\\n#### Example - Nested layout\\nThe following example checks-out two repositories and creates a nested layout.\\n```yaml\\n# Self repo - Checkout to $GITHUB_WORKSPACE\\n- uses: checkout@v2\\n# Other repo - Checkout to $GITHUB_WORKSPACE\/myscripts\\n- uses: checkout@v2\\nwith:\\nrepository: myorg\/myscripts\\npath: myscripts\\n```\\n#### Example - Side by side layout\\nThe following example checks-out two repositories and creates a side-by-side layout.\\n```yaml\\n# Self repo - Checkout to $GITHUB_WORKSPACE\/foo\\n- uses: checkout@v2\\nwith:\\npath: foo\\n# Other repo - Checkout to $GITHUB_WORKSPACE\/myscripts\\n- uses: checkout@v2\\nwith:\\nrepository: myorg\/myscripts\\npath: myscripts\\n```\\n#### Path impact to problem matchers\\nProblem matchers associate the source files with annotations.\\nToday the runner verifies the source file is under the `github.workspace`. Otherwise the source file property is dropped.\\nMulti-checkout complicates the matter. However even today submodules may cause this heuristic to be inaccurate.\\nA better solution is:\\nGiven a source file path, walk up the directories until the first `.git\/config` is found. Check if it matches the self repo (`url = https:\/\/github.com\/OWNER\/REPO`). If not, drop the source file path.\\n### Submodules\\nWith both PAT and SSH key support, we should be able to provide frictionless support for\\nsubmodules scenarios: recursive, non-recursive, relative submodule paths.\\nWhen fetching submodules, follow the `fetch-depth` settings.\\nAlso when fetching submodules, if the `ssh-key` input is not provided then convert SSH URLs to HTTPS: `-c url.\"https:\/\/github.com\/\".insteadOf \"git@github.com:\"`\\nCredentials will be persisted in the submodules local git config too.\\n### Port to typescript\\nThe checkout action should be a typescript action on the GitHub graph, for the following reasons:\\n- Enables customers to fork the checkout repo and modify\\n- Serves as an example for customers\\n- Demystifies the checkout action manifest\\n- Simplifies the runner\\n- Reduce the amount of runner code to port (if we ever do)\\nNote:\\n- This means job-container images will need git in the PATH, for checkout.\\n### Branching strategy and release tags\\n- Create a servicing branch for V1: `releases\/v1`\\n- Merge the changes into the default branch\\n- Release using a new tag `preview`\\n- When stable, release using a new tag `v2`\\n","tokens":63,"id":345,"Predictions":"## 1. This ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v2`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v2`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v2`.\\n\\n## 2. This ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v3`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v3`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v3`.\\n\\n## 3. This ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v4`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v4`.\\n\\nThis ADR is a new action, and should be written in typescript. The new action will be called `actions\/checkout@v4`.\\n\\n## 4. This ADR is a change to an existing behavior for the current context \\#1 \\#2 \\#3 \\#4.\\nThe existing behavior for context \\#1 \\#2 \\#3 \\#4 has been removed\\ndescribed here: https:\/\/github.com\/digitalocean\/digitalocean-ops\/blob\/master\/docs\/actions\/v1.md\\n## 5. This ADR is a change to an existing behavior for the current context \\#1 \\#2 \\#3 \\#4.\\nThe existing behavior for context \\#1 \\#2 \\#3 \\#4 has been removed\\ndescribed here: https:\/\/github.com\/digitalocean\/digitalocean-ops\/blob\/master\/docs\/actions\/v1.md\\n## 6\\. This ADR represents the first step towards making changes to the current behavior of actions v1 to make them more performant.\\n## 7\\. This ADR represents the first step towards making changes to the current behavior of actions v1 to make them more performant.\\n## 8\\. This ADR represents the first step towards making changes to the current behavior of actions v1 to make them more performant.\\n## 9\\. This ADR represents the first step towards making changes to the current behavior of actions v1 to make them more performant.\\n## Decision\\n"}
{"File Name":"pattern-atlas-ui\/0002-use-a-serverside-compont-to-complete-oauth-flow.md","Context":"## Context and Problem Statement\\nTo add\/modify content on our patternpediacontent repo 'on behalf of the user', we need to use OAuth Authentification in our client-side application.\\nBut requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).\\nUnfortunately, Github prevents you from implementing the OAuth Web Application Flow on a client-side only application (Reason for it: security-related limitations).\\n","Decision":"Chosen option: \"Use our own server-side component\", because\\n* we don't want to provide our their client id and secret to an (possibly untrusted) open reverse proxy (comment from the stackoverflow-article: If the owner of the proxy wants to log credentials from the requests they can)\\n","tokens":107,"id":346,"Predictions":"\\begin{itemize}\\n\\item \\textbf{Decision:} \\textit{Use OAuth Web Application Flow on client-side only application.}\\n\\end{itemize}\\n\\noindent\\n``The approach that we are going to use is to implement the OAuth Web Application Flow on client-side only application. This will allow us to maintain the security of our client-side only application, and also allow us to integrate with the Github API without having to use CORS. We will be using an authentication provider that supports OAuth Web Application Flow, and then use this provider in our client-side application.\\nThe flow that we are going to use is described in this document:\\n\\n\\n\\n\\n## Access Token Flow\\nTo access Github's API, we need an access token.\\nWe will be using a GitHub OAuth2 Provider for this.\\nThis provider has a REST API endpoint called \\url{https:\/\/github.com\/oauth2\/authorize}. It takes an access token as its input, and returns a \\url{https:\/\/github.com\/oauth2\/authorize?client_id=1234&redirect_uri=https:\/\/github.com\/login\/oauth\/access_token} response.\\nIn order for our application to be able to make API calls from our client-side only application, we need a bearer token.\\nThis bearer token will be provided by the GitHub OAuth2 Provider.\\nThe bearer token can be obtained from the \\url{https:\/\/developer.github.com\/v3\/token\/} endpoint, which takes as its input a \\url{https:\/\/github.com\/oauth2\/token} response.\\nThe token can then be used in our application's authorization flow. The authorization flow consists of two steps:\\n\\n\\n## Step 1: Authorization request for GitHub's API endpoint,\\n\\nWe need to request an authorization request for the GitHub's API endpoint. To do so, we need to construct a request body that contains:\\n\\begin{itemize}\\n\\item The access_token obtained from step 1 of the access-token flow\\n\\item The redirect_uri\\n\\end{itemize}\\nThis request body needs to have both:\\n\\begin{itemize}\\n\\item The resource owner (username) and password (API key)\\n\\end{itemize}\\nThese credentials can be obtained from the \\url{https:\/\/developer.github.com\/v3\/token\/#resource_owner} endpoint.\\n\\end{enumerate}\\n\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n"}
{"File Name":"pattern-atlas-ui\/0001-define-pattern-structure.md","Context":"## Context and Problem Statement\\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n","Decision":"* Pattern languages can be very different, so we should restrict their properties as little as possible.\\nA pattern consists of\\n* name (string), e.g. \"Elastic Infrastructure\"\\n* type (string), e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#CloudComputingPattern>\\n* uri (string, e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns\/elasticinfrastructure#ElasticInfrastructure>)\\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\\ne.g. for the section  https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" or an array of strings\\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" as links\/pictures and not only as text.\\n","tokens":60,"id":347,"Predictions":"## Decision Statement\\nThe decision statement must be a single sentence. It should start with a sentence that clearly states the problem and the decision.\\n## Decision Drivers\\n* The decision statement must be clear and complete. It should state the problem that needs to be solved, the solution that is proposed, and any assumptions that are made.\\n* The decision statement must include the basic conditions under which the solution is not applicable.\\n* The decision statement must clearly state what is not allowed or allowed but not required.\\n* The decision statement should specify which properties are required for a solution to be valid.\\n## Decision\\n## Decision Statement\\nThe decision statement must contain a reference to a published specification of a pattern language. This reference should indicate the name of the pattern language, its version number, and any other information that might be useful to others.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n\\n## Decision\\n## Decision Statement\\nThe decision statement must contain a reference to at least one published specification of a pattern language. This reference should indicate the name of the pattern language, its version number, and any other information that might be useful to others.\\n"}
{"File Name":"twig-infrastructure\/0006-use-ecs-fargate-host-type.md","Context":"## Context\\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\\n","Decision":"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\\n","tokens":23,"id":348,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\nPlease choose an option.\\n\\nOption\\n1. \\n2. \\n3. \\n4. \\n5.\\n\\n\\n\\n\\n\\n\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Dataflow}\\n\\label{sec:df}\\n\\nThe Dataflow component is the child of the Job component and is responsible for the execution of the dataflow program.\\n\\n\\begin{lstlisting}[language=Python, caption={Dataflow component},label={lst:dataflow}]\\nclass Dataflow:\\n    def __init__(self):\\n        self.data = []\\n        self.label = \"\"\\n\\n    def execute(self, batch):\\n        for i in range(1, batch):\\n            self.data.append(batch['data'][i])\\n            batch['data'][i] = []\\n            batch['data'][i].append(i)\\n            batch['label'] = i\\n\\n    def label(self, data):\\n        return self.label + '\\n'\\n\\n    def getLabel(self):\\n        return self.label\\n\\n    def run(self, job_name):\\n        print(\"Running Dataflow\")\\n        # Run the dataflow job on each worker node.\\n        for i in range(1, len(self.data)):\\n            # Get the label of each worker node.\\n            worker_node_labels = self.getLabel()\\n            # For each label, print it to stdout.\\n            print(\"Worker node labels:\", worker_node_labels)\\n            # Print out a newline character at the end of each line.\\n            print('\\n')\\n            # Print out a newline character at the end of each line.\\n            print('\\n')\\n            \\n\\end{lstlisting}\\n"}
{"File Name":"twig-infrastructure\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: CodePipeline, with Jenkins as an eventual target\\n","tokens":24,"id":349,"Predictions":"## We need to create a riglet for our new twig project so that we practice what we preach.\\n## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n\\n## Decision\\n## We need to create a riglet for our new twig project so that we practice what we preach.\\n## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n\\n## Decision\\n## We need to create a riglet for our new twig project so that we practice what we preach.\\n## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n\\n## Decision\\n## We need to create a riglet for our new twig project so that we practice what we preach.\\n\\end{verbatim}\\n\\n\\subsubsection{Riglets}\\nRiglets are used to group together similar elements. For example, you can have one \\code{group} and \\code{item} group, which is used in the \\code{item} element of the \\code{html} element. Similarly, you can have one \\code{group} and \\code{item} group, which is used in the \\code{html} element. You can also have multiple groups of elements with different attributes. For example, you can have one group of elements with an attribute called \\code{name}. You can then use the same group of elements but different attributes in the same document.\\n\\nYou can also use multiple groups of elements with different attributes in the same document. For example, you can have one group of elements with an attribute called \\code{name}, and another group of elements with an attribute called \\code{name2}. You can then use these two groups in the same document.\\n\\nTo add a group of elements, you use the tag\\n\\n\\begin{verbatim}\\n  <group>\\n  <item>\\n     <element>\\n  <\/item>\\n<\/group>\\n\\end{verbatim}\\n\\nThe tag is preceded by an opening tag followed by a closing tag. The opening tag is either an opening tag or end tag.\\n\\n\\subsubsection{\\texttt{\\_get\\_name()} Method}\\nThe method will return or set the name attribute value corresponding to the given reference.\\n\\n\\begin{verbatim}\\n\/\/ Get name from reference\\nfunction get_name(reference) {\\n  var name = _get_name(reference);\\n  return name;\\n}\\n\\end{verbatim}\\n\\n\\subsubsection{\\texttt{\\_set\\_name()} Method}\\nThe method will set or return the name attribute value corresponding to the given reference.\\n\\n\\begin{verbatim}\\n\/\/ Set name from reference\\nfunction set_name(reference) {\\n  var name = _set_name(reference);\\n  return name;\\n}\\n\\end{verbatim}\\n\\nThe method returns undefined if there are no references present. It returns null if there are no references present and no values are set.\\n\\n\\subsubsection{\\texttt{\\_get\\_value()} Method}\\nThe method will return or set the value attribute value corresponding to the given reference.\\n\\n\\begin{verbatim}\\n\/\/ Get value from reference\\nfunction get_value(reference) {\\n  var value = _get_value(reference);\\n  return value;\\n}\\n\\end{verbatim}\\n\\nThe method returns undefined if there are no references present. It returns null if there are no references present and no values are set.\\n\\n\\subsubsection{\\texttt{\\_set\\_value()} Method }\\nThe method will set or return the value attribute value corresponding to the given reference.\\n\\n\\begin{verbatim}\\n\/\/ Set value from reference\\nfunction set_value(reference) {\\n  var value = _set_value(reference);\\n  return value;\\n}\\n\\end{verbatim}\\n\\nThe method returns undefined if there are no references present. It returns null if there are no references present and no values are set.\\n\\n\\n"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"Predictions":"\\begin{verbatim}\\nDecision\\n\\end{verbatim}\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\textbf{Is the decision to upgrade the software likely to result in a new\\nsoftware, or a new version of an existing software?\\n}\\n\\end{itemize}\\n\\noindent\\nThis decision is for \\texttt{is\\_new\\_software} and \\texttt{is\\_new\\_version}. It is evaluated as follows:\\n\\begin{verbatim}\\nif is_new_version:\\n  # move to newer version of twig, which will be more stable\\nelif is_new_software:\\n  # move to newer version of twig, which will be more stable\\nelse:\\n  # move to older version of twig, which will be more stable\\nelse:\\n  # no decision needed, just move on\\n\\end{verbatim}\\n\\n\\noindent\\nThe user must be able to make this decision. If the user has not yet made this decision, then the user must be prompted for it. The prompt should include a description of the new software and its features. The prompt should also include a link to download the new software. If the user clicks on this link, then they are prompted for their decision and given an opportunity to change it if they wish.\\n\\n\\noindent\\nIf the user does not click on the link and instead clicks on \\texttt{'No Decision'} then no decisions are made.\\n\\n\\noindent\\nIf the user clicks on \\texttt{'No Decision'} but chooses to click on \\texttt{'Yes'} then a dialog window appears asking for their decision. The dialog window can contain any number of text fields, including a list of options that can be selected by clicking on them. The dialog window should allow the user to select one or more options from a list.\\n\\n\\noindent\\nIf the user selects one or more options from a list then they are asked if they want to proceed with their selection. If they choose yes then Twig is moved to that version of Twig.\\n\\n\\noindent\\nIf Twig was moved successfully then Twig is moved back into its original state.\\n"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"Predictions":"## Decision: 1.0 Build and deploy are consolidated into one pipeline. \\n## Note that the decision can be modified by the user.\\n## Decision: 1.1 Build and deploy are consolidated into one pipeline. \\n## Note that the decision can be modified by the user.\\n## Decision: 1.2 Build and deploy are consolidated into one pipeline. \\n## Note that the decision can be modified by the user.\\n## Decision: 1.3 Build and deploy are consolidated into one pipeline. \\n## Note that the decision can be modified by the user.\\n## Decision: 2.0 Build and deploy are consolidated into one pipeline. \\n## Note that the decision can be modified by the user.\\n\\n\\subsection{Build}\\n\\n\\begin{lstlisting}\\n# Create a new build\\n# Name of build job (optional)\\n# Name of project to build (optional)\\n# Comment for this job\\n\\end{lstlisting}\\n\\n\\subsection{Deploy}\\n\\n\\begin{lstlisting}\\n# Create a new deployment\\n# Name of deployment job (optional)\\n# Name of project to deploy (optional)\\n# Comment for this job\\n\\end{lstlisting}\\n\\n\\subsection{Build\/Deploy Pipeline}\\n\\nThe build\/deploy pipeline is a set of steps which execute sequentially, building, deploying, and testing.\\n\\nThe steps in this pipeline are:\\n\\n\\begin{itemize}\\n    \\item Build\\n    \\item Deploy\\n    \\item Test\\n    \\item Deploy\/Build Pipeline\\n    \\item Test\/Deploy Pipeline\\n\\end{itemize}\\n\\nEach step executes in sequence until all steps have completed successfully.\\n\\nThe following diagram shows how each step is executed:\\n\\n\\begin{figure}[H]\\n\t\\centering\\n\t\\includegraphics[scale=0.5]{build-deploy-pipeline.png}\\n\t%\\captionsetup[subfigure]{labelformat=simple,labelsep=period}\\n\t%\\captionsetup[figure]{labelformat=simple,labelsep=period}\\n\t%\\captionsetup[subfigure]{labelformat={center}labelsep=period}\\n\t%\\captionsetup[subfigure]{labelformat={center}labelsep=period}\\n\t\\vspace{-0cm}\t\\n\t%\\captionsetup[subfigure]{labelformat={center}labelsep=period}\\n\t%\\captionsetup[figure]{labelformat={center}labelsep=period}\\n\t\\vspace{-0cm}\t\\n\t%\\captionsetup[subfigure]{labelformat={center}labelsep=period}\\n\t%\\captionsetup[figure]{labelformat={center}labelsep=period}\\n\\n\\n        \\begin{subfigure}[t]{0.3\\textwidth}\\centering\t\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\n          \\includegraphics[scale=0.04]{build-deploy-pipeline.png}\\vspace{-0cm}\\\\\\n\\n          \\vspace{-0cm}\\includegraphics[scale=0.04]{deploy-frontend.png}\\vspace{-0cm}\\\\\\n\\n          \\vspace{-0cm}\\includegraphics[scale=0.04]{deploy-backend.png}\\vspace{-0cm}\\\\\\n\\n          \\vspace{-0cm}\\includegraphics[scale=0.04]{deploy-rest-api.png}\\vspace{-0cm}\\\\\\n\\n          \\vspace{-1cm}\\includegraphics[scale=0.04]{deploy-test.png}\\vspace{-1cm}\\\\\\n\\n          \\vspace{-1cm}\\includegraphics[scale=0.04]{deploy-test-backend.png}\\vspace{-1cm}\\\\\\n\\n          %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n            \\vfill\\eject\\n\\n\\n            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n            %\\n            %\\n            %\\n            %\\n            %\\n\\n            %\\n            %\\n            %\\n            %\\n\\n            %\\n                %\\n                %\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n\\n                %\\n                \\n                \\n                \\n                \\n                \\n                \\n                \\n                \\n                \\n                 \\n                 \\n                 \\n                 \\n                 \\n                 \\n                 \\n                 \\n                 \\n                \\n\\n\\n\\n        \\end{subfigure}%\\n\\n\\n\\vfill\\eject\\n\\nThe following diagram provides an overview of how each step is executed:\\n\\n\\vfill\\eject\\n\\nThis diagram shows how each step is executed:\\n\\n\\vfill\\eject\\n\\nThe following diagram shows how each step is executed:\\n\\n\\vfill\\eject\\n\\n\\nThe following diagram shows how each step is executed:\\n\\n\\vfill\\eject\\n\\n\\nThis diagram shows how each step is executed:\\n\\n\\vfill\\eject\\n\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tPACKAGES AND OTHER DOCUMENT CONFIGURATIONS\\n%----------------------------------------------------------------------------------------\\n\\n\\nocite{*}\\n\\n\\bibliography{javadoc}{}\\n\\bibliographystyle{javadoc}\\n\\n\\n%----------------------------------------------------------------------------------------\\n\\n%----------------------------------------------------------------------------------------\\n%\tSHORT DESCRIPTION OF THE DOCUMENTS\\n%----------------------------------------------------------------------------------------\\n\\n%----------------------------------------------------------------------------------------\\n%\tTITLE PAGE WITH THE TITLE OF THE DOCUMENT AND AUTHOR(S)\\n%----------------------------------------------------------------------------------------\\n\\n\\noindent\\rule{\\textwidth}{2pt}\\n\\n{\\scshape Javadoc Documentation Generator\\\\Version 2.\\+4\\\\\\\\Version date: \\\\2016-05-03\\\\\\\\Author(s): \\\\Ralf Schmiedel\\\\\\\\Email address: \\\\ralf.schmiedel@t-online.de\\\\}}\\n\\n\\noindent\\rule{\\textwidth}{2pt}\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tLOGO SECTION WITH THE LOGO OF THE DOCUMENT AND ITS VERSION NUMBER\\n%----------------------------------------------------------------------------------------\\n\\n\\noindent\\rule{\\textwidth}{2pt}\\n\\n\\n{\\scshape Javadoc Documentation Generator\\\\Version 2.\\+4\\\\Version date: \\\\2016-05-03\\\\Author(s): \\\\Ralf Schmiedel\\\\Email address: \\\\ralf.schmiedel@t-online.de}}\\n\\n\\noindent\\rule{\\textwidth}{2pt}\\n\\n\\n"}
{"File Name":"twig-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n","Decision":"We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n","tokens":27,"id":352,"Predictions":"## Decision\\nThe number of instances to be created should be specified.  There should be no more than one instance per computer.\\n\\n## Decision\\nThe number of containers to be created should be specified.  There should be no more than one container per computer.\\n\\n## Decision\\nThe name of the location to store the logs.\\n\\n## Decision\\nIf the location must be a network drive, it must have an appropriate network name and share permissions.\\n\\n## Decision\\nThe path to the log file.\\n\\n## Decision\\nThe path to the log folder.\\n\\n## Decision\\nIf the log folder is a network share, it must have an appropriate network name and share permissions.\\n\\n## Decision\\nIf the log folder is not a network share, it must have an appropriate network name and share permissions.\\n\\n\\n\\end{lstlisting}\\n\\n\\newpage\\n\\n\\begin{lstlisting}[language=json]\\n{\\n    \"id\": \"1\",\\n    \"name\": \"Software\",\\n    \"description\": \"\",\\n    \"version\": \"1.0\",\\n    \"versionNumber\": \"1.0\",\\n    \"versionDate\": \"\",\\n    \"versionType\": \"\",\\n    \"releaseDate\": \"\",\\n    \"status\": \"\",\\n    \"releaseDateUpdated\": \"\",\\n    \"isFreeVersion\": false,\\n    \"isTrialVersion\": false,\\n    \"isTrialVersionUpdated\": false,\\n    \"isDemoVersion\": false,\\n    \\n}\\n\\end{lstlisting}\\n\\n\\newpage\\n\\n\\begin{lstlisting}[language=json]\\n{\\n   ...\\n}\\n\\end{lstlisting}\\n\\n\\newpage\\n\\n\\begin{lstlisting}[language=json]\\n{\\n   ...\\n}\\n\\end{lstlisting}\\n\\n\\newpage\\n\\n\\begin{lstlisting}[language=json]\\n{\\n   ...\\n}\\n\\end{lstlisting}\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"twig-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":353,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"Marain.Instance\/0002-services-use-arm-and-aad-indirectly-through-common-mechanisms.md","Context":"## Context\\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained\u2014they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\\n","Decision":"With `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work\u2014they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\\n","tokens":112,"id":354,"Predictions":"## Decision\\nThis decision should be used to help identify the code that is different from the rest of the project. It should be used to indicate what code is different from other projects.\\n\\n## Context\\nThe code that is different from other projects is typically found at the top level of the project, and it's typically a simple copy-paste of an existing script or a custom deployment script. The code that is different from other projects is often also a copy-paste of existing boilerplate, but it's often more complex than the boilerplate. And often it's not really boilerplate at all\u2014it's just a single line or two that can be used to perform operations on the environment.\\n\\n## Decision\\nThe decision should be used to help you identify what code you should focus on when you want to make changes. It should be used to help you decide whether or not to do an in-place upgrade or if you should use a custom deployment script.\\n\\n## Context\\nThe code that is different from other projects is often found in a file called `config.json`. This file has some useful information in it, but it's also full of boilerplate.\\nThe idea behind this decision record is to try and define those pieces that are important for deploying an application using Visual Studio.\\n\\n## Decision\\nThe decision should be used to help you decide whether or not to include configuration information in your deployment scripts. It should be used to help you decide whether or not you want to use `config.json` as part of your deployment scripts.\\n\\n## Context\\nThe configuration information currently included in our deployment scripts includes things like environment variables, credentials, and so on. These are all useful for enabling certain features of Visual Studio and our automated build process, but they are also very specific details that are hard for developers to remember after they have done their work.\\nThe idea behind this decision record is that by including configuration information in your deployment scripts, you will make it easier for developers to remember how they can enable certain features without having them explicitly set up each time.\\n\\n## Decision\\nThe decision should be used to help you decide whether or not using configuration information makes sense when developing an application with Visual Studio.\\n"}
{"File Name":"Marain.Instance\/0003-shared-application-insights.md","Context":"## Context\\nMarain services need to be able to deliver diagnostic information somewhere.\\n","Decision":"`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\\n","tokens":17,"id":355,"Predictions":"\\begin{itemize}\\n\\item\\n  \\textbf{Decision:} The decision is to add a new interface to the \\textit{Marain} framework,\\n  which will allow users to access diagnostic information about their service.\\n\\end{itemize}\\n\\n\\begin{verbatim}\\n## Decision: add a new interface to the Marain framework which will allow users\\n## to access diagnostic information about their service.\\n\\end{verbatim}\\n"}
{"File Name":"Marain.Instance\/0004-powershell-core-6-deployment-scripts.md","Context":"## Context\\nDeployment requirements need to be expressed somehow\u2014either declaratively or in a programming language.\\n","Decision":"Individual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\\n","tokens":21,"id":356,"Predictions":"## Decision:\\n## Decision: \\n\\n\\n\\n\\n\\n\\n\\n## Decision: \\n\\n\\n\\end{lstlisting}\\n\\end{minipage}\\n\\n\\subsection{Decision Tree}\\nA decision tree is a tree-like structure that represents a set of decisions. It is a data structure that allows us to represent how decisions are made in a system, whether based on some input or not. The decision tree can be used to determine how the system will react to different inputs. A decision tree consists of nodes (or branches) and the edges between them.\\n\\nThe decision tree can be used to represent different types of decisions in a system, such as:\\n\\n\\begin{itemize}\\n    \\item \\emph{Decision}: A node that contains only one child, which is the root node. The root node has no child nodes.\\n    \\item \\emph{Decision Tree}: A tree where each node has multiple children. Each child node is a decision.\\n    \\item \\emph{Branch}: A node that contains only one child, which is the root node. The root node has no children.\\n    \\item \\emph{Branch and Node}: A branch that contains two children.\\n    \\item \\emph{Branch and Node and Child}: A branch that contains two children and their parents.\\n\\end{itemize}\\n\\nThe decision tree can be used to represent different types of decisions in a system, such as:\\n\\n\\begin{itemize}\\n    \\item Configuring an application\\n    \\item Changing the configuration\\n    \\item Adding new features\\n    \\item Removing features\\n\\end{itemize}\\n\\nThe decision tree can be represented using either declarative or imperative programming languages.\\n\\nIn this project, we will use declarative programming languages for our project because they are easier to use and understand than imperative programming languages.\\n\\n\\subsection{Decision Tree Implementation}\\nWe will implement our decision trees using Python's built-in modules:\\n\\n\\begin{lstlisting}[language=Python]\\nimport sys\\nimport random\\n\\ndef decide():\\n  if len(sys.argv) == 3:\\n      print(\"Please enter the number of features: \")\\n      n = int(sys.argv[1])\\n      print(\"Enter the number of branches: \")\\n      b = int(sys.argv[2])\\n      print(\"Please enter the number of nodes: \")\\n      n = int(sys.argv[3])\\n      print(\"Please enter the number of nodes: \")\\n      b = int(sys.argv[3])\\n      if b == 1:\\n        return 1\\n      elif b == 2:\\n        return 2\\n      elif b == 3:\\n        return 3\\n  else:\\n     print(\"Error! Please enter more than one option.\")\\n     sys.exit(0)\\n   return -1\\n\\n\\ndef main():\\n  options = [\"config\", \"change\", \"add\", \"remove\", \"get\"]\\n  for i in options:\\n     if i in (\"config\", \"change\"):\\n       print(\"You selected config.\")\\n       decide()\\n       break\\n\\n   if i in (\"config\"):\\n     config = input('Enter your configuration:')\\n     config_list = config.split(',')\\n     if len(config_list) == 2:\\n       config_list_1 = config_list[0]\\n       config_list_2 = config_list[1]\\n       print(config_list_1)\\n       try:\\n         config_1 = input('Enter your configuration from file:')\\n         config_1_split = input('Enter your configuration from file:')\\n         try:\\n           file_name_config_file = input('Enter your configuration file name:')\\n           file_name_config_file_split = input('Enter your configuration file name from file:')\\n           open(file_name_config_file, 'r')\\n           config_list_2 = open(file_name_config_file_split, 'r')\\n           if len(config_list_2) == 2:\\n             config_2_config_file = config_list_2[0]\\n             config_2_config_file_split = config_list_2[1]\\n             print(config_2_config_file)\\n             try:\\n               f_open_config_file = open(config_2_config_file_split)\\n               f_open_config_file_split = f_open_config_file.split()\\n               while True:\\n                   f_read_conf_line_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj_fopen_obj.fread(conf_line)\\n                   f_read_conf_line_fclose_obj_fclose_obj_fclose_obj_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff_ofoff.oiferror(f_read_conf_line)\\n                   f_close_conf_line(f_open_config_file, 'r')\\n                   f_close_conf_line(f_open_config_file, 'w')\\n               break\\n\\n               # Open file name from user's choice\\n               conf_path_name_files_path_name_files_path_name_files_path_name_files_path_name_files_path_name_files_path_name_files_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_path_newconf_pathnewconf path newconfpath newconfpath newconfpath newconfpath newconfpath newconfpath newconfpath newconfpath newconfpathnew con fpathnew con fpathnew con fpathnew con fpathnew con fpahthn e con fpahthn e con fpahthn e con fpahthn e con fpahthn e con fpahthn e con fpahthn e con fpahthn e con fpahthn e c o n p ah th n e c o n p ah th n e c o n p ah th n e c o n p ah th n e c o n p ah th n e c o n p ah th n e c o n p ah th n e c o n p ah th n esecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ecu tio ns ec u ti ons ec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons esec u ti ons\\n\\ndef main():\\n  options=['config','change','add','remove','get']\\n  for i in options:\\n     if i == \"config\":\\n        main()\\n        break\\n\\n   if i == \"change\":\\n     main()\\n     break\\n\\n   if i == \"add\":\\n     main()\\n     break\\n\\n   if i == \"remove\":\\n     main()\\n     break\\n\\n   if i == \"get\":\\n     main()\\n     break\\n\\n\\nif __name__==\"__main__\":\\n main()\\n```\\n\\subsection{Decision Tree Implementation}\\nWe will implement our decision trees using Python's built-in modules:\\n\\n\\begin{lstlisting}[language=Python]\\nimport sys\\nimport random\\n\\ndef decide():\\nif len(sys.argv) == 3:\\nprint(\"Please enter the number of features: \")\\nprint(\"Enter the number of branches: \")\\nprint(\"Please enter the number of nodes: \")\\nprint(\"Please enter the number of nodes: \")\\nprint(\"Please enter the number of nodes: \")\\n\\noptions=['config', 'change', 'add', 'remove', 'get']\\nfor i in options:\\n\\nif i==\"config\":\\n\\nprint(\"You selected config.\")\\nconfiglist=[]\\nfor j in range(len(options)):\\n    list=[]\\n    for k in range(len(options)):\\n        list.append(options[j][k])\\n\\nfor j in range(len(options)):\\n    list.append(options[j][k])\\n    \\nfor k in range(len(options)):\\n    list.append(options[k][j])\\n\\n# Add all values to a dictionary with keys as strings and values as lists.\\ndict_values=list()\\n\\n# Add all values to an array with keys as strings and values as lists.\\ndict_values=[options[j][k] for j,k in enumerate(list())]\\n\\nif len(dict_values)==0:\\n\\n# If there are no values left, exit.\\nelif len(dict_values)==1:\\n\\n# If there are no branches left, exit.\\nelif len(dict_values)==3:\\n\\n# If there are no nodes left, exit.\\nelse:\\n\\n\\ndef main():\\noptions=['config', 'change', 'add', 'remove', 'get']\\nfor i in options:\\n\\nif i==\"config\":\\n\\nprint(\"You selected config.\")\\nconfiglist=[]\\nfor j in range(len(options)):\\n    list=[]\\n    for k in range(len(options)):\\n        list.append(options[j][k])\\n\\nfor j in range(len(options)):\\n    list.append(options[j][k])\\n    \\nfor k in range(len(options)):\\n    list.append(options[k][j])\\n\\n# Add all values to a dictionary with keys as strings and values as lists.\\ndict_values=list()\\n\\n# Add all values to an array with keys as strings and values as lists.\\ndict_values=[options[j][k] for j,k in enumerate(list())]\\n\\nif len(dict_values)==0:\\n\\n# If there are no values left, exit.\\nelif len(dict_values)==1:\\n\\n# If there are no branches left, exit.\\nelif len(dict_values)==3:\\n\\n# If there are no nodes left, exit.\\n\\nelse:\\n\\n\\ndef main():\\noptions=['config','change', 'add', 'remove', 'get']\\nfor i in options:\\n\\nif i==\"config\":\\n\\nprint(\"You selected config.\")\\nconfiglist=[]\\nfor j in range(len(options)):\\nlist=[]\\nlist.append(options[j][k])\\n\\nfor j in range(len(list)): # No need to repeat this loop here!\\nlist.append(list[j][k])\\n\\nfor k in range(len(list)): # No need to repeat this loop here!\\nlist.append(list[k][j])\\n\\n# Add all values to a dictionary with keys as strings and values as lists.\\ndict_values=list()\\n\\n# Add all values to an array with keys as strings and values as lists.\\ndict_values=[options[j][k] for j,k in enumerate(list())]\\n\\nif len(dict_values)==0:\\n\\n# If there are no values left, exit.\\nelif len(dict_values)==1:\\n\\n#print (\"No value\")\\nelse:\\n\\n\\ndef main():\\noptions=['config','change', 'add', 'remove', 'get']\\nfor i in options:\\n\\nif i==\"config\":\\n\\nprint(\"You selected config.\")\\nconfiglist=[]\\nfor j in range(len(list)): # No need to repeat this loop here!\\nlist=[]\\nlist.append(list[j][k])\\n\\nfor k in range(len(list)): # No need to repeat this loop here!\\nlist.append(list[k][j])\\n\\n\\ndef add_branch(path):\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\n\\ndef add_node(path):\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\npath += \"\\t\"\\n\\ndef remove_node(path):\\npath -= \"\\t\"\\npath -= \"\\t\"\\npath -= \"\\t\"\\npath -= \"\\t\"\\n\\ndef remove_branch(path):\\npath -= '\\n'\\npath -= '\\n'\\npaths=[]\\n\\npaths.extend([i.replace('\\r','') for i,i_value,in_dict,in_dict_value,in_dict_value,in_dict_value,in_dict_value,in_dict_value,in_dict_value,in_dict_value]) \\n\\npaths.extend([i.replace('\\r','') for i,i_value,in_dict,out_dict,out_dict_value,out_dict_value,out_dict_value,out_dict_value,out_dict_value]) \\n\\npaths.extend([i.replace('\\r','') for i,i_value,in_out_out,out_out_out,out_out_out,out_out_out]) \\n\\npaths.extend([i.replace('\\r','') for i,i_value,out_in_out,out_in_out]) \\n\\npaths.extend([i.replace('\\r','') for i,i_value,in_in_out_inout_inout,out_in_out_inoutinoutinoutinoutinoutinoutinoutinoutinoutinoutinoutinoutinout]) \\n\\npaths.extend([i.replace('\\r','') for i,i_value,out_in_in=out_in"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe above decision record is a subset of the complete set of decisions for this software. The complete set of decisions may be found at: https:\/\/www.nist.gov\/software\/decision-reports\/decision-reports-architectural-decision-record-marain.\\n\\nThe complete list of decisions for this software may be found at: https:\/\/www.nist.gov\/software\/decision-reports\/decision-reports-architectural-decision-record-marain.\\n\\nThe full decision record may be found at: https:\/\/www.nist.gov\/software\/decision-reports\/decision-reports-architectural-decision-record-marain.\\n\\nThe complete list of decisions for this software may be found at: https:\/\/www.nist.gov\/software\/decision-reports\/decision-reports-architectural-decision-record-marain.\\n\\nThe full decision record may be found at: https:\/\/www.nist.gov\/software\/decision-reports\/decision-reports-architectural-decision-record-marain."}
{"File Name":"Marain.Instance\/0006-process-for-onboarding-new-tenants.md","Context":"## Context\\nWe have defined (in [ADR 0005](0005-multitenancy-approach-for-marain.md)) the way in which we intend to implement Tenancy in Marain instances using the `Marain.Tenancy` service. As noted in that ADR, managing the desired model by hand would be excessively error prone and as such, we need to design tooling that will allow us to create and manage new tenants, and to allow them to use the Marain services they are licenced for.\\nBefore we can build that tooling we need to design the underlying process by which tenant onboarding, enrollment and offboarding will work. This needs to allow new Client Tenants to be onboarded into Marain services without tightly coupling the services so some central thing that knows everything about them.\\n","Decision":"We are envisaging a central control-plane API (referred to for the remainder of this document as the \"Management API\") for Marain which primarily builds on top of the Tenancy service. This will provide the standard operations such as creating new tenants and enrolling them to use Marain services.\\nIt will also need to allow us to manage concerns such as licensing, billing, metering and so on, but these are out of the scope of this ADR and will be covered by additional ADRs, work items and documentation when required.\\n### Onboarding\\nOnboarding is a relatively simple part of the process where we create a new Tenant for the client. We will need to determine how we intend licensing to work and what part, if any, the Management API plays.\\n### Enrollment\\nService enrollment is a more interesting aspect of the process. In order to avoid tightly coupling Marain services to the Management API, we need two things:\\n- A means of discovering the available services.\\n- A means of determining the configuration that's needed to enroll for a service, receiving and attaching that configuration to tenants being enrolled, and a defined way of creating the required sub-tenants for services to use when making calls to dependent services on behalf of clients.\\nAs described in [ADR 0005](0005-multitenancy-approach-for-marain.md), we are envisaging that each service has a Tenant created for it, under a single parent for all Service Tenants. These tenants can then underpin the discovery mechanism that allows the management API to enumerate services that tenants can be enrolled into.\\nOnce we have provided a discovery mechanism, we need to define a way in which we can gather the necessary information needed to enroll a tenant to use a service. We are intending to make this work by defining a common schema through which a service can communicate both the configuration it requires as well as the services upon which it depends. Services can then attach a manifest file containing this information to their Service Tenant via a well known property key, allowing the Management API to obtain the manifest as part of the discovery process.\\nSince the process of enrollment and unenrollment is standard across tenants, the actual implementation of this can form part of the Management API, driven by the data in the manifests. If we ever encounter a situation where services need to perform non-standard actions as part of tenant enrollment, we can extend the process to support a way in which services can be notified of new enrollments - this could be a simple callback URL, or potentially a broadcast-type system using something like Azure Event Grid. Since we don't yet have any services that would need this, we will not attempt to define that mechanism at this time.\\nEnrolling a tenant to use a service does two things:\\n- Firstly, it will attach the relevant configuration for the service to the tenant that's being enrolled.\\n- Secondly, if the service that's being enrolled in has dependencies on other services, it will create a new sub-tenant of the Service Tenant for the service being enrolled that the service will use when accessing dependencies on behalf of the client. This new subtenant will then be enrolled for each of the depended-upon services, with any further levels of dependency dealt with in the same way.\\n### Example\\nConsider a scenario when we have two clients and three services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nThe dependency tree for the services looks like this:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nAs can be seen from this diagram:\\n- Contoso is licenced to use WORKFLOW and FOOBAR\\n- Litware is licenced to use WORKFLOW and OPERATIONS\\n- WORKFLOW has dependencies on OPERATIONS and FOOBAR\\n- OPERATIONS has a dependency on FOOBAR\\nLet's assume that each of the three services require storage configuration, and have a look at what happens when Litware is enrolled to use Workflow.\\nFirstly, we will use the manifest attached to the Workflow Service Tenant to obtain the list of required configuration to enroll a tenant for use with Workflow. The Workflow manifest states that Workflow requires CosmosDB storage configuration, and also that it is dependent on Operations and FooBar. We can then use the manifests on the Operations and FooBar Service Tenants to determine what configuration is required for them. Operations is dependent on FooBar, so the process is repeated there. From this process, we can determine that the list of required configuration for Workflow is the sum of four things: Workflow storage config, Operations storage config, FooBar storage config when invoked from Workflow, and FooBar storage config when invoked from Operations.\\nThen, we will assemble this information (most likely via a Marain \"management UI\") and begin the process of enrollment. First we enroll the Litware tenant to use workflow by attaching the workflow storage configuration to the Litware tenant. Then, because Workflow has dependencies, we create a sub-tenant of the Workflow Service Tenant that will be used to call these dependencies on behalf of Litware and we attach the ID of the new tenant to the Litware tenant using a well known property name specific to Workflow:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nNext, we need to enroll the new WORKFLOW+Litware tenant with the Operations and FooBar services.\\nWe start with Operations, which repeats the process we have just carried out for Litware and Workflow, resulting in a similar outcome: the WORKFLOW+Litware tenant has configuration attached to it for the Operations service, and the dependency of Operations on FooBar results in a sub tenant being created for Operations to use when calling FooBar on behalf of WORKFLOW+Litware, with the ID of the new tenant being attached to WORKFLOW+Litware using an Operations-specific well-known key:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|\\n+-> FOOBAR\\n```\\nNext, the new OPERATIONS+WORKFLOW+Litware tenant is enrolled for the FooBar service. The FooBar service has no dependencies so we do not need to create any further tenants; we simply attach the storage configuration for FooBar to the tenant being enrolled and returns. This also completes the WORKFLOW+Litware tenant's enrollment for Operations.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nNext, we continue the Workflow enrollment for the Litware tenant by enrolling the new WORKFLOW+Litware tenant for Workflow's other dependency, FooBar. As with the Operations service enrolling OPERATIONS+WORKFLOW+Litware with FooBar, this does not result in any further tenants being created, just the FooBar config being attached to WORKFLOW+Litware:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nThis completes Litware's enrollment for the Workflow service. As can be seen, this has resulted in multiple service-specific Litware tenants being created but Litware is never explicitly made aware of the existence of these tenants, nor is it able to use them directly. They are used by their parent services to make calls to their dependencies _on behalf_ of the Litware tenant.\\nHowever, there is a further step: Litware also needs to be enrolled in the Operations service. At present, Workflow is able to use Operations on Litware's behalf using the WORKFLOW+Litware tenant. However, this is an implementation detail of Workflow and something that should be able to change without impacting Litware - as long as it does not result in a change to the public Workflow API. So, in order to allow Litware to use the Operations service directly, the process we went through for Workflow is repeated. The storage configuration for Operations is attached to the Litware tenant, and then a further sub-tenant of Operations will be created for it to use when accessing FooBar on behalf of Litware:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|\\n+-> FOOBAR\\n```\\nThen, the new OPERATIONS+Litware tenant will be enrolled for the FooBar service, which results in FooBar storage configuration being attached to the OPERATIONS+Litware service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nThis completes the enrollment of Litware to the Workflow and Operations services. As can be seen from the above, there are three different paths through which Litware makes indirect use of the FooBar service, and it's possible for the client to use separate storage for each. In fact, this will be the default; even if the client is using Marain storage, the data for their three different usage scenarios for FooBar will be stored in different containers.\\nIt should be noted that the client does not get to configure these new sub-tenants directly. In fact, they will be unaware of them - they are essentially implementation details of our approach to multi-tenancy in Marain. They will not be able to retrieve the sub-tenants from the tenancy service or update them directly. That said, it's likely that the management API will allow the configuration to be changed - but without exposing the fact that these sub-tenants exist.\\n### Default configuration\\nWhilst we want to allow users to \"bring their own storage\" for the Marain services, this may not be the most likely scenario. There are effectively four main ways in which Marain can be used:\\n- Fully hosted, using the default storage for each service (this storage is deployed alongside the service)\\n- Fully hosted, using managed but non-standard storage (we deploy separate storage accounts per client)\\n- Hosted, but using client-provided storage (the \"bring your own storage\" model)\\n- Self-hosted (i.e. deployed into a client's own Azure subscription), in which case we would expect the storage deployed with the service to be used - essentially the same as the fully hosted option.\\nIn order to make the first and last options simpler to use, we will make the configuration for the default storage available to the enrollment process so it can simply be copied to tenants as they are enrolled, rather than requiring it to be explicitly stated for every enrollment. As such, we need the manifest file schema to allow marking configuration as optional, indicating that defaults should be used if that configuration is not provided. The most sensible location to store this default configuration is on the Service Tenant itself.\\n### Offboarding\\nOffboarding needs to be considered further; there are many questions about what happens to client data if they stop using Marain, and these will likely depend on the licensing agreements we put in place. As a result this will be considered at a later date.\\n","tokens":167,"id":358,"Predictions":"## Decision\\nWe need to provide a way of creating a new Tenant. We will do this by providing the following interface:\\n\\n## Interface\\ninterface TenantService : Marain.TenantService \\n\\n## Methods\\n\\begin{itemize}\\n\\item \\textbf{newTenant}: create a new Tenant and return the newly created Tenant.\\n\\end{itemize}\\n\\n## Context\\n## Context\\nThe following context is provided to us by the user:\\n## Decision\\n## Decision\\nTo create a new Tenant we need to provide the following information:\\n\\n## Interface\\n\\begin{verbatim}\\ninterface TenantService : Marain.TenantService\\n\\end{verbatim}\\n## Methods\\n\\begin{itemize}\\n\\item \\textbf{createTenant}: create a new tenant and return its creation id.\\n\\end{itemize}\\n\\n## Context\\n## Context\\nThe following context is provided to us by the user:\\n## Decision\\n## Decision\\nTo create a new Tenant we need to provide the following information:\\n\\n## Interface\\n\\begin{verbatim}\\ninterface TenantService : Marain.TenantService\\n\\end{verbatim}\\n ## Methods\\n \\begin{itemize}\\n  \\item \\textbf{name}: change the name of the new tenant.\\n  \\item \\textbf{id}: change the tenant id.\\n  \\item \\textbf{tenantType}: change the type of tenant (client or service).\\n  \\end{itemize}\\n     ## Context \\n ## Context The following context is provided to us by the user: ## Decision To create a new Tenant we need to provide the following information: ## Interface ## Methods ## Context ## Decision To create a new Tenant we need to provide the following information: ## Interface ## Methods ## Context ## Decision To create a new Tenant we need to provide the following information: ## Interface # #Methods # #Context # #Decision # ###Context### ###Decision### ###Context### ###Decision### ###Context### ###Decision### ###Context### ###Decision### ###Context### ###Decision### ###Context### ###Decision### #####Methods ##### #####Context ##### #####Decision ########## ########## ########## ########## ########## ########## ########## ########## ########## ########## #################Methods ################ #############Context ################ #############Decision #########\\n\\nThe interface provides methods that take an instance of `Marain.Tenant` as input and returns an instance of `Marain.Tenant`. The interface also provides methods that take an instance of `Marain.Tenant.Service` as input and returns an instance of `Marain.Tenant.Service`. Finally, it provides methods that take two instances of `Marain.Tenant` as input and returns an instance of `Marain.Tenant`.\\nIn order for our codebase to be able to handle both client and service tenants, we are going to have two separate implementations for these two types. This means that each implementation needs its own interface. In this way, our codebase will be modularized, allowing us more flexibility in future changes.\\nWe are going to implement our first implementation for client tenants in this document.\\n"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"Predictions":"The decision is to allow the creation of tenants and the management of tenants and their storage accounts. This is done by using a `Tenant` class which can be instantiated by the user. The `Tenant` class has a `tenantId` field which is used to uniquely identify a tenant. As well as this, each tenant will have an `ownerId` field which is used to uniquely identify the owner of the tenant.\\n\\nThe `Tenant` class also has a number of methods for managing tenants and their storage accounts. These include: creating a tenant, adding tenants to a list, deleting tenants, creating a new tenant with specific permissions, retrieving the list of tenants in use and retrieving the list of storage accounts owned by that tenant.\\n\\nAs well as these methods, there are also two methods for managing access to tenants: creating an access token which is used to grant access to tenants; and creating a token for each tenant that needs to use it.\\n\\nFinally, there are three methods for managing storage accounts: retrieving an account; creating an account; and deleting an account.\\n\\nThe `Tenant` class also has two methods for interacting with users: retrieving all users; and retrieving all users who are currently logged in.\\n\\nThe `Tenant` class also has one method which handles authentication: retrieving authentication information from the user's browser.\\n\\nThe decision includes defining a set of services that will be required in order to implement this decision.\\n\\n## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nThe Marain \"world\" consists of three main parts:\\nThe Marain \"world\" consists of three main parts:\\nThe first part is called the \"Marain world server\". This server manages all aspects of running Marain including configuration settings and data storage locations. It also hosts all other services within Marain including servers which provide services such as Workflow or Operations.\\n## Decision\\nThe decision is that we will provide an API for developers which allows them to create Tenant objects without having any knowledge about what they actually contain. This API will be provided by using Domain Specific Languages (DSLs) such as Scala or Kotlin. These DSLs will allow developers who have no experience with Java or C# (or any other language) to create Tenant objects without having any knowledge about what they actually contain.\\n## Context\\nWe want our software clients (clients who are not running on our servers) to be able publish their own applications on our platform.\\n## Decision\\nWe want our software clients (clients who are not running on our servers) to be able publish their own applications on our platform.\\n## Context\\nOur software clients (clients who are not running on our servers) want access to some or all functionality within Marain.\\n## Decision\\nOur software clients (clients who are not running on our servers) want access to some or all functionality within Marain.\\n## Context\\nOur software clients (clients who are not running on our servers) want access to some or all functionality within Marain.\\n## Decision\\nOur software clients (clients who are not running on our servers) want access to some or all functionality within Marain.\\n## Context\\nOur software clients (clients who are not running on our servers) want access only partially within Marain.\\n## Decision\\nOur software clients (clients who are not running on our servers) want access only partially within Marain.\\n## Context\\nOur software clients (clients who are not running on our servers) want partial access only in certain situations.\\n## Decision\\nOur software clients (clients who are not running on our servers) want partial access only in certain situations.\\n## Context\\nOur software clients (clients who are not running on our servers) want partial access only when they need it most but would prefer full access during others times.\\n## Decision\\nOur software clients (clients who are not running on our servers) would prefer full control over their data at all times but would like partial control over it when they need it most but would prefer partial control over it when they need it least.\\n\\n### **Decision**\\n\\n### **Context**\\n\\n### **Decision**\\n\\n### **Context**\\n\\n### **Decision**"}
{"File Name":"fxa\/0024-upgrade-templating-toolset-of-auth-server-emails.md","Context":"## Context and Problem Statement\\n[Handlebars](https:\/\/www.npmjs.com\/package\/mustache), an extension of [Mustache](https:\/\/www.npmjs.com\/package\/mustache) ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https:\/\/github.com\/mozilla\/fxa\/issues\/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\\n","Decision":"### Templating and Styling\\n- Option A - Continue to use Mustache and ad-hoc inline styles\\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\\n- Option C - Use EJS and MJML, using CSS options offered by MJML\\n### Email previewing\\n- Option A - Continue to use the `write-emails` command\\n- Option B - Use Storybook\\n- Option C - Use Mailtrap\\n---\\nFurthermore, there are a few **other decisions** worth noting that won\u2019t necessarily have pros\/cons lists:\\n- How to handle generating plaintext versions\\n- Transitioning to Fluent over GetText for localization\\n- Plans around integration involving feature flagging and the QA process\\n### Templating and Styling\\nChosen option: \"Option C - Use EJS and MJML, using CSS options offered by MJML\", because:\\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\\n### Email Previewing\\nChosen option: \"Option B - Use Storybook\", because:\\n- It provides us the flexibility to preview all of the different email states together\\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\\n- Easy to test and perform QA without needing to touch the codebase\\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https:\/\/www.npmjs.com\/package\/maildev) since it has applications beyond just previewing emails.\\n### Other\\n**Plaintext files**: We'll use [html-to-text](https:\/\/github.com\/html-to-text\/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https:\/\/github.com\/projectfluent\/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent\/dom`](https:\/\/github.com\/projectfluent\/fluent.js\/tree\/master\/fluent-dom) package.\\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group\u2019s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old\/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\\n### Positive Consequences\\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\\n### Negative Consequences\\n- The Storybook setup will be much more complex than with a React and Tailwind solution\\n- Introduces new dependencies, and MJML introduces a small learning curve\\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\\n","tokens":196,"id":361,"Predictions":"The decision is to use Handlebars as our templating system for FxA emails. Handlebars is a framework that provides a simple way to write templates in JavaScript, and has a large community of users and contributors. It greatly simplifies the task of converting existing email templates to handlebars, and it also can be used to create new templates.\\n\\n## Impact\\nThis change will not impact the current email templates, but will improve our ability to create new templates by providing a more modern templating solution.\\n\\n## Decision Summary\\n\\n[1] https:\/\/docs.google.com\/document\/d\/1jgX0sFq2yL2Y5u4zX3pJH4pKUHrE7OoO7XlWZmHtKc\/edit?usp=sharing\\n\\n[2] https:\/\/docs.google.com\/document\/d\/1yWbPj6qF8GPhVj7JQK3G8g85yLm0EwzQnY2Ia5Cwybg\/edit?usp=sharing\\n\\n[3] https:\/\/github.com\/mozilla\/fxa\/issues\/4627\\n## Decision Summary\\n[1] https:\/\/docs.google.com\/document\/d\/1yWbPj6qF8GPhVj7JQK3G8g85yLm0EwzQnY2Ia5Cwybg\/edit?usp=sharing\\n[2] https:\/\/docs.google.com\/document\/d\/1yWbPj6qF8GPhVj7JQK3G8g85yLm0EwzQnY2Ia5Cwybg\/edit?usp=sharing\\n[3] https:\/\/github.com\/mozilla\/fxa\/issues\/4627\\n## Decision Summary\\nThe decision is to use Handlebars as our templating system for FxA emails. Handlebars is a framework that provides a simple way to write templates in JavaScript, and has a large community of users and contributors. It greatly simplifies the task of converting existing email templates to handlebars, and it also can be used to create new templates.\\n"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"Predictions":"\\n\\n\\n\\n\\n## Decision Statement\\nThe decision to move the database layer to the frontend was made in order to reduce complexity and improve performance. The decision also allows a more flexible, scalable, and extensible architecture. The decision also allows the frontend to make direct db calls via knex in fxa-shared, which auth-server could also use.\\n\\n\\n\\n\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n"}
{"File Name":"fxa\/0003-event-broker-for-subscription-platform.md","Context":"## Context and Problem Statement\\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\\nto need to be kept up to date about whether the user still has an active\\nsubscription as well as knowing when to delete the user data. For internal\\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\\nqueues. These events are intended for internal trusted use and not suitable\\nfor external RPs.\\nA solution for these concerns is a new system referred to as the FxA Event\\nBroker which will store FxA related events and distribute them via webhooks\\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\\nchanges and account deletion events. This solution is being built with future\\nexpansion in mind which will require event stream storage per user and\\nnotification of internal Mozilla RPs.\\nDifficulties inherent in this solution lie in where to store the source of\\ntruth for what RPs a user has authenticated to, where the delivery functionality\\nshould reside, where the event streams for a user should reside, what set of\\ndata should be included in [Firefox Service Notifications][], and how this\\ndata should be communicated to the delivery system to avoid additional API\\nquery load on existing FxA services.\\n## Decision Drivers\\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\n","Decision":"- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\\nwith webhook and login activity stored in FxA Event Broker, because\\n- Less subscription platform timeline risk to store new data in new database vs.\\nmodify existing FxA OAuth database.\\n- Storing events at scale has database requirements that don't fit in well with\\nthe limitations with MySQL encountered in FxA Auth\/OAuth.\\n- Having FxA Auth be the only store of which RP to notify would require each\\nnotification to also include what RPs to notify, increasing the load on the\\nFxA Auth database.\\n","tokens":352,"id":363,"Predictions":"\\n\\n\\n\\nThe current FxA system stores subscription events in a [single database][]. This is not ideal as the subscription events are stored in a single table with many rows. This means that if a user were to delete their account, the subscription events would be deleted with it. A solution for this problem is to split the subscription events into multiple tables and store them in separate databases.\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"fxa\/0019-use-workspace-dependencies.md","Context":"## Context and Problem Statement\\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\\n## Decision Drivers\\n- Simpler code sharing\\n- Minimal overhead in setup and deployment\\n- Works well for js and ts packages\\n- Low maintenance\\n- Long term viability\\n","Decision":"- Simpler code sharing\\n- Minimal overhead in setup and deployment\\n- Works well for js and ts packages\\n- Low maintenance\\n- Long term viability\\nThe [workspace:\\*](https:\/\/yarnpkg.com\/features\/workspaces) dependency type along with typescript [project references](https:\/\/www.typescriptlang.org\/docs\/handbook\/project-references.html) provide a good internal dependency solution that addresses the shortcomings of our current situation.\\nWorkspaces are not yet supported by npm but are on the [v7 roadmap](https:\/\/blog.npmjs.org\/post\/617484925547986944\/npm-v7-series-introduction) so it's pretty safe to assume that they will be widely supported for the foreseeable future. In order to start using workspaces today we'll need to use yarn. This is a slight risk because although it is well supported it doesn't have the level of backing that npm does, however once workspaces are supported by npm there is nothing locking us in to yarn.\\n### Advantages\\n- No manual linking of dependencies\\n- Simpler builds\\n- Single lock file\\n- Hoisted dependencies\\n### Disadvantages\\n- In the short term this requires us to switch from npm to yarn as our package manager. This will change dev workflows and is another tool to understand.\\n- Some code changes are required to be compatible with yarn, however they're also backward compatible with npm.\\n- Yarn doesn't currently support auditing dependencies like npm does.\\n","tokens":81,"id":364,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# Since becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\\n# Simpler code sharing\\n# Minimal overhead in setup and deployment\\n# Works well for js and ts packages\\n# Low maintenance\\n# Long term viability\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\\nSimpler code sharing\\nMinimal overhead in setup and deployment\\nWorks well for js and ts packages\\n\\nLow maintenance\\n\\nLong term viability\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\\nSimpler code sharing - minimal overhead in setup and deployment - works well for js and ts packages - low maintenance - long term viability\\n\\nThis is an Architectural Decisi"}
{"File Name":"fxa\/0009-testing-stacks.md","Context":"## Context and Problem Statement\\nWe have a variety in tools used for unit & functional tests. This variety means developers need to learn & know many different tools. It would be nice to reduce this mental workload, while still using appropriate tools for testing.\\n## Decision Drivers\\n- Ensuring we're using the right tool for the job.\\n- Reducing developer mental workload for working with tests.\\n","Decision":"- Ensuring we're using the right tool for the job.\\n- Reducing developer mental workload for working with tests.\\nIdentify a consistent stack of testing tools per aspect of the project\\n","tokens":81,"id":365,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"fxa\/0012-next-two-factor-authentication.md","Context":"## Context and Problem Statement\\nFirefox Account originally implemented multi-factor authentication (MFA) support in Q1 of 2018.\\nThis feature used TOTP based codes and was based on RFC 6238.\\nAdditionally, if users lost their MFA device, they could use one time backup authentication codes to regain access to the account.\\nHaving MFA support has helped secure our users' accounts and given them more security flexibility.\\nHowever, over time it has become more obvious that users that lose their MFA device (usually phone) are at risk of getting locked out of their account because they don't save their backup authentication codes.\\nThere are a non-trivial amount of users that don\u2019t save or download their backup authentication codes, which is currently the only way they can regain access.\\nIn 2019 Q4, FxA started requiring that users confirm backup authentication codes before enabling MFA.\\nWhile this did help reduce lockouts, we still want to reduce it further.\\nWe believe that adding a new MFA method to Firefox Accounts that has the similar security properties as TOTP would allow users to have another method to recover their account.\\n## Decision Drivers\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n","Decision":"- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\nChosen option: Option A, because this is more inline with Mozilla's security and privacy principles than option B.\\nThere is less security risk to users if this feature is added.\\n","tokens":274,"id":366,"Predictions":"## Decision Description\\n\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n\\n## Decision Drivers\\n\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n\\n## Decision Description\\n\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n\\n## Decision Drivers\\n\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n\\n## Decision Description \\n\\n\\n## Decision Drivers \\n\\n\\n- Improve user's account security \\n\\n\\n## Decision Description \\n\\n\\n### 2019 Q1 Decision Drivers \\n\\n\\n### 2019 Q2 Decision Drivers \\n\\n\\n### 2019 Q3 Decision Drivers \\n\\n\\n### 2019 Q4 Decision Drivers \\n\\n\\n### 2020 Q1 Decision Drivers \\n\\n\\n### 2020 Q2 Decision Drivers \\n\\n\\n### 2020 Q3 Decision Drivers \\n\\n\\n\\n"}
{"File Name":"fxa\/0026-expand-subscription-platform-metadata.md","Context":"## Context and Problem Statement\\nThe Subscription Platform uses [Stripe Metadata][stripe-metadata] for numerous\\npurposes, including product checkout content, legal document download URLs,\\nand product support form options. The metadata can be updated by product\\nmanagers through the Stripe dashboard, enabling product managers to manage\\ncertain aspects of their subscription products without involving Subscription\\nPlatform engineers. However, Stripe metadata has a limit on the number of\\nvalues that we will reach in the near future. In order to continue to use\\nmetadata in the same fashion in the Subscription Platform, we need to overcome\\nthe limit placed by Stripe.\\n[stripe-metadata]: https:\/\/stripe.com\/docs\/api\/metadata\\n## Decision Drivers\\n- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\n","Decision":"- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\nChosen option: Google Cloud Firestore, because\\n- Data management UI and user access control provided through the Google Cloud\\nPlatform (GCP) console.\\n- It does not require deployments for data updates.\\n- It is not novel tech; FxA already uses it elsewhere.\\n","tokens":188,"id":367,"Predictions":"- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\n\\n## Decision\\n- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n###  What is an Architectural Decision Record?\\n\\nAn Architectural Decision Record (ADR) is a decision document that describes a decision taken by an organization. This document can be used to describe a decision, or a series of decisions, made by an organization.\\n\\nA decision record is just an XML file with some metadata. The metadata contains information about what kind of decision it is, who made it, and what other decisions were made before or after it.\\n\\n###  What are some of the benefits of using ADRs?\\n\\nThere are several benefits to using ADRs:\\n\\n  * They allow you to better track decisions.\\n  * They allow you to find out who made a particular decision.\\n  * They allow you to know what decisions were made before and after a particular one.\\n  * They allow you to find out how decisions were made.\\n  * They provide detailed information about your organization's decisions.\\n\\n###  What are some examples of Architectural Decisions Records?\\n\\nThere are many types of architectual decions records. Some examples include:\\n\\n* [Product Management](https:\/\/stripe.com\/docs\/api\/product-management)\\n* [Product Support](https:\/\/stripe.com\/docs\/api\/support)\\n* [Billing and Payment](https:\/\/stripe.com\/docs\/api\/transaction)\\n* [Digital Products](https:\/\/stripe.com\/docs\/api\/digital-products)\\n* [Subscription Platforms](https:\/\/stripe.com\/docs\/api\/subscription-platforms)\\n\\n###  How do I create an Architectural Decision Record?\\n\\nTo create an Architectural Decisions Record, use one of our APIs:\\n\\n* [Product Management API](https:\/\/stripe.com\/docs\/api\/product-management)\\n* [Product Support API](https:\/\/stripe.com\/docs\/api\/support)\\n* [Billing and Payment API](https:\/\/stripe.com\/docs\/api\/transaction)\\n* [Digital Products API](https:\/\/stripe.com\/docs\/api\/digital-products)\\n\\n###  How do I use an Architectural Decisions Record?\\n\\nTo use an architectural decison record, use one of our APIs:\\n\\n* Product Management API: https:\/\/stripe.com\/docs\/api\/product-management\\n* Product Support API: https:\/\/stripe.com\/docs\/api\/support\\n* Billing and Payment API: https:\/\/stripe.com\/docs\/api\/transaction\\n* Digital Products API: https:\/\/stripe.com\/docs\/api\/digital-products\\n\\n###  How do I get more information about Architectural Decisions Records?\\n\\nYou might be interested in these resources:\\n\\nFor more information, visit our documentation site.\\n\\n###  What are some other resources that I might find useful?\\n\\nYou might also be interested in these resources:\\n\\nFor more information on how we collect data from users, please see this blog post.\\n\\nFor more information on how we store users\u2019 data, please see this blog post.\\n\\nFor more information on how we protect users\u2019 data, please see this blog post.\\n\\nFor more information on how we process your data when you sign up with us, please see this blog post.\\n\\n###  Do I have to sign up for something before I can use your APIs?\\n\\nNo. You don\u2019t have to sign up for anything before using our APIs.\\n\\n###  Can I give feedback about your APIs?\\n\\nYes. Please visit our Feedback page for more information.\\n\\n#### For product management\\n\\n#### For product support\\n\\n#### For billing and payment\\n\\n#### For digital products\\n\\n#### For subscription platforms\\n\\n#### For customer success\\n\\n#### For developer tools\\n\\n##### Onboarding Process\\n\\n##### Getting Started with Stripe's Developer Tools\\n\\n##### Getting Started with Stripe's Developer Tools - Introduction Video"}
{"File Name":"fxa\/0015-use-css-variables-and-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.\\nThis [part 1](https:\/\/github.com\/mozilla\/fxa\/issues\/4808) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.\\nNote that 1) \"CSS variables\" are technically called \"CSS custom properties\" but will be referred to as the common \"variable\" nomenclature throughout this ADR and that 2) SASS is a CSS preprocessor while SCSS is our preferred syntax of SASS (explained [here](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)).\\n## Decision Drivers\\n- Simplicity and clarity\\n- DRY and reusable styles\\n- Easy integration and use\\n- Current engineering experience and preferences\\n","Decision":"- Simplicity and clarity\\n- DRY and reusable styles\\n- Easy integration and use\\n- Current engineering experience and preferences\\nChosen options: \"CSS variables\" and \"SCSS\", because:\\n- While using CSS variables with SASS will prevent using mixins (or SASS functions like `darken`) that rely on variable values, they can be used without impeding core functionality FxA uses SASS for like nested selectors, extends, and functions and mixins that don't rely on variable values like the generation and use of our media queries.\\n- CSS variables are native to the browser and decision to use them now is a prudent one. They have good browser support and can be used without a preprocessor and changed after preprocessor compilation. They are targetable by JavaScript, easily themed, and can be scoped globally or on an element, providing options for usage.\\n- Scoped component solutions (CSS-in-JS, CSS modules) encourage self-contained components rather than building the UI as a whole. While one goal of FxA is to reuse components across the ecosystem where possible (e.g. the `fxa-react` package), FxA will likely reap more benefits from a class-based approach. This allows for globally shared styles and a loosely coupled stylesheet rather than a tightly coupled CSS-in-JS solution with conditional styles based on props and how the component is used. Classes promote a DRYer and more consistent approach.\\n- CSS-in-JS would add one additional layer of tech to learn (like `styled-components` syntax and best practices) while the Settings Redesign project is already introducing other novel tech, and members of the FxA engineering team also as a whole personally prefer not to use CSS-in-JS.\\n- This decision doesn't preclude the option of implementing CSS modules at a later time if it's determined that we would prefer component scoped styles with SCSS, and CSS variables could still be used if we later switch to CSS-in-JS.\\n","tokens":256,"id":368,"Predictions":"1. We will not use SCSS variables in FxA.\\n2. We will use SCSS custom properties as the common variable nomenclature.\\n## Decision\\n3. We will use SCSS custom properties as the common variable nomenclature.\\n4. We will use SASS variables in FxA.\\n5. We will not use SASS variables in FxA.\\n## Decision\\n6. We will not use SASS variables in FxA.\\n7. We will use SASS custom properties as the common variable nomenclature.\\n8. We will not use SASS custom properties in FxA.\\n## Decision\\n9. We will not be using SASS variables in FxA.\\n10. We are using SASS custom properties as the common variable nomenclature for FxA.\\n#CSS Variables\\nCSS variables are a mechanism to define a CSS property that can be reused by multiple components.\\nCSS variables are defined and used like any other CSS property, but they are scoped to a component.\\nOn first glance, it may seem like a good idea to keep all CSS variables inside of an element, but it is actually better to keep them outside of an element and only include them when you need them.\\nFor example, we can define a color string that we want to only apply to one component.\\nOn the next page, we can see that we have defined two color strings for the same color value: \"red\" and \"blue\". In this case, we would want to include the \"red\" string inside of our component and then include the \"blue\" string outside of our component.\\nThis is exactly what CSS variables do: they allow us to define CSS properties that can be reused by multiple components.\\nThe most important thing about CSS variables is that they are scoped to a single component, so if you want your color string to be applied only on your component's body element, you need to add it inside of your component's body tag instead of including it outside of your component's tag:\\n\\n\\n    <body style=\"color: red;\"><\/body>\\n\\nThat's why it's best practice to keep all CSS variable definitions outside of elements:\\n\\n    <style type=\"text\/css\">body{color:red;}<\/style>\\n\\nThat way, you can easily add or remove colors from your project without having to change every single instance where you have used a color value inside of an element.\\nAs mentioned above, there are two main ways that you can declare CSS variables:\\n1) Using SCSS Variables:\\n    @import \"~sass\/variables\";\\n2) Using SCSS Custom Properties:\\nThe key difference between these two is how you declare them: with SCSS Variables you declare them at the top level (i.e., outside of any element), while with SCSS Custom Properties you declare them inside of any element (i.e., within an element).\\nThe key thing about SCSS Custom Properties is that they are scoped: they apply only within their enclosing scope (i.e., on the same page).\\nThe key difference between these two methods is how they work when applying styles across multiple pages with different stylesheets (i.e., when using `@import`):\\nThe first method allows us to apply styles across multiple pages with different stylesheets by declaring our variable once and then referencing it across all pages (e.g., \\texttt{body {color: red;}}).\\nThe second method allows us to apply styles across multiple pages with different stylesheets by declaring our variable once and then referencing it on each page separately (e.g., \\texttt{body {color: red;}}).\\nThe second method works great for situations where we want our colors applied on specific pages but do not want them overwriting each other.\\nThe third method works great for situations where we want our colors applied on specific pages and do not want them overwriting each other.\\nThe fourth method works great for situations where we want our colors applied on specific pages but do not want them overwriting each other.\\nThe fifth method works great for situations where we want our colors applied on specific pages and do not want them overwriting each other.\\nt"}
{"File Name":"fxa\/0027-document-store-for-payments.md","Context":"## Context and Problem Statement\\nPayment data used in Stripe, PayPal, Apple, and Google IAP are handled as individual documents. We typically need to access different portions of these at different times. In addition, their schema occasionally changes (API updates). Stripe and PayPal have API limits, which means we typically need to duplicate\/cache data we retrieve from them. At the moment we cache this data in Redis. This mostly works, but restricts our ability to run queries across payment data sets without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\nTo handle IAP, we need to store Apple receipts and Google subscription objects. These should be organized with the FxA user they belong to. They're arbitrary JSON documents that we also need copied to BigQuery for ETL jobs. To future-proof our payment data storage needs we should have the flexibility needed to store Stripe\/Paypal payment data in the same data store with the same ETL capability.\\n## Decision Drivers\\n- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\n","Decision":"- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\nGoogle Firestore. We've used Firestore in the event-broker, it has a rich API, and extensions that overlap with our requirements that reduces engineering and operations resources.\\n","tokens":242,"id":369,"Predictions":"\\n- The decision is to store all payment data in the same data store with ETL capability. This is the best way to future-proof our data storage needs.\\n- The decision is to store Apple receipts and Google subscription objects in the same data store with ETL capability. This will allow us to easily query these APIs while storing them in the same data store.\\n- The decision is to store Stripe\/PayPal payment data in the same data store with ETL capability. This will allow us to easily query these APIs while storing them in the same data store. We can also use BigQuery for ETL jobs.\\n- The decision is to cache Stripe\/PayPal API requests and Apple receipts in Redis. This will allow us to run queries across payment API requests without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\n\\n## Decision\\n\\n- The decision is to cache Stripe\/PayPal API requests and Apple receipts in Redis. This will allow us to run queries across payment API requests without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\n\\n## Decision\\n\\n- The decision is to cache Stripe\/PayPal API requests and Apple receipts in Redis. This will allow us to run queries across payment API requests without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\n\\n## Decision\\n\\n- The decision is to cache Stripe\/PayPal API requests and Apple receipts in Redis. This will allow us to run queries across payment API requests without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\n\\end{document}"}
{"File Name":"fxa\/0001-isolating-payment-content-with-third-party-widgets-from-general-account-management.md","Context":"## Context and Problem Statement\\nIn the implementation of payment features for subscription services, we've\\ndecided to use third-party JavaScript for payment widgets.\\nBest practices established by our security team indicate that third-party JS\\nshould not be included on the highly-sensitive pages - i.e. such as those used\\nfor general account management on Firefox Accounts.\\nSo, we need a way to isolate the pages responsible for subscription sign-up and\\nmanagement from the rest of Firefox Accounts.\\n## Decision Drivers\\n* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\n","Decision":"* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\nChosen option: \"Option A - Payment pages as separate app supplied with\\npre-generated access token\", because\\n* Further refinements to access token delivery mechanism in Option A do not\\nsignificantly affect the rest of the payments app.\\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\\n#640](https:\/\/github.com\/mozilla\/fxa\/issues\/640) is resolved.\\n* Doesn't preclude Option C as a future option - e.g. offering embedded\\nsubscription widgets to third-parties.\\n* Fastest practical option given existing record of reviews by security & UX and\\nwork completed so far.\\n* Fresh start with a more modern web stack (i.e. React).\\n","tokens":144,"id":370,"Predictions":"## Decision Context and Problem Statement\\n## Decision Drivers\\n## Decision Methodology\\n## Decision Inputs\\n## Decision Outputs\\n## Decision Implementation\\n\\nShare this:\\n\\nLike this:\\n\\nLike \\n\\nRelated\\n\\nComments are closed."}
{"File Name":"fxa\/0023-paypal-support.md","Context":"## Context and Problem Statement\\nThe Subscription Platform was implemented with Stripe as the payment processor and driver of subscription logic. Stripe is used as the source of truth for whether an account has subscriptions associated with it. We would like to add PayPal as a payment option for subscriptions, and integrate it appropriately with the existing system in a least-effort manner that is ideally flexible enough to support future expansions of payment processors, such as IAP.\\n## Decision Drivers\\n- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\n","Decision":"- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\nStripe-driven out-of-band invoice processing. This was originally considered to be the slower of the approaches, but using PayPal subscriptions ended up being approximately the same engineering effort. Given additional user experience benefits of the Stripe-driven approach, it is now the recommend implementation path.\\n","tokens":116,"id":371,"Predictions":"\\n- The decision is to integrate PayPal as a payment option for subscriptions. This decision states that the existing Subscription Platform infrastructure is sufficient to support future expansions of payment processors, such as IAP.\\n\\n## Decision Drivers\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\n- Engineering resources\\n- Future effort needed to integrate IAP\\n\\n## Decision\\n\\n- The decision is to integrate PayPal as a payment option for subscriptions. This decision states that the existing Subscription Platform infrastructure is sufficient to support future expansions of payment processors, such as IAP.\\n\\n## Decision Drivers\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\n\\n## Decision\\n\\n- The decision is to integrate PayPal as a payment option for subscriptions. This decision states that the existing Subscription Platform infrastructure is sufficient to support future expansions of payment processors, such as IAP.\\n\\n## Decision Drivers\\n- Engineering resources\\n- Future effort needed to integrate IAP\\n\\n## Decision\\n\\n- The decision is to integrate PayPal as a payment option for subscriptions. This decision states that the existing Subscription Platform infrastructure is sufficient to support future expansions of payment processors, such as IAP.\\n\\n\\n"}
{"File Name":"fxa\/0002-use-react-redux-and-typescript-for-subscription-management-pages.md","Context":"## Context and Problem Statement\\nA solution for isolating third-party payment widgets from the rest of account\\nmanagement can include building a separate web app on its own dedicated domain.\\nAs a side effect in building that app, we have the opportunity to choose\\ntechnologies for building it that don't necessarily follow the rest of FxA.\\n## Decision Drivers\\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\n","Decision":"* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\nChosen option: \"React, Redux, Typescript\", because\\n* Chance to start with a fresh stack\\n* More vibrant ecosystem\\n* Better tooling & developer ergonomics\\n","tokens":111,"id":372,"Predictions":"## Decision\\n## Decision\\n\\n## Decision\\n## Decision\\n## Decision\\n\\n## Decision\\n## Decision\\n\\n## Decision\\n\\n"}
{"File Name":"fxa\/0006-json-schemas-for-messaging.md","Context":"## Context and Problem Statement\\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this\\nuse-case only and does not apply to JSON used in RESTful APIs.\\nTracking and minimizing drift between the accuracy of message format documentation and\\nthe underlying implementation is an error-prone process. In addition, JSON messages are\\nfrequently produced and consumed from multiple systems, and in multiple langauges. Ensuring\\nvalidity of these messages frequently requires implementing validation logic in each\\nlanguage that consumes these JSON messages to ensure correctness for message handling.\\nAdding a field to a message format is non-trivial, as every consumer must also update\\nits own validation logic. Currently there are a variety of places where messages are consumed\\nand produced, most producers do not validate the message they send as the consumer code is not\\nalways in the same service where the validation code typically resides.\\nThere is currently no versioning for our message formats, which also creates difficulties in\\nknowing when validation logic must change and when existing flexibility is sufficient.\\n## Decision Drivers\\n- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\\n","Decision":"- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\\nChosen option: \"option 3\", because our existing messages can be represented without any\\nmigration needed and external consumers already work with JSON.\\n### [option 1] Keep existing approach\\nCurrently our documentation drifts if a message format is tweaked and the docs are not updated. Ideally\\nour code review process would catch this but sometimes code subleties are not caught that alter the\\nmessage format. Validation changes can also fail to appear in the documentation, even though what is\\nconsidered a valid message key\/value should be documented.\\nWe have to occasionally duplicate `joi` schemas between repos, which should likely be moved to `fxa-shared`\\nto benefit re-use, but this does not help non-Node projects consume our messages.\\n- Pros\\n- No additional work is needed that we don't already do.\\n- Cons\\n- Our documentation frequently drifts.\\n- It's additional work to maintain accurate `joi` schemas spread throughout the code-base _or_\\n- It's additional work to consolidate our `joi` schemas in `fxa-shared`.\\n- `joi` schemas can't be used by consumers not written in JS.\\n### [option 2] Use protobuf\\nUsing protobuf would allow us to validate and document valid message formats that would also consume\\nless space and be more performant to serialize\/deserialize. Validation is built-in as invalid messages\\nwill not deserialize, and documenation can be generated based on the protobuf spec for a message.\\nProtobuf tooling is widely available for most langauges.\\n- Pros\\n- High throughput messaging could benefit from higher efficiency of serializing\/deserializing of messages\\n- Documentation stays in sync with message spec\\n- Validation is built-in to the message serialization\/deserialization process\\n- Cons\\n- Messages are in binary and not easily introspectable\\n- Existing message consumers will need to be updated to handle protobuf\\n- Migration procedure will need to be created\/implemented to shift from existing JSON format\\n### [option 3] Use JSON-Schemas\\nJSON-Schemas can define existing FxA JSON messages used in our queue system. There are libraries in\\nmost languages to consume these schemas for validation purposes, and libraries exist to generate\\ndocumenation from a JSON-Schema.\\n[SchemaVer] can be introduced to existing FxA JSON messages as consumers ignore unknown keys, while\\nconsumers using the JSON-Schema can verify they can handle new message formats or if they're incompatible.\\nSome of this logic is manual, as [SchemaVer] is not an official part of the JSON-Schema specification. There\\nis a recommendation of [SchemaVer] in [Best Web Practices for Data](https:\/\/www.w3.org\/TR\/dwbp\/#dataVersioning).\\n- Pros\\n- No changes needed for existing consumers\\n- Validation can be done based on the JSON-Schema without custom code\\n- Documentation can be generated to stay in sync with message spec\\n- JSON is human readable without further processing.\\n- JSON is widely used on the web and our increasing use of JWTs also result in JSON.\\n- JSON-Schema is widely supported in a variety of langauges.\\n- Cons\\n- [SchemaVer] is a draft that isn't officially used by the JSON-Schema committees, but no other versioning\\nscheme for JSON-Schema appear to exist as of this writing.\\n[schemaver]: https:\/\/snowplowanalytics.com\/blog\/2014\/05\/13\/introducing-schemaver-for-semantic-versioning-of-schemas\/\\n","tokens":269,"id":373,"Predictions":"## Context and Problem Statement\\nThe following ADR addresses the problem of tracking drift between the accuracy of message format\\ndocumentation and the underlying implementation.\\n\\n\\nThis ADR addresses the problem of tracking drift between the accuracy of message format\\ndocumentation and the underlying implementation.\\n\\n## Decision Drivers\\n- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\\n## Decision\\n## Context and Problem Statement\\nThe following ADR addresses the problem of tracking drift between the accuracy of message\\nformat documentation and the underlying implementation.\\n\\n## Decision Drivers\\n# This is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# ## Context and Problem Statement\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\n\\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this use-case only and does not apply to JSON used in RESTful APIs.\\nTracking and minimizing drift between the accuracy of message format documentation and \\nthe underlying implementation is an error-prone process. In addition, JSON messages are \\nfrequently produced and consumed from multiple systems, and in multiple langauges. Ensuring \\nvalidity of these messages frequently requires implementing validation logic in each \\nlanguage that consumes these JSON messages to ensure correctness for message handling.\\n\\nAdding a field to a message format is non-trivial, as every consumer must also update \\nits own validation logic. Currently there are a variety of places where messages are consumed \\nand produced, most producers do not validate the message they send as the consumer code is not \\nalways in the same service where the validation code typically resides.\\n\\nThere is currently no versioning for our message formats, which also creates difficulties in \\nknowing when validation logic must change and when existing flexibility is sufficient.\\n\\n## Decision Drivers\\n- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\n\\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this use-case only and does not apply to JSON used in RESTful APIs.\\nTracking and minimizing drift between the accuracy of message format documentation and \\nthe underlying implementation is an error-prone process. In addition, JSON messages are \\nfrequently produced and consumed from multiple systems, and in multiple langauges.\\n\\nAdding a field to a message format is non-trivial, as every consumer must also update its own validation logic.\\n\\nCurrently there are a variety of places where messages are consumedand produced, most producers do not validate themessage they send asthe consumer codeis not alwaysin themessage processing systemwherethe validationcodetypically resides.\\n\\nThere is currently no versioning for ourmessage formats, which also creates difficultiesinknowing whenvalidationlogicmustchangeandwhenexistingflexibilityis sufficient.\\n\\n## Decision Drivers\\n- Accurate documentation viaJSON-Schema doc generationValidation logic generated fromJSON-Schemato reduce manually written\/maintained code\\n\\nThis document describes how we plan on implementing versioning for our Message Format Specifications (MFS) using Semantic Versioning (SV).\\n\\nSemantic Versioning (SV) provides backwards compatibility while allowing us to add new features without breaking existing implementations.\\n\\nWe will be using Semantic Versioning (SV) as our primary mechanism for managing MFS version numbers.\\n\\nWe will be using Semantic Versioning (SV) as our primary mechanism for managing MFS version numbers.\\n"}
{"File Name":"fxa\/0007-subscription-claim-jwt-access-token.md","Context":"## Context and Problem Statement\\nA mechanism is needed to inform service providers (SP) that a user has paid for a given subscription. A SP could fetch the user's profile information, however, we prevent some 3rd party SPs from doing so, and instead give them all the information they need in a JWT format access token. Adding subscription information in JWT access tokens will give SPs the information they need to verify users have paid for a subscription.\\n## Decision Drivers\\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\n","Decision":"- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\nChosen option: Adding subscription info into its own claim was chosen because FxA's lax scope checking means bad acting users could grant themselves access to subscriptions they have not paid for. See [this bug regarding FxA's lax scope checking][#lax-scope-checking].\\n### Positive Consequences\\n- The JWT access token contains all the information an SP needs to verify the user has paid for a subscription.\\n- Users are unable to grant themselves access to subscriptions they have not paid for.\\n### Negative Consequences\\n- SPs must now check two claims from the JWT to ensure a user is able to access a protected resource.\\n- An additional claim is added that is not defined in [the JWT access token draft spec][#jwt-draft-spec].\\n","tokens":163,"id":374,"Predictions":"## Decision Statement\\nThe decision statement for this decision is as follows:\\n\\n\\n\\n\\n## Decision Drivers\\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\n"}
{"File Name":"fxa\/0010-transition-fxa-from-backbone-to-react.md","Context":"## Context and Problem Statement\\nWhen Firefox Accounts was created some 6-7 years ago, the front-end Javascript framework and HTML templating options were limited significantly to those available at the time. This ADR details the pros and cons of various FE libraries and frameworks, and why it is desirable for FxA to move to a different technology. It does not cover _how_ we will begin the conversion which can be [read about here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md).\\nWhile Backbone and Mustache have served their purposes for FxA thus far, other modern libraries and frameworks not only offer more appealing architecture and developer conveniences, but also offer better data predictability, potential performance boosts, and a strong community of online support.\\n## Decision Drivers\\n- The desire for a more modern FE architecture and a more efficient UI\\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\\n","Decision":"- The desire for a more modern FE architecture and a more efficient UI\\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\\nChosen option: \"Option B - Move to client-side React\", because\\n- The team desires the benefits we will gain from moving to a more modern library (excluding option A)\\n- The team has experience with (and likes) React\\n- We're already using it in other parts of the ecosystem, i.e. the payments server\\n- FxA does not need server-side rendering (option C)\\n- Angular (option D) is heavy, opinionated with a steep learning curve, and offers at least some features we won't need\\n- Web components and libraries around them (option F) are quite new, yet a distant potential architecture shift from React to web components appears to be far less dramatic than from Backbone to React\\nIn the end, it is recognized that Vue _may_ have been a good option for FxA, but React was chosen over Vue due to:\\n- existing FxA engineering team knowledge\\n- the FxA payments server is already using React\\n- React has already been reviewed by our security team and \"passed\" the trial run when it was novel tech in our stack\\n- As one of our desires is to best protect against such a major refactor proposal in the future, we have to allow package age, online support, popularity and backing, and stability to be factors in our decision.\\n","tokens":235,"id":375,"Predictions":"\\n- The team is comfortable with the decision to move to a more modern FE architecture and a more efficient UI\\n- The team also understands that moving to a different technology will require some planning and effort\\n- The team is comfortable with the decision to move to a different technology\\n\\n## Decision\\n\\n- The team is comfortable with the decision to move to a different technology\\n\\n## Decision\\n\\n- The team is comfortable with the decision to move to a different technology\\n\\n## Decision\\n\\n- The team is comfortable with the decision to move to a different technology\\n\\n## Decision\\n\\n- The team is comfortable with the decision to move to a different technology\\n\\n## Decision\\n\\n- The team is comfortable with the decision to move to a different technology\\n\\n\\n\\end{document}\\n"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"Predictions":"## Decision Description\\n\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n## Decision\\n## Decision Description\\n\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n## Decision\\n## Decision Description\\n\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n## Decision\\n## Decision Description\\n\\n\\nDRAFT\\n\\n# 2020-02: Use the following template for your user stories. Please use the template to create a story that is unique to your use case. If you want to create multiple stories, please use the following template for each story.\\n\\n### Use Case Name: ####### (add more details below)\\n\\n### Use Case Summary:\\n\\n### Use Case Statement:\\n\\n### Use Case Scope: \\nThe scope of this use case is to allow users to interact with an app in a way that is not possible without an account. This can be done by allowing users to log into an app without having an account and then be able to interact with the app in a way that requires an account.\\n\\n### Use Case Requirements: \\nThe user must be able to sign into an app without having an account and then be able to interact with the app in a way that requires an account.\\n\\n### Use Case Solution:\\n\\n### Use Case Implementation:\\nSign into the webapp without having an account.\\nWhen prompted, enter your email address.\\nWhen prompted, enter your password.\\nAfter you have logged in successfully, you will be taken back to the webapp.\\n\\n### Use Case Validation:\\nUsers can only interact with webapps if they have a valid email address. Users cannot interact with webapps if they do not have a valid email address.\\nUsers cannot interact with webapps if they do not have a valid password.\\nUsers cannot interact with webapps if they do not have a valid phone number.\\nUsers cannot interact with webapps if they do not have a valid birthday.\\n\\n### Use Case Deployment:\\nAll existing users can sign-in without having an account. New users are required to create and verify their accounts before using any of the apps.\\n\\n#### Your Name Here ####\\n\\n#### Your Email Address ####\\n\\n#### Your Phone Number ####\\n\\n#### Your Birthday ####\\n\\n#### Your Social Security Number ####\\n\\n#### Your Email Address ####\\n\\n#### Your Password ####\\n\\n#### Your Password XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX \\n\\n#### Your Social Security Number ###-\\n\\n#####Your Phone Number ###-\\n\\n#####Your Email Address ###-\\n\\n#####Your Password ###-\\n\\n#####Your Birthday ###-\\n\\n#####Your Social Security Number ###-\\n\\n#####Your Email Address #####-\\n\\n#####Your Password #####-\\n\\n#####Your Phone Number #####-\\n\\n#####Your Email Address #####-\\n"}
{"File Name":"fxa\/0017-switch-settings-auth-to-sessiontoken.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\nIn [ADR-0014](0014-auth-for-settings-redesign.md), we'd decided to use OAuth2 as the mechanism. But, upon attempting to implement, we discovered that [context missing from OAuth2 tokens][missing-oauth2-context] rendered the option unusable.\\n[missing-oauth2-context]: https:\/\/github.com\/mozilla\/fxa\/pull\/4931#discussion_r411828476\\n## Decision Drivers\\nBasically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"Basically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Reuse existing sessionToken on content-server with auth-server APIs\", because it's the least novel option requiring fewest changes to auth-server.\\n### Positive Consequences\\n- Easiest path to authenticating on auth-server for API requests, works almost exactly like the settings client does already.\\n### Negative Consequences\\n- We defeat the security promises of Hawk, but we're not really relying on them. We'd also like to move away from Hawk in the future anyway.\\n","tokens":171,"id":378,"Predictions":"## Decision:\\\\\\n\\n\\n\\nThe user is currently logged in to the Settings app. The Settings app is also the OAuth2 provider for the Settings Redesign app. The OAuth2 provider has access to the protected user data.\\n\\nThe OAuth2 provider is a third party service and doesn't have any user data.\\n\\nThe OAuth2 provider has been granted access to the protected user data by the Settings Redesign app. The OAuth2 provider needs to be able to read and write that data.\\n\\nThe OAuth2 provider needs to be able to read and write that data from HTTP requests (via POST and GET). And, since we're using an existing app, we need to provide some form of authentication & authorization for that.\\n\\n## Decision Drivers\\\\\\n\\n- Smooth UX\\\\\\n- Security\\\\\\n- Development velocity\\\\\\n- Ease of integration\\\\\\n\\end{table}\\n\\n\\begin{table}[h]\\n\\centering\\n\\caption{Summary of Decision Drivers}\\n\\vspace{-0.3in}\\n\\begin{tabular}{|l|c|c|}\\n\\hline \\textbf{Decision Driver} & \\textbf{Description} & \\textbf{\\# of decisions made}\\\\ \\hline \\nSmooth UX & Smooth UX, smooth transition between apps & 1\\\\ \\hline \\nSecurity & Secure, secure access in both apps & 1\\\\ \\hline \\nDevelopment velocity & Faster development, less bugs, easier feature changes & 1\\\\ \\hline \\nEase of integration & Easy integration with existing apps & 1\\\\ \\hline \\n\\end{tabular}\\n\\label{tab:decision_drivers}\\n\\vspace{-0.3in}\\n\\end{table}\\n\\n\\n% TODO: This is a great summary of what I want my decision record to look like! I think this should go in here.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n\\n% TODO: Make sure you include a table with all your decisions and their drivers.\\n"}
{"File Name":"fxa\/0021-use-internjs-testing.md","Context":"## Context and Problem Statement\\nFor over the past 5 years (at least), we have been using [InternJS](https:\/\/github.com\/theintern\/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer\/faster\/better supported testing frameworks.\\n## Decision Drivers\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\n","Decision":"- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async\/await. The biggest factor was not having to introduce another testing framework for developers to learn.\\n","tokens":104,"id":379,"Predictions":"## Decision\\n## Decision\\n\\n- 0\\n\\n\\n\\#2 - \\textbf{Decision}\\n\\#1 - \\textbf{Decision}\\n\\#2 - \\textbf{Decision}\\n\\n## Decision Drivers\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\n## Decision\\n## Decision\\n## Decision\\n\\n\\#3 - \\textbf{Decision}\\n\\#3 - \\textbf{Decision}\\n\\#3 - \\textbf{Decision}\\n\\n## Decision\\n## Decision\\n## Decision\\n\\n\\n\\n"}
{"File Name":"fxa\/0028-retiring-email-service.md","Context":"## Context and Problem Statement\\nThe goal of fxa-email-service was to spin off the email sending responsibilities of FxA into a shared service that multiple Mozilla projects could use. From its readme:\\n> The FxA team had an OKR for Q2 2018 about decoupling the auth server from SES and making it possible to send email via different providers. Subsequently, some other teams expressed an interest in depending on a standalone email service too.\\n> This repo started as our experiment to see what a decoupled email-sending service would look like, written in Rust. It is now handling all FxA email traffic in production, and we are gradually separating it from the FxA stack to run as a standalone service in its own right.\\nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its \"weight\", being a fairly large codebase for what it does and our only Rust service, has never been a concern. Recent work to eliminate fxa-auth-db-mysql meant we either needed make changes to it or rethink how FxA sends email. It turned out the work to update email-service was larger than eliminating it and replacing it by sending email directly from auth-server via SES or SMTP.\\n## Decision Drivers\\n- Future maintenance\\n- Email provider flexability\\n","Decision":"- Future maintenance\\n- Email provider flexability\\nWe will eliminate email-service.\\n","tokens":315,"id":380,"Predictions":"\\nThe decision was made to decouple the email sending service from the auth-server and use SES instead. This means that the email service will no longer be reliant on FxA and will not suffer any data loss in case the auth-server fails. It also means that we don't need to maintain both services, as the email service can be run as a standalone application. The only thing left to do is to remove fxa-auth-db-mysql from SES and replace it with SES directly.\\n## Decision Impact\\n- No impact\\n- No change\\n- No impact\\n- No change\\n- No impact\\n\\n\\n## Decision\\n\\nThe decision was made to decouple the email sending service from the auth-server and use SES instead. This means that the email service will no longer be reliant on FxA and will not suffer any data loss in case the auth-server fails. It also means that we don't need to maintain both services, as the email service can be run as a standalone application. The only thing left to do is to remove fxa-auth-db-mysql from SES and replace it with SES directly.\\n## Decision Impact\\n- No impact\\n- No change\\n- No impact\\n\\n\\n## Decision\\n\\nThe decision was made to decouple the email sending service from the auth-server and use SES instead. This means that the email service will no longer be reliant on FxA and will not suffer any data loss in case the auth-server fails. It also means that we don't need to maintain both services, as the email service can be run as a standalone application. The only thing left to do is to remove fxa-auth-db-mysql from SES and replace it with SES directly.\\n## Decision Impact\\n"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"Predictions":"## Decision Description\\nThe approach chosen for this ADR is to use a CSS framework which provides a simple way to define common CSS classes. We will use the [SASS](https:\/\/sass-lang.com\/) language, which provides a way to define common CSS classes and variables. The approach is chosen as it is the only supported approach in the FxA ecosystem.\\n\\n## Decision Steps\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\\n## Decision Steps\\n## Decision Description\\nThe approach chosen for this ADR is to use a CSS framework which provides a simple way to define common CSS classes. We will use the [SASS](https:\/\/sass-lang.com\/) language, which provides a way to define common CSS classes and variables. The approach is chosen as it is the only supported approach in the FxA ecosystem.\\n\\n## Decision Steps\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\\n## Decision Steps\\n## Decision Description\\nThe approach chosen for this ADR is to use a CSS framework which provides a simple way to define common CSS classes. We will use the [SASS](https:\/\/sass-lang.com\/) language, which provides a way to define common CSS classes and variables. The approach is chosen as it is the only supported approach in the FxA ecosystem.\\n\\n## Decision Steps\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will"}
{"File Name":"fxa\/0014-auth-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Authenticate via OAuth2 to use auth-server APIs\", because:\\n- It requires minimal changes to existing auth-server implementation and infrastructure.\\n- It relies on an authentication mechanism with relatively well-known security properties.\\n### Positive Consequences\\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\\n### Negative Consequences\\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\\n","tokens":66,"id":382,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n\\n## Decision\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n\\n## Decision\\n\\nThis is an Architectural Decisi"}
{"File Name":"fxa\/0013-react-toolchain-for-settings-redesign.md","Context":"## Context and Problem Statement\\nFirefox Accounts is introducing a new visual design for Settings (the interface that exists at the address `https:\/\/accounts.firefox.com\/settings`). The changes involved are significant enough such that it [has been decided](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) that an entirely new React App will be built more or less independent of the existing Settings web app. With this we can beta test and roll out the new design with minimal disruption.\\nAs there are a multitude of ways a React App can be set up and configured, this decision has introduced the question, \"What [toolchain](https:\/\/en.wikipedia.org\/wiki\/Toolchain) should we set up a new React App with?\". This ADR serves to answer that question by going over the various approaches we can take to set up a new React App and how it might integrate with our existing FxA ecosystem.\\nIt's important to note that we are not deciding the languages, libraries, or other tools that we will use within the React development environment, but rather, the system that will be used to develop the React App with.\\nFinally, we are not making decisions about the App's production server here. We'll be using [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server)'s Express server for the production build.\\n## Decision Drivers\\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n","Decision":"- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\nChosen option: \"B3 - Create React App and use Rescripts\", because:\\n- Our team has sufficient knowledge in working with CRA over the other options.\\n- We don't need to re-invent the wheel. Our needs for the Settings project are not so great that it would be worth spending time creating a ground-up toolchain. To some extent this option allows us to do what we do best without needing to worry about the specifics of the development workflow.\\n- Rescripts allows us to layer additional configuration overtop of the existing configuration without jeopardizing the integrity or trading away any of the powers of CRA. Specifically, we can use Rescripts to modify CRA to allow external React components to be imported, as demonstrated in [this repo](https:\/\/github.com\/jodyheavener\/rescripts-react-app) (technical notes [here](https:\/\/github.com\/jodyheavener\/rescripts-react-app\/blob\/master\/.rescriptsrc.js)). This capability could be implemented in our other React Apps.\\n- If, down the road, we end up in a place where we absolutely need access to the internals we can always eject.\\n","tokens":437,"id":383,"Predictions":"## Decision Statement\\nThe decision for this ADR is to use the [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server) as the production build system. This is a decision that was made based on a combination of factors, including but not limited to: \\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n\\n## Decision Statement\\nThe decision for this ADR is to use the [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server) as the production build system. This is a decision that was made based on a combination of factors, including but not limited to: \\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n\\n## Decision Statement\\nThe decision for this ADR is to use the [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server) as the production build system. This is a decision that was made based on a combination of factors, including but not limited to: \\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n\\n## Decision Statement\\nThe decision for this ADR is to use the [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server) as the production build system. This is a decision that was made based on a combination of factors, including but not limited to: \\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\nThe final decision driver was \"maintainability\", which led us in this direction.\\n"}
{"File Name":"fxa\/0008-redis-lua-scripts.md","Context":"## Context and Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n","Decision":"- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\nUse lua scripts. The proof-of-concept [PR](https:\/\/github.com\/mozilla\/fxa\/pull\/3278) showed lua as viable and an overall simpler option.\\n### Positive Consequences\\n- Future use cases should be simpler to implement\\n- Less custom Redis glue code and simpler stack traces\\n- Improved maintainability\\n### Negative Consequences\\n- Additional developer cost of understanding lua\\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\\n","tokens":134,"id":384,"Predictions":"\\end{quote}\\n\\subsection*{Decision Context}\\n\\begin{quote}\\nThe context for this decision is the OAuth2 authentication system. We currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n\\n## Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n\\n## Decision\\n\\end{quote}\\n\\subsection*{Decision Drivers}\\n\\begin{quote}\\nThe decision driver is the reason that this decision was made.\\n\\n## Decision\\n\\end{quote}\\n\\subsection*{Decision Context}\\n\\begin{quote}\\nThe context for this decision is the OAuth2 authentication system. We currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n\\n## Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n\\n## Decision\\n\\end{quote}\\n% \\begin{figure}[H]\\n% \\centering\\n% \\includegraphics[width=0.8\\textwidth]{figs\/decision.png}\\n% \\caption{\\label{fig:decision}Decision Diagram}\\vspace{-1ex}\\end{figure}\\n\\n"}
{"File Name":"fxa\/0028-evaluate-playwright.md","Context":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n","Decision":"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n","tokens":160,"id":385,"Predictions":"## Decision: The decision to continue with the current architecture and test pipeline is made based on the following considerations:\\n## 1. The cost of running the tests is too high. We should evaluate other testing options (e.g. CircleCI, Travis CI) to reduce this cost.\\n## 2. The test suite does not currently scale well, and we need a better testing stack to increase reliability.\\n## 3. We need a real solution to the issue of failing PRs and keep our sanity while trying to improve our rate of success.\\n## 4. We need a more stable testing environment in CI.\\n## 5. The CI build is too expensive for us to run every day.\\n## Decision: The decision to continue with the current architecture and test pipeline is made based on the following considerations:\\n## 1. The cost of running the tests is too high. We should evaluate other testing options (e.g. CircleCI, Travis CI) to reduce this cost.\\n## 2. The test suite does not currently scale well, and we need a better testing stack to increase reliability.\\n## 3. We need a real solution to the issue of failing PRs and keep our sanity while trying to improve our rate of success.\\n## 4. We need a more stable testing environment in CI.\\n\\n### Test Plan\\n\\n### Summary\\n\\n### Test Cases\\n\\n### Expected Results\\n\\n### Actual Results\\n\\n### Impact (If any)\\n\\n### Solution\\n\\n#### CircleCI\\n\\n#### Travis CI\\n\\n#### CircleCI & Travis CI Integration Test (CircleCI)\\n\\n#### CircleCI & Travis CI Integration Test (Travis)\\n\\n#### CircleCI & Travis Integration Test (Travis)\\n\\n#### CircleCI & Travis Integration Test (Travis)\\n\\n#### CircleCI & Travis Integration Test (CircleCI)\\n\\n#### Travis Integration Test (Travis)\\n\\n#### Travis Integration Test (Travis)\\n\\n\\n\\n### Additional Information\\n\\n\\n\\nWe are using GitLab CE v11.5.\\n\\nWe are using GitLab Runner v11.5.\\n\\nWe are using GitLab Runner v11.\\n\\nWe are using GitLab CE v12.\\n\\nWe are using GitLab CE v11.\\n\\nWe are using GitLab CE v10.\\n\\nWe are using GitLab Runner v10.\\n\\n\\n\\nOur current deployment process uses:\\n\\n* `gitlab-ci` runs `circleci` on each commit\\n* `circleci` runs `circleci-ci` for each commit\\n\\nOur current deployment process uses:\\n\\n* `gitlab-ci` runs `circleci` on each commit\\n* `circleci-ci` runs `circleci-ci` for each commit\\n\\n\\n\\nOur current deployment process uses:\\n\\n* `gitlab-ci` runs `circleci` on each commit\\n* `circleci-ci` runs `circleci-ci` for each commit\\n\\n\\n\\nOur current deployment process uses:\\n\\n* `gitlab-ci` runs `circleci-ci` on each commit\\n* `circleci-ci` runs `circleci-ci` for each commit\\n\\n\\n\\nOur current deployment process uses:\\n\\n* **CircleCI** run **Circle** **Test** **Test** **Runner** **Runner**\\n* **Circle** run **Circle** **Test** **Test** **Runner**\\n* Circle Team creates an empty repository in an empty branch\\n* Team creates an empty repository in an empty branch\\n* Team creates an empty repository in an empty branch\\n* Team creates an empty repository in an empty branch\\n"}
{"File Name":"fxa\/0004-product-capabilities-for-subscription-services.md","Context":"## Context and Problem Statement\\nFor subscription services, we need a way to inform a Relying Party about the\\nrelevant subscription status to enable features for a given user.\\n## Decision Drivers\\n* Engineering simplicity\\n* Security\\n* Business Operations\\n* Marketing\\n","Decision":"* Engineering simplicity\\n* Security\\n* Business Operations\\n* Marketing\\nChosen option A: Single profile assertions consisting of a list of subscription capability\\nstrings based on requesting client ID.\\n* Subscription capabilities are conveyed as a single additional profile assertion.\\n* FxA maintains a mapping of client-to-capabilities, capabilities-to-product\\n* Relying Parties can provide multiple subscription-gated capabilities.\\n* Example:\\n* **RP-A** provides `goldBadge` and `silverBadge`\\n* **RP-B** provides `goldBadge` and `unlimitedStorage`\\n* **RP-C** provides `freePuppies`\\n* FxA can bundle capabilities into cross-RP products as needed for marketing &\\nbusiness purposes.\\n* Example:\\n* **Product A** bundles `goldBadge` & `unlimitedStorage`\\n* RPs are not granted visibility into the user's entire subscription status.\\n* Example:\\n* User subscribes to **Product A**.\\n* **RP-A** will see `goldBadge` but not `unlimitedStorage` in User's `subscriptions` claim in profile.\\n* **RP-B** will see both `goldBadge` and `unlimitedStorage`\\n* **RP-C** sees no capabilities listed\\n","tokens":54,"id":386,"Predictions":"## Decision Drivers\\n## Engineering simplicity\\n## Security\\n## Business Operations\\n## Marketing\\n\\n\\subsection{Decision Drivers}\\n\\label{sec:decision-drivers}\\n\\nThis section describes the decision drivers for the software.\\n\\nThe decision drivers are:\\n\\n\\begin{itemize}\\n    \\item Engineering simplicity\\n    \\item Security\\n    \\item Business Operations\\n    \\item Marketing\\n\\n    \\item Engineering simplicity - avoid duplication of effort in maintaining code base and infrastructure. For example, if a user has multiple subscriptions, they will have to remember which subscriptions they have subscribed to and which subscriptions they haven't subscribed to. This will cause a duplication of effort.\\n    \\item Security - prevent unauthorized access to data. For example, if a user has multiple subscriptions, they may not know which subscription is associated with which user. This will cause a duplication of effort.\\n    \\item Business Operations - provide better service to the Relying Party by informing them about the subscription status of each user. For example, if a user has multiple subscriptions, they can be notified about their subscription status and provide feedback on their experience.\\n    \\item Marketing - provide better service to the Relying Party by informing them about the subscription status of each user. For example, if a user has multiple subscriptions, they can be notified about their subscription status and provide feedback on their experience.\\n\\end{itemize}\\n\\n\\subsection{Engineering simplicity}\\n\\label{sec:engineering-simplicity}\\n\\nThis decision driver is related to the decision driver Engineering simplicity.\\n\\nThe software should be designed so that it is easy for users to understand and use.\\n\\nThe decisions are:\\n\\n\\begin{itemize}\\n    \\item Engineering simplicity - avoid duplication of effort in maintaining code base and infrastructure. For example, if a user has multiple subscriptions, they will have to remember which subscriptions they have subscribed to and which subscriptions they haven't subscribed to. This will cause a duplication of effort.\\n    \\n        The software should be designed so that it is easy for users to understand and use.\\n\\n        The decisions are:\\n\\n        \\begin{enumerate}\\n            \\setlength{\\itemsep}{0pt}\\n            \\setlength{\\parskip}{0pt}\\n\\n            \\item Engineering simplicity - avoid duplication of effort in maintaining code base and infrastructure. For example, if a user has multiple subscriptions, they will have to remember which subscriptions they have subscribed to and which subscriptions they haven't subscribed to. This will cause a duplication of effort.\\n\\n                The software should be designed so that it is easy for users to understand and use.\\n\\n                The decisions are:\\n\\n                \\begin{enumerate}\\n                    \\setlength{\\itemsep}{0pt}\\n                    \\setlength{\\parskip}{0pt}\\n\\n                    \\item Engineering simplicity - avoid duplication of effort in maintaining code base and infrastructure. For example, if a user has multiple subscriptions, they will have to remember which subscriptions they have subscribed to and which subscriptions they haven't subscribed to. This will cause a duplication of effort.\\n\\n                        The software should be designed so that it is easy for users to understand and use.\\n                        The decisions are:\\n\\n                        \\begin{enumerate}\\n                            \\setlength{\\itemsep}{0pt}\\n                            \\setlength{\\parskip}{0pt}\\n\\n                            \\setlength{\\parsep}{1em} % <--- this line causes an error\\n                            {\\bfseries If possible}:\\n                                Avoid duplicate coding for services provided by different teams.\\n                            {\\bfseries If possible}:\\n                                Avoid duplicate coding for services provided by different teams.\\n\\n                        % 1) You can find out how many services you're providing with your current setup.\\n                        % 2) You may want more services than you currently provide because you're using the same setup as another team.\\n\\n                        % 3) You may want more services than you currently provide because you're using the same setup as another team.\\n\\n                        % 4) You may want more services than you currently provide because you're using the same setup as another team.\\n\\n                        % 5) You may want more services than you currently provide because you're using the same setup as another team.\\n                        \\n                    % 6) Try adding new services or removing some existing ones from your existing setup.\\n                    % 7) Try adding new services or removing some existing ones from your existing setup.\\n\\n                    % 8) Try adding new services or removing some existing ones from your existing setup.\\n                    % 9) Try adding new services or removing some existing ones from your existing setup.\\n                    \\n                %\\n                %\\n                %\\n                %\\n                %\\n                %\\n\\n                %\\n            %\\n        %\\n        \\n        %\\n\\n        %\\n\\n        \\n        %\\n\\n        \\n        %\\n\\n        \\n        %\\n\\n        \\n        %\\n        \\n        %\\n\\n        \\n    \\n\\n    \\n            \\n            \\n           \\n    \\n    \\n    \\n            \\n           \\n        \\n            \\n    \\n\\n    \\n            \\n            \\n           \\n    \\n    \\n    \\n            \\n           \\n        \\n            \\n    \\n\\n    \\n            \\n            \\n           \\n    \\n    \\n    \\n            \\n           \\n        \\n            \\n    \\n\\n    \\n            \\n            \\n           \\n    \\n    \\n    \\n            \\n            \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n                    \\n                    \\n                \\n                \\n            \\n            \\n        \\n    \\n    \\n\\n    \\n            \\n            \\n            \\n            \\n        \\n    \\n    \\n\\n    \\n            \\n            \\n            \\n            \\n        \\n    \\n    \\n\\n    \\n    \\n    \\n        \\n        \\n        \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n     \\n\\n\\n\\n\\subsection{Security}\\n\\label{sec:security}\\n\\nThis decision driver relates directly with Security decision driver below\\n\\nThe decisions are:\\n\\n\\begin{enumerate}[noindent]\\n\t\\setlength{\\leftmargini}{1em}%\\n\t\\addtolength{\\leftmargini}{-1em}%\\n\t%\\addtolength{\\leftmargini}{-1em}%\\n\t%\\addtolength{-1em}{-1em}%\\n\t%\\addtolength{-1em}{-1em}%\\n\t%\\addtolength{-2em}{-2em}}\\n\t%\\addtolength{-2em}{-2em}}\\n\\n\t\\noindent\\n\t\\noindent\\n\t\\noindent\\n\t\\noindent\\n\t\\noindent\\n\t\\noindent\\n\\n\\tabcolsep=1mm\\n\\n\\tabcolsep=2mm\\n\\n\\tabcolsep=3mm\\n\\n\\tabcolsep=4mm\\n\\n\\tabcolsep=5mm\\n\\n\\tabcolsep=6mm\\n\\n\\tabcolsep=7mm\\n\\n\\tabcolsep=8mm\\n\\n\\n\\n% No indent \\n% No indent \\n% No indent \\n% No indent \\n\\n% Table heading \\n% Table heading \\n\\n\\n\\n\\upshape\t\\n\\vspace*{-4ex}\\noindent\t\\n\\vspace*{-4ex}\\noindent\t\\n\\vspace*{-4ex}\\noindent\t\\n\\vspace*{-4ex}\\noindent\t\\n\\vspace*{-4ex}\\noindent\t\\n\\vspace*{-4ex}\\noindent\t\\n\\n\\noindent\\n\t\\n\tSecurity issues related with exposing sensitive data on remote servers,\\n\tor manipulating data on client-side or client-server side,\\n\tor accessing private data on server-side,\\n\tor accessing private data on client-side or client-server side,\\n\tor any other security-related issue raised by users,\\n\tcould result in loss of money or personal information,\\n\tand therefore affect business operations negatively.\\newline\\n\t\\n\tThis type of issues could be avoided by implementing proper security measures at all levels.\\newline\\n\t\\n\tThe following security measures could help protect against such issues:\\n\t\\n\t\\n\t\\n\t\\n\t\\n%% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n%% \\n \\n%% (c) Macro created by Macro Package [2017\/01\/17 v5.00e]\\n%%\\n%% Generated on Mon Jan 30 10:19:14 CEST 2017\\n \\n \\n%% (c) Macro created by Macro Package [2016\/03\/02 v6.00e]\\n%%\\n%% Generated on Mon Jan 30 10:19:14 CEST 2017\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n\\n \\n \\n\\n \\n\\n\\n  \\n   \\n  \\n   \\n  \\n   \\n  \\n   \\n   \\n  \\n   \\n  \\n  \\n \\n  \\n \\n  \\n  \\n   \\n  \\n  \\n \\n  \\n  \\n\\n  \\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n\\n\\n  \\n\\n    \\n    \\n    \\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n     \\n\\n\\n\\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"Predictions":"## Decision Details\\n- [Apollo Server](https:\/\/www.apollographql.com\/docs\/intro\/apollo-server\/) is a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\n- [Apollo Client](https:\/\/www.apollographql.com\/docs\/intro\/apollo-client\/) is a client library for GraphQL. It provides a rich set of primitives for building GraphQL clients.\\n- [GraphQL](https:\/\/graphql.org\/) is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n- [GraphQL Redesign](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) will be created as a new React application and in turn, has opened the door to assess certain pieces of our technology stack.\\n- [GraphQL Redesign](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) will be created as a new React application and in turn, has opened the door to assess certain pieces of our technology stack.\\n## Decision\\n## Decision Details\\n- [Apollo Server](https:\/\/www.apollographql.com\/docs\/intro\/apollo-server\/) is a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\n- [Apollo Client](https:\/\/www.apollographql.com\/docs\/intro\/apollo-client\/) is a client library for GraphQL. It provides a rich set of primitives for building GraphQL clients.\\n- [GraphQL Redesign](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) will be created as a new React application and in turn, has opened the door to assess certain pieces of our technology stack.\\n- [GraphQL Redesign](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) will be created as a new React application and in turn, has opened the door to assess certain pieces of our technology stack.\\n## Decision\\n## Decision Details\\n- [GraphQL Redesign](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) will be created as a new React application and in turn, has opened the door to assess certain pieces of our technology stack.\\n## Decision\\n## Decision Details\\n- The following decision drivers are used when deciding whether or not to use Apollo Server or Apollo Client:\\n\\n* Performance implications; consideration around the number of network requests and data transferred\\n* Ease of setup for client-side API calls\\n* Clarity around expected data, React integration, and developer tooling\\n* Development speed, both around initial setup and as new features roll out\\n\\n## Decision\\n## Decision Details\\n- The following decision drivers are used when deciding whether or not to use GraphQL Redesign or Apollo Server:\\n* Performance implications; consideration around the number of network requests and data transferred\\n* Ease of setup for client-side API calls\\n* Clarity around expected data, React integration, and developer tooling\\n* Development speed, both around initial setup and as new features roll out\\n## Decision\\n## Decision Details\\nThe following decision drivers are used when deciding whether or not to use GraphQL Redesign or Apollo Client:\\nPerformance implications; consideration around the number of network requests and data transferredEase of setup for client-side API callsClarity around expected data, React integration, and developer toolingDevelopment speed,\\n"}
{"File Name":"fxa\/0011-create-new-react-app-for-settings-redesign.md","Context":"## Context and Problem Statement\\n\"Settings\" refers widely to the `accounts.firefox.com\/settings` page where users can manage their Firefox account. The \"Settings Redesign Project\" refers to a 2019\/2020 project centered around giving this page a fresh user interface. The changes are fairly significant and will require a lot of component shuffling and some functionality refactoring. [See the PRD](https:\/\/docs.google.com\/document\/d\/18zu7JCYIsUp8tUMJqb2uErNlzL9f6CQvefLy9HFZ4UY\/edit?pli=1#heading=h.cf57dt1i8634).\\nThe FxA Engineering team desires to migrate from [Backbone and Mustache to React](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0010-transition-fxa-from-backbone-to-react.md). This ADR addresses questions around what level of action we should take regarding this desire and the Settings Redesign Project - should a conversion to React be done at a later time, or are the changes significant enough to justify propelling us into React now?\\n","Decision":"Chosen option: \"Option C - Take the 'ground up' approach and create a new settings React application\", because\\n- The overall goal of the Settings Redesign Project is to offer more flexibility to expand Settings functionality. New features have already been planned. It could be argued that heavy refactoring later (which would be needed for option A or B) would take just as much time as recreating this page with React now.\\n- This approach allows us an opportunity to set up Storybook for the content server and review our tests, CSS, and a11y implementations as we implement the UI changes.\\n- This approach also simplifies our GH workflow and A\/B testing the entire redesign.\\nWe can mitigate risks by avoiding a \"big bang\" surprise replacement by implementing, deploying, and launching smaller pieces of MVP functionality along the way under a new front-end route serving the React application while keeping the Backbone Settings live. While this may be tedious, it can be a safety net against losing integrity and wisdom earned in the original system.\\n","tokens":233,"id":388,"Predictions":"\\begin{itemize}\\n\\item[1.]\\n  \\item[2.]\\n  \\item[3.] We will move forward with the transition to React at a later time, after we have completed the FxA Engineering team's migration to React.\\n\\end{itemize}\\n\\n\\end{document}"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\end{document}"}
{"File Name":"asb-client-spa\/0002-use-netlify-for-deployments-and-hosting.md","Context":"## Context\\nGitHub Pages is a perfectly good static site host, however, not having the\\nability for preview builds is problematic when wishing to test changes prior to\\nthem being merged into the main branch. Netlify has the ability to create\\npreview builds for each PR or even each branch along with a number of other\\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\\n[comparison](https:\/\/www.netlify.com\/github-pages-vs-netlify\/).\\n","Decision":"The decision is to move hosting and deployment onto Netlify\\n","tokens":100,"id":390,"Predictions":"\\begin{itemize}\\n    \\item Yes - Netlify is a good choice for static site hosting\\n    \\item No - GitHub Pages is a good choice for static site hosting\\n\\end{itemize}\\n\\end{document}\\n"}
{"File Name":"asb-client-spa\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":391,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"structurizr-python\/0008-package-structure.md","Context":"## Context\\nWe try to structure our package in logical sub-units but we want to maintain a\\nconsistent public interface.\\n","Decision":"We allow for arbitrarily nested sub-packages but export important classes and\\nfunctions to the top level thus exposing a public interface. Our unit tests\\nshould reflect this package structure.\\n","tokens":26,"id":393,"Predictions":"## Our package should be able to identify the main category of a package.\\n\\n## Context\\n## We should be able to identify the main subcategory of a package.\\n\\n## Decision\\n## A package should be able to support the main subcategory of an application.\\n\\n## Context\\n## A package should be able to support the main category of an application.\\n\\n## Decision\\nOur package supports the main category of an application, so it should\\nsupport this category.\\n\\n## Context\\nA package should not support a category that is not supported by any\\napplication.\\n\\n## Decision\\nOur package does not support any categories that are not supported by any\\napplication.\\n\\n## Context\\nA package should not support categories that are not supported by any\\napplication.\\n\\n## Decision\\nOur package should support all categories.\\n\\n\\n# If you want, you can also specify your own context here:\\n# \\item \\textbf{Context} : The context for your decision record.\\n# \\item \\textbf{Decision} : An object of class \\texttt{Decision}.\\n\\n\\end{lstlisting}\\n\\n\\subsubsection*{Example}\\n\\n\\begin{lstlisting}\\npackage foo;\\n\\npublic interface IFoo {\\n    public void bar(int i);\\n}\\n\\npublic class Bar implements IFoo {\\n    public void bar(int i) {\\n        System.out.println(\"Bar\");\\n    }\\n}\\n\\npublic class Foo {\\n    private String name;\\n    private int age;\\n    \\n    public void bar() {\\n        System.out.println(\"Foo\");\\n        System.out.println(\"Name: \" + name);\\n        System.out.println(\"Age: \" + age);\\n    }\\n    \\n    public String getName() {\\n        return name;\\n    }\\n    \\n    public int getAge() {\\n        return age;\\n    }\\n    \\n}\\n\\end{lstlisting}\\n\\n\\subsubsection*{Output}\\n\\n\\begin{lstlisting}\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Foo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nFoo Name: foo.Foo@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 14\\n\\nBar Name: foo.Bar@4d2e3c0f Age: 13\\n\\nFoo Name: foo.Foo@4d2e3c0f Age: 13\\n\\n\\end{lstlisting}\\n\\nWe can see that we have two different types of packages:\\n\\n\\begin{itemize}\\n\t\\item The first one is for applications. It is called \\texttt{\"Application\"}.\\n\t\\n\tWe can see that it has two sub-packages:\\n\\n\t\\begin{itemize}\\n\t\t\\item \\texttt{\"Application\"}. It contains all the classes and methods related to the application.\\n\t\t\\n\t\tWe can see that there are three classes in this sub-package:\\n\t\t\\n\t\t\t\\begin{lstlisting}\\n\t\t\t\tpublic class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 {\\n\t\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\t\t\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\tpublic static class Application1 extends Application implements UserInterface1,\\n\t\t\t\t\tUserInterface2 { \\n\t\t\\n\t\t\t\tpublic void run() { \\n\t\t\\n\t\t\t\t\tSystem.out.println(\"Hello, world!\");\\n\t\t\t\t}\t\t\\n\t\t\t\t\\n\t\t\t...\\n\t\t\t\t\t\t\\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t      \\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"structurizr-python\/0007-unit-tests.md","Context":"## Context\\nWe need to make a decision on the testing framework for our project.\\n","Decision":"We will make use of pytest. It is a de facto standard in the Python community\\nand has unrivaled power.\\n","tokens":18,"id":394,"Predictions":"## We need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n## Decision\\nWe need to make a decision on the testing framework for our project.\\n\\n\\n\\subsection{Software}\\n\\label{software}\\n\\subsubsection{Decision}\\n\\label{decision}\\nThe following decisions are made in this document.\\n\\begin{itemize}\\n    \\item The software is decided not suitable.\\n    \\item The software is decided suitable but needs more work.\\n    \\item The software is decided suitable and needs further development.\\n    \\item The software is decided suitable and needs further development.\\n    \\item The software is decided unsuitable.\\n    \\item The software is decided unsuitable and needs further development.\\n    \\item The software is decided unsuitable and needs no further development.\\n\\end{itemize}\\n\\n\\subsubsection{Context}\\nThe context of this document is `Web Application Development'. This means that we are developing an application that will be used by many different people, so we have chosen not to use any special terms. In this context, we are calling `Web Application Development' `WebApp' or `WebApp'.\\n\\n\\subsection{Software Architecture}\\n\\nThe following diagram shows how we have organised all of the components in this application. We have chosen not to name any of these components because they do not play any part in deciding which components should be used.\\n\\n\\begin{figure}[h]\\n    \\centering\\n    \\includegraphics[width=1.0\\textwidth]{figures\/WebApp_architecture.png}\\n    \\caption{WebApp Architecture Diagram}\\label{fig:webapp_architecture_diagram}\\n\\end{figure}\\n\\nThis diagram shows that each component has its own interface. These interfaces allow us to use other components without needing another component's help.\\n\\n\\subsection{Software Design}\\n\\nThe next diagram shows how we have organised all of the components in this application. We have chosen not to name any of these components because they do not play any part in deciding which components should be used.\\n\\n\\begin{figure}[h]\\n    \\centering\\n    \\includegraphics[width=1.0\\textwidth]{figures\/WebApp_design.png}\\n    \\caption{WebApp Design Diagram}\\label{fig:webapp_design_diagram}\\n\\end{figure}\\n\\nThis diagram shows that each component has its own interface. These interfaces allow us to use other components without needing another component's help.\\n\\n"}
{"File Name":"structurizr-python\/0006-code-testing.md","Context":"## Context\\nSetting up different testing environments and configurations can be a painful\\nand error prone process.\\n","Decision":"We use tox to define, configure, and run different test scenarios.\\n","tokens":21,"id":395,"Predictions":"## 1. Use a dedicated test environment for each application.\\n## 2. Use a shared test environment for all applications, but use different\\nenvironments for each application.\\n## 3. Use a shared test environment for all applications, but use different\\nenvironments for each application and provide the user with the option to choose\\nwhich environment to use.\\n## 4. Provide a shared test environment for all applications, but provide the user with the option to choose which environment to use (e.g. by having different tests run in each of the environments).\\n## Decision\\n## 1. Provide a shared test environment, but provide the user with the option to choose which environment to use (e.g. by having different tests run in each of the environments).\\n## Decision\\n## 1. Create a separate test suite for each application.\\n## 2. Create a separate test suite for each application, but allow users to choose between them.\\n## Decision\\n## 1. Allow users to choose between using one or more test suites.\\n\\end{lstlisting}\\n\\n\\subsection{Use Cases}\\n\\n\\subsubsection{Use Case: \\texttt{Test Suite}}\\n\\label{sec:test_suite}\\n\\nThe \\texttt{TestSuite} class allows you to create and manage Test Suites.\\index{Test Suite} \\n\\nConsider an \\texttt{Application} that needs testing.\\index{Testing Application} The Application will have many Test Suites.\\index{TSTES!Test Suites} Each Test Suite will have one or more \\texttt{Test Objects}. Each Test Object will be associated with one or more Test Cases.\\index{TSTES!Test Cases} A Test Suite can be used by multiple Testing Applications.\\index{TSTES!Testing Applications} \\n\\nThe \\texttt{TestSuite} class provides methods that allow you to create and manage Test Suites.\\index{TSTES!Test Suites}\\index{TSTES!Testing Application}\\index{TSTES!Application}\\index{TSTES!Testing Application}\\index{TSTES!Application}\\index{TSTES!Application}\\index{TSTES!Application} \\n%\\begin{lstlisting}[language=Java]\\n%import java.util.ArrayList;\\n%import java.util.List;\\n%import org.junit.Test;\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%public class TestSuite {\\n%\\n%    public static List<TestObject> getTestObjects() {\\n%        return new ArrayList<TestObject>();\\n%    }\\n%\\n%    public static void setTestObjects(List<TestObject> objects) {\\n%        this.testObjects = objects;\\n%    }\\n%\\n%    public static void addObject(TestObject object) {\\n%        this.testObjects.add(object);\\n%    }\\n%\\n%%      public static void main(String[] args) {\\n%%          List<TestObject> list = new ArrayList<TestObject>();\\n%%          list.add(new TestObject(\"A\"));\\n%%          list.add(new TestObject(\"B\"));\\n%%          list.add(new TestObject(\"C\"));\\n%%          list.add(new TestObject(\"D\"));\\n%%          list.add(new TestObject(\"E\"));\\n%%          list.add(new TestObject(\"F\"));\\n%%          System.out.println(list);\\n%%      }\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%\\end{lstlisting}\\n%\\n%\\begin{lstlisting}[language=Java]\\n%\\end{lstlisting}\\n%\\n\\nYou can also create new Tests Objects from existing ones using the \\texttt{\\_set()} method.\\index{TSTES!\\_set()}\\n\\begin{sloppypar}\\n\\textbf{\\_set()} takes an Object as an argument and adds it as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\begin{sloppypar}\\n\\textbf{\\_set(Object)} adds an Object as a member of this object's Set:\\n\\end{sloppypar}\\nExample:\\label{}\\n\\begin{sloppypar}\\npublic class MyTestClass {\\n   private final MyTestClass myTestClass;\\n   public MyTestClass(MyTestClass myTestClass) {\\n      this.myTestClass = myTestClass;\\n   }\\n   public void method1(){\\n      System.out.println(myTestClass);\\n   }\\n   public void method2(){\\n      System.out.println(myTestClass);\\n   }\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\n   @Override\\n   protected void setUp() throws Exception {\\n\\n      super.setUp();\\n      myTestClass = new MyTestClass();\\n      myTestClass.method1();\\n      myTestClass.method2();\\n   }\\n\\n   @Override\\n   protected void tearDown() throws Exception {\\n\\n      super.tearDown();\\n      myTestClass = null;\\n   }\\n\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\n   private final List<MyTestCase> m_testCases;\\n\\n   public MyTestCase(List<MyTestCase> m_testCases) {\\n      m_testCases = m_testCases;\\n   }\\n\\n   @Override\\n   protected void setUp() throws Exception {\\n\\n      super.setUp();\\n         m_testCases.clear();\\n\\n         \/\/ Assign values here and then call super.tearDown()\\n         m_testCases.add(m_myClass);        \\n         \/\/ And then call super.tearDown()\\n         m_myClass.method1();       \\n         \/\/ And then call super.tearDown()\\n\\n         \/\/ Assign values here and then call super.tearDown()\\n         m_myClass.method2();       \\n         \/\/ And then call super.tearDown()\\n         m_myClass.method1();       \\n         \/\/ And then call super.tearDown()\\n\\n       }\\n\\n}\\n\\npublic class MyTestCase extends TestCase {\\n\\nprivate final List<MyTestCase> m_tests;\\n\\npublic MyTestCase(List<MyTestCase> tests) {\\n     m_tests = tests;\\n}\\n\\n\/\/Now when we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\n@Test(expected=\"A\")\\nvoid methodA() {\\n     ttest.methodA();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"B\")\\nvoid methodB() {\\n     ttest.methodB();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"C\")\\nvoid methodC() {\\n     ttest.methodC();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"D\")\\nvoid methodD() {\\n     ttest.methodD();\\n     assertTrue(true);\\n}\\n\\n@Test(expected=\"E\")\\nvoid methodE() {\\n     ttest.methodE();\\n     assertTrue(true);\\n}\\n\\n\\n\/\/And so on...\\n\\n\/\/Now when we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\n@Test(expected=\"A\")\\nvoid methodA() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"B\")\\nvoid methodB() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"C\")\\nvoid methodC() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"D\")\\nvoid methodD() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n@Test(expected=\"E\")\\nvoid methodE() throws Exception{\\n    System.out.println(ttest.getTests().get(0).get());\\n}\\n\\n\\n\/\/And so on...\\n\\n\/\/When we want to run our tests we can do so using \\n\/\/the following code:\\n\\nMyTestCase ttest=new MyTestCase(m_tests);\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\nttest.testMethod();\\n\\n\\n\/\/And so on...\\n%\\end{sloppypar}\\n%\\n\\nIn addition you can create your own subclasses of TestCase that extend from TestCase.\\index{TSTES!\\_extends}}\\nThe subclassing mechanism allows you to define your own specializations of TestCase methods that are not defined in TestCase itself.\\n\\nThe subclassing mechanism also allows you to define your own specializations of TestCase methods that are not defined in TestCase itself.\\n\\nFor example, suppose you have created your own subclass of TestCase called \\texttt{\\_Mock}. The \\texttt{\\_Mock} subclass has no methods defined in TestCase itself since it is only there as part of its superclass.\\n\\nYou may want your subclasses' methods in order to implement additional functionality that is not provided by either the superclass or its subclasses.\\n\\nTo achieve this, you can override any function defined in either the superclass or its subclasses.\\index{TSTES!\\_\\_super}}\\nThe overriding mechanism allows you to define your own specializations of function definitions in either the superclass or its subclasses.\\n\\nFor example, suppose you have created your own subclass of TestCase called \\texttt{\\_\\_Mock}. The \\_\\_Mock subclass has no methods defined in either its superclass or its subclasses.\\n\\nYou may want your subclasses' functions in order to implement additional functionality that is not provided by either its superclass or its subclasses.\\n\\nTo achieve this, you can override any function defined in either its superclass or its subclasses.\\n\\n\\nYou may also need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\nAccessors and mutators are useful if you want accessors and mutators that are specific only within your subclass.\\n\\nAccessors and mutators are also useful if you want accessors and mutators that are specific only within your subclass.\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''. The \\_\\_Mock'' subclass has no accessor functions defined since it is only there as part of its superclass.\\n\\nYou may need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\nAccessors and mutators are useful if you want accessors and mutators that are specific only within your subclass.\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''''''''''''. The \\_\\_Mock''''''''''''''''''''''''''''''''''''''''''''' '''''''''\\n\\nYou may need accessors and mutators (methods that specify what properties are read\/write\/updated).\\n\\n\\nFor example, suppose you have created your own subclass of TestCase called \\_\\_Mock''. The \\_\\_Mock'\\'s accessor functions were just added since they were not included within their superclass.\\n\\n\\nSince these accessor functions were not included within their superclass they would not be accessible from within their subclasses.\\n\\n\\nIn order for them to be accessible from within their subclasses they must be overridden.\\n\\n\\nBy overriding these accessor functions it is possible for them to be accessed from within their respective subclasses.\\n\\n\\nThis means that even though they were not included within their respective classes they now exist.\\n\\n\\nHowever, because they were not included within their respective classes they could not be accessed from within their respective classes.\\n\\n\\n\\n"}
{"File Name":"structurizr-python\/0003-python-3-6-only.md","Context":"## Context\\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n","Decision":"We make an early decision to only support Python 3.6 and above.\\n","tokens":38,"id":396,"Predictions":"## 1. Do not support f-strings.\\n## 2. Support f-strings.\\n## 3. Support f-strings.\\n## Decision\\n## 1. Do not support f-strings.\\n## 2. Support f-strings.\\n## 3. Support f-strings.\\n\\end{Verbatim}\\n\\end{frame}\\n\\n\\begin{frame}{Conditional Statements}\\n\\framesubtitle{Conditional Statements}\\n\\begin{itemize}\\n\\item \\textbf{if} and \\textbf{elif} statements are used to execute different code based on the value of a variable\\n\\item If the value of a variable is true, then the corresponding statement will be executed\\n\\item If the value of a variable is false, then the corresponding statement will not be executed\\n\\end{itemize}\\n\\begin{Verbatim}[fontsize=\\footnotesize]\\nif condition:\\n    # code to execute when condition is satisfied\\nelif condition:\\n    # code to execute when condition is not satisfied\\nelse:\\n    # code to execute when the condition is satisfied\\nelse:\\n    # code to execute when the condition is not satisfied\\n\\n# some statements here\\n\\nif name == 'Bob':\\n    print('Hello Bob')\\nelif name == 'Alice':\\n    print('Hello Alice')\\nelse:\\n    print('Hello Bob')\\nprint('You are Bob')\\n\\n# some statements here\\n\\n# some statements here\\n\\nif name == 'Bob':\\n    print('Hello Bob')\\nelif name == 'Alice':\\n    print('Hello Alice')\\nelse:\\n    print('Hello Bob')\\nprint('You are Bob')\\n\\n# some statements here\\n\\n# some statements here\\n\\nprint(\"Hello\")\\n```\\n\\end{Verbatim}\\n\\end{frame}\\n\\n% \\begin{frame}{If Statement}\\n% \\framesubtitle{If Statement}\\n% \\begin{itemize}\\n% \\item The basic IF statement of Python uses a ternary operator (||) that evaluates either true or false depending on whether its value is true or false.\\par\\n% \\n% \\n% \\n% \\n% \\n% \\n% \\item An example of an IF statement would be: if x > 0, y = x else z = y.\\par\\n% \\n% \\n% \\n% \\n% \\item The ternary operator can be expanded into a more complex IF statement by adding more conditions.\\par\\n% \\n% \\n%\\end{itemize}\\n%\\end{frame}\\n\\n\\begin{frame}{If Statement (cont'd)}\\n\\framesubtitle{\\texttt{\\_if\\_else\\_if\\_else}}\\n\\begin{Verbatim}[fontsize=\\footnotesize]\\ndef _if_else_if_else():\\n      if variable_1: return True\\n      if variable_2: return True\\n      if variable_n: return True\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if variable_1: return True\\n      if variable_2: return True\\n      if variable_n: return True\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if variable_n>0 and variable_2>0 and variable_n>0 and variable_3>0 and ...\\n      else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_3>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_n>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_n>0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_1,variable_2,variable_n):\\n      if varibale_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_else(variable_m<0 and ...\\n          else:\\n          return False\\n\\n\\ndef _if_else_if_none():\\n        #do nothing.\\n\\ndef _if_none():\\n        #do nothing.\\n\\ndef _not_none():\\n        #do nothing.\\n\\ndef _not_none():\\n        #do nothing.\\n\\n#some other statements here\\n\\n\\n\\n#print(\"You are Bob\")\\n\\n#some other statements here\\n\\n\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n\\n#print(\"You are Bob\")\\n```\\nGives output as follows:\\par\\nPython 3 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n\\n\\n\\n\\n\\n\\n\\\\nSo I'm sure you can see how this works.\\par\\nFor example:\\par\\n\\t\\t\\t\\t\\t\\t\\t\\\\n\\t\\t\\t\\t\\t\\t\\\\n\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\\\n\"Bob\"\\n\\end{Verbatim}\\n\\end{frame}\\n\\n"}
{"File Name":"structurizr-python\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":397,"Predictions":"## The version of the package is 4.0.1.\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n## ## Decision\\nWe need a simple way to manage our package version.\\n\\n\\n## The decision part is only required if the decision is not present in the context.\\n\\n\\end{verbatim}\\n\\n\\subsection{The \\texttt{Decision} class}\\n\\label{sec:decision}\\nThis class provides an object that encapsulates the decision logic.  It also provides some convenience methods for working with the decision object in different ways.\\n\\n\\subsubsection{Decision Object}\\nThe \\texttt{Decision} class is an encapsulation of the decision logic.  It can be used to create and manipulate decisions, and it can also be used as an interface between decisions and other objects.  The class also contains a constructor that takes no arguments, and which returns an instance of \\texttt{Decision}.  This constructor takes two arguments: \\texttt{Context} and \\texttt{DecisionType}.  The first argument is optional, and defaults to \\texttt{\\textless{}Context>\\textgreater{}}, which specifies the context in which this decision should be made.  The second argument specifies the type of decision that should be made.  There are four possible types:\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item {\\bf Default}: This is the default type of decision, which means that this specific instance will make this type of decision.\\n\\item {\\bf Left}: This is a right-hand-side (RHS) decision, meaning that this specific instance will make this type of RHS decision.\\n\\item {\\bf Right}: This is a left-hand-side (LHS) decision, meaning that this specific instance will make this type of RHS decision.\\n\\item {\\bf Final}: This is a final-type decision, meaning that this specific instance will make this type of final-type decision.\\n\\end{itemize}\\n\\nThese types are defined in Section~\\ref{sec:types}.  A default value for each type can be specified by passing its name as an argument to the constructor.\\n\\nThe class also provides some convenience methods for working with decisions.\\n\\nThe {\\tt get\\_decision()} method returns an instance of \\texttt{Decision}.  It takes one argument: \\texttt{\\_decisionType}.  If no type was supplied when calling this method, then it returns an empty object.\\n\\nThe {\\tt set\\_decision()} method sets the value of an existing instance's property using its property name as its argument (see Section~\\ref{sec:convenience}).  If no property name was supplied when calling this method, then it sets all properties using their names as their arguments (see Section~\\ref{sec:convenience}).  \\n\\nThe {\\tt set\\_decisionType()} method sets or gets the value of an existing instance's property using its property name as its argument (see Section~\\ref{sec:convenience}).  If no property name was supplied when calling this method, then it sets all properties using their names as their arguments (see Section~\\ref{sec:convenience}).  \\n\\nFinally, there are convenience methods for dealing with multiple decisions at once.  \\n\\nFor example:\\n%\\n% TODO: Add more convenience methods here...\\n%\\n{\\small\\n\\begingroup\\n\\begingroup\\n\\begingroup\\n\\begingroup\\n\\n% TODO: Add more convenience methods here...\\n%\\n{\\small\\n\\n%\\begin {verbatim}\\n%    def get_decision_type(self):\\n%        return self._decisionType\\n\\n%    def set_decision_type(self, _decision_type):\\n%        self._decisionType = _decision_type\\n\\n%    def get_decision_context(self):\\n%        return self._context\\n\\n%    def set_decision_context(self, _context):\\n%        self._context = _context\\n\\n%\\end {verbatim}\\n%\\n%\\n\\n{\\small\\n\\n\\begingroup\\n\\n%\\begin {verbatim}\\n%    def get_decision_context(self):\\n%        return self._context\\n\\n%    def set_decision_context(self, _context):\\n%        self._context = _context\\n\\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\n%\\endgroup}\\n\\nYou can use these convenience methods like so:\\n\\n{\\small\\n\\begingroup\\n\\begingroup\\n\\n%% TODO: Add more convenience methods here...\\n%\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_context(self):\\n%%         return self._context\\n \\n%%     def set_decision_context(self, _context):\\n%%         self._context = _context\\n \\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\n%\\endgroup}\\n\\nYou can also use these convenience methods like so:\\n\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_type(self):\\n%%         return self._decisionType\\n \\n%%     def set_decision_type(self, _decision_type):\\n%%         self._decisionType = _decision_type\\n \\n%\\end {verbatim}\\n\\n%\\endgroup}\\n\\nYou can use these convenience methods like so:\\n\\n{\\small\\n \\n%\\begin {verbatim}\\n%%     def get_decision_context(self):\\n%%         return self._context\\n \\n%%     def set_decision_context(self, _context):\\n%%         self._context = _context\\n \\n%\\end {verbatim}\\n\\n\\n%\\n%\\n\\n{\\small\\n  \\n\\begingroup\\n  \\n  \\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n}\\n\\n\\n \\n\\n%\\n\\n}\\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n \\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n \\n \\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"Predictions":"## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n## Decision\\n## We need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\\n\\n\\begin{Verbatim}[commandchars=\\\\\\{\\}]\\n# Version Control System (VCS)\\n# https:\/\/en.wikipedia.org\/wiki\/VCS\\n\\n$ git init --bare\\n\\n$ git add .\\n\\n$ git commit -m 'Initial commit'\\n\\n$ git remote add origin <URL>\\n\\n$ git push origin master\\n\\n$ git pull origin master\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools init --bare --branch master --url <URL>\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n# Command line tool for working on the repository.\\n# https:\/\/github.com\/idevtools\/idevtools-cli\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\n$ idevtools merge master --branch master --url <URL>\\n\\end{Verbatim}\\n\\n\\item[3] \\textbf{What is the primary purpose of this software?} \\hfill \\break \\\\\\nThe primary purpose of this software is:\\n\\n\\begin{itemize}\\n    \\item To organize your project files into directories.\\n    \\item To manage your project's history.\\n    \\item To share your project's history with others.\\n    \\item To make it easier for other people who work on your project.\\n\\end{itemize}\\n\\n\\end{enumerate}\\n\\n\\section*{6.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{7.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{8.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{9.1}\\nThe software will be used as part of my dissertation research.\\n\\n\\section*{10.1}\\nThe software will be used as part of my dissertation research.\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX A - WORKFLOW DIAGRAMS\\n%----------------------------------------------------------------------------------------\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX B - PROJECT DESCRIPTIONS AND REQUIREMENTS (NOT REQUIRED)\\n%----------------------------------------------------------------------------------------\\n\\n%\\begin{figure}[h]\\n%\\centering\\n%\\includegraphics[width=0.5\\textwidth]{images\/project_diagram.png}\\n%\\caption{}\\n%\\label{}\\n%\\end{figure}\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDIX C - PROPOSED DEVELOPMENT METHODOLOGY (NOT REQUIRED)\\n%----------------------------------------------------------------------------------------\\n\\n%\\begin{figure}[h]\\n%\\centering\\n%\\includegraphics[width=0.5\\textwidth]{images\/proposed_development_methodology.png}\\n%\\caption{}\\n%\\label{}\\n%\\end{figure}\\n\\n\\n\\n"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"Predictions":"## Decision\\nThe decision should provide a JSON representation of the workspace.\\n\\n## Decision\\nIf the decision is successful, the JSON representation is returned.\\n\\n## Decision\\nIf the decision fails, an error message is displayed.\\n\\n## Decision\\nThe result of this decision is a JSON representation of the workspace.\\n\\n## Decision\\nThe decision should return a 200 response code and a JSON representation of the workspace.\\n\\n## Decision\\nIf the decision succeeds, the specified number of bytes are returned as a JSON representation of the workspace.\\n\\n## Decision\\nIf the decision fails, an error message is displayed and an error code is returned.\\n\\n## Decision\\nIf an error occurs, this failure message will be displayed. The error code will be returned to indicate which failure was encountered.\\n\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n# TODO: This should probably be merged into #1144.\\n\\ndef parse_response(response):\\n    \"\"\"Parse response from server\"\"\"\\n    if response.status == 200:\\n        response_text = response.text()\\n        return response_text\\n\\ndef parse_error(response):\\n    \"\"\"Parse error from server\"\"\"\\n    if response.status == 400:\\n        return \"Bad Request\"\\n\\ndef parse_error_response(response):\\n    \"\"\"Parse error from server\"\"\"\\n    if response.status == 400:\\n        return \"Bad Request\"\\n\\ndef parse_response_json(response):\\n    \"\"\"Parse response from server\"\"\"\\n    if response.status == 200:\\n        json_response = json.loads(response.text())\\n        return json_response\\n\\ndef parse_error_response(response):\\n    \"\"\"Parse error from server\"\"\"\\n    if response.status == 400:\\n        return \"Bad Request\"\\n\\ndef parse_response_json(json_response):\\n    \"\"\"Parse response from server\"\"\"\\n    if json_response.status == 200:\\n        json_response = json.loads(json_response)\\n        result = {\\n            \"message\": json_response.message,\\n            \"errorCode\": json_response.errorCode,\\n            \"statusCode\": json_response.statusCode,\\n            \"errorDescription\": json_response.errorDescription,\\n            \"errorInfo\": {\\n                \"errorMessage\": json_response.errorMessage,\\n                \"errorCode\": json_response.errorCode,\\n                \"code\": json_response.code,\\n                \"reasonPhrase\": json_response.reasonPhrase\\n            }\\n        }\\n        return result\\n\\ndef main():\\n    parser = ParseResponseParser()\\n    \\n    try:\\n        \\n        main_parser.parse_request()\\n        \\n        main_parser.parse_error()\\n        \\n        main_parser.parse_request()\\n\\nif __name__ == \"__main__\":\\n    \\nmain()\\n\\nYou can\u2019t perform that action at this time."}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"Predictions":"## \\Notoes\\nThe following tools are recommended by the Software Engineering Institute:\\n## \\n\\n\\n\\n\\n\\n\\n\\n## \\n\\n\\end{verbatim}\\n\\n\\subsection{Software Engineering Institute}\\nThe Software Engineering Institute (SEI) is a nonprofit organization that provides training, education, and consulting services for software engineering professionals and organizations. SEI's mission is to advance the software engineering profession through education, research, and standards development. Its five core competencies are:\\n\\begin{itemize}\\n\\item People: Providing training and education for the software engineering profession.\\n\\item Processes: Providing training and education for software engineering processes.\\n\\item Products: Providing services to support the software engineering profession.\\n\\item Tools: Providing tools that facilitate the implementation of software engineering processes.\\n\\end{itemize}\\nSEI provides training in its core competencies through its Center for Software Engineering Education (CSEE) and its Center for Software Excellence (CSE). The CSEE offers a wide range of courses on various aspects of software development. The CSE offers a wide range of courses on various aspects of software quality.\\n\\nIn addition to providing training in its core competencies, SEI also provides consulting services to help organizations improve their software development practices.\\n\\nSEI has developed a set of standards that are used by organizations around the world. These standards are called \"best practices.\" In addition to providing training in these best practices, SEI also provides consulting services to help organizations implement these best practices.\\n\\nSEI's standards include:\\n\\n\\begin{itemize}\\n\\item ISO\/IEC 29119:2009 Software Engineering - Guidelines for Process Improvement\\n\\item ISO\/IEC 29118:2009 Software Engineering - Guidelines for Software Process Improvement\\n\\end{itemize}\\n\\nThe ISO\/IEC 29119 standard is designed to help organizations improve their processes. Organizations can use this standard as a reference when they want to improve their processes.\\n\\nThe ISO\/IEC 29118 standard is designed to help organizations improve their products. Organizations can use this standard as a reference when they want to improve their products.\\n\\nSEI's website contains a list of all of its current standards:\\n\\nhttps:\/\/www.sei.cmu.edu\/\\n\\nYou can find out more about SEI's current standards by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/csee\/\\n\\nhttps:\/\/www.sei.cmu.edu\/cse\/\\n\\nYou can find out more about SEI's current projects by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/projects\/\\n\\nYou can find out more about SEI's current publications by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/publications\/\\n\\nYou can find out more about SEI's current conferences by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/conferences\/\\n\\nYou can find out more about SEI's current workshops by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/workshops\/\\n\\nYou can find out more about SEI's current events by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/events\/\\n\\nYou can find out more about SEI's past events by visiting:\\n\\nhttps:\/\/www.sei.cmu.edu\/events\/past-events\/\\n"}
{"File Name":"structurizr-python\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":401,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"amf-core\/0003-flattened-json-ld-parsing-emission-for-self-encoded-dialects.md","Context":"## Context\\nSelf-encoded dialects define dialect instances which share the same ID between the instance document and the dialect\\ndomain element encoded in such document. This allows the document and the encoded dialect domain element to be treated\\nas the same resource.\\nOn the other hand Flattened JSON-LD emission only renders one node for each ID.\\n","Decision":"We merge both nodes (the document and the encoded domain element) and emit the single merged node with the shared ID.\\nThe merged node contains both the properties from the document and the encoded domain element.\\nWhen parsing the resulting flattened JSON-LD we parse the merged node twice: first as a domain element and then as a\\ndocument. Parsing the properties from the domain element ignores the properties from the document and vice-versa.\\n","tokens":71,"id":402,"Predictions":"## 0: No dialect is defined\\n\\n## 1: A dialect is defined\\n\\n## 2: A dialect is defined with a different ID than the one defined in the document\\n\\n## 3: A dialect is defined with a different ID than the one defined in the document and with a different ID\\n\\n## than the one defined in the document.\\n\\n## 4: A dialect is defined with a different ID than the one defined in the document and with a different ID\\n\\nthan the one defined in the document and with a different ID than the one defined in the document.\\n\\n## 5: A dialect is not specified\\n\\n## 6: The specified dialect does not exist or has been deleted from this domain*\\n\\n# Note that this decision record can be used to generate an additional decision record for each of these cases:\\n# - The specified dialect does not exist or has been deleted from this domain*\\n# - The specified dialect exists but has been deleted from this domain* by an upgrade to Flattened JSON-LD version 1.0.0 or later\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.*\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.*\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.*\\ndomain element must be given as well as an optional last-draft version of this domain element, so that we can distinguish between old versions of this domain element and new versions of it.*\\ndomain elements are allowed to have any number (including none) on their \\ldots{} attributes, but they cannot have any \\ldots{} attribute on their \\ldots{} attributes.\\ndomain elements are allowed to have any number (including none) on their \\ldots{} attributes, but they cannot have any \\ldots{} attribute on their \\ldots{} attributes.*\\ndomain elements are allowed to have any number (including none) on their \\ldots{} attributes, but they cannot have any \\ldots{} attribute on their \\ldots{} attributes.*\\ndomain elements are allowed to have any number (including none) on their \\ldots{} attributes, but they cannot have any \\ldots{} attribute on their \\ldots{} attributes.*\\ndomain elements are allowed to have any number (including none) on their \\ldots{} attributes, but they cannot have any \\ldots{} attribute on their \\ldots{} attributes.*\\ndomain elements are allowed to have any number (including none) on their \\ldot{}, however they cannot have any *attribute* on them.\\ndomain elements are allowed to have any number (including none) on their *attribute* properties. This means that you may not specify *any* attribute for your *domain* or *domain* instance property. This also means you may not specify *any* attribute for your *domain* or *domain* instance property. This also means you may not specify *any* attribute for your *domain* or *domain* instance property. This also means you may not specify *any* attribute for your *domain* or *domain* instance property. This also means you may not specify *any* attribute for your *domain* or *domain* instance property.\\n## Decision Content\\nFlattened JSON-LD only renders documents which contain at least one node which contains a content field which matches exactly the content field found at node level in the source document.\\nThe content field value must match exactly what was found at node level in both source documents.\\nIf no content field exists then no content field value will be rendered.\\nThe content field value must match exactly what was found at node level in both source documents.\\nIf no content field exists then no content field value will be rendered.\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n"}
{"File Name":"amf-core\/0005-expose-non-scalajs-types-in-amf-client-remote-content.md","Context":"## Context\\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\\nit from export.\\n","Decision":"- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\\n- Add the `toString()` method in `Content` that returns the content in `stream`\\n","tokens":67,"id":403,"Predictions":"## \\n\\nThe ScalaJSTypings plugin exports the following types to the Scala interface:\\n\\n\\nScalaType[\\_String]: String\\nScalaType[\\_Number]: Int\\nScalaType[\\_Boolean]: Boolean\\nScalaType[\\_Date]: Date\\nScalaType[\\_Time]: Time\\nScalaType[\\_Double]: Double\\nScalaType[\\_Float]: Float\\nScalaType[\\_StringMap]: Map[String, String]\\nScalaType[\\_LongMap]: Map[String, Long]\\nScalaType[\\_DoubleMap]: Map[Double, Double]\\n\\nclass ScalaJSTypings extends Serializable\\nclass ScalaJSTypings extends Serializable\\nclass ScalaJSTypings extends Serializable\\nclass ScalaJSTypings extends Serializable\\nclass ScalaJSTypings extends Serializable\\nclass ScalaJSTypings extends Serializable\\nclass JavaTerm:\\nEnum[java.lang.String]():\\nscala.reflect.internal.JavaMethod[java.lang.String](java.lang.String)()\\nclass JavaTerm:\\nEnum[java.lang.String]():\\nscala.reflect.internal.JavaMethod[java.lang.String](java.lang.String)()\\nclass JavaTerm:\\nEnum[java.lang.String]():\\nscala.reflect.internal.JavaMethod[java.lang.String](java.lang.String)()\\nclass JavaTerm:\\nEnum[java.lang.String]():\\nscala.reflect.internal.JavaMethod[java.lang.String](java.lang.String)()\\nclass JavaTerm:\\nEnum[java.lang.String]():\\nscala.reflect.internal.JavaMethod[java.lang.String](java.lang.String)()\\nclass JavaTerm:\\nEnum[java.util.List]():\\nThe `Content.stream` field is an instance of `org.scalajs.runtime.AbstractHtmlStream`, which is a subclass of `org.scalajs.runtime.CharStream`. The original implementation of `Content.stream` was implemented as a singleton method in ScalaJS. However, this implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original implementation of `Content.toString` was implemented as a singleton method in AbstractHtmlStream. However, this implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original implementation of `Content.toString` was implemented as a singleton method in AbstractHtmlStream. This implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original implementation of `Content.toString` was implemented as a singleton method in AbstractHtmlStream. This implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original implementation of `Content.toString` was implemented as a singleton method in AbstractHtmlStream. This implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original implementation of Content.toString was implemented as a singleton method in AbstractHtmlStream. This implementation can be replaced by a more efficient implementation by using the `isSingleton` method in AbstractHtmlStream.\\nThe original Implementation of Content.toString was implemented as a singleton method in Abstract Html Stream. This Implementation can be replaced with an efficient Implementation by using the is Singleton Method from abstract Html stream class.\\nThe original Implementation of Content.toString was implemented as a singleton method in abstract Html stream class.This class has no public constructor and no public methods.\\n## Decision\\n## \\begin{itemize}\\n##   \\item 1: The content stream is not exported to scala.\\n%%   \\item 2: The content stream is exported to scala.\\n%%   \\item 3: The content stream is not exported to scala.\\n%%   \\item 4: The content stream is exported to scala.\\n%%   \\item 5: The content stream is not exported to scala.\\n%%   \\item 6: The content stream is exported to scala.\\n%%   \\item 7: The content stream is not exported to scala.\\n%%   \\item 8: The content stream is not exported to scala.\\n%%   \\item 9: The content stream is not exported to scala.\\n%%   \\item 10: The content stream is not exported to scala.\\n%%   \\item 11: The content stream is not exported to scala.\\n%\\end{enumerate}\\n%\\end{decision}\\n\\n%===============================================================================\\n%===============================================================================\\n%===============================================================================\\n%===============================================================================\\n%===============================================================================\\n%===============================================================================\\n%===============================================================================\\n%\\n%\\begin{figure}[t]\\n%\\begin{center}\\n%\\includegraphics[scale=0.2]{figs\/decision.pdf}\\n%\\caption{Decision}\\n%\\label{fig:decision}\\n%\\end{center}\\n%\\end{figure}\\n\\n"}
{"File Name":"amf-core\/0008-field-deprecation.md","Context":"## Context\\nWe need to start deprecating old fields and have no mechanism to do so.\\n","Decision":"We will start deprecating fields. Getter\/setter methods in Scala and Platform classes will be deprecated also. Deprecated fields will still be set alongside new fields until these are removed in the next major version.\\nHow to deprecate a field:\\n1. Deprecate field definition in model class\\n1. Deprecate field definition with `@deprecated` annotation\\n2. Deprecate field definition with the `deprecated=true` paramter from the `Field` class\\n3. Annotate the `fields` value assignment with `@silent(\"deprecated\")` annotation to avoid compilation errors from deprecated fields\\n2. Deprecate getter\/setter methods in Scala and Platform classes with `@deprecated` annotation\\n3. Update usages of getter\/setter methods to use both legacy and new fields (with the `@silent(\"deprecated\")` annotation)\\n","tokens":21,"id":404,"Predictions":"## The old fields are deprecated. We need to have a mechanism to deprecate them.\\n\\n## Decision\\n## We need to have a mechanism to deprecate them.\\n\\n## Decision\\n## We need to have a mechanism to deprecate them.\\n\\n\\n\\end{verbatim}\\n\\caption{Decision Record for the \\texttt{room} field.}\\n\\label{fig:decisions}\\n\\end{figure}\\n\\nThe user can use the \\texttt{decide} and \\texttt{deprecate} functions of the \\texttt{decisions} library to determine whether a given field is deprecated or not.\\n\\n\\subsection{Decision Functions}\\n\\nThe \\texttt{decide} function is used to determine whether a given field is deprecated or not. The \\texttt{\\_depreceatable} flag is used to indicate if the field is deprecated or not. The function returns 1 if the field is deprecated, and 0 if it isn't.\\n\\n\\begin{verbatim}\\ndef decide():\\n    return decision_function()\\n\\end{verbatim}\\n\\nThe \\texttt{\\_depreceatable} flag can be used in conjunction with other flags in order to indicate whether a given field is deprecated or not. For example, if you want to know whether an attribute in your model has been deprecated, you can use the \\texttt{\\_deprecated} flag together with the \\texttt{\\_depreceatable} flag as shown below:\\n\\n\\begin{verbatim}\\ndef decide():\\n    return decision_function()\\n    def check_deprecated_attribute(attr):\\n        return attr.deprecated or attr._deprecated\\n\\n    if check_deprecated_attribute('room') == 1:\\n        print('Room has been depreciated.')\\n    else:\\n        print('Room has not been depreciated.')\\n\\ndef check_deprecated_attr(attr):\\n    return attr.deprecated or attr._deprecated\\n\\ndef check_deprecated_attr(attr):\\n    return attr.deprecated or attr._deprecated\\n\\nif __name__ == '__main__':\\n    try:\\n        room = MyModel()\\n        room.decide()\\n    except DecidedError as e:\\n        print(f'Decided error: {e}')\\n    else:\\n        print(f'Decided by {check_deprecated_attr(room)}')\\n        print(f'Decided by {check_deprecated_attribute(room)}')\\n\\end{verbatim}\\n\\nIn this example, we call the function in order to determine whether an attribute called `room' has been deprecated or not. If we call it on an object called `MyModel', we will get back a DecidedError object that indicates that room has been depreciated.\\n\\nIf we call it on an object called `MyModel', we will get back a DecidedError object that indicates that room has been depreciated.\\n\\nIf we want to know whether an attribute in our model has been deprecated, you can use the \\texttt{\\_deprecated} flag together with the \\texttt{\\_depreceatable} flag as shown below:\\n\\n\\begin{verbatim}\\ndef decide():\\n    return decision_function()\\n    def check_deprecated_attribute(attr):\\n        return attr.deprecated or attr._deprecated\\n\\n    if check_deprecated_attribute('room') == 1:\\n        print('Room has been depreciated.')\\n    else:\\n        print('Room has not been depreciated.')\\n\\ndef check_deprecated_attr(attr):\\n    return attr.deprecated or attr._deprecated\\n\\ndef check_deprecated_attr(attr):\\n    return attr.deprecated or attr._deprecated\\n\\nif __name__ == '__main__':\\n    try:\\n        room = MyModel()\\n        room.decide()\\n    except DecidedError as e:\\n        print(f'Decided error: {e}')\\n    else:\\n        print(f'Decided by {check_deprecated_attr(room)}')\\n        print(f'Decided by {check_deprecated_attribute(room)}')\\n\\end{verbatim}\\n\\nIn this example, we call the function in order to determine whether an attribute called `room' has been deprecated or not. If we call it on an object called `MyModel', we will get back a DecidedError object that indicates that room has been depreciated.\\n\\nIf we want to know whether an attribute in our model has been deprecated, you can use the \\texttt{\\_deprecated} flag together with the \\texttt{\\_depreceatable} flag as shown below:\\n\\nYou can also use other flags such as those provided by Python's built-in library for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!='). For instance, you can use the built-in library's built-in function for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!=') as follows:\\n\\nIf you want to know which of two objects are equal, you can use either one of these functions:\\n\\nIf you want to know which of two objects are different, you can use either one of these functions:\\n\\nYou can also use other flags such as those provided by Python's built-in library for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!='). For instance, you can use the built-in library's built-in function for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!=') as follows:\\n\\nIf you want to know which of two objects are equal, you can use either one of these functions:\\n\\nYou can also use other flags such as those provided by Python's built-in library for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!='). For instance, you can use the built-in library's built-in function for deciding which of two objects are equal (e.g., `==') and which of two objects are different (e.g., `!=') as follows:\\n\\nTo see what all possible flags available for determining whether an object is equal or different from another object using Python's built-in libraries, please refer here.\\n\\nThe following code snippet shows how all possible flags available for determining whether an object is equal or different from another object using Python's built-in libraries looks like when run in Python 3.x.\\n\\nSuppose that we have an instance named ``MyObject'' and another instance named ``OtherObject''. Both ``MyObject'' and ``OtherObject'' have attributes named ``name'' and ``age''. Suppose further that both ``MyObject'' and ``OtherObject'' have attributes named ``age'' but they do not have attributes named ``name''. Let us say further that both ``MyObject'' and ``OtherObject'' do have attributes named ``age'', but they do not have attributes named ``name''. Now let us say further that both ``MyObject'' and ``OtherObject'' do have attributes named ``age'', but they do have attributes named ``name''. Finally let us say further that both instances share several common properties including their age.\\n\\nNow suppose that there exists some code written inside some class called MyClass where some methods defined inside MyClass method body look like this:\\n\\nclass MyClass(object):\\n   def __init__(self):\\n      self.name = \"Bob\"\\n      self.age = 30\\n   def name(self):\\n      return self.name\\n   def age(self):\\n      return self.age\\n\\n\\nNow suppose further that there exists some code written inside some class called OtherClass where some methods defined inside OtherClass method body look like this:\\n\\nclass OtherClass(object):\\n   def __init__(self):\\n      self.name = \"Jane\"\\n      self.age = 40\\n   def name(self):\\n      return self.name\\n   def age(self):\\n      return self.age\\n\\n\\nNow suppose further that there exists some code written inside some class called Person where some methods defined inside Person method body look like this:\\n\\nclass Person(object):\\n   def __init__(self):\\n      self.name = \"Bob\"\\n      self.age = 30\\n   def name(self): \\n      return 'Bob'\\n   def age(self): \\n      return 'Bob'\\n\\nNow suppose further that there exists some code written inside SomeMethod() where SomeMethod() method body looks like this:\\n\\ndef SomeMethod():\\n   # Code here...\\n\\nNow suppose further that there exists some code written inside SomeMethod() where SomeMethod() method body looks like this:\\n\\nclass MyClass(object):\\n   # Code here...\\n\\nNow suppose further that there exists some code written inside SomeMethod() where SomeMethod() method body looks like this:\\n\\nclass OtherClass(object):\\n\\nNow suppose further that there exists some code written inside SomeMethod() where SomeMethod() method body looks like this:\\n\\nclass Person(object):\\n\\nNow suppose further that there exists some code written inside SomeMethod() where SomeMethod() method body looks like this:\\n\\nclass AnotherPerson(object):\\n\\nFinally let us say further that Both instances share several common properties including their age.\\n\\nSuppose now I write a program such as following in Python3.x using PyCharm IDE: \\n\\nimport sys\\nimport os.path \\n\\nfrom functools import wraps\\n\\nfrom datetime import datetime\\n\\n\\n# This class defines methods related with creating new instances.\\n# It defines methods related with creating new instances.\\nclass MyClass(object):\\n\\n   # Create new instance.\\n   # Create new instance.\\n   def __init__(self,name= None,name_age= None,name_age_age=None,name_age_age_age=None,name_age_age_age_age=None,name_age_age_age_age=None,name_age_age_age_age=None,name_name= None,\\n               name_name_date= None,\\n               name_name_date_month= None,\\n               name_name_date_year= None,\\n               name_name_date_year_month= None,\\n               name_name_date_year_month_day= None,\\n               name_name_date_year_month_day_month= None,\\n               name_name_date_year_month_day_day_month= None,\\n               name_name_date_dayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayyearmonthdayweekdayweekdayweekdayweekdayweekdayweekdayweekdayweekdayweekdayweekdayweekday)\\n         :\\n            pass\\n\\n            # Create new instances.\\n            # Create new instances.\\n            # Create new instances.\\n            pass\\n\\n\\n# This class defines methods related with creating new instances.\\n# It defines methods related with creating new instances.\\nclass OtherClass(object):\\n\\n     # Create new instance.\\n     # Create new instance.\\n     pass\\n\\n\\n# This class defines methods related with creating new instances.\\n# It defines methods related with creating new instances.\\nclass Person(object):\\n\\n     # Create new instances.\\n     # Create new instances.\\n     pass\\n\\n\\n# This class defines methods related with creating new instances.\\n# It defines methods related with creating new instances.\\n\\n\\ndef create_new_instance():\\n     try: \\n         MyObj = MyClass(\"Bob\", \"Jane\")\\n         MyObj.name=\"Bob\"\\n         MyObj.age=\"30\"\\n         MyObj.name=\"Jane\"\\n     except DecidedError as e: \\n         pass\\n\\n\\nnew_instance1 = create_new_instance()\\n\\n\\nif __name__ == '__main__':\\n     try: \\n         create_new_instance()\\n     except DecidedError as e: \\n         pass\\n\\n\\nprint(create_new_instance())\\n```\\nOutput :\\n\\nHere I am calling my program via PyCharm IDE using run button. You may notice following things about my program output when running it via PyCharm IDE:\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via PyCharm IDE via run button.\\n\\n\\nI am passing initial values ('Bob', 'Jane', etc.) when calling my program via Py"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"Predictions":"## The custom AMF validator needs to show the location of the file from which each error was generated.\\n## Given the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n## Result\\n## The custom AMF validator needs to show the location of the file from which each error was generated.\\n## Given the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n"}
{"File Name":"amf-core\/0004-domainelementmodel-isexternallink-field.md","Context":"## Context\\nLinking nodes between different graphs is a feature not provided by AMF\/AML. This feature is required by some adopters to:\\n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)\\n* Link nodes between different parsed dialect instances (ANG)\\n","Decision":"Add a boolean field to DomainElementModel called IsExternalLink that marks that a domain element is a reference to a domain element defined in another graph.\\n","tokens":65,"id":406,"Predictions":"## Decision: The decision is to link the nodes between the two graphs. The decision is made by the GraphNodeLinkingHandler.\\n## Decision: The decision is to link the nodes between the two graphs. The decision is made by the GraphNodeLinkingHandler.\\n## Decision: The decision is to link the nodes between the two graphs. The decision is made by the GraphNodeLinkingHandler.\\n## Decision: The decision is to link the nodes between the two graphs. The decision is made by the GraphNodeLinkingHandler.\\n## Decision: Linking of nodes between a parsed API specification graph with a parsed dialect instance graph\\n## Decision: Linking of nodes between a parsed API specification graph with a parsed dialect instance graph, and a parsed dialect instance graph with a parsed API specification graph\\n## Decision: Linking of nodes between a parsed API specification graph with a parsed dialect instance graph, and a parsed dialect instance graph with an API specification diagram\\n## Decision: Linking of nodes between a parsed dialect instance graph and an API specification diagram\\n## Decision: Linking of nodes between an API specification diagram and an API specification diagram\\n## Decision: Linking of nodes between an API specification diagram and an API specification diagram\\n## Decision:\\n#1  If no node in this DAG has been linked to any node in this DAG, then this node should be linked to all other DAGs that have been linked to it.\\n#2  If no node in this DAG has been linked to any node in this DAG, then this node should be linked to all other DAGs that have been linked to it.\\n\\n[[Category:Decision]]"}
{"File Name":"amf-core\/0006-custom-domain-properties-json-ld-rendering.md","Context":"## Context\\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\\nmay or may not include their own semantic information (when used with semantic extensions).\\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\\nmodel.\\n","Decision":"We will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\\ngenerated by a dynamic model while still representing the model statically.\\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\\nrepresent the definition and value of the domain extension.\\nThis in the \"traditional & static\" parts of the model would be rendered like this (summary):\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myDomainExtension\",\\n\"definedBy\": {\\n\"@id\": \"myCustomDomainProperty\"\\n},\\n\"extension\": {\\n\"@id\": \"myDataNode\"\\n}\\n}\\n]\\n}\\n```\\nInstead a \"dynamic\" model is simulated\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"myCustomDomainProperty\": {\\n\"@id\": \"myDataNode\"\\n},\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myCustomDomainProperty\"\\n}\\n]\\n}\\n```\\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\\nwhich properties of the serialized node \"are dynamic\".\\n","tokens":89,"id":407,"Predictions":"## Decision\\n\\begin{itemize}\\n## \\item The ADR is a valid JSON-LD document. The domain is specified by the `*Model.scala` object and the annotations are\\nspecified by the `*Annotation.scala` object.\\n\\end{itemize}\\n## Context\\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\\nmay or may not include their own semantic information (when used with semantic extensions).\\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\\nmodel.\\n\\n## Decision\\n## Decision\\n\\begin{itemize}\\n## \\item The ADR is a valid JSON-LD document. The domain is specified by the `*Model.scala` object and the annotations are\\nspecified by the `*Annotation.scala` object.\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"amf-core\/0007-no-spec-mediatypes-in-amf-operations.md","Context":"## Context\\nOn AMF 5 Beta's first iteration \"domain+syntax\" mediatypes like \"application\/swagger20+yaml\" were\\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\\nThis is controversial as:\\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\\ncouldn't be parsed.\\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\\nalthough the configuration they used already specified their intended domain and syntax.\\n","Decision":"Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\\n","tokens":165,"id":408,"Predictions":"## Decision\\nThe decision is to continue with the current implementation. The existing mediatypes\\nare not deprecated, but will be removed in the next release. The new mediatypes are\\nexpected to be more comparable to the existing ones and will make it easier for clients to\\ncreate new configuration units.\\n\\n## Decision\\n## Decision\\nThe decision is to continue with the current implementation. The existing mediatypes\\nare not deprecated, but will be removed in the next release. The new mediatypes are\\nexpected to be more comparable to the existing ones and will make it easier for clients to\\ncreate new configuration units.\\n\\n## Decision\\n## Decision\\nThe decision is to continue with the current implementation. The existing mediatypes\\nare not deprecated, but will be removed in the next release. The new mediatypes are\\nexpected to be more comparable to the existing ones and will make it easier for clients to\\ncreate new configuration units.\\n\\n## Decision\\n## Decision\\nThe decision is to continue with the current implementation. The existing mediatypes\\nare not deprecated, but will be removed in the next release. The new mediatypes are\\nexpected to be more comparable to the existing ones and will make it easier for clients to\\ncreate new configuration units.\\n\\n## Decision\\n## Decision\\nThe decision is to continue with the current implementation. The existing mediatypes\\nare not deprecated, but will be removed in the next release. The new mediatypes are\\nexpected to be more comparable to the existing ones and will make it easier for clients to\\ncreate new configuration units.\\n"}
{"File Name":"amf-core\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":409,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"amf-core\/0002-extracttolink-in-json-ld-expanded-emission.md","Context":"## Context\\nExpanded JSON-LD emission embeds node links. This means that whenever a node references another node, instead of using a\\nJSON-LD link we render the referenced node in place. This is because the advantage of the JSON-LD expanded emission is\\nto allow consumer applications (e.g. API Console) to consume JSON-LD as regular JSON; and for that we cannot use JSON-LD\\nlinks.\\n**The problem**: Embedding nodes causes the resulting JSON-LD to be very big (because the same node is rendered many times).\\n_Note: Flattened JSON-LD emission does not suffer this problem because it only renders each node once and then uses\\nJSON-LD links for every node reference._\\n","Decision":"`extractToLink` is a logic we developed for the expanded JSON-LD emission that introduces _AMF links_ (as opposed to\\nJSON-LD links) for some node references. Specifically, it introduces an AMF link and extracts the link target to a\\n_declaration_.\\n_AMF links_ are only available for `Linkable` domain elements, so we use it in only for certain elements. These links\\nare part of the model as opposed to JSON-LD links which are simple graph references.\\n","tokens":153,"id":410,"Predictions":"## Decision\\nThe decision is to render the node in place, even though it is a reference to a node.\\n\\n\\n## Decision\\n## Decision\\nThe decision is to render the node in place, even though it is a reference to a node.\\n```\\n\\nOP 2019-05-09: I found the answer. It was because I was using `@` in the JSON-LD. \\n"}
{"File Name":"documentation\/0003-transfere-hash-in-jwt-claim.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the json data model.\\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\\nThe validity of the hash must be verified.\\n## Decision Drivers <!-- optional -->\\n* JWT should be used\\n","Decision":"* JWT should be used\\nChosen option: \"Transfer hash in JWT Claim\", because it's the only option when using JWT.\\n### Positive Consequences <!-- optional -->\\n* multiple hashes for different json documents can be added in one JWT\\n","tokens":93,"id":411,"Predictions":"## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision\\n## Decision Drivers\\n\\n## Decision\\n\\n\\end{lstlisting}\\n\\section{Decision}\\n\\subsection{Decision Driver-1}\\nThe decision driver has two inputs: the json data and the hash. The hash is generated by the software using the sha256 function. The hash must be transferred to the authorization server. The validity of the hash must be verified.\\n\\subsection{Decision Driver-2}\\nThe decision driver has two inputs: the json data and the hash. The hash is generated by the software using the sha256 function. The hash must be transferred to the authorization server. The validity of the hash must be verified.\\n\\subsection{Decision Driver-3}\\nThe decision driver has two inputs: the json data and the hash. The hash is generated by the software using the sha256 function. The hash must be transferred to the authorization server. The validity of the hash must be verified.\\n\\subsection{Decision Driver-4}\\nThe decision driver has two inputs: the json data and a hashed version of it. The hashed version is created by using an algorithm called base64url encoding.\\nThe valid and invalid hashes must be compared.\\nIf both hashes are valid, then we can send a response with a 200 status code.\\nOtherwise, if one of them is invalid, then we have to verify that it really came from our server.\\nIf both hashes are invalid, then we have to generate a new one.\\nWe can use a library called jwt for this purpose.\\nWe can use jwt library to generate an access token for our application and send it to our client.\\nWe can check if this token is valid or not using jwt.verify() method, which takes an access token as input and returns a boolean value indicating if it was successfully verified or not.\\nIf this token was successfully verified, then we can send a response with a 200 status code,\\nOtherwise, if this token was not successfully verified, then we have to generate another one.\\nThe verification process will take some time because it involves checking whether both tokens are valid or not.\\n"}
{"File Name":"documentation\/0001-use-json-web-tokens.md","Context":"## Context and Problem Statement\\nExternal services must authorize web clients to the WebSocket API.\\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\\nOnly little data should be stored for an open WebSocket connection.\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n","Decision":"* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\nChosen option: \"JWT from pre-shared keys\", because the WebSocket API is loosely coupled and it is well supported on many platforms.\\n### Positive Consequences\\n* Simple to implement\\n* Authorization data can be send in a portable and verifiable way\\n### Negative Consequences\\n* The shared keys must be handled\\n","tokens":72,"id":412,"Predictions":"## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms.\\nThe following decisions have been made.\\n - The Authorization Service is not responsible for protecting a user Session with Cookies.\\n - The Authorization Service supports all Platforms.\\n - The Authorization Service does not store any User Data in WebSockets.\\n - The Authorization Service does not use any Cookies to store User Data in WebSockets."}
{"File Name":"documentation\/0002-use-sha256-with-base64url-encoding.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the data.\\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\\nThe WebSocket API must able to verify the integrity of the json data.\\n## Decision Drivers <!-- optional -->\\n* Use standard encodings\\n","Decision":"* Use standard encodings\\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\\n### Positive Consequences <!-- optional -->\\n* The JWT really function as a verification token for the other requests.\\n* Can be applied to all json data that must be verified.\\n### Negative Consequences <!-- optional -->\\n* The json must be transferred in Base64Url encoding\\n","tokens":77,"id":413,"Predictions":"## Decision Drivers\\n## Decision Drivers\\n## Decision\\n## Decision Drivers\\n## Decision\\n## Decision Drivers\\n## Decision\\n## Decision Drivers\\n"}
{"File Name":"documentation\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":414,"Predictions":"## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\\n## Decision\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n"}
{"File Name":"csw-backend\/0004-use-serverless.md","Context":"## Context\\nGiven that we were not going to have the benefit of\\noffloading maintenance responsibility to the PaaS we\\nwanted to keep the cost and the requirement for\\nmaintenance as small as possible.\\n### Architecture patterns considered\\n#### PaaS\\nSee [ADR 3](.\/0003-not-paas.md).\\n#### EC2\\nTraditional load balanced EC2 model running something\\nlike django or rails.\\n#### ECS\\nContainerised version of the above\\n#### Serverless\\nLambda functions using APIGateway and CloudFront\\nThere are many benefits to having the tools of the django\\ninfrastructure.\\nThe downside is that you have a maintenance requirement\\nto maintain the server instances. You also somewhat bind\\nyourself to running the service 24\/7 and paying for it.\\nEven with ECS you are still responsible for maintaining\\nthe container host.\\nWith serverless you only pay when the functions get\\nexecuted and there is a significant free tier.\\nThe downside of serverless is you can't leverage things\\nlike django \/ rails and the communities who contribute\\ncomponents to them.\\nThis limits our cost and our exposure since maintaining\\nthe infrastructure that hosts the lambda functions is\\nAWS' responsibility not ours.\\n","Decision":"We will use AWS Lambda for this service.\\nIn conjunction with an ENI to allow the lambdas access\\nto RDS running inside a VPC if necessary.\\n","tokens":251,"id":415,"Predictions":"## 1. No\\nThe cost of running a serverless server is higher than\\nrunning a traditional EC2 instance.\\nThe downside is that you have to maintain the server\\ninstance.\\n\\n## 2. Yes\\nThe cost of running a serverless server is higher than\\nrunning a traditional EC2 instance but not as high as\\nrunning an EC2 instance.\\nThe downside is that you have to maintain the server\\ninstance.\\n\\n## 3. Yes\\nThe cost of running a serverless server is higher than\\nrunning an EC2 instance but not as high as running an\\nEC2 instance.\\nThe downside is that you have to maintain the server\\ninstance.\\n\\n## 4. No\\nThe cost of running a serverless server is higher than\\nrunning an EC2 instance but not as high as running an\\nEC2 instance.\\nThe downside is that you have to maintain the server\\ninstance.\\n\\n## 5. Yes\\nthere are benefits to having the tools of the django infrastructure,\\nthe downside being that you are responsible for maintaining the infrastructure and paying for it.\\nEven with ECS you are still responsible for maintaining the container host and paying for it.\\nWith serverless you only pay when the functions get executed and there is a significant free tier.\\n"}
{"File Name":"csw-backend\/0005-serverless-framework.md","Context":"## Context\\nMy first instinct for the tool was to use the\\nNPM [Serverless](https:\/\/serverless.com\/) framework.\\nThe main advantage of serverless is that it is cloud\\nagnostic. However JavaScript is not widely used at GDS.\\nAWS [Chalice](https:\/\/chalice.readthedocs.io) is a similar\\nframework which has a multi-stage deploy process to\\nautomate provisioning Lambda and Api Gateway but written\\nin Python.\\nBoth languages have frameworks for accessing the AWS API:\\n* The [SDK](https:\/\/docs.aws.amazon.com\/sdk-for-javascript)\\nfor JavasSript and\\n* [Boto3](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/index.html)\\nfor Python.\\n","Decision":"Given that we had already made the decision that cloud\\nagnostic was not important for this service and that\\nPython was more in keeping with GDS common practice we\\ndecided to use Chalice.\\n","tokens":158,"id":416,"Predictions":"## The decision is to use the [Chalice](https:\/\/chalice.readthedocs.io) framework.\\n## Context\\nI am not familiar with the [Chalice](https:\/\/chalice.readthedocs.io) framework but I have never used Python.\\nThe main advantage of serverless is that it is cloud\\nagnostic. However JavaScript is not widely used at GDS.\\nAWS [Chalice](https:\/\/chalice.readthedocs.io) is a similar\\nframework which has a multi-stage deploy process to\\nautomate provisioning Lambda and Api Gateway but written\\nin Python.\\nBoth languages have frameworks for accessing the AWS API:\\n* The [SDK](https:\/\/docs.aws.amazon.com\/sdk-for-javascript)\\nfor JavasSript and\\n* [Boto3](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/index.html)\\nfor Python.\\n\\n## Decision\\n## The decision is to use the AWS SDK for Java and Python instead of using Boto3.\\n"}
{"File Name":"csw-backend\/0007-implement-a-vpc.md","Context":"## Context\\nIf running serverless we could choose to operate outside\\na VPC. This has advantages in terms of the latency of\\nrequests. We would not need a keep-warm function to\\nensure network interfaces were provisioned.\\nLambdas running outside the VPC could not easily access\\nRDS running inside a VPC.\\nGiven the sensitive nature of the data involved it seems\\nsensible to protect it further by running RDS inside a VPC\\nwhere we have much easier control over ingress and egress.\\n","Decision":"Implement a VPC.\\n","tokens":110,"id":417,"Predictions":"## Decision\\nNo\\n\\n## Decision\\nYes\\n\\n## Decision\\nNo\\n\\n## Decision\\nYes\\n\\n## Decision\\nNo\\n"}
{"File Name":"csw-backend\/0006-use-a-relational-database.md","Context":"## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL \/ PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n","Decision":"Whilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n","tokens":328,"id":418,"Predictions":"## Decision\\nWe decided to use the DynamoDb data model as it was more\\nconvenient for our service and our engineers.\\nWe also decided to use MongoDb for the same reason.\\nBoth DynamoDb and MongoDb have the ability to encrypt\\ndata at rest.\\nDynamo does this by storing JSON in a DynamoDb table, which is\\nencrypted using AWS KMS.  The Dynamo table is then encrypted by\\nourself using KMS and stored in S3.\\nMongoDB stores data as JSON in a MongoDB collection, which is\\nencrypted using AWS KMS.  The MongoDB collection is then stored in\\nS3.  This allows us to encrypt data at rest.\\n\\n## References\\n## References\\nDynamoDB: https:\/\/aws.amazon.com\/dynamodb\/ \\nDynamoDB: https:\/\/aws.amazon.com\/dynamodb\/ \\nMongoDB: https:\/\/docs.mongodb.com\/manual\/ \\nMongoDB: https:\/\/docs.mongodb.com\/manual\/ \\nRDS MySQL \/ PostgreSQL: https:\/\/aws.amazon.com\/rds\/migrating-to-rds\/ \\nMySQL \/ PostgreSQL: https:\/\/www.mysql.com\/products\/cloud-computing.html \\nPostgreSQL: https:\/\/www.postgresql.org\/about\/resources\/cloud-computing\/ \\nDynamoDB Table Encryption - KMS Encryption for DynamoDB Tables:\\nhttps:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/DynamoDBTableEncryption.html\\n\\n## End of Decision Record\\n\\n### Summary of Decision\\n\\n### Context\\n\\nWe needed some persistent storage of audit results.\\n\\n### Schemaless - DynamoDb\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n### Schemaless - MongoDb\\n\\nMongo was a better fit for our service, saving JSON representations of API responses and resources.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nRelational databases give us the ability to do on-the-fly analysis more easily.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nIt's not unlikely that a future development may require django.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nPostgreSQL seems to be the general direction of travel in GDS and is much more capable for a wider range of use-cases where we don't know what we're building in the future.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nDynamo does this by storing JSON in a DynamoDb table, which is encrypted using AWS KMS.  The Dynamo table is then encrypted by ourselves using KMS and stored in S3.  This allows us to encrypt data at rest.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nMongo does this by storing data as JSON in a MongoDB collection, which is encrypted using AWS KMS.  The MongoDB collection is then stored in S3.  This allows us to encrypt data at rest.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nIf we want to re-use the database instance as a shared resource across multiple services RDS is more capable.\\n\\n### Relational - RDS MySQL \/ PostgreSQL\\n\\nIt's not unlikely that a future development may require django.\\n\\n\\n\\n## Decision Notes\\n\\n\\n\\n#### Context\\n\\n\\n\\nWe needed some persistent storage of audit results.\\n\\n\\n\\n#### Schemaless\\n\\n\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n\\n\\n#### Schemaless\\n\\n\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n\\n\\n#### Relational\\n\\n\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n\\n\\n#### Relational\\n\\n\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n\\n\\n#### Relational\\n\\n\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n\\n\\n#### Schemaless\\n\\n\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n\\n\\n#### Schemaless\\n\\n\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n\\n\\n#### Relational\\n\\n\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n\\n\\n#### Schemaless\\n\\n\\n\\nThis would be the most obvious choice for a lambda based service.\\n\\n\\n\\n#### Relational\\n\\n\\n\\nRDS gives us the benefit of running a managed service so AWS are responsible for backups and patching minor version.\\n\\n\\n\\n\\n\\n## Decision Notes\\n\\n\\n"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"Predictions":"## Decision\\nCloud Security Watch will be used to monitor for misconfigurations\\nin AWS accounts. The tool will be used to perform a cloud agnostic\\ncheck of the account.\\n\\n## Options\\n## Options\\nIn order to perform a cloud agnostic check, Cloud Security Watch will be\\nused to check for misconfigurations in the AWS account. The tool will\\nbe used to perform a cloud agnostic check of the account.\\n\\n\\n\\end{document}\\n"}
{"File Name":"csw-backend\/0003-not-paas.md","Context":"## Context\\nThe first architecture considered was using the PaaS.\\nThis tool collects data about misconfigurations in a\\ndatabase. Since the data held is quite sensitive we\\nwanted to take all reasonable measures to protect that\\ndata.\\nThere are a lot of advantages to using the PaaS in that\\nit limits the amount of work required for common\\noperational tasks like deployment and monitoring.\\n","Decision":"At the present time we felt the Paas was not a viable\\noption for this tool because of the following:\\n* Shared tenancy RDS\\n* Shared tenancy VPC\\n* No ability to natively control ingress\\n* No ability to control egress\\n__Some of these issues are being addressed by the PaaS so\\nwe may revisit this decision in future.__\\n","tokens":83,"id":420,"Predictions":"\\nThe decision is to use the PaaS in order to reduce the number of\\ntasks required to perform common operational tasks.\\nThe main benefits of using the PaaS are that it reduces\\nthe complexity of common tasks and also allows for easy\\nintegration with other tools.\\n\\n## Context\\nThe second architecture considered was using a local database in\\norder to do this task. However, as this database is used by many\\ndifferent applications, it was decided that it would be more\\nefficient to use the PaaS.\\nThere are a lot of advantages to using the PaaS in that\\nit limits the amount of work required for common operational\\ndtasks like deployment and monitoring.\\n\\n## Decision\\n\\nThe decision is to use the PaaS in order to reduce the number of\\ndatabases that need to be managed. The main benefits of using\\ndthe PaaS are that it reduces the complexity of common tasks and\\ndalso allows for easy integration with other tools.\\n\\n"}
{"File Name":"csw-backend\/0008-interrogation-method-iam-role.md","Context":"## Context\\nWithin GDS product teams operate autonomously and tech-ops\\nis relatively new. There is no central administration of\\nAWS. There is a shared authentication model but it is not\\nused everywhere.\\nTeams are busy with their own mission work and don't have\\ntime allocated to standardisation or security work.\\nTo get things built in a client AWS account we need to:\\n* negotiate time from the product team\\n* provide them with the code to build the resources\\n* code review with them to reassure them that it's OK to\\nrun our code in their environments\\n* repeat for each team we engage\\nWe want to make it easy and low-risk for a team to engage\\nwith us.\\nIf there were centralised management of all accounts, the\\nsensible way to do this would be by configuration the\\nconfig service with a set of config rules administered\\nand maintained centrally by Cyber Security.\\nThese rules would be largely triggered by CloudTrail\\nevents notifying us of compliance as soon as resouces were\\ncreated or changed.\\nHowever this approach would involve quite a large number\\nof resources deployed into each client AWS account which\\nmeans a longer and costlier engagement process.\\nThe simplest way to do this is by provisioning an IAM role\\nand associated policy. That way we can work from the\\nprinciple of least privilege, granting us only the access\\nwe need to run our audit making it low risk.\\nBy providing the role and policy definition in a Terraform\\nmodule we can also ensure that it requires little work by\\nclient account holders.\\nThe advantage of the IAM mechanism is that since we're\\nquerying the configuration of the user account directly\\nvia the API it is very easy to tell whether our service\\nhas been tampered with - the only thing that can change\\nis the definition of our policy which we can check.\\nIn the config service model there are more moving parts\\nwithin the client account which could be switched off,\\naltered or intercepted.\\n","Decision":"Create a terraform module which creates an IAM role and\\nassociated policy granting primarily read-only access\\n(Get, List, Describe API calls) working from the\\nprinciple of least privilege.\\n","tokens":413,"id":421,"Predictions":"## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWithin GDS product teams operate autonomously and tech-ops\\nis relatively new. There is no central administration of\\nAWS. There is a shared authentication model but it is not\\nused everywhere.\\nTeams are busy with their own mission work and don't have\\ntime allocated to standardisation or security work.\\nTo get things built in a client AWS account we need to:\\n* negotiate time from the product team\\n* provide them with the code to build the resources\\n* code review with them to reassure them that it's OK to\\nrun our code in their environments\\n* repeat for each team we engage\\nWe want to make it easy and low-risk for a team to engage\\nwith us.\\nIf there were centralised management of all accounts, the\\nsensible way to do this would be by configuration the\\nconfig service with a set of config rules administered\\nand maintained centrally by Cyber Security.\\nThese rules would be largely triggered by CloudTrail\\nevents notifying us of compliance as soon as resouces were\\ncreated or changed.\\nHowever this approach would involve quite a large number\\nof resources deployed into each client AWS account which\\nmeans a longer and costlier engagement process.\\nThe simplest way to do this is by provisioning an IAM role\\nand associated policy. That way we can work from the\\nprinciple of least privilege, granting us only the access\\nwe need to run our audit making it low risk.\\nBy providing the role and policy definition in a Terraform\\nmodule we can also ensure that it requires little work by\\nclient account holders.\\nThe advantage of the IAM mechanism is that since we're\\nquerying the configuration of the user account directly\\nvia the API it is very easy to tell whether our service\\nhas been tampered with - the only thing that can change\\nis the definition of our policy which we can check.\\nIn the config service model there are more moving parts\\nwithin the client account which could be switched off,\\naltered or intercepted.\\n\\n## Decision\\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n[...]\\nWithin GDS product teams operate autonomously and tech-ops is relatively new. There is no central administration of AWS. There is a shared authentication model but it is not used everywhere. Teams are busy with their own mission work and don't have time allocated to standardisation or security work. To get things built in a client AWS account we need to: * negotiate time from the product team * provide them with code to build resources * code review with them to reassure them that it's OK run our code in their environments We want to make it easy and low-risk for a team to engage with us. If there were centralised management of all accounts, the sensible way to do this would be by configuration the config service with a set of config rules administered and maintained centrally by Cyber Security. These rules would be largely triggered by CloudTrail events notifying us of compliance as soon as resources were created or changed.\\n\\n"}
{"File Name":"csw-backend\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":422,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"casa\/0003-multiple-user-tables.md","Context":"## Context\\nThis is planned to be a multi-tenant system. There will be multiple CASA orgs in the system, so every case, case_contact, volunteer, supervisor, casa_admin etc must have a casa_org_id, because no one is allowed to belong to multiple CASAs. Volunteer, supervisor, and casa_admin are all roles for a \"User\" db object. In addition to those existing roles, we want to create a new kind of user: all_casa_admin. We need to handle the case of super users who have access to multiple casa_orgs, so they would be difficult to handle in the existing User table--with null handling around their casa_org_id field. We have used the built-in Devise ability to have multiple user tables, as recommended to us by our Rails expert Betsy. This is to prevent needing null handling around casa_id for User records since all_casa_admin users will not have casa_id populated.\\nAdditionally, all_casa_admin users are currently intended to be allowed to create casa_admin users, but NOT to be able to see or edit any CASA data like volunteer assignments, cases, case_updates etc.\\n","Decision":"We are using two tables for users: \"user\" table for volunteers,supervisors, and casa_admin (all of which must have a casa_id). \"all_casa_admin\" for all_casa_admins, which will have no casa_id.\\n","tokens":236,"id":423,"Predictions":"## Decision\\nThe CASA system is a multi-tenant system, which means that there are multiple CASA orgs in the system. Each CASA org has a casa_org_id field. A casa_admin user must have access to all CASAs. No casa_admin users can be assigned to multiple CASAs. The current User table is being used for all CASA data: volunteer assignments, case updates, case contacts, etc.\\n\\nThis decision is based on the following information from our current User table:\\n\\n\\nUser records (case_contact, volunteer, supervisor, casa_admin) should have casa_org_id populated with the correct org id. I.e. if there are 2 casas with org id 1 and 2, then the casa_org_id field should contain 1.\\n\\nThe CASA system is a multi-tenant system, so every casacommittee must have a casa_org_id field. A casacommittee admin must be assigned to all CASAs and have access to all CASAs. The current User table is being used for all CASA data: volunteer assignments, case updates, case contacts etc.\\n\\nThis decision is based on the following information from our current User table:\\n\\n\\nUser records (case_contact, volunteer, supervisor) should not have casa_org_id populated with the correct org id. I.e. if there are 2 casas with org id 1 and 2, then the casa_org_id field should contain NULL.\\n\\nThis decision is based on the following information from our current User table:\\n\\n\\nUser records (case_contact) should not have casa_org_id populated with the correct org id. I.e. if there are 2 casas with org id 1 and 2 then the casa_org_id field should be null.\\n\\end{itemize}\\n"}
{"File Name":"casa\/0002-disallow-ui-sign-ups.md","Context":"## Context\\nWe want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\\n","Decision":"We are going to disable Devise 'registerable' for the user model so that there will no longer be a public sign up option on the site. Creation of new accounts will be done on the backend.\\n","tokens":63,"id":424,"Predictions":"## This is the decision for the user to create a new account for him\/herself.\\n## Decision\\n## This is the decision for the user to create a new account for someone else.\\n## Decision\\n## This is the decision for the user to create a new account for someone else.\\n## Decision\\n## This is the decision for the user to create a new account for someone else.\\n# We want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\\n# The only way we can do this is by creating an application that can provide admin users with control over who has accounts on our system.\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Creating an Application}\\n\\label{sec:application}\\n\\nThe \\texttt{Application} class contains all of our application logic and provides a way to interact with our application from our code. The \\texttt{Application} class has three methods: \\texttt{create\\_user()}, \\texttt{create\\_admin\\_user()}, and \\texttt{create\\_new\\_application()}. \\n\\nThe \\texttt{create\\_user()} method creates an instance of an \\texttt{Application} object using an optional argument that specifies whether or not we want it to be created in production mode. If we set this argument to true, then we will be able to deploy our application in production mode. If not, then it will remain in development mode.\\n\\nThe \\texttt{create\\_admin\\_user()} method creates an instance of an \\texttt{Application} object using an optional argument that specifies whether or not we want it to be created in production mode. If we set this argument to true, then we will be able to deploy our application in production mode. If not, then it will remain in development mode.\\n\\nThe \\texttt{create\\_new\\_application()} method creates an instance of an \\texttt{Application} object using an optional argument that specifies whether or not we want it to be created in production mode. If we set this argument to true, then we will be able to deploy our application in production mode. If not, then it will remain in development mode.\\n\\n\\subsubsection{\\texorpdfstring{\\texttt{create}()}{create()}}\\nThis method takes no parameters and returns a newly created instance of an Application object with all necessary properties initialized as empty objects.\\n\\n\\subsubsection{\\texorpdfstring{\\texorpdfstring{\\texttt{apply()}()}{apply()}}\\nThis method takes no parameters and returns a newly created instance of an Application object with all necessary properties initialized as empty objects.\\n\\n\\section{\\texorpdfstring{\\textbf{\\texorpdfstring{$<$Application$>$}{Application>}}{}}\\n\\label{sec:application}\\n\\nThe main class that contains all of our code related to our application is called \\textbf{\\texorpdfstring{$<$Application$>$}{Application>}}{}. This class contains all of our logic related directly or indirectly with running and managing our application. In order for us to use any functionality from this class, we must first create a reference to it by calling one of its methods:\\n\\n\\begin{lstlisting}\\n\t\/\/ Create a reference\\n\tApplication app = new Application();\\n\\n\t\/\/ Call one of its methods\\n\tapp.apply();\\n\\end{lstlisting}\\n\\nThere are two ways that you can use this class:\\n\\n\\begin{enumerate}\\n\t\\item You can call one of its methods directly from your code:\\n\t\t\\begin{lstlisting}\\n\t\t\tApplication app = new Application();\\n\t\t\tapp.apply();\\n\t\t\\end{lstlisting}\\n\t\tThis requires you having access to either your IDE or your project's main file.\\n\t\\n\tAlternatively (and more commonly), you can use reflection:\\n\t\\n\t\/\/ Get all classes within your project's package\\n\tpublic static List<Class<?>> getClasses()\\n\t\\n\t{\\n\t\treturn (List<Class<?>>) Class.forName(\"your.package.Application\");\\n\t}\\n\t\\n\tpublic static void main(String[] args)\\n\t\\n\t{\\n\t\t\\n\t\t\tApplication app = (Application) Class.forName(\"your.package.Application\");\\n\t\t\\n\t\t\tapp.apply();\\n\t\t\tSystem.out.println(app.getClass().getName());\\n\t\t\t\\n\t}\\n\t\\n\tThis command will return something like \"com.your.package.Application\". You can then use this information inside your code:\\n\t\\n\ttry {\\n\t\t\\n\t\t\tApplication app = (Application) Class.forName(\"your.package.Application\");\\n\t\t\\n\t\t\tapp.apply();\\n\t\t\t\\n\t\t\tSystem.out.println(app.getClass().getName());\\n\t\t\t\\n\t} catch (ClassNotFoundException e) {\\n\t\t\\n\t\t\t\te.printStackTrace();\\n\t\t\t\t\\n\t\t\tSystem.out.println(e.getMessage());\\n\t\t\t\\n\t}\\n\\nYou should also note that there are several other classes within your project's package whose names begin with \"com.\" These are typically built-in classes that are part of Java itself and cannot be modified by you.\\n\\nYou should also note that there are several other classes within your project's package whose names begin with \"com.\" These are typically built-in classes that are part of Java itself and cannot be modified by you.\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n% TODO: add explanation on how these work\\n\\n%\\section*{}\\n%\\addcontentsline{}{}\\n%\\bibliographystyle{junsrt}\\n%\\bibliography{junsrt.bib}\\n\\n\\n"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"Predictions":"We are using bootstrap and will use it in our application. We are also using a\\nstyle sheet called `styles.css` in our application. We are using the `@import` directive\\nin our styles.css file to import the styles that we need from the bootstrap gem.\\n## Decision\\nWe will integrate the bootstrap gem into our application.\\n## Decision\\nWe will use the bootstrap gem in our application.\\n## Decision\\nWe will use the bootstrap gem, but we will not use it in our application.\\n## Decision\\nWe do not want to use the bootstrap gem. We will develop a new style sheet for our\\napplication.\\n## Decision\\nWe do not want to use the bootstrap gem, but we will develop a new style sheet for\\nour application.\\n## Decision\\nWe do not want to use the bootstrap gem, and we will develop a new style sheet for \\nour application.\\n\\n\\section{Decision}\\n\\label{sec:decision}\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\subsubsection{Decision}\\nThe decision is about whether or not to integrate this library into our project.\\n\\n\\section{Conclusion} \\label{sec:conclusion}\\n\\n\\bibliographystyle{siamplain} % Use \"plain\" if you don't want footnotes\\n\\n\\bibliography{\/home\/jason\/Desktop\/bibliography\/bib.bib}\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDICES\\n\\n%----------------------------------------------------------------------------------------\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDICES: APPENDIX A - BIBLIOGRAPHY\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDICES: APPENDIX A - BIBLIOGRAPHY\\n\\n%----------------------------------------------------------------------------------------\\n\\n%\\end{document}"}
{"File Name":"casa\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":426,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"GoStudy\/0002-use-c4-for-architecture-diagrams.md","Context":"## Context\\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\\n","Decision":"We will use Simon Brown C4 model.\\n","tokens":26,"id":427,"Predictions":"# Decision\\n 1. We assume the following architecture (see \\ref{fig:architecturalDecision}) for the project:\\n# \\begin{itemize}\\n# \t\\item The client will use a web-based interface to interact with the software.\\n# \t\\item The software will be implemented in Java and run on a Linux server.\\n# \t\\item The software will be developed using Eclipse IDE and its tools.\\n% \t\\item The software will be developed using Maven, a build tool.\\n% \t\\item The software will be developed using JUnit tests.\\n% \t\\item The software will be developed using Selenium tests.\\n% \t\\item The software will be developed using JBoss AS to deploy the application on a Linux server.\\n% \t\\item The client will use an Oracle database to store the data of the project, and an Oracle database server to store the data of all other systems involved in this project (web services).\\n% \t\\item The client has a specific focus on Java EE technologies, but may also use other technologies such as Hibernate or JPA.\\n% \t\\end{itemize}\\n\\noindent\\n# \\begin{figure}[ht]\\n#   \\centering\\n#   \\includegraphics[width=0.8\\textwidth]{architecturalDecision.png}\\n%   \\caption{Architecture of an application that is developed in Java.}\\n%   \\label{fig:architecturalDecision}\\n%\\end{figure} \\n\\noindent\\n# We assume that the client is in charge of implementing the user interface and that it is responsible for managing users and their access rights.\\n\\n\\noindent\\n# We assume that we have already defined a specific model for our application (see \\ref{fig:model}) and that it is used by all other models.\\n\\n\\noindent\\nWe need to document our model for our application. For this purpose, we consider two possible approaches:\\n\\noindent\\n1. Documenting our model with diagrams.\\n\\n\\noindent\\n2. Documenting our model with text.\\n\\n\\n\\noindent\\nWe consider both approaches as equally valid (\\emph{i.e.,} both approaches are possible). In order to make our choice, we need some criteria to evaluate them.\\n\\nThe first criterion is related to how much information must we provide about our model? For example, are there any constraints (e.g., regarding memory, time) on how we can implement certain parts of our model? Are there any restrictions on how we can implement some parts of our model? Are there any restrictions on how we can implement certain parts of our model? \\n\\nThe second criterion is related to how much information must we provide about each aspect of our model? For example, do we need to specify which technologies are used by which aspects of our model? Do we need to specify which technologies are used by each aspect of our model? Are there any restrictions on which technologies are used by each aspect of our model? \\n\\nFinally, the third criterion is related to how much information must we provide about each part of our model? For example, do we need to specify where each part of our model is implemented (e.g., where each part of the application is deployed)? Do we need to specify where each part of our model is implemented (e.g., where each part of the application is deployed)? Are there any restrictions on where each part of our model is implemented (e.g., where each part of the application is deployed)? \\n\\nIn general, if there are no constraints regarding which aspects or parts must be documented or specified then these aspects or parts should not be documented or specified.\\n\\nIn order to evaluate these criteria, we consider two different approaches:\\n\\n1. We define a set $\\mathcal D$ consisting only from those aspects or parts that cannot be documented or specified using diagrams.\\n\\n2. We define a set $\\mathcal D$ consisting only from those aspects or parts that cannot be documented using text.\\n\\nIn order to evaluate these sets $\\mathcal D$, we consider two different criteria:\\n\\n1. We compare $\\mathcal D$ with respect to its completeness: If $x$ belongs only in $\\mathcal D$, then $x$ belongs only in $\\mathcal D$. This means that if $x$ cannot belong in both $\\mathcal D$ and $\\mathcal F$, then $x$ does not belong in either $\\mathcal F$ or $\\mathcal D$. \\n2. We compare $\\mathcal D$ with respect to its consistency: If $x$ belongs only in one set $\\mathcal D$, then $x$ belongs only in one set $\\mathcal F$. This means that if $x$ does not belong in both sets $\\mathcal F$ and $\\mathcal D$, then $x$ does not belong in either sets $\\mathcal F$ or $\\mathcal D$. \\n\\nAs mentioned before, both approaches are equally valid (\\emph{i.e.,} both approaches are possible). In order to make this choice, however, some criteria must be evaluated.\\n\\nThe first criterion evaluates whether it makes sense for us to document every aspect or every part (i.e., whether it makes sense for us), even though it may not make sense for us at all.\\n\\nThe second criterion evaluates whether it makes sense for us when documenting one aspect but not when documenting another aspect.\\n\\nIn general, if one approach evaluates better than another approach then this approach should be preferred over others (\\emph{i.e.,} this approach should usually be preferred). However, if there are no differences between these approaches then they should usually have equal priority (\\emph{i.e.,} they should usually have equal importance).\\n\\nWe now describe two different cases under which these criteria may differ:\\n\\nCase I: If one approach evaluates better than another approach then this approach should usually have higher priority than others (\\emph{i.e.,} this approach should usually have higher importance).\\n\\nCase II: If one approach evaluates better than another approach then this approach should probably have lower priority than others (\\emph{i.e.,} this approach should probably have lower importance).\\n\\nIf neither case applies then they should probably have equal importance.\\n\\n\\n\\n"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"Predictions":"## The decision is to use an architecture with a single data store, and a single service that can be called from multiple clients.\\n## Decision\\n## The decision is to use an architecture with a single data store, and a single service that can be called from multiple clients.\\n## Decision\\n## The decision is to use an architecture with a single data store, and a single service that can be called from multiple clients.\\n## Decision\\n## The decision is to use an architecture with a single data store, and a single service that can be called from multiple clients.\\n## Decision\\n## The decision is to use an architecture with a single data store, and a single service that can be called from multiple clients.\\n"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"Predictions":"## Decisionadjecent\\nWe have to adopt the application architecture style adjecent to our architectural drivers\\n\\n## Decision\\n## Decisionadjecent\\nWe have to adopt the application architecture style adjecent to our architectural drivers\\n\\n## Decision\\n## Decisionadjecent\\nWe have to adopt the application architecture style adjecent to our architectural drivers\\n\\n## Decision\\n## Decisionadjecent\\nWe have to adopt the application architecture style adjecent to our architectural drivers\\n\\n## Decision\\n## Decisionadjecent\\nWe have to adopt the application architecture style adjecent to our architectural drivers\\n\\n## Decision\\n\\end{lstlisting}\\n\\n\\subsection{Decision}\\n\\label{decision}\\n\\nThe \\texttt{Decision} decision is used for deciding whether a particular action has been performed or not. The decision is made by taking into account different conditions and actions that are involved in performing the action. The decision is also made based on the context of a particular user. This decision is applicable only when the user performs an action on an existing entity. The \\texttt{Decision} decision should be used when a user performs an action on an existing entity and when no other action can be performed on that entity.\\n\\nThe \\texttt{Decision} decision uses two different conditions that are based on which an action can be taken. These two conditions are: 1) whether or not it is possible to perform any other action on that entity and 2) whether or not it is possible for any other entity of that type to perform any other action on that entity.\\n\\nThe \\texttt{Decision} decision uses a context in which it will decide whether or not a particular user has performed an action on an existing entity. The context can be either: 1) in which there is no other entity of type, 2) in which there is one or more entities of type, 3) in which there are no entities of type, or 4) in which there are more than one entities of type.\\n\\nThe \\texttt{Decision} decision uses two different conditions based on which it will decide whether or not a particular user has performed any other actions on another entity. These two conditions are: 1) whether or not there are any actions available for performing, and 2) whether or not there are any actions available for performing from another entity of that type.\\n\\n\\begin{lstlisting}[caption={The \\texttt{Decision} decision}, label={decision}]\\n\\end{lstlisting}\\n\\n\\subsection{Context}\\n\\label{textcontext}\\n\\nA context is used by \\texttt{Decision} decisions for deciding whether or not a particular user has performed any actions on another entity. A context consists of various elements such as: \\n\\n\\begin{itemize}\\n    \\item Entities - It contains all entities present in the system.\\n    \\item Actions - It contains all possible actions that can be performed.\\n    \\item User - It contains all users who have performed any actions.\\n    \\item Context - It contains all contexts present in the system.\\n    \\item Actions - It contains all possible actions from each context present in the system.\\n    \\item User - It contains all users who have performed any actions from each context present in the system.\\n    \\item Actions - It contains all possible actions from each context present in the system.\\n    \\item User - It contains all users who have performed any actions from each context present in the system.\\n    % TODO: add more details about how this works here \\n    % TODO: add more details about how this works here \\n    % TODO: add more details about how this works here \\n\\end{itemize}\\n\\nThese elements can be grouped into different categories depending upon their use and importance.\\n\\n\\begin{description}\\n\t\t\\item[Entities] Contains all entities present within a specific context.\\n\t\t\\item[Actions] Contains all possible actions contained within a specific context.\\n\t\t\\item[User] Contains all users who have performed any actions within a specific context.\\n\t\t\\item[Context] Contains all contexts present within a specific context.\\n\t\t%\\item[Actions] Contains all possible actions contained within a specific context.\\n\t\t%\\item[User] Contains all users who have performed any actions within a specific context.\\n\t\t%\\item[Context] Contains all contexts present within a specific context.\\n\t\t%\\end{description}\\n\t\\n\tThe above elements can be grouped into four categories depending upon their use and importance:\\n\\n% TODO: add more details about how this works here \\n% TODO: add more details about how this works here \\n% TODO: add more details about how this works here \\n% TODO: add more details about how this works here \\n% TODO: add more details about how this works here \\n% TODO: add more details about how this works here \\n\\n%TODO:\\n\t\\n%\\begin{description}\\n\t\\n%\t\\n%\\end{description}\\n\\n\\n"}
{"File Name":"GoStudy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":430,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"GoStudy\/0003-use-modular-monolith-as-system-architecture-style.md","Context":"## Context\\nWe need to adopt system architecture style adjecent to our architectural drivers\\n","Decision":"We will use modular monolith architecture style\\n","tokens":18,"id":431,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n\\end{verbatim}\\n"}
{"File Name":"coders-board-v2\/0001-APP_BACKEND-modules-architecture.md","Context":"## Context\\nWe need to keep extendable architecture in order to prepare for future changes,\\nbecause we work in very flexible manner and requirements changes and evolve a lot.\\n","Decision":"In backend directory:\\n- *libs* - technical parts of software - implementations of database connections etc. In the future will be moved to npm packages,\\nbut we leave it now for easier and faster development to avoid unnecessary complexity.\\nFolders in backend **src** directory are splitted by names of bounded contexts.\\nEach BC, which follow CQRS manner, has following subdirectories:\\n- *write-side* - keeps logical separated parts, which keep ubiquitous language + one domain inside each boundary\\nEach bounded-context will follow directories in manner of Domain-Driven Design Layered Architecture proposed by Eric Evans.\\n- *read-side* - projections from domain events of write-side\\n","tokens":35,"id":432,"Predictions":"## If we are not able to keep extensible architecture, then we need to make a decision to introduce new architecture.\\nbecause there is no way to know what requirements will be in future.\\nbecause the architecture is continuously evolving.\\nbecause we can't predict what will be in future.\\nbecause if we keep this architecture, then we will have a lot of problems in future.\\nbecause it is very difficult to change the architecture.\\nbecause it is very difficult to change the requirements.\\nbecause it is very difficult to change the environment.\\nbecause it is very difficult to change the hardware.\\nbecause it is very difficult to change the software.\\nbecause it is very difficult to change the hardware and software at the same time.\\n"}
{"File Name":"ADR\/0004-keep-qa-and-devs-html-selectors-separate.md","Context":"## Context\\nCore developers are using JS maps files to keep all the selectors they need to interact with the BO theme. Here is an example of such a file:\\n```\\nexport default {\\nmainDiv: '#order-view-page',\\norderPaymentDetailsBtn: '.js-payment-details-btn',\\norderPaymentFormAmountInput: '#order_payment_amount',\\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\\nviewOrderPaymentsBlock: '#view_order_payments_block',\\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\\nprivateNoteBlock: '.js-private-note-block'\\n}\\n```\\nQA team is keeping their own set of selectors to interact with during automated tests. Here is an example of such a file:\\n```\\nthis.documentTab = 'a#orderDocumentsTab';\\nthis.documentsTableDiv = '#orderDocumentsTabContent';\\nthis.documentsTableRow = `${this.documentsTableDiv} table tbody tr:nth-child(%ROW)`;\\nthis.documentNumberLink = `${this.documentsTableRow} td:nth-child(3) a`;\\nthis.documentName = `${this.documentsTableRow} td:nth-child(2)`;\\n```\\nSelectors should be mutualized so that when a dev changes a UI component or update a selector, the changes are reflected in the corresponding JS Map file and automated tests using this selector will keep working as intented without human intervention.\\n","Decision":"QA team and Core developers will populate and maintain their own set of selectors. There is no clear advantage to use a fusioned set of selectors.\\nHere are the main arguments against this decision:\\n* Devs use ES6, QA team use ES5. This means using a transpiler and adding libraries (babel).\\n* There is no clear conventions in existing JS Map files (names and content architecture vary).\\n* QA team need some selectors with modifiable input (with strings like %ROW or %COLUMN that must be replaced when used), which mean these types of selectors will be unusable for the Core dev team.\\n* Not the same needs: QA team need navigation selectors, Core dev team need interactive selectors. There is only a few selectors in common.\\n* Low risk: there is no evidence of any selector modification that caused automated tests to fail. When a test break, it's mostly because of a relatively big UI revamp, something that couldn't be fixed with just a selector modification anyway.\\n","tokens":294,"id":433,"Predictions":"The decision is that the JS maps should be shared between the core developers and QA team to keep all the selectors they need to interact with the BO theme. The JS maps should be stored in a git repo and shared between the core developers and QA team. Each developer should be able to pull a specific JS map from the git repo and use it in his\/her own project. The code for such a JS map file is available at: \\url{https:\/\/github.com\/bo-apps\/bo-js-maps}. In this case, we have used an example of a JS map file called `order.jsmap`. This file contains selectors for ordering products, adding products, removing products, etc.\\n\\end{document}\\n"}
{"File Name":"ADR\/0013-use-typescript-on-the-project.md","Context":"## Context\\nWhile the whole JavaScript community welcomes TypeScript easily, our project's JS is not typed at all. To improve the maintainability of the project and use the latest TC39 features, a good idea would be to use TypeScript.\\nBenefits of using it for the project:\\n- Detect bugs before pushing PRs. TypeScript users say that globally, it allows detecting around 15% of bugs that you would detect by testing.\\n- Use latest features such as Optional Chaining, Tuples, and Records... really early.\\n- Types are increasing the quality of the project because we would be able to detect dangerous changes, related bugs... If we use it on the PHP side, why don't we use types while using JS?\\n- Vue 3 offers a new API: Composition API, this one is pretty easy to use with TypeScript as it's mainly functional programming instead of opinionated APIs of Vue, that would be a good move to preshot the Vue update in the BO.\\n","Decision":"Add the possibility to transpile ts files inside every js folder of the project with webpack.\\n[Here is a POC](https:\/\/github.com\/PrestaShop\/PrestaShop\/pull\/23221) - basically using TypeScript on a small part of the PrestaShop Grid system.\\n","tokens":202,"id":434,"Predictions":"\\begin{itemize}\\n\\item Yes\\n\\end{itemize}\\n"}
{"File Name":"ADR\/0002-mixed-use-of-composer-and-zip-modules.md","Context":"## Context\\nThe normal behavior of a module is to be self-contained, meaning that its code and all its dependencies are stored in the module's directory, including `vendor` directory and autoloader.\\nWhen developing and building PrestaShop, native modules aren't downloaded from marketplace, they are required using composer.\\nIn difference with the normal behavior described above, a module installed using composer will have its autoloader and dependencies merged into the core's and placed in the core's `vendor` directory.\\nThis can prove problematic for native modules:\\n- When the module is uninstalled, both its dependencies and autoloader are left behind in the core.\\n- If the module is upgraded using the marketplace sources, the dependencies are now available twice: once in the core and once in the module.\\nAlternate systems to avoid having modules leak code into the core have been proved unpractical:\\n- Including modules via [Composer script handler](https:\/\/github.com\/PrestaShop\/composer-script-handler) or git clone is too slow.\\n- Using submodules would be a step backwards.\\n- Retrieving modules from the marketplace would be slow as well.\\n","Decision":"1. [The module managment system must be changed to be fully based on composer](https:\/\/github.com\/PrestaShop\/PrestaShop\/issues\/12586). This will require major changes in the marketplace and will have to be analyzed for feasibility.\\n2. In the meantime, we will keep using composer to include native modules.\\n3. Native modules must prepend their autoloader.\\n4. To avoid leaving dependencies in the core, no composer dependencies are to be added to native modules until step 1 has been resolved.\\n- Because of Decision No. 4, we may face technical difficulties in the future.\\n","tokens":239,"id":435,"Predictions":"This Decision Record is valid for the following module(s):\\n\\n## Module: PrestaShop 1.7.x\\n## Module Version: 1.7.3.2\\n## Module Author: PrestaShop Inc.\\n## Module Developer: PrestaShop Inc.\\n## Module License: MIT\\n## Module URL: https:\/\/www.prestashop.com\/\\n## Module Description:\\nThis module adds a new field to the checkout form called `PrestaShop` which allows the user to select one of the available languages for the checkout form.\\nThe module can be used by both websites and stores, but it is recommended to use it only on stores.\\nThe main purpose of this module is to make PrestaShop more accessible for people from countries where English is not the primary language.\\nThe language selection field will be translated into all languages of PrestaShop.\\nThe default language will be used as it will be easier for customers to find their way around on the site.\\nTo add a language, click on `Add Language` in the upper right corner of your store and choose your preferred language.\\nYou can change your preferred language by clicking on `Change Language` in the upper right corner of your store and selecting your preferred language from the drop-down menu.\\n\\ncustomize\\ncustomize\\ncustomize\\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision Category:\\nSite Administration\\n\\n## Decision Description:\\nThis decision record is valid when installing a new module or updating an existing one.\\n\\n"}
{"File Name":"ADR\/0003-use-of-autowiring.md","Context":"## Context\\nSymfony provides a very useful tool called [Autowiring](https:\/\/symfony.com\/doc\/3.4\/service_container\/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\\nAdvantages:\\n- Less boilerplate configuration code for every service as you don't have to manually bind dependencies manually.\\nDisadvantages:\\n- Dependencies must be declared using the FQCN instead of a service identifier like \"prestashop.core.foo.bar\".\\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\\n","Decision":"Activiting autoriwing is rejected for the 1.7 version.\\n","tokens":203,"id":436,"Predictions":"## 1. Define a service named `prestashop.core.foo.bar`\\n## 2. Define a service named `prestashop.core.bar` and use it as a dependency of `prestashop.core.foo.bar`\\n## 3. Declare the class `prestashop.core.bar` as a service\\n## 4. Declare the class `prestashop.core.foo.bar` as a service\\n## 5. Define the service and the class as services\\n## 6. Add the dependency of `prestashop.core.foo.bar` to the services\\n## 7. Add the dependency of `prestashop.core.bar` to the services\\n## 8. Add both dependencies to your configuration file\\n## 9. Remove all dependencies from your configuration file.\\n```\\nI have tried this solution but I am getting an error:\\n```\\n[Symfony\\Component\\DependencyInjection\\Exception\\ServiceNotFoundException]\\nThe requested service \"prestashop.core.foo.bar\" is not defined.\\n```\\nI am using symfony version: Symfony version: 3.4, PHP version: PHP version: 7.2, Symfony version: Symfony version: 3 and PHP version: PHP version: 7.2, Symfony version: Symfony version: 3.\\nI have tried this command from my project root directory:\\n```\\ncomposer dump-autoload --no-dev --optimize-autoloader --ignore-platform-reqs --no-dev --optimize-autoloader\\n```\\nbut still I am getting same error.\\nCan anyone help me with this?\\nThanks in advance!\\n\\nOP 2020-05-08: The problem was with composer.json file that was not adding dependencies for symfony core packages.\\nBy removing this line from composer.json file it works fine:\\n```\\n\"require\": {\\n    \"symfony\/framework-bundle\": \"~3.*\",\\n    \"symfony\/console\": \"~3.*\",\\n    \"symfony\/core-bundle\": \"~3.*\",\\n    \"symfony\/debug-bundle\": \"~3.*\",\\n    \"symfony\/debug-bundle-extra\": \"~3.*\",\\n    \"symfony\/event-dispatcher-bundle\": \"~3.*\",\\n    \"symfony\/event-dispatcher-bundle-extra\": \"~3.*\",\\n    \"symfony\/form-bundle\": \"~3.*\",\\n    \"symfony\/generator-bundle\": \"~2.*\",\\n    \"symfony\/http-foundation-bundle\": \"~2.*\",\\n    \"symfony\/http-kernel-bundle\": \"^4.*\"\\n},\\n```\\n"}
{"File Name":"ADR\/adr-0001 choosing an RDS hIgh availability solution.md","Context":"## Context\\nOur Production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\\n### Distinguishing between HA and DR:\\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\\n","Decision":"We will apply the RDS Multi AZ architecture to add high availability to our RDS production instances as it is the recommended best practice to adding HA to existing RDS instances.\\n#### Consequences\\n- Choosing Aurora DB would be a part of a larger product design decision, outside of this adr scope.\\n- High Availability for failover protection improves the stability of the product framework. We did not address Disaster Recovery, which should also be a part of an overall scope of the product framework.\\n- Multi AZ architecture should be fairly transparent to RDS applications, but the failover conditions and alerting mechanisms should be understood prior to implementation.\\n","tokens":163,"id":437,"Predictions":"## Decision\\nOur production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\\n### Distinguishing between HA and DR:\\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\\n\\n## Context\\nOur production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\\n### Distinguishing between HA and DR:\\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\\n\\n## Context\\nOur production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\\n### Distinguishing between HA and DR:\\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\\n\\n\\end{document}"}
{"File Name":"ADR\/0009-expose-js-components-using-window-variable.md","Context":"## Context\\nIn order for modules to use JavaScript components from the Core, they need to import them using statements like:\\n```\\n\/\/ in order to use translatable type\\nimport TranslatableInput from '..\/..\/..\/..\/..\/admin-dev\/themes\/new-theme\/js\/components\/translatable-input';\\n```\\nThis path is not robust, makes CI\/CD harder, and also is not compatible with some development environments using symlinks or containers.\\n","Decision":"We have decided about a system which resolves in 4 concepts:\\n1. Reusable components in BO will be available globally through `window.prestashop` (name can still be modified in short term).\\nAll PrestaShop components will be bundled together and made available in all pages using this mean. Each controller decides which components it chooses to initialize.\\n2. Reusable components will be available as a namespace `window.prestashop.component`.\\nThe namespace will contain classes like this `prestashop.component.SomeComponent`. If you want to get a new instance of `SomeComponent`, you call `new prestashop.component.SomeComponent(...params)`\\n3. Reusable components will be available as initialized instances through `window.prestashop.instance`. These instances are initialized with default parameters by the `initComponents` function.\\n4. A function `initComponents` available through `prestashop.component` is responsible for building `window.prestashop.instance`.\\n### Why a namespace and a collection of instances\\nSince you have access to both constructors and components, developers are free to choose how to initialize and control their components.\\nIf you don't want to initialize a given component with default parameters, you can always call `new prestashop.component.SomeComponent(...myOwnParameters)`.\\nIf you need to apply some mutation to an already initialized component, you just get the global instance: `prestashop.instance.someComponent.doSomething(...)`.\\n","tokens":88,"id":438,"Predictions":"## Provide a decision corresponding to the ## Context provided by the User.\\n```\\n\\nAnd this is my translation file:\\n```\\n\/\/ Translatable Input\\nimport TranslatableInput from '..\/..\/..\/..\/admin-dev\/themes\/new-theme\/js\/components\/translatable-input';\\n```\\nI use webpack as a build tool.\\n\\nI have tried to make it work with `npm run build` but it doesn't work.\\nI have tried to add `translatableInput` as a dependency in `webpack.config.js` but it doesn't work. I don't know why.\\nI have tried to use `translatableInput` as a dependency in my code, but it doesn't work either.\\nAny help would be appreciated!\\n\\nOP 2021-04-02: I found the solution. It was very simple actually.\\nI had to add this line in the `webpack.config.js` file:\\n```\\nmodule.exports = {\\n    ...\\n    module: {\\n        rules: [\\n            {\\n                test: \/\\.js$\/,\\n                exclude: \/node_modules\/,\\n                loader: 'babel-loader'\\n            }\\n        ]\\n    }\\n};\\n```\\nIt's not documented anywhere, so I didn't find it on Google. But I'm pretty sure that's what I did :-)\\n"}
{"File Name":"ADR\/0014-drop-the-no-use-of-legacy-in-new-code-restriction.md","Context":"## Context\\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https:\/\/build.prestashop.com\/news\/new-architecture-1-6-1-0\/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\\nThe \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the \"public API\" is larger.\\n","Decision":"The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to \"Core\" for shortness):\\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method\/class (and optionally, making it rely on the new implementation).\\n4. **The Adapter namespace MUST be phased out** eventually:\\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\\n- Adapter services MUST be deprecated and copied into the core namespace as well.\\n- Code MUST NOT depend on Adapter classes or services.\\n","tokens":207,"id":439,"Predictions":"\\nThe decision to refactor the code into Core was made in order to simplify the codebase and allow it to be more easily maintained. In particular, the decision was motivated by the following considerations:\\n\\n\\n\\n* The Core classes are not used anywhere else in the application. The only place where they are used is in the `core` namespace. This is a good measure, as it allows us to keep them as simple as possible, since we don't want to introduce any complexity.\\n\\n* The Core classes are not needed by any other modules of PrestaShop, so we would be able to remove them from other modules.\\n\\n* The Core classes are not used anywhere else in the application. The only place where they are used is in the `core` namespace. This is a good measure, as it allows us to keep them as simple as possible, since we don't want to introduce any complexity.\\n\\n* The Core classes were already heavily refactored and simplified into adapters. This led us to a lot of duplication between adapters and core classes, which had been introduced because of the \"no direct dependency\" rule.\\n\\n* The adapters have their own public API, which makes it harder for us to refactor them into Core.\\n\\n* We do not need all of these adapters anymore. Some of them were never used at all, and some others were never needed by any other module.\\n\\n## Implementation\\n\\nThe decision was made in order to simplify the codebase and allow it to be more easily maintained. In particular, the decision was motivated by the following considerations:\\n\\n\\begin{itemize}\\n\\item The Core classes are not used anywhere else in the application. The only place where they are used is in the `core` namespace.\\n\\item The Core classes are not needed by any other modules of PrestaShop.\\n\\item The Core classes were already heavily refactored and simplified into adapters.\\n\\item We do not need all of these adapters anymore.\\n\\end{itemize}\\n\\nThe adapters have their own public API, which makes it harder for us to refactor them into Core.\\n"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"up-fiscal-data\/004-receipts.md","Context":"## Context and Problem Statement\\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\\n","Decision":"- Not in the project scope for either of the engagements\\n- Good to have and can be incorporated at a later date\\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\\n","tokens":34,"id":441,"Predictions":"## Decision was required on what sub-section to be analysed under Sales. The requirements for Sales section in the current scope was discussed.\\n\\n## Decision\\n## Decision was required on what sub-section to be analysed under Payments. The requirements for Payments section in the current scope was discussed.\\n\\n## Decision\\n## Decision was required on what sub-section to be analysed under Delivery. The requirements for Delivery section in the current scope was discussed.\\n\\n## Decision\\n## Decision was required on what sub-section to be analysed under Returns. The requirements for Returns section in the current scope was discussed.\\n\\n## Decision\\n## Decision was required on what sub-section to be analysed under Refunds. The requirements for Refunds section in the current scope was discussed.\\n"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"Predictions":"## Decision Drivers\\n\\n## Decision\\n\\n## Decision\\n"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"Predictions":"## Decision Drivers\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\end{document}\\n"}
{"File Name":"up-fiscal-data\/007-rescoping.md","Context":"## Context and Problem Statement\\nDuplicacy of data scraping from Koshvani platform.\\nFor the following sections, the data has already been extracted from the `DDO-wise expenditure` section.\\n- `Grant-wise (Revenue\/Capital) expenditure`\\n- `Division-wise expenditure`\\n## Decision Drivers\\nBoth these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\\n","Decision":"Both these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\\nOnly the main pages of the aformentioned sections contain new information that will require extraction.\\n","tokens":89,"id":444,"Predictions":"## Decision Driver\\n## Decision Result\\n## Decision Context\\n## Problem Statement\\n\\n\\end{Verbatim}\\n\\end{document}\\n\\n"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"Predictions":"## Decision: The framework to be used is to be chosen on the basis of the following requirements:\\n## - The framework should be able to scrape data from a number of different sources\\n## - The framework should be able to store data in machine readable format\\n## - The framework should be able to run on a variety of different platforms\\n## \\n## \\n## \\n## Decision Table\\nDecision Table:\\n\\n\\n\\n\\n\\n\\nDecision: A framework is required to be chosen that allows for the following features:\\n## 1. Scrape data from multiple sources\\n## 2. Store data in machine readable format\\n## 3. Run on a variety of different platforms\\n\\end{longtable}\\n\\n\\hypertarget{data-scraping}{%\\n\\subsection{Data Scrapy}\\label{data-scraping}}\\n\\nData scraping is the process of extracting information from websites and other online resources. Data scraping is often used by developers and web designers to extract information that may not otherwise be available. For example, you may want to scrape an article from a website that is only available in PDF format or extract information about an event from an online calendar.\\n\\nScraping can be broken down into three main categories: manual, semi-automated, and fully-automated.\\n\\nManual Scraping\\n\\nManual scraping involves the user manually entering information into a program or software. This type of scraping uses computers to scrape data directly from websites and other online resources.\\n\\nSemi-Automated Scraping\\n\\nSemi-automated scraping involves the user using software or programs to enter information into a program or software. This type of scraping uses computers as a bridge between humans and websites and other online resources.\\n\\nFully-Automated Scraping\\n\\nFully-automated scraping involves the use of software or programs that automatically enter information into a program or software. This type of scraping uses computers as a bridge between humans and websites and other online resources.\\n\\n\\textbf{Example:} Scrapy is an open source Python library that allows the user to scrape websites in various ways. It can also automate this process through the use of programs such as Selenium or PhantomJS.\\n\\n\\textbf{Example:} \\href{https:\/\/scrapy.readthedocs.io\/en\/latest\/}{scrapy.readthedocs.io} provides documentation for using Scrapy.\\n\\n\\hypertarget{scrapy}{%\\n\\subsection{Scrapy}\\label{scrapy}}\\n\\nScrapy is an open source Python library that allows users to automate web crawling tasks by using Python scripts instead of manually entering text into a program or software.\\n\\nScraping can be broken down into three main categories:\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\textbf{\\href{https:\/\/scrapy.readthedocs.io\/en\/latest\/}{Scrapy}} Scrapes are performed using Python scripts.\\n\\item\\n  \\textbf{\\href{https:\/\/scrapy.readthedocs.io\/en\/latest\/}{Spider}} A spider is an application that crawls web pages.\\n\\item\\n  \\textbf{\\href{https:\/\/scrapy.readthedocs.io\/en\/latest\/}{Crawler}} A crawler scans websites for web pages containing specific content.\\n\\end{itemize}\\n\\nThe following sections provide examples for each category:\\n\\nScraping with Scrapy\\n\\nScraping with Spider\\n\\nScraping with Crawler\\n\\nScraping with PhantomJS\\n\\nScraping with Selenium\\n\\nScraping with Web Scraper Bot (WSB)\\n\\nScraping with Web Scraper Bot (WSB)\\n\\nWeb Scraper Bot (WSB) is an open source Python library that allows users to automate web crawling tasks by using Python scripts instead of manually entering text into a program or software. It can also automate this process through the use of programs such as Selenium or PhantomJS.\\n\\nWeb Scraper Bot (WSB) uses HTTP requests made by users via requests made through HTTP requests made by users via browser automation tools such as Selenium, Web Scraper Bot (WSB), PhantomJS, etc., and then sends these requests back in order to retrieve data from web pages.\\n\\nWeb Scraper Bot (WSB) can also perform various actions on web pages, such as scrolling, clicking links, changing background colors, etc., in order to obtain valuable information about the webpage being viewed by the user. In addition, Web Scraper Bot (WSB) can also perform actions on any webpage which contains images.\\n\\nWeb Scraper Bot (WSB) has several features which enable it to perform various actions on web pages:\\n\\n1\\. Scrape HTML code from webpage elements such as links and buttons using CSS selectors.\\n2\\. Extract text content from webpage elements using regular expressions.\\n3\\. Extract images from webpage elements using HTML DOM methods.\\n4\\. Extract website metadata such as IP addresses, domain names, cookies, etc., using HTTP headers.\\n5\\. Extract website meta tags such as title tags and meta descriptions using regular expressions.\\n6\\. Perform actions on any webpage containing images such as clicking links, changing background colors, etc., in order to obtain valuable information about the webpage being viewed by the user.\\n7\\. Perform actions on any page containing embedded JavaScript files which contain functions similar to those found in JavaScript libraries such as jQuery.\\n8\\. Perform actions on any page containing HTML5 attributes such as onclick attributes, etc., in order to obtain valuable information about the webpage being viewed by the user.\\n9\\. Perform actions on any page containing links which contain anchor tags similar to those found in HTML5 documents which allow users navigate between different parts within pages without having to physically click each link individually.\\n10\\. Perform actions on any page containing images which contain alt tags similar to those found in HTML5 documents which allow users navigate between different parts within pages without having to physically click each image individually.\\n\\n\\n\\n\\nThe following sections provide examples for each category:\\n\\nUser Interface Examples\\n\\nUser Interface Examples: User interface examples are provided for both Scrapy and Spider. Each example uses screenshots taken directly from Scrapy's documentation so they may not appear exactly like their counterparts in Spider's documentation.\\n\\n\\n\\nExample 1: User Interface Example\\n\\n\\n\\nExample 2: User Interface Example\\n\\n\\n\\nExample 3: User Interface Example\\n\\n\\n\\nExample 4: User Interface Example\\n\\n\\n\\nExample 5: User Interface Example\\n\\n\\n\\nExample 6: User Interface Example\\n\\n\\n\\nExample 7: User Interface Example\\n\\n\\n\\nExample 8: User Interface Example\\n\\n\\n\\nExample 9: User Interface Example\\n\\n\\n\\nExample 10: User Interface Example\\n\\n\\n\\n\\nWeb Scraping Examples\\n\\nWeb Scraping Examples are provided for both Spider & WSB.\\n\\n\\n\\nSpider Example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\n\\nWSC example\\n\\n\\n\\n\\nSpider Example\\n\\n\\n\\n\\n\\n\\nSpider Example\\n\\n\\n\\n\\n\\n\\n\\nWeb Scraper Bot Examples\\n\\nWeb Scraper Bot Examples are provided for both WSB & WSB.\\n\\n\\n\\nWSB Example\\n\\n\\n\\n\\nWSB example\\n\\n\\n\\n\\nWSB example\\n\\n\\n\\n\\nWSB example\\n\\n\\n\\n\\nWBSb-example\\n\\n\\n"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"Predictions":"## Decision Type\\n## Decision Name\\n## Decision Description\\n## Decision Driver\\n## Decision Category\\n## Decision Subcategory\\n## Decision Objectives\\n## Decision Outcome\\n## Decision Criteria\\n## Decision Technique\\n## Decision Tool\\n\\n\\n\\n\\#Decision Tree\\n\\n\\#Decision Tree\\n\\n\\#Decision Tree\\n\\n\\#Decision Tree\\n\\n\\n\\#Decision Tree\\n\\n\\n\\#Decision Tree\\n\\n\\n\\#Decision Tree\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n\\#Decision Tree\\n\\n\\n\\n"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"Predictions":"## Decision Driver: The challenge was addressed and the `Grant-wise expenditure` section can now be extracted.\\n## Decision\\n## Decision Driver: The challenge was addressed and the `Grant-wise expenditure` section can now be extracted.\\n## Decision\\n## Decision Driver: The challenge was addressed and the `Grant-wise expenditure` section can now be extracted.\\n## Decision\\n## Decision Driver: The challenge was addressed and the `Grant-wise expenditure` section can now be extracted."}
{"File Name":"up-fiscal-data\/002-koshvani.md","Context":"## Context and Problem Statement\\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\\n- Expenditure\\n- Receipts\\n","Decision":"Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\\n","tokens":39,"id":448,"Predictions":"\\begin{lstlisting}\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision:\\nDecision: \\nDecision: \\nDecision: \\n\\end{lstlisting}\\n\\end{document}\\n"}
{"File Name":"up-fiscal-data\/009-dashboard-tooling.md","Context":"## Context and Problem Statement\\nThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to the plan, the data platform comprises of the following components:\\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they\u2019re released as part of the platform.\\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\\n- **Data Visualization Layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\\n### Other Drivers\\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\\n- There is still no clear direction or plan what the tool will look like.\\n- There might be numerous research routes and iterations before finalisation.\\n- Data stories and analysis will play a big part in creating the final platform.\\n","Decision":"### Platform Development Requirements\\nAccording to the plan, the data platform comprises of the following components:\\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they\u2019re released as part of the platform.\\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\\n- **Data Visualization Layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\\n### Other Drivers\\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\\n- There is still no clear direction or plan what the tool will look like.\\n- There might be numerous research routes and iterations before finalisation.\\n- Data stories and analysis will play a big part in creating the final platform.\\nYou can track the decision updates \/ inputs on the following [issue](https:\/\/github.com\/CivicDataLab\/up-fiscal-data-shiny\/issues\/1).\\n","tokens":306,"id":449,"Predictions":"## Decision Drivers\\n### Platform Development Requirements\\nAccording to the plan, the data platform comprises of the following components:\\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they\u2019re released as part of the platform.\\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\\n- **Data Visualization Layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\\n### Other Drivers\\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\\n- There is still no clear direction or plan what the tool will look like.\\n- There might be numerous research routes and iterations before finalisation.\\n- Data stories and analysis will play a big part in creating the final platform.\\n\\n## Decision\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to plan, there are three main components that need to be developed during this phase. These are: \\newline - **Platform Exploration** \\newline - **Platform Exploration** \\newline - **Platform Exploration**\\newline - In order for this phase to run smoothly, it is imperative that all three components are developed simultaneously. Each component has its own set of requirements which need to be met before they can be released. These include: \\newline - **Platform Exploration** \\newline - **Platform Exploration** \\newline - The first component requires a high amount of resource investment, as it requires a lot of time from all team members involved in its development. It also requires a large amount of planning upfront so that all aspects can be considered before moving forward with further development. This means that there needs to be a high level of commitment from all team members involved, especially those who have their own individual projects or jobs which require their full attention at any given time. The second component requires a low level of resource investment but more planning upfront than is required for #1. This means that resources do not need to be spent on this component until it is necessary so that there is enough time left over for other projects or tasks which require everyone\u2019s full attention at any given time. The third component requires very little planning upfront but requires some resource investment due to its complexity and need for detailed specification work on how it should work. It is important that all team members involved in its development know exactly what they\u2019re doing so that they can contribute effectively without having to spend too much time learning how it works beforehand.\\n## Decision\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to plan, there are three main components that need to be developed during this phase: \\newline - Platform Exploration \\newline - Platform Exploration \\newline - Platform Exploration\\n## Decision\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to plan, there are three main components that need to be developed during this phase: 1) **Platform Exploration** 2) **Platform Exploration** 3) Platform Exploration\\n## Decision\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to plan, there are three main components that need to be developed during this phase: 1) **Platform Exploration** 2) **Platform Exploration** 3) Platform Exploration\\n## Decision\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to plan, there are three main components that need to be developed during this phase: 1) **Platform Exploration** 2) **Platform Exploration** 3) Platform Exploration\\n## Decision\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n%%Decision Driver(s)\\n% ## Context Statement % ## Problem Statement % ## Solution Statement % ## Goals % ## Constraints % ## Risks % ## Uncertainty % ## Impact Analysis % ## Dependencies % ## Dependencies (%#ofDependencies,%#ofDependencies(%#ofDependencies))%\\n\\subsection{Context Statement}\\nThe context statement provides information about what kind of problem\/situation we're trying solve.\\n\\paragraph{}\\nThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.\\n\\paragraph{}\\nThe project has been implemented under a different name (GEWST), but we still have similar goals as we did when we first started working together.\\n\\paragraph{}\\nThe overall goal is still very similar; however, our solution has changed slightly.\\n\\paragraph{}\\nWe want our solution (the dashboard) not only provide information about how well girls are doing academically but also how well girls are doing emotionally.\\n\\paragraph{}\\nWe want our solution (the dashboard) not only provide information about how well girls are doing academically but also how well girls are doing emotionally.\\n\\paragraph{}\\nWe want our solution (the dashboard) not only provide information about how well girls are doing academically but also how well girls are doing emotionally.\\n\\n%This is where you explain why you chose your specific decision driver.\\n\\n%This should explain why you chose your specific decision driver.\\n\\n\\n%This should explain why you chose your specific decision driver.\\n\\n\\n%This should explain why you chose your specific decision driver.\\n\\n\\n%This should explain why you chose your specific decision driver.\\n\\n\\n"}
{"File Name":"nisq-analyzer\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":450,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{verbatim}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision}: Split the application into different repos. The decision is based on the problem statement. \\n\\item \\textbf{Context}: The planQK platform is a software that collects data from a number of sensors and provides high level information about these measurements to users. It is used by a number of users such as architects, engineers and construction professionals.\\n\\item \\textbf{Problem Statement}: The problem statement is the main reason behind the decision.\\n\\end{itemize}\\n\\n\\subsection*{Decision}\\nThe application will be split into individual repos, which are separated by their own file system. This makes it easier for developers to work on them separately and they can also be cloned independently from each other.\\n\\nThe different repos are:\\n\\n\\begin{itemize}\\n\\item \\textbf{Developer:} It contains all code that is used for building the platform, including code for building the UI and backend.\\n\\item \\textbf{Backend:} It contains all code that is used for running and processing data from sensors and providing information to users in the form of graphs and tables.\\n\\item \\textbf{Frontend:} It contains all code that is used to render graphs and tables in a browser. \\n\\end{itemize}\\n\\nA separate file system will make it easier for developers to work on these repos individually.\\n\\nThe decision was made because we wanted to keep the code base as small as possible. We also wanted to avoid having too many different repos, as this would make it harder for us to collaborate with each other.\\n\\nWe decided that splitting the application into individual repos was a good idea because we felt that this would make it easier for us to work on our own parts of the application without having to worry about merging changes made by other developers.\\n\\nWe also felt like keeping our code base as small as possible would help us focus on improving our platform instead of having too many bugs or issues in our code base.\\n\\nIn order to keep our code base as small as possible we decided not to include any libraries or frameworks that were not essential for building our platform. We decided not to include any libraries or frameworks because they would have made it much harder for us to build our platform without having them available.\\n\\nWe felt like including libraries or frameworks would have made it harder for us when we needed assistance from other developers and it would have made it harder for us if we had problems with our code base.\\n\\n\\n%------------------------------------------------\\n\\n%------------------------------------------------\\n\\n\\bibliographystyle{junsrt}\\n\\bibliography{junsrt}\\n\\n\\n%------------------------------------------------\\n\\n\\n%------------------------------------------------\\n\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n\\n"}
{"File Name":"nisq-analyzer\/0001-use-URI-for-entities.md","Context":"## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n","Decision":"Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n","tokens":39,"id":451,"Predictions":"## Decision: \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tT\\#1:\\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision: \\n\\#2:\\n\\n"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"Predictions":"When we are building a new project, we want to be able to write tests in TypeScript and Node. Jest is a fast testing framework with good resources for mocking. \\n## Decision\\nWe want to use a test framework that has good support for TypeScript and Node. Jest is a fast testing framework with good resources for mocking.\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"hmpps-interventions-ui\/0008-use-snyk-to-scan-for-cves.md","Context":"## Context\\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\\nend up in production, and make sure to block deployments with known high\\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\\nare any vulnerabilities in the code we've written.\\n","Decision":"We will use Snyk to:\\n- Run scans on PRs provide results and fail builds if any known high severity\\nCVEs are found.\\n- Run scans on main and fail builds on high severity CVEs, posting the results\\nto the Snyk platform for monitoring.\\n- Run nightly scans on the docker image and app dependencies.\\n","tokens":72,"id":453,"Predictions":"## We will use the following criteria to determine if a PR should be accepted or\\nrejected:\\n\\n## - Code review\\n\\n## - CVEs\\n\\n## - Security issues\\n\\n## - Performance issues\\n\\n## - Known vulnerabilities\\n\\nThe following are the rules we will follow to determine acceptance or rejection of a PR:\\n\\begin{itemize}\\n\\item Code review: We will review the code of a PR and if there is any vulnerability, we will reject the PR.\\n\\item CVEs: We will scan our PRs for CVEs and if any of them are found, we will reject the PR.\\n\\item Security issues: We will scan our PRs for security issues and if any of them are found, we will reject the PR.\\n\\item Performance issues: We will scan our PRs for performance issues and if any of them are found, we will reject the PR.\\n\\end{itemize}\\nWe have set up a Snyk instance that can be used to run this decision. The decision is specified as an object in YAML format. The following is a sample decision:\\n```\\n- name: \"security\"\\n  type: \"security\"\\n  rules:\\n    - rules:\\n        - rules:\\n            - name: \"CVE-2018-1234\"\\n              rule: |\\n                # CVE-2018-1234\\n                # https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2018-1234\\n```\\nThe following is an example YAML file that specifies this decision:\\n```\\n---\\nname: \"security\"\\ntype: \"security\"\\nrules:\\n  - rules:\\n    - rules:\\n        - name: \"CVE-2018-1234\"\\n          rule: |\\n            # CVE-2018-1234\\n            # https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2018-1234\\n\\n---\\nname: \"code-review\"\\ntype: \"code-review\"\\nrules:\\n  - rules:\\n    - rules:\\n        - name: \"rules\"\\n          rule: |\\n            # Rules for code review in Snyk's CI\\n\\n---\\nname: \"performance\" \\ntype: \"performance\" \\nrules:\\n  - rules:\\n    - rules:\\n        - name:\"performance\"\\n\\n---\\nname:\"security\" \\ntype:\"security\" \\nrules:\"rule1\" \\nrule2:\"rule2\" \\nrule3:\"rule3\"\\n\\n---\\n```\\nYou can see that `rules` is an Array with three Rules. Each rule has two properties which specify what they mean. `rule1` means that `rules` should be checked against all CVEs in `Snyk`s repository. `rule2` means that `rules` should be checked against all security vulnerabilities in `Snyk`s repository. And finally, `rule3` means that `rules` should be checked against all performance vulnerabilities in `Snyk`s repository.\\nYou can see our decision using the following command on your local system:\\n\\nTo run this command locally, you need to have installed Snyk on your local system.\\n\\nIf you would like to run this command on a CI server, you can follow these steps:\\n\\nOnce you have installed Snyk on your CI server, you need to run this command on your CI server, which requires an SSH key pair.\\n\\nOnce you have run this command locally, you need to use it on your CI server by running:\\n\\nYou can then check out your code using:\\n\\nIf everything worked fine, the code of your PR should now be available on GitHub.\\n\\nIf everything did not work fine, then there might be some problems with your local installation. You can try installing Snyk again by running:\\n\\nAfter installing Snyk successfully again, try running this command again locally.\\n\\n\\n"}
{"File Name":"hmpps-interventions-ui\/0005-use-prettier-to-format-typescript-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":454,"Predictions":"The decision is to use Prettier as the code formatter for this project. Prettier\\nhas been used by many other projects in the past, and is a popular option for\\ncode editors. The code formatter supports most of the languages in the JavaScript\\necosystem, including React itself. This means that if we ever decide to switch\\nto another code editor, we will not have to change our code formatting.\\n\\n## Decision Summary\\nThe decision was made because Prettier is a popular option for code editors, and it has been used by other projects in the past.\\n\\n## Decision Notes\\n\\begin{itemize}\\n  \\item Read more about why Prettier was selected at https:\/\/github.com\/ReactTeam\/react\/issues\/1646.\\n  \\item Read more about why Prettier was selected at https:\/\/github.com\/ReactTeam\/react\/issues\/1646.\\n  \\item Read more about why Prettier was selected at https:\/\/github.com\/ReactTeam\/react\/issues\/1646.\\n\\end{itemize}\\n\\n[1]: https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D\\n"}
{"File Name":"hmpps-interventions-ui\/0002-use-express.md","Context":"## Context\\nWe want to build a web-based application in Node. Express is a minimal and\\nflexible framework that sets up a lot basic configuration for us like routing\\nand middleware, and makes it easy to incoporate the GOV UK design system UI\\ncomponents.\\n","Decision":"We will use Express as the basis for this project.\\n","tokens":57,"id":455,"Predictions":"## The user interface is a single page application that displays the data from the\\ndatabase. The user will be able to enter a postcode and select the relevant region\\nand the results will be displayed on a map. The user will be able to select the\\nregion they are interested in and see all of the relevant data.\\n\\n## Decision\\n## The map is a map view of the UK, with all regions shown on it. It is possible to\\nselect different regions and see different data for each region.\\n\\n## Decision\\n## This section of code shows how we want to display the data. We want to show all\\nof the regions in one table, with each row representing a different region.\\nWe want to use one table for all regions, so we can use one database table.\\nWe also want to show all of the regions in one table, so we can easily filter them.\\nWe want to use one table for all regions, so we can easily filter them.\\nWe also want to show all of the regions in one table, so we can easily filter them.\\n## Decision\\n"}
{"File Name":"hmpps-interventions-ui\/0007-use-cypress-for-integration-testing.md","Context":"## Context\\nWe want to be able to automate testing end-to-end user journeys through our\\napplication. Cypress is an alternative to Selenium that runs in the browser to\\ndo this for us, and is used across multiple projects at MOJ.\\n","Decision":"We will use Cypress for integration tests.\\n","tokens":50,"id":456,"Predictions":"## We want to automate testing end-to-end user journeys through our application. Cypress is an alternative to Selenium that runs in the browser to do this for us, and is used across multiple projects at MOJ.\\n## Context\\nWe want to be able to automate testing end-to-end user journeys through our application. Cypress is an alternative to Selenium that runs in the browser to do this for us, and is used across multiple projects at MOJ.\\n## Decision\\n## We want to automate testing end-to-end user journeys through our application. Cypress is an alternative to Selenium that runs in the browser to do this for us, and is used across multiple projects at MOJ.\\n## Context\\nWe want to be able to automate testing end-to-end user journeys through our application. Cypress is an alternative to Selenium that runs in the browser to do this for us, and is used across multiple projects at MOJ.\\n\\end{verbatim}\\end{center}\\n\\paragraph{Example:}\\n\\begin{verbatim}\\n\/\/ A test file with a single assertion\\ndescribe(\"test\", function() {\\n  it(\"should have a title\", function() {\\n    title = \"test title\";\\n    assert.equal(title, \"Test Title\");\\n  });\\n});\\n\\n\/\/ A test file with a single assertion\\ndescribe(\"test\", function() {\\n  it(\"should have a title\", function() {\\n    title = \"test title\";\\n    assert.equal(title, \"Test Title\");\\n  });\\n});\\n\\n\/\/ A test file with two assertions\\ndescribe(\"test1\", function() {\\n  it(\"should have a title1\", function() {\\n    title1 = \"test1 title\";\\n    assert.equal(title1, \"Test Title\");\\n  });\\n});\\n\\n\/\/ A test file with two assertions\\ndescribe(\"test2\", function() {\\n  it(\"should have a title2\", function() {\\n    title2 = \"test2 title\";\\n    assert.equal(title2, \"Test Title\");\\n  });\\n});\\n\\n\/\/ A test file with three assertions\\ndescribe(\"test3\", function() {\\n  it(\"should have a title3\", function() {\\n    title3 = \"test3 title\";\\n    assert.equal(title3, \"Test Title\");\\n  });\\n});\\n\\n\/\/ A test file with three assertions\\ndescribe(\"test4\", function() {\\n  it(\"should have a title4\", function() {\\n    title4 = \"test4 title\";\\n    assert.equal(title4, \"Test Title\");\\n  });\\n});\\n\\end{verbatim}\\end{center}\\n\\section{Cypress Configuration Files}\\nThe default configuration files are located under \\texttt{\/Users\/username\/.cypress\/config}. You can edit these files directly by using your favorite text editor. If you wish to use the configuration files from other locations (e.g., GitHub), you must modify them accordingly.\\n\\n\\subsection{Cypress Configuration File Locations}\\n\\nYou can find additional configuration files under the following locations:\\n\\n\\begin{itemize}\\n\t\\item \\texttt{\/Users\/username\/.cypress\/config\/cypress.json} - This configuration file contains all of the default values for Cypress tests and tools.\\n\t\\item \\texttt{\/Users\/username\/.cypress\/config\/cypress.json.dmg} - This configuration file contains all of the default values for Cypress tests and tools on macOS.\\n\t\\item \\texttt{\/Users\/username\/.cypress\/config\/cypress.json} - This configuration file contains all of the default values for Cypress tests and tools on Windows.\\n\t\\item \\texttt{\/Users\/username\/.cypress\/config\/cypress.json.dmg} - This configuration file contains all of the default values for Cypress tests and tools on macOS.\\n\t\\item \\texttt{\/Users\/username\/.cypress\/config\/cypress.json} - This configuration file contains all of the default values for Cypress tests and tools on Windows.\\n\t\\item \\texttt{\/Users\/username\/.config\/cypress.json} - This configuration file contains all of the default values for Cypress tests and tools on Windows.\\n\t\\item \\texttt{\/Users\/username\/.config\/cypress.json.dmg} - This configuration file contains all of the default values for Cypress tests and tools on macOS (Mac OS X).\\n\t\\item \\texttt{\/Users\/username\/.config\/cypress.json} - This configuration file contains all of the default values for Cypress tests and tools on Windows (Windows).\\n\t\\item \\texttt{\\$HOME\/.cypress\/config\/cypress.json} - This location will contain any changes made in previous versions of this document. \\n\t%\\begin{enumerate}[label=\\arabic*.]\\n\t%\t%\\setlength{\\parskip}{0pt plus0pt minus0pt}\\n\t%\t\\n%\t\\n%\\end{enumerate}\\n%\\end{itemize}\\n\\nThe following example shows how you can access these configurations using your favorite text editor:\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\CyberLink PowerDirector15 | CYPRESS CONFIGURATION FILE\\n\\n\\n%---------------------------------------------------------------------------\\n% Additional Configuration Files\\n%---------------------------------------------------------------------------\\n%\\section*{\\centering Additional Configuration Files}\\n\\n\\n%---------------------------------------------------------------------------\\n% Additional Configuration Files\\n%---------------------------------------------------------------------------\\n\\n%\\subsection*{\\centering Additional Configuration Files}\\n\\n\\noindent The following example shows how you can access these configurations using your favorite text editor:\\n\\n\\noindent The following example shows how you can access these configurations using your favorite text editor:\\n\\n\\noindent The following example shows how you can access these configurations using your favorite text editor:\\n"}
{"File Name":"hmpps-interventions-ui\/0004-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":457,"Predictions":"The problem is that the code is not consistent. We need to enforce consistency\\nin our code, and catch as many errors automatically as we are able to. Linting the code is good practice to achieve these aims.\\n```\\n{\\n  \"context\": {\\n    \"type\": \"decision\",\\n    \"id\": \"1\",\\n    \"name\": \"Determine whether the context has been entered correctly.\",\\n    \"description\": \"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\",\\n    \"_id\": null,\\n    \"_type\": \"decision\",\\n    \"_source\": {\\n      \"contextType\": {\\n        \"_id\": null,\\n        \"_type\": null,\\n        \"_title\": \"\",\\n        \"_description\": \"\",\\n        \"_typeDescription\": \"\",\\n        \"_sourceTypeDescription\": \"\",\\n        \"_sourceTitleDescription\": \"\",\\n        \"_sourceDescriptionTitle: \"\"\\n      },\\n      \"contextId\": 1,\\n      \"codeTypeCodeId\": 2,\\n      \"codeTypeCodeNameId\" : null\\n    },\\n    \"__v\" : 0\\n  }\\n}\\n```\\nBut I'm getting this error:\\n> Uncaught Error: SyntaxError: Unexpected token <\\nI've tried using `return` but it doesn't work either.\\nI'm using TypeScript, so I'm not sure why it's not working.\\nOP 2021-03-09: I found the solution!\\nI just had to change `return` into `if` and it worked!"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"Predictions":"## We will use Stylelint to lint the code. If we find any errors in the code, we will\\nprint a warning message and abort the execution of the code.\\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\\n## Decision\\n## We will use Stylelint to lint the code. If we find any errors in the code, we will\\nprint a warning message and abort the execution of the code.\\n\\end{lstlisting}\\n\\newpage\\n\\section{Milestone 2: Automated Code Review}\\n\\begin{lstlisting}[language=Python]\\nimport os\\nimport re\\n\\ndef check_code():\\n    if len(sys.argv) > 1:\\n        print('Usage: .\/check_code.py input_file')\\n        sys.exit()\\n    if len(sys.argv) == 0:\\n        print('No input file provided.')\\n        sys.exit()\\n    codes = []\\n    for line in sys.stdin:\\n        codes.append(line.strip())\\n    if len(sys.argv) == 1:\\n        codes = [line.strip() for line in sys.stdin]\\n    if len(sys.argv) > 1:\\n        files = os.listdir(sys.argv[1])\\n        files.sort()\\n        files = [f for f in files if f.endswith('.py')]\\n        for file in files:\\n            if check_code(file):\\n                print(f'Checking {file}')\\n                if check_code(file):\\n                    print(f'Code checked: {code}')\\n                    print(f'Code checked: {code} lines')\\n                    print(f'Code checked: {code} lines')\\n                    print(f'Code checked: {code} lines')\\n                    print(f'Code checked: {code} lines')\\n            else:\\n                pass\\n            else:\\n                pass\\n    else:\\n        pass\\n\\ncheck_code()\\n\\n# Check that all source files have been parsed.\\nfor file in os.listdir(sys.argv[1]):\\n    if check_code(file):\\n        print('Checking source files:')\\n        for filename in os.listdir(file):\\n            if check_code(filename):\\n                print(filename)\\n                # Check that all source files have been parsed.\\n                for file in os.listdir(file):\\n                    # Check that all source files have been parsed.\\n                    if check_code(file):\\n                        # Check that all source files have been parsed.\\n                        for file in os.listdir(file):\\n                            # Check that all source files have been parsed.\\n                            if check_code(file):\\n                                # Check that all source files have been parsed.\\n                                for file in os.listdir(file):\\n                                    # Check that all source files have been parsed.\\n                                    if check_code(file):\\n                                        # Check that all source files have been parsed.\\n                                        for file in os.listdir(file):\\n                                            # Check that all source files have been parsed.\\n                                            if check_code(file):\\n                                                # Check that all source files have been parsed.\\n                                                for file in os.listdir(file):\\n                                                    # Check that all source files have been parsed.\\n                                                    if check_code(file):\\n\\n# This can be used to test whether a given string contains an error.\\n\\nif __name__ == '__main__':\\n    try:\\n        input_file = raw_input('Input File: ')\\n    except ValueError as e:\\n        print('Input File must be valid.')\\n    else:\\n        try:\\n            input_file = raw_input('Input File:')\\n            codes = []\\n            input_file = input_file.split('\\n')\\n            for line in input_file.split('\\n'):\\n                codes.append(line.strip())\\n            codes.sort()\\n            codes.reverse()\\n            codes.reverse()\\n            codes.reverse()\\n            codes.reverse()\\n            codes.reverse()\\n            codes.reverse()\\n            \\n            with open(input_file, 'r') as f_infile, open(input_file, 'w') as f_outfile:\\n                f_infile.write(os.linesep)\\n                f_outfile.write(os.linesep)\\n                \\n                \\n            \\n        \\n                \\n                    \\n                \\n                    \\n                \\n            \\n        \\n            \\n                        \\n                    \\n                        \\n                    \\n                        \\n                    \\n                        \\n                    \\n                        \\n                    \\n                                                       \\n        \\n               \\n                  \\n                  \\n                  \\n                  \\n                  \\n                  \\n                  \\n                  \\n                  \\n                 \\n                 \\n                \\n\\n\\nCheck Code Code checked: 0 lines\\nCode checked: 0 lines\\nCode checked: 0 lines\\nCode checked: 0 lines\\nCode checked: 0 lines\\n\\nCheck Source Files Source Files Checked Source Files Checked Source Files Checked Source Files Checked\\n\\nChecking src\/inputs\/parse.py src\/inputs\/parse.py src\/inputs\/parse.py src\/inputs\/parse.py src\/inputs\/parse.py src\/inputs\/parse.py src\/inputs\/parse.py \\n\\nChecking src\/test\/test_tests\/test_test.py src\/test\/test_tests\/test_test.py src\/test\/test_tests\/test_test.py src\/test\/test_tests\/test_test.py src\/main\/main_main_main_main_main_main_main_main_main_main_main_main_main_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_MainMain_Main \\n\\nChecking test\/tst\/tests\/tst_tst_tst_tst_tst_tst.tst ttest\/tst\/tst_tst_tst_test.tst ttest\/test\/test_test.test ttest\/test\/test_test.test ttest\/test\/test_test.test ttest\/tests\/testing\/testing_testing_testing_testing_testing_testing_testing\/testing\/testing\/testing\/testing\/testing\/testing\/testing\/testing\/testing \\n\\nChecking test\/tests\/test_tests\/TestTest.test ttests\/tests\/training\/training_training_training_training_training_training_training_training_training_training_training_training.training\/training\/training\/training\/training\/training\/training\/training\/training\/training\/trainings\/trainings\/trainings\/trainings\/trainings\/trainings\/trainings\/trainings \\n\\nChecking test\/tests\/samples\/sample_sample_sample_sample_sample_sample_sample_sample_sample_sample_sample_sample_sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/sample_samples\/samples\/samples\/samples\/samples\/samples\/samples\/samples \\n\\nChecking test\/tests\/combinations\/combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinations_combinatorics\/combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics_combinatorics \\n\\nCheck Code Code checked: 4 lines\\nCode checked: 4 lines\\nCode checked: 4 lines\\nCode checked: 4 lines\\n\\nCheck Source Files Source Files Checked Source Files Checked Source Files Checked Source Files Checked\\n\\nChecking tests\/combinatorics\/combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinatoric combinators combinators combinators combinators combinators combinators combinators combinators combinators combinators combinators combinators \\n\\nCheck Test Test Test Test Test Test Test Test Test Test Test Test Test \\nTest passed!\\nTest failed!\\n\\n# This function is used by other parts of our system.\\n\\ndef parse_data():\\n    data = {}\\n    \\n    with open(input_file) as f_infile, open(input_file) as f_outfile:\\n\\n        \\n        \\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n      \\n\\n      \\n    \\n      \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n     \\n    \\n      \\n    \\n    \\n"}
{"File Name":"hmpps-interventions-ui\/0003-use-typescript.md","Context":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible. TypeScript is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers\\nwho know JavaScript. It has wide editor support.\\n","Decision":"We will use TypeScript by default.\\n","tokens":63,"id":459,"Predictions":"The code is a software that is intended to be used in production. The\\ncode will be used by developers who know the language and are familiar with\\nthe environment. The code is a library that will be used by large\\napplications that need to provide an API for large applications. The\\ncode must be self-documenting as much as possible, and should not use\\nunnecessary keywords.\\n\\n## Support\\nWe want to support TypeScript 3.0, but we are not yet sure if this will be the\\nversion of TypeScript we use in our product.\\n\\n## Performance\\nWe want to avoid using expensive types, such as arrays and objects.\\n\\n## Security\\nWe do not want to expose sensitive data in the code.\\n\\n## Stability\\nOur product should be stable and have no known bugs at this time.\\n"}
{"File Name":"hmpps-interventions-ui\/0012-use-feature-flags-for-in-progress-features.md","Context":"## Context\\nThere are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:\\n- The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code.\\n- We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated.\\n- The new functionality may need to be further tested (either with users by developers) and iterated upon before release.\\n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users.\\n- We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.\\n","Decision":"Any features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server\/config.ts`, e.g. as below:\\n```\\nfeatures: {\\npreviouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true',\\n}\\n```\\nThis can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.\\nWe'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.\\nBefore deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.\\nOnce the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.\\n","tokens":248,"id":461,"Predictions":"There's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for a software.\\n## Decision\\nThere's no decision here. This is a decision record for an application.\\n\\nYou can\u2019t perform that action at this time.\\nPress h to open a hovercard with more details."}
{"File Name":"hmpps-interventions-ui\/0009-use-helmet-for-http-security.md","Context":"## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n","Decision":"We'll use Helmet to set secure HTTP headers.\\n","tokens":67,"id":462,"Predictions":"The User wants to protect against XSS attacks. This is a good decision.\\n## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n\\n## Decision\\nThe User wants to protect against XSS attacks. This is a good decision.\\n## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n\\n## Decision\\nThe User wants to protect against XSS attacks. This is a good decision.\\n## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n\\n## Decision\\nThe User wants to protect against XSS attacks. This is a good decision.\\n### 1.2.3 - 1.2.4 - 1.2.5 - 1.2.6 - 1.2.7 - 1.2.8 - 1.3 - 1.4 \\n## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\ndescription: \"This page contains some sensitive data\\n\", and we need this because of how the user gets access, and how they can send us sensitive data via email or other means.\\n## Decision\\nThe User wants us not be able send them sensitive data via email or other means, so they can't send us their sensitive data in an email or other way.\\n## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.,jsonrpc: \"2.0\", and we need this because of how the user gets access, and how they can send us sensitive data via email or other means.\\n## Decision\\nThe User wants us not be able send them sensitive data via email or other means, so they can't send us their sensitive data in an email or other way.\\n### 1.5 - 1.6 \\n## Context\\nWe want our users not be able to send us their sensitive data in an email or other way, so they can't send us their sensitive data in an email or other way.\\n## Decision\\nThe User does not want us not be able send them their sensitive data in an email or other way, so they can't send us their sensitive data in an email or other way.\\n\\n"}
{"File Name":"hmpps-interventions-ui\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael\\nNygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":463,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"rems\/014-userids.md","Context":"## Problems with the current approach\\n- The `sub` attribute might be opaque and not usable between systems, e.g. when sending entitlements to other systems\\n- Different login methods may have different formats for external user identifiers\\n- Different login methods might use conflicting values for claims\\n- E.g. `foo@csc.fi` via HAKA or CSC SSO (TODO: do we know of cases like this?)\\n- A single user might have multiple external identifiers (TODO: do we know of cases like this?)\\n","Decision":"We'll add internal random user ids to REMS. This internal user id will\\nbe the key that users are referred to within REMS. The user's external\\nid will be stored in the attributes JSON blob.\\nThis will allow us more flexibility in the future when identity\\nrequirements and use cases change, and might also make all sorts of\\nmigrations easier (since internal user ids don't need to be touched).\\n","tokens":110,"id":464,"Predictions":"## The current approach seems to be the best one.\\n\\n\\n## Decision Reasoning\\n## This decision is based on the following considerations:\\n\\n- The `sub` attribute will be opaque to the user, so it will not have any meaning for external systems.\\n- Different login methods may use different formats for external user identifiers, so it is not possible to use the same format for both systems.\\n- Different login methods might use conflicting values for claims, so it is not possible to use the same value for both systems.\\n- A single user might have multiple external identifiers, so it is not possible to use the same value for both systems.\\n\\n\\n## Decision Reasoning\\n## The current approach seems to be the best one.\\n```\\nI think that this is a good start.  I'm not sure what all of these things mean, but I think we should make a decision whether we want to store them or not.  I don't know if we should store them at all.  If we decide that they are important enough, then we should store them.  If they are less important, then we shouldn't store them.\\n"}
{"File Name":"rems\/017-applicant-ux.md","Context":"# Problems with the current applicant UX\\n","Decision":"Here's a step-by-step plan for fixing these.\\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https:\/\/github.com\/CSCfi\/rems\/issues\/2766)\\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https:\/\/github.com\/CSCfi\/rems\/issues\/2614)\\n3. (Problem B) Autosave in the background by replacing\/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https:\/\/github.com\/CSCfi\/rems\/issues\/2767)\\n","tokens":8,"id":465,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n# Problem with the current applicant UX\\n\\n### Problem with the current applicant UX\\n\\n### Problem with the current applicant UX\\n\\n### Problem with the current applicant UX\\n\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{User Interface}\\n\\label{sec:ui}\\nThe user interface is composed of a set of \\texttt{UIButtons} which allow to switch between different views. Each \\texttt{UIButton} is associated with a \\texttt{UIView} which defines the user interface and its appearance. The \\texttt{UIView}s are composed by a set of \\texttt{UIViews} which are responsible for displaying information in a specific way. For example, in Figure~\\ref{fig:ui}, a \\texttt{MyView} is used to display a list of all my projects. This view has an associated \\texttt{UIViewList} which allows to navigate through all the projects in my account. The \\texttt{UIViewsList} is composed by an array of \\texttt{UIViews}. Each instance has an associated label and an associated style.\\n\\n\\begin{figure}[h!]\\n\t\\centering\\n\t\\includegraphics[width=0.9\\linewidth]{.\/img\/ui.png}\\n\t\\caption[Image]{The user interface. The user can navigate through all the projects in my account by selecting one from the list. The list displays each project's name and its associated style.}\\n\t\\label{fig:ui}\\n\\end{figure}\\n\\nThe user can select one or more projects from the list and then click on one of them to display its corresponding view.\\n\\nEach \\texttt{UIView} has an associated \\texttt{text}, an associated style and an associated label.\\nIn this section, we will describe each element of these components.\\n\\n\\subsubsection{\\texttt{text}}\\nThe text element represents any text that can be displayed on screen.\\nThis text may be displayed as a single line or as multiple lines.\\nIt can also contain images or other elements that are not visible on screen.\\n\\nA text element may be added to any UIView.\\nWhen adding a text element to a UIView, it is automatically added at the bottom of that view.\\n\\nA text element may be removed from any UIView.\\nWhen removing a text element from a UIView, it is automatically removed from all other UIViews that contain that same view.\\n\\nA text element may have different styles depending on its content.\\nDifferent styles may be applied to different parts of the same text element.\\n\\nA text element may have different labels depending on its content.\\nDifferent labels may be applied to different parts of the same text element.\\n\\nEach UIView has two properties:\\nthe first property represents the type of content that should be displayed;\\nthe second property represents how this content should be displayed.\\nThese two properties are described below:\\n\\n\\begin{itemize}\\n    \\item {\\bf Content Type}: This property represents how this content should be displayed (see Figure~\\ref{fig:content_type}). \\n    It may take one value (for example, \"Title\") or multiple values (for example, \"Title 1\", \"Title 2\", \"Title 3\").\\n    In this case, when setting this property, you must specify exactly what values you want for each part (for example, \"Title 1\" and \"Title 2\").\\n    You can also use placeholder values (for example, \"First Title\" or \"Second Title\") if you want to specify placeholders for each part instead of specifying specific values for each part.\\n    If you set this property using placeholder values, it will automatically insert placeholders in your code if they do not exist already.\\n\\n    {\\bf Label}: This property represents how this content should be displayed (see Figure~\\ref{fig:label}). \\n    It can take any value (for example, \"Label 1\", \"Label 2\", \"Label 3\").\\n    In this case, when setting this property, you must specify exactly what value you want for each part (for example, \"Label 1\" or \"Label 2\").\\n    You can also use placeholder values (for example, \"First Label\" or \"Second Label\") if you want to specify placeholders for each part instead of specifying specific values for each part.\\n    If you set this property using placeholder values, it will automatically insert placeholders in your code if they do not exist already.\\n\\n\\n    {\\bf Style}: This property represents how this content should look like (see Figure~\\ref{fig:style}). \\n    It can take one value ({\\itshape e.g., bold}), multiple values ({\\itshape e.g., italic}), or no value ({\\itshape e.g., normal}) and takes no special meaning within CSS rules.\\n    \\n    {\\bf Label}: This property represents how this content should look like (see Figure~\\ref {fig:label}). \\n      It can take any value ({e.g., bold}), multiple values ({e.g., italic}), or no value ({e.g., normal}) and takes no special meaning within CSS rules.\\n    \\n      {\\bf Style}: This property represents how this content should look like (see Figure~{\\itshape .}) \\n      It can take one value ({e.g., bold}), multiple values ({e.g., italic}), or no value ({e.g., normal}) and takes no special meaning within CSS rules.\\n\\n\\n      Each UIView has two properties:\\n      {\\bf Style}: This property represents how this content should look like (\\emph{i.e.,} it describes what kind of style should apply to it).\\n      It can take any value ({e.g., bold}, etc.), multiple values ({e.g., italic}, etc.), or no value ({e.g., normal}) and takes no special meaning within CSS rules.\\n\\n\\n      {\\bf Label}: This property represents how this content should look like (\\emph{i.e.,} it describes what kind of label should apply to it).\\n      It can take any value ({e.g., bold}, etc.), multiple values ({e.g., italic}, etc.), or no value ({e.g., normal}) and takes no special meaning within CSS rules.\\n\\n\\n      Each UIView has two properties:\\n      {\\bf Label}: This property represents how this content should look like (\\emph{i.e.,} it describes what kind of label should apply to it).\\n      It can take any value ({e.g., bold}, etc.), multiple values ({e.g., italic}, etc.), or no value ({e.g.,\\n        normal}) and takes no special meaning within CSS rules.\\n\\n\\n      {\\bf Style}: This property represents how this content should look like (\\emph{i.e.,\\n        e.g.. Bold}. It describes what kind of style apply to it).\\n      It can take any value ({e.g.,\\n        bold}, etc.), multiple values ({e.g.,\\n        italic}, etc.), or no value ({e.g.,\\n        normal}) and takes no special meaning within CSS rules.\\n\\n\\n      Each UIView has two properties:\\n      {\\bf Style}: This property represents how this content should look like (\\emph{i.e.,\\n        e.g.. Bold}. It describes what kind of style apply to it).\\n      It can take any value({ e,g,\\n        bold},{etc},{etc}) , multiplevalues({ e,g,\\n       italic},{etc},{etc})ornovalue({ e,g,\\n       normal}))andtakesnospecialmeaningwithinCSSrules.\\n\\n\\n   }\\n   %{\\item Content Type: The type of contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Label: The label representing contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Style: The style representing contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Label}: The label representing contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Style}: The style representing contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Label}: The label representing contents that will appear on screen as shown in Figure~{\\itshape .}}\\n   %{\\item Style}: The style representing contents that will appear on screen as shown in Figure~{\\itshape .}}   \\n%{\\n%     Content Type: The type(s)ofcontentsthatwillappearonscreenasshowninFigure%\\n%     {%\\n%         {%\\n%             {%\\n%                 {%\\n%                     {%\\n%                         {%\\n%                         {%\\n%                             {%\\n%                             {%\\n%                                 {%\\n%                                 {%\\n%                                     {%\\n%                                     {%\\n%                                     }{}{}{}{}{}\\n%                                     }{}{}{}\\n%                             }{}{}\\n%                         }{}{}\\n%%                         }\\n%%                         }\\n%%                     }{}\\n%%                 }{}\\n%%             }{}\\n%%         }{}\\n%%     }%\\n%%     ]\\n%%\\n%\\vspace{-0.5cm}\\n%\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/label.png}\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/style.png}\\vspace{-0.5cm}}\\n\\n%\\vspace{-0.5cm}\\n%\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/label.png}\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/style.png}\\vspace{-0.5cm}}\\n\\n%\\vspace{-0.5cm}\\n%\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/label.png}\\hspace{-0.5cm}\\includegraphics[height=4cm]{.\/img\/style.png}\\vspace{-0.5cm}}\\n\\n\\vspace*{-1mm}\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\vspace*{-1mm}\\n\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\noindent\\rule[-2mm]{100mm}{3mm}\\n\\n\\n"}
{"File Name":"openlobby-server\/0010-replace-flask-with-django.md","Context":"## Context\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n","Decision":"We will switch to Django. It's not only well written server but it has also\\n\"batteries included\" like a good ORM layer. And some other features like\\nmiddlewares will simplify things.\\n","tokens":47,"id":466,"Predictions":"## Decision\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n\\n## Decision\\n## Decision\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n\\n## Decision\\n## Decision\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n\\n\\end{document}\\n"}
{"File Name":"openlobby-server\/0004-graphql-api.md","Context":"## Context\\nOpen Lobby Server will be written in API First design. Frontend applications\\nwill be based on it's API.\\n","Decision":"We will use GraphQL as API standard. From other options like REST and Graph API\\nit's the most client friendly approach. GraphQL query language is easy to use,\\nvalidated and self documenting.\\nGraphQL allows clients to get everything they need in one request without any\\noverhead of not needed data. That is very important for mobile frontends.\\nGraphiQL tool also provides easy way for developers to inspect and try API. So\\nit's easy to adopt by frontend applications developers or other API users.\\n","tokens":27,"id":467,"Predictions":"## Decision\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe front-end will be written in API First design. Backend applications\\nwill be based on it's API.\\n\\n## Decision\\n## Decision\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe back-end will be written in API First design. Front-end applications\\nwill be based on it's API.\\n\\n## Decision\\n## Decision\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe back-end will be written in API First design. Front-end applications\\nwill be based on it's API.\\n\\n## Decision\\n## Decision\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe front-end will be written in API First design. Backend applications\\nwill be based on it's API.\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Application Architecture}\\n\\begin{lstlisting}\\n# Application Architecture, Version 1\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Application Architecture, Version 2}\\n\\begin{lstlisting}\\n# Application Architecture, Version 2\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Application Architecture, Version 3}\\n\\begin{lstlisting}\\n# Application Architecture, Version 3\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{Application Architecture, Version 4}\\n\\begin{lstlisting}\\n# Application Architecture, Version 4\\n\\n\\end{lstlisting}\\n\\n"}
{"File Name":"openlobby-server\/0012-use-postgresql.md","Context":"## Context\\nWe want to add relational database.\\n","Decision":"We will use PostgreSQL. It's a mature database with handy features like JSON\\nand hstore data types. It's fully ACID compliant including schema changes. It\\nhas very good support in Django's ORM.\\nAnother popular option is MySQL\/MariaDB. But because it has a major bug `#28727`\\n(10 years since it has been reported and it's still not fixed) breaking ACID in\\nschema changes it can't be used for any serious project.\\n","tokens":11,"id":468,"Predictions":"## Add the relational database.\\n\\n## Decision\\n## Remove the relational database.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a web application. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe want to add relational database.\\n\\n## Decision\\n## Add the relational database.\\n\\n## Decision\\n## Remove the relational database.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a mobile application. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe want to add relational database.\\n\\n## Decision\\n## Add the relational database.\\n\\n## Decision\\n## Remove the relational database.\\n\\n%% A decision record is a sequence of actions that have been taken on one or more items.\\n%% The actions are listed in order of their priority, with higher priorities having higher impact.\\n%% Actions are ordered by priority in sequence, from highest priority to lowest priority.\\n\\n\\begin{DoxyCode}\\n\\end{DoxyCode}\\n\\n\\n\\subsection{Examples}\\n\\n\\hypertarget{decision-records-example}{%\\n\\subsubsection{Example}\\label{decision-records-example}}\\n\\nThis is an example of a \\hyperlink{decision-records-example}{Decision Record}.\\n\\n%% A decision record is a sequence of actions that have been taken on one or more items.\\n%% The actions are listed in order of their priority, with higher priorities having higher impact.\\n%% Actions are ordered by priority in sequence, from highest priority to lowest priority.\\n\\n\\begin{DoxyCode}\\n\\NormalTok{do\\_add\\_database <-}\\StringTok{ }\\ControlFlowTok{function}\\NormalTok{(}\\DataTypeTok{id =} \\DecValTok{1}\\NormalTok{) }\\ControlFlowTok{function}\\NormalTok{(}\\DataTypeTok{id =} \\DecValTok{2}\\NormalTok{) }\\ControlFlowTok{function}\\NormalTok{(}\\DataTypeTok{id =} \\DecValTok{T }\\OperatorTok{-> }\\KeywordTok{return}\\NormalTok{(id))  \\{}\\n    \\KeywordTok{return}\\NormalTok{(id)}\\n\\NormalTok{\\}}\\n\\end{DoxyCode}\\n\\n\\nThe following branches can be considered as decisions:\\n\\n\\begin{DoxyItemize}\\n\\tightlist\\n\\item\\n  do\\_add\\_database() -\\\/ Add a new SQL statement to be executed when an item is added to a table.\\n\\item\\n  do\\_add\\_database(id) -\\\/ Add an item to a table using its id as primary key.\\n  \\begin{itemize}\\n  \\tightlist\\n  \\item\\n    id should be unique and cannot be null. This will create an index on id (primary key).\\n  \\item\\n    The default value for id can be any number between 1 and $>$= max(1, nrow(mtcars)) (see below).\\n  \\end{itemize}\\n\\item\\n  do\\_add\\_database(id = T) -\\\/ Add an item using its id as primary key.\\n  \\begin{itemize}\\n  \\tightlist\\n  \\item\\n    id can be any number between 1 and $>$= max(1, nrow(mtcars)) (see below).\\n  \\end{itemize}\\n\\end{DoxyItemize}\\n\\nThe following branches can also be considered as decisions:\\n\\n\\begin{DoxyItemize}\\n\\tightlist\\n\\item\\n  do\\_add\\_database(id = T) -\\\/ Create an index on id (primary key).\\n\\end{DoxyItemize}\\n\\nThe following branches can also be considered as decisions:\\n\\n\\begin{DoxyItemize}\\n\\tightlist\\n\\item\\n  do\\_add\\_database(id = T) -\\\/ Create and index on id (primary key).\\n\\end{DoxyItemize}\\n\\nThe following branches can also be considered as decisions:\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n% FIGURE: Branches\\n\\n\\n"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"Predictions":"## The Webserver is chosen. This is a decision record. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe Webserver is chosen. This is a decision record. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## The Webserver is chosen. This is a decision record. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe Webserver is chosen. This is a decision record. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n\\end{lstlisting}\\n\\n\\subsubsection{API}\\nWe define an API for our program which consists of two parts: \\texttt{WebServer} and \\texttt{WebServerFactory}. The first part contains methods that return an instance of \\texttt{WebServer} and methods that return instances of \\texttt{WebServerFactory}. The second part contains methods that return an instance of \\texttt{WebServerFactory}.\\n\\n\\subsection{Implementation}\\n\\subsubsection{Design}\\nAs we have seen in section \\ref{sec:design}, we have implemented our design in Java using JUnit and Eclipse IDE (Figure~\\ref{fig:design}). In this section, we will describe how we implemented our design.\\n\\n\\begin{figure}[H]\\n  \\centering\\n  \\includegraphics[width=0.9\\linewidth]{Design.png}\\n  \\caption{\\label{fig:design}Implementation of our design using JUnit and Eclipse IDE}\\n\\end{figure}\\n\\n\\nWe started with defining some concepts, such as classes, interfaces, and packages. We also defined data types, such as integers, strings, dates, and floats.\\n\\nIn this project, we have created two classes:\\n\\begin{lstlisting}[language=Java]\\nclass WebServer implements Runnable\\n{\\n    public void run()\\n    {\\n        System.out.println(\"running...\");\\n        try\\n        {\\n            Thread.sleep(1000);\\n        }\\n        catch (InterruptedException e)\\n        {\\n            e.printStackTrace();\\n        }\\n\\n        \/\/ To give this method priority over other threads.\\n        \/\/ If the main thread sleeps for too long,\\n        \/\/ it will cause other threads to be awakened.\\n    }\\n}\\n\\nclass WebServerFactory implements Factory<WebServer>\\n{\\n    public static <T> WebServer create(T instance)\\n    {\\n        if(instance == null)\\n            throw new NullPointerException(\"instance must be initialized\");\\n\\n        T t = (T) instance;\\n\\n        if(t instanceof WebServer)\\n            return (WebServer)t;\\n        \\n        throw new IllegalArgumentException(\"instance must be initialized\");\\n    }\\n}\\n\\end{lstlisting}\\n\\nIn addition to these classes, we also defined some methods which are used in our application:\\n\\n\\begin{lstlisting}[language=Java]\\npublic class Application extends Application\\n{\\n    public static void main(String[] args) throws Exception\\n    {\\n        \\n         \/\/ Create web server object.\\n         final String serverName = \"localhost\";\\n         final String port = \"80\";\\n        \\n         final WebServer webServer = new WebServer(serverName,port);\\n        \\n         \/\/ Create web server factory object.\\n         final Factory<WebServer> factory = new Factory<WebServerFactory>();\\n        \\n         \/\/ Set up web server factory object.\\n         factory.setFactory(factory);\\n        \\n         \/\/ Start web server process on given port number.\\n         webServer.start();\\n        \\n         \/\/ Check whether there are any running servers on given port number.\\n         if(webServer.isRunningOnPort(port))\\n            System.out.println(\"Running on port \" + port + \"...\");\\n\t}\\n}\\n\\end{lstlisting}\\n\\nThe application class provides functionality for creating instances of different classes in order to implement our design.\\n\\nOnce all of these concepts are defined, we can start implementing our design using JUnit tests.\\n\\nIn this project, we have written tests for each class which covers all of its functionality. We have written tests to ensure that all methods work as expected:\\n\\n\\begin{lstlisting}[language=Java]\\npublic class TestApplication extends Application\\n{\\n    @Test\\n    public void testStart()\\n    {\\n        \\n      final String serverName = \"localhost\";\\n      final String port = \"80\";\\n      \\n      final String url = \"http:\/\/localhost:\" + serverName + \":\" + port;\\n\\n      System.out.println(url);\\n\\n      final WebClient client = new WebClient();\\n      \\n      client.get(url);\\n\\n      assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n\\n      client.close();\\n\\n      assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n      \\n      Thread.sleep(1000);\\n      \\n      assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n      \\n     }\\n\\n     @Test\\n     public void testGet()\\n     {\\n       \\n       final String url =\"http:\/\/localhost:\" + serverName + \":\" + port;\\n       \\n       System.out.println(url);\\n\\n       final WebClient client = new WebClient();\\n       \\n       client.get(url);\\n\\n       assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n       \\n     }\\n\\n     @Test\\n     public void testPut()\\n     {\\n       \\n       final String url =\"http:\/\/localhost:\" + serverName + \":\" + port;\\n       \\n       System.out.println(url);\\n\\n       final WebClient client = new WebClient();\\n       \\n       client.put(url);\\n\\n       assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n       \\n     }\\n\\n     @Test\\n     public void testDelete()\\n     {\\n       \\n       final String url =\"http:\/\/localhost:\" + serverName + \":\" + port;\\n       \\n       System.out.println(url);\\n\\n       final WebClient client = new WebClient();\\n       \\n       client.deleteUrl(url);\\n\\n       assertEquals(url+\":\"+port,\"http:\/\/localhost:\"+serverName+\":\"+port);\\n       \\n     }\\n\\n }\\n    \\n   @Test(expectedExceptions=IllegalArgumentException.class)\\n   public void testIllegalArgumentException()\\n   {\\n   \\n   \\n   }\\n   \\n   \\n   @Test(expectedExceptions=NullPointerException.class)\\n   public void testNullPointerException()\\n   {\\n   \\n   \\n   }\\n   \\n \\n }\\n \\n\/\/...\\n}\\n \\n\/\/...\\n}\\n \\n\/\/...\\n}\\n\\nThe tests are written using JUnit's predefined TestNG framework which allows us to easily write tests for any kind of application:\\n\\n\\begin{itemize}\\n\t\\item Tests are grouped into groups called Test Suites which contain one or more Test Classes each containing a single Test Method (see Figure~\\ref{fig:testSuite}).\\n\t\tTests will therefore be grouped into groups according to their type (see Figure~\\ref{schemata}) and then executed within each group according to their order (see Figure~\\ref{schemata}). For example:\\n\t\t\\tab \\tab The class `testCreate` contains a single method `testGet` which will be executed when `testCreate` is executed within `testSuite`. When `testCreate` is executed within `testSuite`, it will execute `testGet` twice: once when it first runs (`@Before`) and again when it runs (`@After`) (`@After`). Similarly, when it runs (`@After`) (`@Before`) (`@After`), it will execute all its tests again (`@After`). Therefore if you want your tests to run only once per execution cycle you would use an array with two elements: one element containing your initial setup code (`@Before`) and another element containing your teardown code (`@After`). In this case you would write the following code:\\n\t\t\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\tab\\\\\\n \t\t\\tab 1 * 2 * 3 * 4 * 5 * 6 * 7 * 8 * 9 \\\\\\n \t\t\\tab $=$ $1$ $2$ $3$ $4$ $5$ $6$ $7$ $8$ $9$\\n \t\t\\\\\\n \t\tThe above code shows how you could group your tests into groups so that they execute only once per execution cycle (i.e., so that they run only once per execution cycle). You could also group them into suites so that they run only once per suite but then you would need to add extra logic for grouping them into suites according to their order (i.e., so that they run only once per suite).\\n\t\t\\n\t\tTips:\\n\t\t\\n\t\t - You can also group your tests by name using something like this:\\n\t\t\t\\tab Testing A -> testing A -> testing B -> testing C ...\\n\t\t\t\\n\t\t - You can also group your tests by name using something like this:\\n\t\t\t\\tab Testing A -> testing A -> testing B -> testing C ...\\n\t\t\t\\n\t\t - You can also group your tests by name using something like this:\\n\t\t\t\\tab Testing A -> testing A -> testing B -> testing C ...\\n\t\t\t\\n\t\t - You can also group your tests by name using something like this:\\n\t\t\t\\tab Testing A -> testing A -> testing B -> testing C ...\\n\t\t\t\\n\t\t - If you want multiple groups or suites within one test suite then use something like this:\\n\t\t\t\tTAB Testing A1 --> Testing A1 --> Testing B1 --> Testing C1 ... TAB Testing A2 --> Testing A2 --> Testing B2 --> Testing C2 ... TAB Testing A3 --> Testing A3 --> Testing B3 --> Testing C3 ... TAB TESTING A4 --> TESTING B4--> TESTING C4 ... TAB TESTING AB--> TESTING CB--> TESTING CD ...\\n\t\t\t\t\\n\t\t\t\tTAB Tests should always be grouped according to their type (e.g., unit or integration).\\n\t\t\t\tTAB Tests should always be grouped according to their order (e.g., before\/after).\\n\t\t\t\tTAB Tests should always be grouped according to their name.\\n\t\t\t\t\\n\t\t\t\tTAB Each Test Class should contain at least one method called `setUp()` which performs all initial setup tasks necessary for executing all its other methods (`tearDown()`).\\n\t\t\t\t\\n\t\t\t\tTAB Each Test Class should contain at least one method called `tearDown()`.\\n\t\t\t\t\\n\t\t\t\tTAB Each Test Class should contain at least one method called `tearDown()`.\\n\t\t\t\t\\n\t\t\t\tTAB Each Test Class should contain at least one method called `setUp()`.\\n\t\t\t\t\\n\t\t\t\tTAB Each Test Class should contain at least one method called `tearDown()`.\\n\t\t\t\t\\n\t\t\t\tThe above code shows how you could group your tests into suites so that they run only once per suite but then you would need extra logic for grouping them into suites according to their order (i.e., so that they run only once per suite). You could also group them into suites so that they run only once per suite but then you would need extra logic for grouping them into suites according to their order (i.e., so that they run only once per suite).\\n\t\t\t\t\\n\t\t\t\tTips:\\n\t\t\t\t\t\\n\t\t\t\t - If you want multiple groups or suites within one test suite then use something like this:\\\\\\n\\n\t\t\t\t TAPTests should always be grouped according to their type(e.g., unit or integration).\\n\\n\t\t\t\t TAPTests should always be grouped according to their order(e.g., before\/after).\\n\\n\t\t\t\t TAPTests should always be grouped according to their name.\\n\\n\\n\t\t\t\t\t\\n\t\t\t\t - If you want multiple groups or suites within one test suite then use something like this:\\\\\\n\\n\t\t\t\t TAPTests should always be grouped according to their type(e.g., unit or integration).\\n\\n\t\t\t\t TAPTests should always be grouped according to their order(e.g., before\/after).\\n\\n\t\t\t\t TAPTests should always be grouped according to their name.\\n\\n\\n\t\t\t\t\t\\n\t\t\t\t - Each Test Class should contain at least one method called 'setUp()' which performs all initial setup tasks necessary for executing all its other methods ('tearDown()').\\n\t\t\t\t\t\\n\t\t\t\t - Each Test Class should contain at least one method called 'tearDown()'.\\n\\n\\n\t\t\t\t\t\\n\t\t\t\t - Each Test Class should contain at least one method called 'setUp()'.\\n\\n\\n\t\t\t\t\t\\n\t\t\t\t - Each Test Class should contain at least one method called 'tearDown()'.\\n\\n\\n\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\t\\n\t\t\t\t\t\tIn addition, each class in our application can optionally implement certain interfaces in order perform certain tasks:\\n\\n\t\t\t\t\t\tIn addition each class in our application can optionally implement certain interfaces in order perform certain tasks:\\n\\n\t\t\t\t\t\tIn addition each class in our application can optionally implement certain interfaces in order perform certain tasks:\\n\\n\t\t\t\t\t\tIn addition each class in our application can optionally implement certain interfaces in order perform certain tasks:\\n\\n\t\t\t\t\t\tIn addition each class in our application can optionally implement certain interfaces in order"}
{"File Name":"openlobby-server\/0007-adopt-graphql-relay-specification.md","Context":"## Context\\nWe need to make API friendly for clients and design pagination.\\n","Decision":"We will adopt GraphQL Relay specification. It solves pagination so we don't\\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\\nIt has a way to define inputs in mutations.\\nGraphene lib has good support for creating API following Relay specifications.\\n","tokens":16,"id":470,"Predictions":"## 1. Design a pagination interface for the website.\\n## 2. Make API friendly for clients.\\n## 3. Implement pagination in the website.\\n## Context\\nWe need to make API friendly for clients and design pagination.\\n\\n## Decision\\n## 1. Design a pagination interface for the website.\\n## 2. Make API friendly for clients.\\n## 3. Implement pagination in the website.\\n\\nThe decision record is used to decide whether the given decision is correct or not.\\n\\nThe following are the types of data that can be entered in this record.\\n\\n- Decision\\n- Context\\n\\nThe following are the fields of a decision record.\\n\\n- Decision: This is an integer that represents a decision number. The value is either one or zero, depending on whether this is the first or last decision in a set of decisions.\\n\\n- Context: This is an integer that represents a context number. The value is either one or zero, depending on whether this context has been used before.\\n\\nExample\\n\\nExample of Decision Record\\n\\nInput:\\n\\nDecision Record:\\n\\nDecision: 1\\nContext: 1\\n\\nOutput:\\n\\nDecision Record\\n\\nDecision: 1\\nContext: 2\\n\\nSource Code:\\n\\nimport json\r\n\\nfrom pprint import pprint\r\n\\n\r\n\\ndef main():\r\n\\n    json_data = {\r\n\\n        'decision': {\r\n\\n            't': {\r\n\\n                'i': {\r\n\\n                    'value': '123'\r\n\\n                },\r\n\\n                'c': {\r\n\\n                    'value': '123'\r\n\\n                }\r\n\\n            },\r\n\\n            'context': {\r\n\\n                'i': {\r\n\\n                    'value': '123'\r\n\\n                },\r\n\\n                'c': {\r\n\\n                    'value': '',\r\n\\n                    'i': {'value': ''\r\n\\n                        if (isnull(t.value)):\r\n\\n                            t.value = t.value + ', '\r\n\\n                        else:\r\n\\n                            t.value = t.value + ',' + t.value\r\n\\n                        }\r\n\\n                    }\r\n\\n                }\r\n\\n            }\r\n\\n        }\r\n\\n    }\r\n\\n\r\n\\n    pprint(json_data)\r\n\\n\r\n\\nif __name__ == '__main__':\r\n\\n    main()\\n\\nOutput:\\n\\n{u't': u'123', u'c': u'123', u'i': u'123'}\\n\\n4 thoughts on \u201cPython | Decision Record\u201d\\n\\nPingback: Python | Decision Tree | Pythoneers\\nPingback: Python | Decision Tree | Pythoneers\\nPingback: Python | Decision Tree | Pythoneers\\nPingback: Python | Decision Tree | Pythoneers"}
{"File Name":"openlobby-server\/0013-black-code-formatter.md","Context":"## Context\\nWe would like to simplify code reviews and unify code style.\\n","Decision":"Use Black code formatter: https:\/\/github.com\/ambv\/black\\n","tokens":16,"id":471,"Predictions":"## (1) We would like to see a uniform style for code reviews and code style guides.\\n\\n## (2) We would like to see some code style guidelines and an outline of the code style guidelines.\\n\\n## (3) We would like to see a set of rules for code reviews and code style guides.\\n\\n## (4) We would like to see a set of rules for code reviews and code style guidelines.\\n\\n## (5) We would like to see a set of rules for code reviews and code style guidelines.\\n\\n## (6) We would like to see a set of rules for code reviews and code style guidelines.\\n\\n\\n\\end{document}\\n"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"Predictions":"## This is a software project. We need to choose main programming language for this project.\\n## Decision\\n## The decision is: C++.\\n## Decision\\n## This is a software project. We need to choose main programming language for this project.\\n## Decision\\n## The decision is: C++.\\n## Decision\\n## This is a software project. We need to choose main programming language for this project.\\n## Decision\\n## The decision is: C++.\\n\\end{verbatim}\\n\\n\\subsubsection{Decision}\\n\\begin{verbatim}\\nThis is a software project. We need to choose main programming language for this project.\\n\\end{verbatim}\\n\\n\\subsubsection{Decision}\\n\\begin{verbatim}\\nThe decision is: C++.\\n\\end{verbatim}\\n\\n\\subsubsection{Decision}\\n\\begin{verbatim}\\nThis is a software project. We need to choose main programming language for this project.\\n\\end{verbatim}\\n\\n\\subsubsection{Decision}\\n\\begin{verbatim}\\nThe decision is: C++.\\n\\end{verbatim}\\n\\n\\n"}
{"File Name":"openlobby-server\/0002-use-elasticsearch-for-fulltext-search.md","Context":"## Context\\nWe need a database with fulltext search capable of searching in various\\nlanguages especially in Czech.\\n","Decision":"We will use Elasticsearch. It's well known database with great fulltext search\\ncapabilities based on Apache Lucene. It has also aggregations, highlighting of\\nresults, and many other useful features.\\nWe will use it as database for all data so we have just one database in the\\nsystem.\\n","tokens":23,"id":473,"Predictions":"## Decision: We need a database with fulltext search capable of\\nsearching in various languages especially in Czech.\\n## Result\\n## Result: The database is ready for use.\\n\\n\\end{lstlisting}\\n\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Database Query Language}\\n\\begin{lstlisting}\\nCREATE TABLE `search` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `name` varchar(255) NOT NULL,\\n  `language` text COLLATE utf8_unicode_ci NOT NULL DEFAULT 'Czech',\\n  PRIMARY KEY (`id`),\\n  INDEX `idx_name_language` (`name`,`language`)\\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci;\\n\\end{lstlisting}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Database Query Language}\\n\\begin{lstlisting}\\nINSERT INTO search(`id`, `name`, `language`) VALUES\\n(1, 'Czech', 'czech'),\\n(2, 'English', 'english'),\\n(3, 'German', 'german');\\n\\end{lstlisting}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Database Query Language}\\n\\begin{lstlisting}\\nSELECT * FROM search WHERE language = 'english';\\n+----+-------------------+-------+--------+\\n| id | name              | language | \\n+----+-------------------+-------+--------+\\n| 1 | Czech               | english | \\n| 2 | English            | english |\\n| 3 | German              | german |\\n+----+-------------------+-------+--------+\\n3 rows in set (0.00 sec)\\n\\end{lstlisting}\\n\\end{frame}\\n\\n% \\begin{frame}[fragile]{Database Query Language}\\n% \\begin{lstlisting}\\n% SELECT * FROM search WHERE language = 'czech';\\n% +----+-------------------+-------+--------+\\n% | id | name              | language   |\\n% +----+-------------------+-------+--------+\\n% | 1 | Czech               | english |\\n% +----+-------------------+-------+--------+\\n% 1 row in set (0.00 sec)\\n%\\n% SELECT * FROM search WHERE language = 'german';\\n%\\n% +----+-------------------+-------+--------+\\n% | id | name              | language   |\\n% +----+-------------------+-------\/--------\\n% | 1 | Czech               | english |\\n% +----\/--------\\n%\\n%\\end{lstlisting}\\n%\\n%\\end{frame}\\n\\n"}
{"File Name":"openlobby-server\/0008-pytest.md","Context":"## Context\\nWe need to choose framework for tests.\\n","Decision":"We will use Pytest. It's much more Pythonic and simplier to use than Unittest\\nfrom standrd library.\\n","tokens":12,"id":474,"Predictions":"## Framework is selected.\\n## Context\\nWe need to choose framework for tests.\\n\\n## Decision\\n## Framework is selected.\\n## Context\\nWe need to choose framework for tests.\\n\\n## Decision\\n## Framework is selected.\\n## Context\\nWe need to choose framework for tests.\\n\\n## Decision\\n## Framework is selected.\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Architecture decisions}\\n\\begin{verbatim}\\n# 1: 1.0 - 1.5 (1.0-1.5) - 2.0 (2.0-2.5) - 3.0 (3.0-3.5)\\n# 2: 4.0 (4.0-4.5) - 5.0 (5.0-5.5) - 6.0 (6.0-6.5)\\n# 3: 7.0 (7.0-7.5) - 8.0 (8.0-8.5) - 9.x (9.x-9.f)\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Architecture decisions}\\n\\begin{verbatim}\\n# Task: \"Create a test suite\"\\n# Task:\\n#   Task: \"Build the test suite\"\\n#   Task:\\n#     Task: \"Run the test suite\"\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\nTask:\\n```\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]\\frametitle{Architecture decisions}\\n\\begin{verbatim}\\n# Task: \"Create a test suite\"\\n# Task:\\n\\nclass TestSuite:\\n\\n    def __init__(self, config):\\n        self.config = config\\n\\ndef create_test_suite(config):\\n    \"\"\" Create a Test Suite \"\"\"\\n    # Create the TestSuite object with the given configuration.\\n\\n    # Define a list of tasks.\\n    tasks = []\\n\\n    # Add all tasks to the list.\\n    for task in config.tasks_list():\\n        tasks.append(task)\\n\\n    # Create an empty test suite object and add it to the list of Test Suites.\\n    self.test_suite = unittest.TestLoader().loadTestsFromTestCase(tests)\\n\\ndef run(self):\\n    \"\"\" Run this Test Suite \"\"\"\\n    # Run each task in the Test Suite.\\n\\ndef build(self):\\n    \"\"\" Build the Test Suite \"\"\"\\n    # Build each task in the Test Suite.\\n\\ndef run_test_suite(self, test_suite=None):\\n    \"\"\" Run this Test Suite \"\"\"\\n    # Run all tests in the Test Suite.\\n\\ndef run_test_suite_with_tests(self, unittests=()):\\n    \"\"\" Run this Test Suite with Tests \"\"\"\\n    # Run all tests in the Test Suite with tests added from unittests.\\n\\ndef run_test_suite_with_tests_with_tests(self, unittests=()):\\n    \"\"\" Run this Test Suite with Tests added from unittests and then run all tests.\"\"\"\\n```\\nThe function `run_test_suite` creates a `TestSuite` object with configured tasks and runs them using `unittest.TestLoader.loadTestsFromTestCase`. The function `run_test_suite_with_tests` creates a `TestSuite` object and adds it to `unittest.TestLoader.loadTestsFromTestCase`. The function `run_test_suite_with_tests_with_tests` creates a `TestSuite` object and adds it to `unittest.TestLoader.loadTestsFromTestCase`, then calls both functions above to create a new `TestSuite`. \\nThe function `run_test_suite` runs each task in its own separate thread and returns True if it succeeds or False if it fails.\\nThe function `run_test_suite_with_tests`, on the other hand, runs all tasks in their own separate thread and returns True if it succeeds or False if it fails.\\nThe function `run_test_suite_with_tests_with_tests`, on the other hand, runs all tasks in their own separate thread and returns True if it succeeds or False if it fails.\\n\\n\\nThe functions above are nothing more than wrapper functions around existing Python functions that do exactly what they say they do.\\n\\n\\nThe code above shows how classes can be used as simple ``containers'' for other objects that provide functionality.\\n\\n\\nIn Python, classes are used as ``containers'' that encapsulate functionality provided by other objects.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\nFor example, we can use classes as containers for objects that provide functionality similar to what we have seen in previous slides.\\n\\n\\n\\n"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"Predictions":"## Decision: The Elasticsearch 6.4.0 release is a major release in terms of capabilities and\\nfeatures. Elasticsearch 6.x is the first release to have fulltext capabilities\\nand also the first release to have a dedicated engineering team working on fulltext\\ncapabilities.\\n\\n## Context\\n## Context\\nElasticsearch 6.x is designed to provide a high performance, scalable, and\\nsecure search experience for large-scale search tasks. It features a new query engine that\\nreduces the time complexity of queries and also improves the performance of aggregation queries.\\n\\n## Decision\\n## Decision: The Elasticsearch 6.4.0 release is a major release in terms of capabilities and\\nfeatures. Elasticsearch 6.x is the first release to have fulltext capabilities\\nand also the first release to have a dedicated engineering team working on fulltext\\ncapabilities.\\n\\n## Context\\n## Context\\nElasticsearch 6.x is designed to provide a high performance, scalable, and\\nsecure search experience for large-scale search tasks. It features a new query engine that\\nreduces the time complexity of queries and also improves the performance of aggregation queries.\\n\\n## Decision\\n## Decision: The Elasticsearch 6.4.0 release is a major release in terms of capabilities and\\nfeatures. Elasticsearch 6.x is the first release to have fulltext capabilities\\nand also the first release to have a dedicated engineering team working on fulltext\\ncapabilities.\\n\\n\\n\\subsection{Elasticsearch Query Language}\\n\\label{sec:elasticsearch-query-language}\\n\\begin{figure}[ht]\\n    \\centering\\n    \\includegraphics[width=1\\linewidth]{figures\/elasticsearch-query-language.pdf}\\n    \\caption[Query Language]{Query Language}\\n    \\label{fig:elasticsearch-query-language}\\n\\end{figure}\\n\\nThe query language used by ElasticSearch uses JSON syntax for describing RESTful APIs.\\nThe documentation for this language can be found in section~\\ref{sec:elasticsearch-query-language}.\\nThe following sections will provide an overview of this language.\\n\\n\\subsubsection{Query Syntax}\\n\\nThe query syntax consists of two parts:\\n\\n\\begin{itemize}\\n    \\item An operator which can be one or more operators.\\n    \\item A set of keywords which are separated by commas.\\n\\end{itemize}\\n\\nAn operator can be either an operator or keyword.\\n\\nAn operator can be:\\n\\n\\begin{itemize}\\n    \\item An operator which has no keyword attached.\\n    \\item A keyword which has no operator attached.\\n    \\item A keyword which has both an operator attached as well as a keyword.\\n    \\item A keyword which has both an operator attached as well as no keyword attached.\\n    \\item A keyword which has neither an operator nor keyword attached.\\n    \\item An empty set (a comma-separated list).\\n    \\n    Although it is possible to omit keywords from an operator, it is not recommended because it makes it harder for other developers to understand what you mean.\\n    \\n    If you are using ES v5.x or earlier, you should not use keywords without operators because they are not supported in ES v5.x or earlier versions.\\n    \\n    If you are using ES v5.x or earlier, you should use keywords with operators so that your queries will work with ES v5.x or earlier versions.\\n    \\n\\end{itemize}\\n\\nA set of keywords can be separated by commas.\\n\\nA set of keywords can either be separated by commas or by spaces.\\n\\nIf there are multiple keywords in a set then they must be separated by commas.\\n\\nIf there are multiple keywords in a set then they must be separated by spaces.\\n\\nThere will never be more than one space after each keyword in an ES query.\\n\\nEach keyword must end with either \/ or \/?.\\n\\n\\nThe following table shows how each part works:\\n\\n%TODO add examples\\n\\nFor example:\\n\\n%TODO add example\\n\\n%TODO add example\\n\\n%TODO add example\\n\\n%TODO add example\\n\\n%TODO add example\\n\\n\\n\\n"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n## \\n## \\n"}
{"File Name":"openlobby-server\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis article:\\nhttp:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":477,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"openlobby-server\/0009-openid.md","Context":"## Context\\nWe need an authentication mechanism for users. It must be secure and\\nfrontend application independent.\\n","Decision":"We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\\nfor a frontend applications. Ideally over the GraphQL API.\\n","tokens":22,"id":478,"Predictions":"## We decide to go with the following solution. It is secure and easy to use. The\\nbackend application will be developed in C++.\\n\\n## Decision\\n## We will implement a token based authentication mechanism for the frontend\\napplication. The backend application will be developed in C++.\\n\\n%% Commented out for now.\\n%% This is a sample of the token based authentication mechanism.\\n%% \\end{verbatim}\\par\\noindent\\n%% \\noindent\\texttt{token\\_auth\\_token\\_type} - This field specifies the type of token used for authentication.\\n%% \\texttt{token\\_auth\\_user} - This field specifies the user name used for authentication.\\n%% \\texttt{token\\_auth\\_password} - This field specifies the password used for authentication.\\n\\n\\subsection{Authentication}\\n\\label{sec:authentication}\\n\\nThe user can be authenticated using one of two mechanisms: password or token-based. The user can be authenticated using either mechanism or both, depending on the requirements and constraints of an application. The user can also be authenticated using both mechanisms simultaneously.\\n\\nThe user can enter his\/her credentials in a form that looks like this:\\n\\begin{verbatim}\\nuser_name: password\\n\\end{verbatim}\\n\\nThe user name can either be empty or contain a valid username and password combination. If it contains an empty username, then it is assumed that no credentials are provided by the user and therefore no authentication is required. If it contains a valid username and password combination, then it is assumed that the username entered by the user matches with one of those stored in the database.\\n\\nIf a valid username\/password combination is found, then it is assumed that the user has been authenticated successfully. Otherwise, an error message indicating failure to authenticate is displayed.\\n\\nIf both mechanisms are enabled, then both mechanisms are used to authenticate the user. The first mechanism (password) requires that an additional step of verification by sending a request containing all necessary information to the server (such as a hash of all input variables). The second mechanism (token-based) requires additional steps of verification by sending a request containing all necessary information to an external server (such as verifying whether an uploaded file contains valid data).\\n\\nWhen using token-based authentication, only tokens from tokens.xml file are accepted. Tokens.xml file may contain any number of tokens, ranging from one token up to 256 tokens per line.\\n\\n\\subsubsection{Token-Based Authentication}\\nThe following table shows how tokens are processed during token-based authentication:\\n\\n\\begin{center}\\n\\captionof{Tokens Processing}{Table showing how tokens are processed during token-based authentication}\\n\\label{Tokens Processing}\\n\\tabcolsep=1mm\\n\\tabcolsep=2mm\\n\\tabcolsep=2mm\\n\\tabcolsep=2mm\\n\\tabcolsep=3mm\\n\\nToken type & Value & Description & Example\\\\\\n%\\tabcolsep=1mm\\n\\n% \\rowcolor[HTML]{EFEFEF}\\n% \\hline\\n\\n% 0 & User's ID & User's ID \\\\\\n% 1 & Password & User's password \\\\\\n% 2 & Token Type & Token type \\\\\\n% 3 & Token Value & Token value \\\\\\n% 4 & Token Length & Token length \\\\\\n% 5 & Signature Algorithm Type & Signature algorithm type \\\\\\n% 6 - 9& Signature Algorithm Parameters& Signature algorithm parameters \\\\\\n\\n%\\rowcolor[HTML]{EFEFEF}\\n%\\hline\\n\\n%\\rowcolor[HTML]{EFEFEF}\\n%\\hline\\n\\n%\\rowcolor[HTML]{EFEFEF}\\n%\\hline\\n\\n%\\rowcolor[HTML]{EFEFEF}\\n%\\hline\\n\\n%\\n%\\n%\\n%\\n%\\n%\\n\\n\\end{center}\\n\\nThe following table shows how tokens are processed during token-based authentication when multiple tokens are present:\\n\\n\\begin{center}\\n\\captionof{Tokens Processing}{Table showing how tokens are processed during token-based authentication when multiple tokens are present}\\n\\label{Tokens Processing Multiple Tokens}\\n\\tabcolsep=1mm\\n\\tabcolsep=2mm\\n\\tabcolsep=2mm\\n\\tabcolsep=3mm\\n\\nToken type & Value & Description\\\\\\n%\\tabcolsep=1mm\\n\\n% \\rowcolor[HTML]{EFEFEF}\\n\\n% %0&User's ID&User's ID\\\\ %0&Password&User's password\\\\ %0&Token Type&Token type\\\\ %0&Token Value&Token value\\\\ %0&Signature Algorithm Type&A signature algorithm type\\\\ %0&Sig Algo Params&Sig Algo parameters\\\\ %0&Sig Algo Length&Sig Algo length\\\\ %0%\\n\\n%. .\\n\\n%.\\n\\n%.\\n\\n%.\\n\\n%.\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n\\n%\\n%\\n%\\n%\\n%\\n%\\n\\n%\\n%\\n%\\n%\\n\\n\\n\\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n %\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\t\\n\t\\n\t\\n\t\\n\\n\\n\\n\\n\t\\n\t\\n\t\\n\\n\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\\n\t\\n\\n\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\\n\t\t\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n\t\t\\n\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\t\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\t\\n\t\\n\\n\\n\t\\n\t\\n\t\\n\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\t\\n\t\\n\t\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\t\\n\\n\t\t\\n\t\t\\n\\n\t\t\t\\n\t\t\\n\\n\t\t\t\\n\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\\n\t\t\t\\n\t\t\t\\n\t\t\\n\\n\\n\\n\t\t\\n\t\\n\\n\\n\\n\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\\n\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\t\\n\t\\n\\n\\n"}
{"File Name":"nearby-services-api\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":479,"Predictions":"The framework provides a high level abstraction over the low level language features. Effort spent solving domain specific problems should be kept to an absolute minimum. The framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n## Context\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Decision\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Context\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Decision\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Context\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Decision\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Context\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Decision\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Context\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n## Decision\\nThe framework has been designed to provide a flexible and extensible architecture allowing for the addition of new features or components as required.\\n\\n\\n\\end{document}"}
{"File Name":"nearby-services-api\/0003-use-mongodb-for-data-storage.md","Context":"## Context\\nInformation about pharmacies is required by the application in order to display\\nto users. A Docker image of pharmacies\\n([pharmacy-db](https:\/\/hub.docker.com\/r\/nhsuk\/pharmacy-db\/)) running a MongoDB\\ninstance has been created for use by other applications.\\nNodeJS uses a single threaded event loop architecture and as such works best\\nwhen the work it is doing is non-CPU intensive. Searching through datasets is\\npotentially CPU intensive.\\n","Decision":"We have decided to use the existing Docker image rather than spend effort\\nacquiring the data again.\\n","tokens":98,"id":480,"Predictions":"\\nThe decision is to allow the user to search for pharmacies in the UK and display\\nthem in a list view. The list view will also show the number of pharmacies\\navailable in the UK. The decision is based on a number of factors, which are\\nto be considered when making this decision. These factors include:\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  The number of users who will be using this application.\\n\\item\\n  The number of users who will be using this application on mobile devices.\\n\\item\\n  The time it takes to display each pharmacy and how long it takes to display\\neach pharmacy.\\n\\item\\n  The amount of data that needs to be displayed and how much data is needed per\\npharmacy.\\n\\end{itemize}\\n\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displayed.\\nThe decision was made that the user should not have to wait for each pharmacy to\\nbe displayed as they might not want to wait until all pharmacies are displaying.\\nThe decision was made that the user should not have t"}
{"File Name":"nearby-services-api\/0005-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":481,"Predictions":"## \\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n## \\n\\n\\n##       \\n\\n##   \\n\\n##       \\n\\n##       \\n\\n##   \\n\\n##       \\n\\n##       \\n\\n##   \\n\\n##     1. We need to know what the application is doing in a more light weight way than scraping logs. We need to be able to monitor KPIs of the application in order to understand the health of the application. This will allow us to react and potentially pro-actively initiate measures as to ensure the application's health if sound. Ultimately providing a better service for our users.\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"nearby-services-api\/0006-split-nearby-and-open-results-into-separate-endpoints.md","Context":"## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n","Decision":"The decision is to add a new endpoint i.e. `\/open` alongside the current\\n`\/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n","tokens":100,"id":482,"Predictions":"The decision is to add the ability to show open and nearby services on separate\\npages. The consumer needs to be able to tell the difference between\\nopen and nearby services. The consumer needs to be able to tell if a\\nservice is available or not. This information should be displayed in the\\nconsumer's main page and in a separate page for each of the consumer's\\nservices.\\n\\n## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing\\nease with which it can be used both by the current and future consumers.\\n\\n## Decision\\nThe decision is to add the ability to show open and nearby services on separate\\npages. The consumer needs to be able to tell the difference between\\nopen and nearby services. The consumer needs to be able to tell if a\\nservice is available or not. This information should be displayed in the\\nconsumer's main page and in a separate page for each of the consumer's\\nservices.\\n\\n## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing\\nease with which it can be used both by the current and future consumers.\\n\\n## Decision\\nThe decision is to add the ability to show open and nearby services on separate pages (and more of them). The consumer needs to be able to tell \\nthe difference between \\nopen and nearby services. The consumer needs \\nto be able \\nto tell if a service is available or not. This information should be displayed in \\nthe \\nconsumer's main page \\nand in a separate page for each \\nof  \\nthe consumer's \\nservices.\\n"}
{"File Name":"nearby-services-api\/0004-use-elastic-search.md","Context":"## Context\\nElasticsearch is configured as a cluster for reliability and failover, and\\nprovides a single point for data updates. MongoDB runs as a single instance and\\nis not clustered.\\n","Decision":"nearby-services-api will consume data from Elasticsearch rather than MongoDB.\\n","tokens":41,"id":483,"Predictions":"## Decision: The decision is that Elasticsearch is the preferred cluster for\\nthe application. The decision is based on the availability of data, the\\navailability of other clusters, and the availability of connectivity to other\\nclusters.\\n## Decision: The decision is that MongoDB runs as a single instance and\\nis not clustered.\\n## Decision: The decision is that Elasticsearch is the preferred cluster for\\nthe application. The decision is based on the availability of data, the\\navailability of other clusters, and the availability of connectivity to other\\nclusters.\\n## Decision: The decision is that MongoDB runs as a single instance and\\nis not clustered.\\n## Decision: The decision is that Elasticsearch is the preferred cluster for\\nthe application. The decision is based on the availability of data, the\\navailability of other clusters, and the availability of connectivity to other\\nclusters.\\n"}
{"File Name":"nearby-services-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":484,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"Predictions":"## This is a decision record for the application. The decision is intended to be used by the user to make a decision about the application.\\n## Decision\\n## This is a decision record for the application. The decision is intended to be used by the user to make a decision about the application.\\n## Decision\\n## This is a decision record for the application. The decision is intended to be used by the user to make a decision about the application.\\n## Decision\\n## This is a decision record for the application. The decision is intended to be used by the user to make a decision about the application.\\n## Decision\\n## This is a decision record for the application. The decision is intended to be used by the user to make a decision about the application.\\n## Decision\\n## This is an Architectural Decision Record for software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\end{verbatim}\\n\\begin{verbatim}\\n#1: 0, 1, 2, 3, 4, 5\\n\\end{verbatim}\\n\\begin{verbatim}\\n#1: 0, 1, 2, 3, 4\\n\\end{verbatim}\\n\\begin{verbatim}\\n#1: 0, 1, 2\\n\\end{verbatim}\\n\\nThe \\texttt{decision} field of an \\texttt{Decision} object specifies whether this particular \\texttt{Decision} should be considered as an architectural decision or not.\\n\\nThe \\texttt{context} field of an \\texttt{ArchitecturalDecision} object specifies if this particular \\texttt{ArchitecturalDecision} should be considered as an architectural or not.\\n\\nThe \\texttt{name} field of an \\texttt{ArchitecturalDecision} object specifies if this particular \\texttt{ArchitecturalDecision} should be considered as an architectural name or not.\\n\\nThe \\texttt{id} field of an \\texttt{ArchitecturalDecision} object specifies if this particular \\texttt{ArchitecturalDecision} should be considered as an architectural id or not.\\n\\nThe \\texttt{name\\_type\\_id\\_id\\_name\\_type\\_id\\_name\\_type\\_id\\_name\\_type\\_id} field of an \\texttt{NameTypeID}, which defines how many nodes are allowed in each id type and how many nodes are allowed in each name type and name id and how many nodes are allowed in each name id and how many nodes are allowed in each name type. For example:\\n\\begin{itemize}\\n\t\\item NameTypeID: \"a\", \"b\", \"c\", \"d\"\\n\t\\item NameTypeID: \"a\", \"b\"\\n\t\\item NameTypeID: \"a\", \"b\", \"c\"\\n\t\\item NameTypeID: \"a\", \"b\", \"c\", \"d\"\\n\t\\item NameTypeID: \"a\", \"b\", \"c\",\"d\"\\n\t\\item NameTypeID: \"a\",\"b\",\"c\",\"d\"\\n\t\\item NameTypeID: \"%d\"\\n\t\\item NameTypeID: \"%d\"\\n\t%\\item NameTypeID: \"%d\",\\n    %%d,\\n    %%d,\\n    %%d,\\n    %%d,\\n    %%d,\\n    %%d,%d,%%d,%%d,%%d,%%d,%%d,%dd,%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.%dd.\\end{itemize}\\n\\nThe following example shows how much number of nodes are allowed in each id type and name node types:\\n\\n\\todo[inline]{This should probably be split into two different examples?}\\n\\n\\noindent\\n\\n\\todo[inline]{This should probably be split into two different examples?}\\n\\n\\noindent\\n\\n\\todo[inline]{This should probably be split into two different examples?}\\n\\n\\noindent\\n\\n\\todo[inline]{This should probably be split into two different examples?}\\n\\n\\noindent\\n\\n\\nA value of zero means that there are no restrictions on any node types.\\n\\nIf you want more flexibility on how many nodes can belong to each node type you can use one of these options:\\n\\n\\todo[inline]{This could potentially become pretty complicated depending on what you want. For example you might want something like:\\n\\vspace{-0.10cm}\\noindent\\n\\n\\tikzstyle{\\n\tnode = [draw=none,line width=0pt]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]}\\n\\vspace{-0.10cm}\\noindent\\n\\n\\vspace{-0.10cm}\\noindent\\n\\n\\noindent\\n\\n\\vspace{-0.10cm}\\noindent\\n\\n\\noindent\\n\\n\\nthat would allow you to specify that you want only one node per node type but allow up to four nodes per name id and up to five nodes per name type:\\n\\vspace{-0.10cm}\\noindent\\n\\n\\n\\tikzstyle{\\n\tnode = [draw=none,line width=0pt]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]\\n\tnode = [minimum size=5mm]]\\n\\vspace{-0.10cm}\\noindent\\n\\n\\nIf you want more flexibility on how many nodes can belong to each node type you can use one of these options:\\n\\vspace{-0.10cm}\\noindent\\n\\n\\n\\tikzstyle{\\n\tnode = [draw=none,line width=0pt]\\n\tnode = [minimum size=25mm]\\n\tnode = [minimum size=25mm]]\\n\\vspace{-0.10cm}\\noindent\\n\\n\\nIf you want more flexibility on how many nodes can belong to each node type you can use one of these options:\\n\\vspace{-0.10cm}\\noindent\\n\\n\\n\\tikzstyle{\\n\tnode = [draw=none,line width=0pt]\\n\tnode = [minimum size=<number> mm]]\\n\\vspace{-0.10cm}\\noindent\\n\\n\\nIf you want more flexibility on how many nodes can belong to each node type you can use one of these options:\\n\\vspace{-0.10cm}\\noindent\\n\\n\\n\\tikzstyle{\\n\tnode = [draw=none,line width=<number> mm]]\\n\\vspace{-0.10cm}\\noindent\\n\\n\\noindent\\n\\n\\nIn any case it's recommended that when using multiple ids that they all have at least one unique value so that search results don't get muddled up (see below). Also note that if there are no restrictions on any node types then all ids will have at least one unique value (see below).\\n\\nIn case there were no restrictions on any node types then all ids will have at least one unique value (see below).}\\n\\n\\nA value of zero means that there are no restrictions on any node types.\\n\\nFor example:\\n\\vspace{-9pt}\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n\\nYou may also specify multiple names with distinct values for each id:\\n\\nFor example:\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\noindent\\n\\nYou may also specify multiple names with distinct values for each id:\\n\\nFor example:\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\noindent\\n\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\nYou may also specify multiple names with distinct values for each id:\\n\\nFor example:\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\noindent\\n\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nAlthough it's possible for multiple names with distinct values for each id:\\n\\n\\n\\n%\\todo[inline]{I think we need some kind of documentation here? It would probably help us understand what we're doing better!}\\n\\n\\nHowever, using multiple ids does introduce some limitations depending on your requirements (for example if your data structure has nested arrays then it might become difficult):\\n\\nFor example:\\n\\n\\n"}
{"File Name":"elasticsearch-updater\/0003-remove-unwanted-indexes.md","Context":"## Context\\nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.\\nRunning two updaters simultaneously, as happens for pull requests that last several days, can leave\\ntwo indexes with the same alias.\\nElasticsearch watches are run to export data to Prometheus throughout the day.\\nTwo new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\\n","Decision":"Orphaned and indexes with duplicate aliases will be removed as part of the update.\\nThe indexes created every day by the watch will also be deleted within the elastic-search updater.\\nThis could be done by an Elasticsearch watch, but a license is required to use this functionality.\\nTo avoid adding more infrastructure components to the system rather than having a standalone service, the cleanup will be\\nperformed in the elastic-search updater. This can be moved into another service in future if required.\\n","tokens":96,"id":486,"Predictions":"## - Elasticsearch timeout can cause 'orphaned' indexes from remaining in the cluster after an update.\\n## - Running two updaters simultaneously, as happens for pull requests that last several days, can leave\\n## two indexes with the same alias.\\n## - Elasticsearch watches are run to export data to Prometheus throughout the day.\\n## - Two new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\\n\\n\\end{DoxyItemize}\\n\\n\\n\\begin{DoxyItemize}\\n\\item \\href{https:\/\/github.com\/elastic\/elasticsearch\/issues\/13540}{\\tt https\\+:\/\/github.\\+com\/elastic\/elasticsearch\/issues\/13540}\\n\\end{DoxyItemize}\\n\\n\\n\\begin{DoxyItemize}\\n\\item \\href{https:\/\/github.com\/elastic\/elasticsearch\/issues\/15119}{\\tt https\\+:\/\/github.\\+com\/elastic\/elasticsearch\/issues\/15119}\\n\\end{DoxyItemize}\\n\\n\\n\\begin{DoxyItemize}\\n\\item \\href{https:\/\/github.com\/elastic\/elasticsearch\/issues\/15212}{\\tt https\\+:\/\/github.\\+com\/elastic\/elasticsearch\/issues\/15212}\\n\\end{DoxyItemize}\\n\\n\\n\\begin{DoxyItemize}\\n\\item \\href{https:\/\/github.com\/elastic\/elasticsearch\/issues\/15555}{\\tt https\\+:\/\/github.\\+com\/elastic\/elasticsearch\/issues\/15555}\\n\\end{DoxyItemize}\\n\\n\\n"}
{"File Name":"elasticsearch-updater\/0004-remove-snyk.md","Context":"## Context\\nMany of the vulnerabilities identified by snyk have no fixes, and can only be ignored in the synk config file for a set number of days.\\nAfter the number of days are up, the vulnerabilities will fail the build and prevent merge on new PRs, with the only option to ignore the\\nvulnerabilities again.\\nMany of the snyk failures are for build tools that are not exposed when applications are deployed.\\nThe overhead of ignoring many snyk vulnerabilities to get builds passing outweighs the potential risks.\\n","Decision":"Snyk will be removed and replaced with a combination of Greenkeeper and Github vulnerability subscriptions.\\nGreenkeeper will ensure the dependencies are up to date, and the subscription notifications will provide visibility of serious vulnerabilities.\\nAn automated replacement will be put in place in the near future.\\n","tokens":111,"id":487,"Predictions":"## Decision\\nIgnore\\n\\n## Decision\\nIgnore\\n"}
{"File Name":"elasticsearch-updater\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":488,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"the-zoo\/0004-graphql-api.md","Context":"## Context\\nWe need API for the Zoo to expose issues, analytics data, etc.\\n","Decision":"We will create API by GraphQL specification. Pagination will be done according\\nto Relay server specification. We considered to build REST API, but we decided\\nthat for fresh new APIs is the GraphQL right choice.\\n","tokens":19,"id":489,"Predictions":"## 1. We should implement a data API so that the Zoo can expose data to the web. The Zoo will need to expose issues, analytics data, etc. This API will also be used by other applications such as HubSpot.\\n## References\\nhttps:\/\/www.zookeeper.com\/blog\/2017\/06\/20\/zookeeper-data-api\/\\n## Context\\nWe have a Zoo which is a distributed system with multiple servers. The Zoo has a web interface and it can be accessed from multiple places like mobile apps, desktop apps, etc. The Zoo uses ZooKeeper to coordinate the distributed system.\\n\\n## Decision\\n## 1. We should make ZooKeeper more robust and reliable.\\n\\n## References\\nhttps:\/\/www.zookeeper.com\/blog\/2017\/06\/20\/zookeeper-data-api\/\\n## Context\\nWe have a Zoo which is a distributed system with multiple servers. The Zoo has a web interface and it can be accessed from multiple places like mobile apps, desktop apps, etc. The Zoo uses ZooKeeper to coordinate the distributed system.\\n\\n## Decision\\n## 1. We should make ZooKeeper more robust and reliable.\\n\\n## References\\nhttps:\/\/www.zookeeper.com\/blog\/2017\/06\/20\/zookeeper-data-api\/\\n"}
{"File Name":"the-zoo\/0002-use-black-code-formatter.md","Context":"## Context\\nWe would like to use code formatter to standardize code look so we stop bothering about it\\nin code reviews and focus on how code actually works.\\n","Decision":"We will use Black: https:\/\/github.com\/ambv\/black\\n","tokens":34,"id":490,"Predictions":"## You may provide a pre-defined decision as a string. The decision is used as a\\n## context for the formatter.\\n## Decision\\n\\n## You may provide a pre-defined decision as a string. The decision is used as a\\n## context for the formatter.\\n\\n\\end{verbatim}\\n\\n\\subsubsection*{Decision}\\n\\nThe \\texttt{decision} option accepts the following values:\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\textbf{None}: no decision is given.\\n\\item\\n  \\textbf{Code Formatter}: code formatter is applied to all source files in the project.\\n  This option is useful if you want to apply code formatter to the source files of all projects in your workspace or if you want to apply code formatter to all new files added to your workspace.\\n  This option will be automatically applied by the tool.\\n  It will not be necessary to use this option if you are using a tool that implements code formatting, such as \\texttt{git diff}, \\texttt{git diff --decorate}, or \\texttt{git diff --graph}.\\n\\item\\n  \\textbf{Code Formatter}: code formatter is applied to all source files in the project. This option is useful if you want to apply code formatter to all new files added to your workspace or if you want to apply code formatter only on specific file types (e.g., .c, .h, .cpp, etc.). If you use this option, then it will not be necessary for you to use this option if you are using a tool that implements code formatting, such as \\texttt{git diff}, \\texttt{git diff --decorate}, or \\texttt{git diff --graph}.\\n\\item\\n  \\textbf{\\#define}: defines an identifier which can be used in one or more lines of source files. The identifier must start with a letter and can contain any character except whitespace and control characters. The identifier must end with either an equal sign (=) or an open brace ({}) and can contain any character except whitespace and control characters.\\n\\item\\n  \\textbf{\\#pragma}: defines an identifier which can be used in one or more lines of source files. The identifier must start with a letter and can contain any character except whitespace and control characters. The identifier must end with either an equal sign (=) or an open brace ({}) and can contain any character except whitespace and control characters.\\n\\end{itemize}\\n\\nThe value of the default argument is ``none'', which means that no decision has been given.\\n\\nThe default value for the default argument is ``none''.\\n\\nFor example, consider the following two rules:\\n\\n\\begin{verbatim}\\nrule: {name = \"Foo\"; type = \"function\"}\\nrule: {name = \"Bar\"; type = \"function\"}\\nrule: {name = \"Foo\"; type = \"function\"} -> {name = \"Foo\", type = \"function\"}\\nrule: {name = \"Bar\"; type = \"function\"} -> {name = \"Bar\", type = \"function\"}\\nrule: {name = \"#define Foo Foo; Foo(); #define Bar Bar; Bar(); #endif Foo(); Bar();}\\nrule: {name = \"#pragma Foo; Foo(); #pragma Bar; Bar();}\\nrule: {name = \"#pragma Foo; Foo(); #pragma Bar; Bar(); #endif Foo(); Bar();}\\nrule: {name = \"#define Foo Foo; Foo(); #define Bar Bar; Bar(); #endif Foo(); Bar();}\\nrule: {name = \"#pragma foo foo {} foo{} foo{} foo{} foo{} foo{} foo{} foo{}\";\\nrule: {name = \"#pragma bar bar {} barbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarba\";\\nrule: {name = \"#pragma foo foo {} foo{} foo{} foo{}foo{}\\nfoo{}foo{}foo{}foo{}foo{}foo{};\\n# pragma bar bar {} barbarbarbarbarberba\";\\n# pragma foofoofoofoofoofoofoofoofoofoofoofoofoo\";\\n# pragma foofoofoobarbaramba\";\\n# pragma foobaroobarba\";\\n# pragma foobarbazaba\";\\n# pragma foobarbazabababa\";\\n\"Compile error\" on line \"\\n\" of rule \"\\nRule name:\\n\\n\\n\\n\\nRule name:\\n\\n\"\\n```\\nHere we have defined two rules:\\n```\\nRule name:\\nName of rule\\n\\nRule name:\\nName of rule\\n\\nRule name:\\nName of rule\\n\\nRule name:\\nName of rule\\n\\nHere we have defined two rules:\\n```\\n```\\nRule name:\\n```\\n\\nHere we have defined two rules:\\n```\\n\\nHere we have defined two rules:\\n```\\nHere we have defined two rules:\\n\\nHere we have defined two rules:\\n\\nHere we have defined two rules:\\n\\nHere we have defined two rules:\\n\\nHere we have defined two rules:\\n\\n```\\nThis means that each line of source files containing ``#define'' statements will be automatically formatted according to these four rules.\\n\\nIf none of these four rules are applicable, then no formatting will be performed on those lines.\\n\\nThe default value for the default argument is ``none''.\\n\\nFor example, consider the following three options:\\n\\n\\begin{verbatim}\\noption1: (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n    rule2 : (default) {\\n        \/\/ Rule2...\\n    }\\n}\\noption2 : (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n    rule2 : (default) {\\n        \/\/ Rule2...\\n    }\\n}\\noption3 : (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n    rule2 : (default) {\\n        \/\/ Rule2...\\n    }\\n}\\noption4 : (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n}\\noption5 : (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n}\\noption6 : (default) {\\n    rule1 : (default) {\\n        \/\/ Rule1...\\n    }\\n}\\noption7 : (default) {\\n\/\/ ...\\n}\\noption8 : (default)\\n{\\n\/\/ ...\\n}\\noption9 :\\n{\\n\/\/ ...\\n}\\noption10 :\\n{\\n\/\/ ...\\n}  \\n\/\/ ...\\n};\\n```\\n\\n\\n\\nNote that this list does not include options that specify how many times these formatting options should be applied.\\n\\n\\nIn addition, it may be possible for some formatting options to override formatting options specified by other formatting options.\\n\\n\\nFor example, consider the following three options:\\n\\n\\n\\begin{verbatim}\\noption8 :\\n{\\n\/\/ ...\\n};\\n```\\n\\n\\n\\nNote that this list does not include options that specify how many times these formatting options should be applied.\\n\\n\\nIn addition, it may be possible for some formatting options to override formatting options specified by other formatting options.\\n\\n\\nFor example,\\n\\n\\n% define number_format() format %format string %format string %format string %format string %format string %format string %format string\\n\\n\\n% define number_format() format %format string %format string %format string %format string %\\n% format string %\\n\\n% define number_format() format %\\n% format %\\n% format %\\n% format %\\n% format %\\n% format %\\n\\n% define number_format() format %\\n% format %\\n% format %\\n\\n%% define number_format() format %\\n\\n%% define number_format() format %\\n\\n%% define number_format() format %\\n\\n%% define number_format() format %\\n\\n%% define number_format() format %\\n\\n%% define number_format() format %\\n\\n%%% Define Number Formatting Options\\n\\n%%% Define Number Formatting Options\\n\\n%%% Define Number Formatting Options\\n\\n%%% Define Number Formatting Options\\n\\n\\n"}
{"File Name":"the-zoo\/0006-fomantic-ui.md","Context":"## Context\\nWe were using [Semantic UI](https:\/\/semantic-ui.com\/), as it's an open source and robust framework.\\nIt has a wide range of components, it's easily customizable and it uses modern\\napproaches. It's really well documented, although usually documentation is not even\\nneeded as it's really intuitive to work with it.\\nUnfortunately, the project is not as maintained as before, and the development of new components\\nis stale.\\n[Fomantic UI](https:\/\/fomantic-ui.com\/) picked up where Semantic UI left, and offers a drop-in replacement,\\nwith a higher quantity of components, and the development can be considered active.\\n","Decision":"We will use [Fomantic UI](https:\/\/fomantic-ui.com\/), the community fork of `Semantic UI`. The\\nsupport on this project is fast and on point, and the roadmap aligns nicely with our development,\\nas the maintainers are planning to offer first party support for Vue components, and ditch the\\njQuery dependency.\\n","tokens":139,"id":491,"Predictions":"## 1. We should drop Semantic UI, it's not as maintained as before, and the development of new\\ncomponents is stale.\\n\\n\\n\\n## 2. We should replace Fomantic UI with [Semantic UI](https:\/\/semantic-ui.com\/) instead.\\n\\n## 3. We should use [Semantic UI](https:\/\/semantic-ui.com\/) instead.\\n\\n## 4. We should use [Fomantic UI](https:\/\/fomantic-ui.com\/) instead.\\n\\n## 5. We should use [Semantic UI](https:\/\/semantic-ui.com\/) instead.\\n\\n## 6. We should use [Fomantic UI](https:\/\/fomantic-ui.com\/) instead.\\n\\n\\end{lstlisting}\\n"}
{"File Name":"the-zoo\/0007-conventional-commits.md","Context":"## Context\\nWe always wanted to have a consistent commit message styling, we used coala's CommitBear to help with\\nthis task but besides just a bit more clarity in the commit history, it doesn't bring that much value.\\nNow that we'll be getting rid of coala in favour of pre-commit hooks, we need an alternative solution,\\nand we can use this change to also step up our game regarding commit messages.\\n","Decision":"We will use the [conventional commits](https:\/\/www.conventionalcommits.org) convention for writing our commit messages\\nfrom now on. Besides forcing us to keep a consistent commit message styling, it will also bring much\\nmore value, like:\\n* Automatically generating CHANGELOGs\\n* Automatically determining a semantic version bump (based on the types of commits landed)\\n* Communicating the nature of changes to teammates, the public, and other stakeholders\\n* Having different CI steps based on the nature of the commits added to the MR\\n* Having insights about how much time\/effort is spent on the different areas of the development\\n","tokens":89,"id":492,"Predictions":"\\begin{itemize}\\n\\item Commit Message Style\\n\\end{itemize}\\n\\noindent\\noindent The commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\noindent\\noindent\\noindent\\noindent\\noindent\\noindent\\n## Context\\n\\begin{itemize}\\n\\item Maintainability\\n\\end{itemize}\\n\\n## Decision\\n\\begin{itemize}\\n\\item Maintainability\\n\\end{itemize}\\n\\n## Context\\n\\begin{itemize}\\n\\item Documentation\\n\\end{itemize}\\n\\n## Decision\\n\\begin{itemize}\\n\\item Documentation\\n\\end{itemize}\\n\\n## Context\\n\\nThe commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\nThe commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\nThe commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\nThe commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\nThe commit message style is not consistent between the different repositories, we'll be changing this. The new style will be more readable and clear, it'll also help with the maintainer's sanity.\\n\\nThe commit message style is not consistent between the different repositories, we've selected to use pre-commit hooks instead of coala to change this.\\n\\nThe commit message style is not consistent between the different repositories, we've selected to use pre-commit hooks instead of coala to change this.\\n\\nThe commit message style is not consistent between the different repositories\\n\\nThe commit message style is not consistent between the different repositories\\n\\nThis tool allows us to create a list of decisions that need to happen in order for our software to move forward. This tool works by having a list of statements that can happen when a decision has been made. A statement can either have one or multiple actions that need to happen in order for a decision to move forward.\\n\\n\\nThis tool allows us to create a list of decisions that need to happen in order for our software to move forward. This tool works by having a list of statements that can happen when a decision has been made. A statement can either have one or multiple actions that need to happen in order for a decision to move forward.\\n\\n\\n\\nThis tool allows us to create a list of decisions that need to happen in order for our software to move forward. This tool works by having a list of statements that can happen when a decision has been made. A statement can either have one or multiple actions that need to happen in order for a decision to move forward.\\n\\n\\nThis tool allows us to create a list of decisions that need to happen in order for our software to move forward. This tool works by having a list of statements that can happen when a decision has been made. A statement can either have one or multiple actions that need t"}
{"File Name":"the-zoo\/0003-save-adrs-into-adr-dir.md","Context":"## Context\\nWe need dir to save ADRs into.\\n","Decision":"We will save ADRs into top level `adr` dir. Another option was to put them into `docs\/adr`\\nalong with Sphinx docs, but they might be bit hidden there.\\n","tokens":14,"id":493,"Predictions":"## The user wants to save ADRs in dir.\\n## Decision\\n## The user wants to save ADRs in dir.\\n## Decision\\n## The user wants to save ADRs in dir.\\n## Decision\\n## The user wants to save ADRs in dir.\\n## Decision\\n## The user wants to save ADRs in dir.\\n## Decision\\n## The user wants to save ADRs in dir.\\n\\nExample: Saving an ADR into a directory:\\n\\n# Save an ADR into a directory\\ndir = '\/home\/USER\/dir'\\nif dir == '\/tmp':\\n    print ('Save an ADR into a directory')\\nelse:\\n    print ('Save an ADR into a directory')\\n\\n# Save an ADR into a directory\\ndir = '\/home\/USER\/dir'\\nif dir == '\/tmp':\\n    print ('Save an ADR into a directory')\\nelse:\\n    print ('Save an ADR into a directory')\\n\\n# Save an ADR into a directory\\ndir = '\/home\/USER\/dir'\\nif dir == '\/tmp':\\n    print ('Save an ADR into a directory')\\nelse:\\n    print ('Save an ADR into a directory')\\n\\n# Save an ADR into a directory\\ndir = '\/home\/USER\/dir'\\nif dir == '\/tmp':\\n    print ('Save an ADR into a directory')\\nelse:\\n    print ('Save an ADR into a directory')\\n\\n# Save an ADR into a directory\\ndir = '\/home\/USER\/dir'\\nif dir == '\/tmp':\\n    print ('Save an ADR into a directory')\\nelse:\\n    print ('Save an ADR into a directory')\\n\\n# Save an ADR in the home folder of USER\\n\\n# Save the contents of the home folder of USER.\\n# HOME is the name of the home folder.\\nHOME = 'HOME'\\n\\n# Save the contents of the home folder of USER.\\nHOME = 'HOME'\\n\\n# Save the contents of HOME.\\n\\nIf you want to save folders, you can use this code:\\n\\nsave_folder('C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop', 'C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop',\\n        'C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop')\\n\\nThis will create C:\\Users\\USERNAME\\Desktop and C:\\Users\\USERNAME\\Desktop. If you want to make it more generic, you can use this code:\\n\\nsave_folder('C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop', '', '', '', '', '', '',\\n        'C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop')\\n\\nThis will create C:\\Users\\USERNAME\\Desktop and C:\\Users\\USERNAME\\Desktop. If you want to make it more generic, you can use this code:\\n\\nsave_folder('C:\\\\Users\\\\USERNAME\\\\Desktop', '', '', '', '', '', '',\\n        'C:\\\\Users\\\\USERNAME\\\\Desktop', 'Desktop')\\n\\nThis will create C:\\Users\\USERNAME\\Desktop and C:\\Users\\USERNAME\\Desktop. If you want to make it more generic, you can use this code:\\n\\nsave_folder('C:\\\\Program Files (x86)\\\\Microsoft Office\\\\Office16', '')\\n\\nThis will create C:\\Program Files (x86)Microsoft OfficeOffice16. If you want to make it more generic, you can use this code:\\n\\nsave_folder('C:\\\\Program Files (x86)\\\\\\Microsoft Office\\\\\\Office16', '')\\n\\nThis will create C:\\Program Files (x86)Microsoft OfficeOffice16. If you want to make it more generic, you can use this code:\\n\\nsave_folder('C:\\\\Program Files\\\\\\Microsoft Office\\\\\\Office16','')"}
{"File Name":"the-zoo\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":494,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"the-zoo\/0005-semantic-ui.md","Context":"## Context\\nWe were using [Material Design Lite](https:\/\/getmdl.io\/) as our front-end CSS framework.\\nThe selection of offered components is quite limited and most of them just include\\nthe basic logic. We have to dedicate a big amount of time writing front-end code and it's\\nhard to compose the provided components as they are mainly meant to be used in a single\\nway because of the strong Material design guidelines.\\nWe aim to focus on logic, not on developing front-end components. We need a solution\\nthat can offer a high number of already built components. The components should be\\neasy to configure and to compose together as our UI sometimes needs components that\\nare not contemplated on the common guidelines.\\n","Decision":"We will use [Semantic UI](https:\/\/semantic-ui.com\/), as it's an open source and robust framework.\\nIt has a wide range of components, it's easily customizable and it uses modern\\napproaches. It's really well documented, although usually documentation is not even\\nneeded as it's really intuitive to work with it.\\n","tokens":148,"id":495,"Predictions":"\\nWe have decided to use the Material Design Lite for our front-end. The provided\\ncomponents are very limited and it's difficult to compose them as they are meant to be used\\nonly in a single way.\\nWe aim to focus on logic, not on developing front-end components. We need a solution\\nthat can offer a high number of already built components. The components should be\\neasy to configure and to compose together as our UI sometimes needs components that\\nare not contemplated on the common guidelines.\\n\\n## Comments\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n"}
{"File Name":"front-end-monorepo\/adr-34.md","Context":"## Context\\nOn PFE, workflows have a complex logic of programmatic workflow selection. [ADR 18](adr-18.md) has documented the original context. This ADR is a partial superseding of the decision on how to select the workflow when navigating to `\/classify`\\n","Decision":"We will continue to use the routing behavior defined by [ADR 18](adr-18.md)\\n### `\/classify` behaviour\\nWe will not programmatically select a workflow except for one case: when there is only one active workflow. For all other cases, we have a UI prompt for the volunteer to manually select which workflow they wish to contribute to.\\n### Error handling\\nCases when the workflow is not available are:\\n- The workflow does not actually exist, so this will 404.\\n- The workflow exists, but is in an inactive state. The activeness state effectively functions as a permissions mechanism since users with the project owner, collaborator, or expert role or Zooniverse admins can still request and load inactive workflows. Users with the correct role should be able to load an inactive workflow with a visual indication in the UI it is inactive. All other users will receive a 404.\\n- The workflow exists and is active, however, the project uses workflow assignment and the workflow has not been assigned to the volunteer yet. The classify page should load, the classifier itself doesn't, and the workflow selection prompt is rendered for the volunteer to choose between the workflows they have been assigned.\\n","tokens":58,"id":496,"Predictions":"## Decision: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at the same time. \\n## Context\\n## Context: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at the same time.\\n## Decision\\n## Decision: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at the same time.\\n## Context\\n## Context: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at the same time.\\n## Decision\\n## Decision: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at the same time.\\n## Context\\n## Context: There are two workflows available in the PFE, `classify` and `classify`, that are mapped to the same workflow in the SIS. The user is allowed to select either workflow. The user is also allowed to select both workflows at"}
{"File Name":"front-end-monorepo\/adr-28.md","Context":"## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n","Decision":"### Changes from the TESS LCV\\nThe TESS LCV is built only for their use case and is not configurable. It is hard coded to expect brush stroke annotation for the classifications, zoom only in the x-axis direction, and for only one data series. We will build a generally configurable plots so that other projects can have the flexibility they need. The new plot viewers will be modular so that it can be placed into a composite, complex subject viewer as needed.\\nPreviously, the TESS LCV was built using d3.js, however, mixing d3 and react can be dangerous. The decision at the time was to use d3 because of the custom requirements needed for the TESS LCV and the react + d3 libraries were too opinionated to be used for our needs. The library d3 is also difficult to write tests for because of its chaining API. For this reason, the original TESS LCV is largely untested.\\nSince then, a library called [vx](https:\/\/vx-demo.now.sh\/) containing reusable low-level visualization react component that uses the d3 math utilities, has become more mature to start using. This fits our needs to have the DOM solely rendered by React, but still has the usefulness of a mature library like d3 to do calculations as needed. The new plots will be built using vx.\\nThe long term goal is to swap the TESS LCV over to the new `ScatterPlotViewer`, however, this means adding support for brush annotations which will be investigated at a later time.\\n### ScatterPlotViewer\\nThe scatter plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Pan and zoom in both axes directions or constrainable\\n- Axis inversion\\n- Customizable axis label\\n- Customizable number of axis ticks and direction\\n- Customizable margin and padding for the plot area\\n### BarChartViewer\\nA bar chart plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Labels for axes and for individual bars\\n### VariableStarViewer\\nThe scatter plot and the bar chart together along with the `SingleImageViewer` and a few additional controls will be a complex composite viewer built as the `VariableStarViewer`. The `VariableStarViewer` will have its own control bar that has a toggle for axis inversion, period, data series visibility, and phase focus. Each scatter plot will be individually pan and zoomable.\\n### DataImageViewer\\n_Note: Naming still TBD_\\nThis will be a complex composite consisting of a scatter plot and a single image. We may want to support up to N images, but this is still TBD. The initial build will be just the single scatter plot and single image.\\n### Future plots\\nThere may be requests to build more plot types like a line plot or map plot that renders GeoJSON. We will continue to evaluate our usage of vx at that time and ideally will continue to use it.\\n","tokens":99,"id":497,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n\\n\\end{document}\\n"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"Predictions":"## Decision\\n1. Add a multiFrame Viewer to the classifier.\\n2. Provide a way for users to search and select multi-frame subjects.\\n3. Provide a way for users to display multi-frame subjects in the classification interface.\\n4. Provide a way to select and copy multi-frame subjects from the classifier.\\n5. Provide a way for users to add new features to the classifier.\\n6. Provide a way for users to view multi-image features in the classification interface.\\n7. Provide a way for users to view multi-image features in the classification interface.\\n## Context\\n## Context\\nThis decision document is intended to be used by the user of the classifier as they make decisions regarding which features are needed and how they will be implemented.\\n## Decision\\n## Decision\\n1. Add a multiFrame Viewer to the classifier.\\n## Context\\n## Context\\nThe PagesViewer will provide an easy-to-use, high-level interface that allows users to view and manipulate filmstrip clips (see Figure 1). The PagesViewer will use multiFrame objects as its base class, which allows it access to all information about filmstrips (i.e., image, time stamp, etc.). The PagesViewer will also provide an easy-to-use, high-level interface that allows users to select multiple clips from filmstrips and simultaneously display them in real-time (see Figure 2). The PagesViewer will use multiFrame objects as its base class, which allows it access to all information about filmstrips (i.e., image, time stamp, etc.). The PagesViewer will also provide an easy-to-use, high-level interface that allows users to modify multiple clips from filmstrips at once (see Figure 3). The PagesViewer will use multiFrame objects as its base class, which allows it access to all information about filmstrips (i.e., image, time stamp, etc.). The PagesViewer will also provide an easy-to-use, high-level interface that allows users to view multiple clips from filmstrips simultaneously (see Figure 4). The PagesViewer will use multiFrame objects as its base class, which allows it access to all information about filmstrips (i.e., image, time stamp, etc.). The PagesViewer will also provide an easy-to-use high-level interface that allows users to add new features such as transcriptions or annotations.\\n## Decision\\n## Decision\\n1. Add a multiFrame Viewer to the classifier.\\n## Context\\n## Context\\nThe pages viewer is currently only available in two views: single frame and multiple frame viewing modes. We plan on adding more views over time so that we can provide different levels of functionality based on user needs and preferences.\\n## Decision\\n## Decision\\n2. Provide a way for users to search and select multi-frame subjects.\\n## Context\\n## Context\\nUsers should be able to search for specific topics or individuals within their films using various criteria such as subject name or keyword(s) and have those results displayed in their browser window or within their text editor without having additional software installed on their computer.\\n## Decision\\n## Decision\\n3. Provide a way for users to display multi-frame subjects in the classification interface.\\n# # # # # # # # # # ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ## Background ##\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ###\\n### \\n# # \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n#### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #### Conclusion #####"}
{"File Name":"front-end-monorepo\/adr-29.md","Context":"## Context\\nFuture projects using the `front-end-monorepo` will require a video player. Some projects may simply need a video to play. Some projects will require users to interact with a video file; such as make annotations on the video to mark locations or item sizes. The video player must be highly customizable, able to be used in a React code base project and support all major browsers.\\n","Decision":"We will implement `react-player` [Github](https:\/\/github.com\/CookPete\/react-player).\\nDemo Page: [Demo](https:\/\/cookpete.com\/react-player\/)\\nThe lengthy list of attributes (props) and Callback Props makes `react-player` a great choice for developers who need customization. Many of these attributes are simply booleans. For example, to play a video, pass in `playing={boolean}`. Full list of [Props](https:\/\/github.com\/CookPete\/react-player#props)\\nCompared to the native HTML video player, `react-player` makes it easy to customize styling so the player looks the same across different browsers.\\nMaking `react-player` responsive is easy by targeting pre-defined classNames.\\n[Responsiveness](https:\/\/github.com\/cookpete\/react-player#responsive-player)\\nOne of the biggest wins of using `react-player` is ease-of-use. This will reduce developer time and reduce the amount of custom methods in our code base.\\n### Media\\n`react-player` is a React component for playing a variety of URLs, including file paths, YouTube, Facebook, Twitch, SoundCloud, Streamable, Vimeo, Wistia, Mixcloud, and DailyMotion. [Supported Media](https:\/\/github.com\/CookPete\/react-player#supported-media)\\nAlthough `react-player` supports a number of externally hosted videos, we will only support video files uploaded to the Zooniverse platform.\\nFile types will be validated to ensure the files are mp4.\\n### Customization\\nA wide range of [Props](https:\/\/github.com\/CookPete\/react-player#props) can be passed in to control playback and react to events.\\nPlease read through the [Github](https:\/\/github.com\/CookPete\/react-player) for a full list of features.\\n","tokens":81,"id":500,"Predictions":"\\begin{itemize}\\n    \\item No\\n    \\item Yes\\n\\end{itemize}\\n\\end{decision}\\n"}
{"File Name":"front-end-monorepo\/adr-24.md","Context":"## Context\\nPanoptes-Front-End's drawing tools largely developed from previous custom projects and\/or were added one at a time to support a specific project. Because of this, several inconsistencies have been discovered in downsteam analysis and aggregation. To explain the inconsistencies, a few definitions are needed:\\n- RHC: A right handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the +y axis with angle=0 along the +x axis\\n- LHC: A left handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the -y axis with angle=0 along the +x axis\\n- Domain: The range of values a number can take [ or ] is inclusive, ( or ) is exclusive.\\n- Upper origin: The point 0, 0 is in the upper left of the plot\\n- Lower origin: The point 0, 0 is in the lower left of the plot\\nThe inconsistencies comprise of:\\n- The browser SVG coordinate systems use _RHC_ with an _upper origin_ resulting in positive angles rotating clockwise. Most plotting software (R, Python, Matlab) are _RHC_ with a _lower origin_ resulting in positive angles rotating counter-clockwise.\\n- The position of origin has been inconsistent between tools which has an effect on the final annotation too. Most use the center x, y point, but some don't\\n- Some of the drawing tools use _LHC_\\n- Some tools' annotation use `angle` some use `rotation`\\n- It's unclear when the x, y annotation refers to the center point of the shape\\n- It's unclear when the x, y annotation is being used as the point of rotation\\nSome of the mark annotation models have a few other issues as well:\\n- Some shapes have default values which an create bias. For example, the ellipse has a default axis ratio of 0.5 and many volunteers have left the default creating a bias ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/500#issuecomment-516788821))\\n- The freehand drawing tool has peformance impact on the browser as the drawing is being created and with the job to create classification exports as well. This is because the current annotation consists of every single x, y point created\\n","Decision":"The shape's mark annotation models should change for consistency and improved post-classification analysis in the following ways:\\n- The annotation should use the mathematical standard of _RHC_ with a domain of `[-180, 180]` for consistent angle calculation\\n- The annotation model should use `angle` for naming rotation angles. This replaces usage of `rotation`.\\n- The annotation model should replace `x` and `y` with `x_center` and `y_center` for shapes where that is applicable\\n- The exceptions are non-shapes like point, line, and transcription line tools, and non-symmetric shapes like fan.\\n- Shape clustering in aggregation is always done with the center\\n- All rotations should be defined about `x_center` and `y_center` point. If the rotation cannot be defined around the center point, then the point used should be clearly recorded in the annotation as `x_rotation` and `y_rotation`\\n- Conditional logic in component code can be avoided by using a Mobx-State-Tree's computed view functions to get either `x_center` or `x_rotation` mapped to `mark.x`. Code example:\\n```js\\nconst CircleModel = types\\n.model('CircleModel', {\\nx_center: types.optional(types.number, 0),\\ny_center: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_center,\\ny: self.y_center\\n}\\n}\\n}))\\n```\\n```js\\n\/\/ non-symmetrical shapes like fan use x_rotation, y_rotation\\nconst FanModel = types\\n.model('FanModel', {\\nx_rotation: types.optional(types.number, 0),\\ny_rotation: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number),\\nspread: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_rotation,\\ny: self.y_rotation\\n}\\n}\\n}))\\n```\\n- Default values should be removed wherever possible. We will replace these with project builder configurable values set in the project builder lab when the tools are setup.\\n- The parameters will _not_ have default values suggested by us. If the parameters are not set in the lab, then when attempting to use that drawing tool in the classifier, the classifier should display an error message that the tool is not fully setup yet. The lab should also prompt for inputing a value.\\n- The lab should include instructions and a warning about the biasing effect.\\n- The tools that have defaults are ellipse, rotate rectangle, fan.\\n- The freehand drawing tools mark annotation will be a string of the SVG's path and it will be the responsibility of post-classification analysis to convert this to usable x,y points. We will include a sample script in the `DataDigging` repo for project owners to reference on how to do this. Aggregation in Caesar will have to be updated to do the conversion first.\\n- `tool` will change to `toolIndex` to clarify it is referring to the index of the input from the task area. The `toolIndex` is useful to distinguish between multiple instances of the same tool tip in a drawing task.\\n- Drawing annotations and drawing tool marks will have a `taskType` and `toolType` attribute added that map to an enumeration of the type of task or tool like `drawing` or `point`, `ellipse`, etc respectively. This enables the aggregation for Caesar code to auto-configure which extractor to use without checking data types ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/823#issuecomment-493896524)).\\n- Certain annotation models may have internal properties used denoted by a preceding underscore. These properties will be removed with the classification complete action. We will remove them to help prevent confusion by project owners in downstream analysis.\\n","tokens":494,"id":501,"Predictions":"## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n## Decision\\n\\n\\n\\end{document}\\n"}
{"File Name":"front-end-monorepo\/adr-09.md","Context":"## Context\\nThe big question: which Subject Viewer should we use to view a given Subject?\\nAt the moment, the Classifier tries to \"guess\" which Subject Viewer to use (see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) by analysing the Subject\\nitself. e.g. if the Subject has multiple images, show a multi-image viewer. If\\nthe Subject has a video, show a video viewer.\\nWhile this method works in a majority of projects, certain projects with\\nextremely specific Subject structures breaks the generalised \"guessing\" logic,\\nor else warps the logic with so many \"what if\" clauses that it collapses into a\\nwrithing mass of nonsense.\\nFor example, a TESS Planet Hunters Subject looks like...\\n```\\nSubject 12345 = {\\n...\\nlocations: [\\n{ \"image\/png\": \"tess-data-12345.png\" },\\n{ \"application\/json\": \"tess-data-12345.json\" }\\n]\\n}\\ntess-data-12345.json = {\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\nIf we were to add to the \"guess the Subject Viewer\" logic by stating that _\"any\\nSubject that has a JSON, and that JSON has x-y coordinates, should use the Light\\nCurve Viewer\",_ then we'd have trouble if, say, a future project needs similar\\nJSONs with x-y coordinates for a Map Viewer or a Line Graph Viewer, etc.\\nSide note:\\n- The Light Curve Viewer can still also serve as a generic Scatterplot Viewer\\ngiven proper tweaks - this should be marked for future dev so JSON data with\\ngeneric x-y data can be \"guessed\".\\n","Decision":"The \"Choose a Subject Viewer\" logic (again, see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) will _first_ try to check\\nif there's a **Workflow Configuration** stating the preferred Subject Viewer.\\nIf this specific configuration does not exist, the logic will _continue_ to\\n\"guess\" the correct Subject Viewer to use, as per the current system.\\n```\\n\/\/Example of a workflow.configuration for TESS project:\\n{\\nsubject_viewer: \"lightcurveviewer\"\\n}\\n```\\nWe've also decided _not_ to specify the \"Custom\/Specific Subject Viewer\" logic\\nwithin the _Subject_ itself, since it'll make the Subjects more complicated and\\nmaintenance troublesome. (Compare changing the config field of one Workflow vs\\nupdating the metadata or JSON of a million Subjects.)\\ni.e. we should **not** do something like:\\n```\\ntess-data-12345.json = {\\ntype: 'lightcurve',\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\n","tokens":385,"id":502,"Predictions":"\\nThe decision is based on the following considerations:\\n- If the Subject has a JSON, show a Light Curve Viewer. If it doesn't, show a\\nMap Viewer.\\n- If it has multiple images, show a multi-image viewer. If it doesn't have image\\ndata, show a video viewer.\\n- If the Subject has a video, show a video viewer.\\n- If the Subject has a multi-image and video files, show an Overlay Viewer.\\n- If the Subject has a multi-image and video files and an Image Viewer, show an\\nOverlay Viewer.\\n\\ntess-data-12345.json = {\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\nThis is currently only implemented for JSONs with x-y coordinates. It will be\\nimplemented for all other types of data later on.\\n\\n## Result\\n\\nThe result is as follows:\\n- The Subject Viewer which uses the Light Curve Viewer as its default is shown\\nfirst. This is the default if no other Subjects have a JSON or if they are not\\npresent in the Subject's Image Data or Video Data.\\n## Notes\\n\\begin{itemize}\\n\\item The `Light Curve Viewer` is not necessarily used if there are multiple Images or Videos available.\\n\\item The `Light Curve Viewer` can also be used for scatterplot viewers.\\n\\end{itemize}\\n"}
{"File Name":"front-end-monorepo\/adr-07.md","Context":"## Context\\nMaking changes to drawing tools in production - specifically, to the structure of the data they output - is an occasionally necessary evil. Unfortunately, there is currently no way to link a marking tool, whether by version, commit or whatever, to its output in a classification.\\nThis means that when changes happen to the tools, we need to be careful to communicate those changes to project managers, since there is every chance that they will break their data pipelines. And in the absence of a schema for the tools' output, these changes have to be handled manually in code by project managers.\\n","Decision":"We include a schema for each marking tool used as part of the classification object submitted to the API.\\nThe schema will be written in JSON, and will live alongside to the marking tool code. It should describe the expected data structure of a marking tool.\\n### Example\\nFor a point tool with the following example output:\\n```json\\n{\\n\"x\": 152.96875,\\n\"y\": 164\\n}\\n```\\nThe schema could look like:\\n```json\\n{\\n\"$schema\": \"http:\/\/json-schema.org\/draft-06\/schema#\",\\n\"$id\": \"https:\/\/zooniverse.org\/schemas\/tools\/point.schema.json\",\\n\"type\": \"object\",\\n\"title\": \"Point\",\\n\"description\": \"A single point on the subject\",\\n\"properties\": {\\n\"x\": {\\n\"description\": \"X value of the point, where `0` is on the left\",\\n\"type\": \"number\"\\n},\\n\"y\": {\\n\"description\": \"Y value of the point, where `0` is at the top\",\\n\"type\": \"number\"\\n}\\n},\\n\"required\": [\\n\"x\",\\n\"y\"\\n]\\n}\\n```\\n### Alternatives\\nWe could version the schema, and simply reference it within the classification by its URL. For example, the point tool, instead of including the entire schema in the classification, has a reference to `https:\/\/zooniverse.org\/schemas\/tools\/point.schema.v1.json`. A change to the point tool output then becomes a new version number.\\nThis would result in smaller classification object sizes, although forgoes the convenience of having a self-contained classification object.\\n","tokens":120,"id":503,"Predictions":"\\nThe project manager can use the tool's output to determine whether it is a classification or a non-classification. If it is a classification, then the tool will be used to classify the data. If it is not, then it will be used to generate an annotation.\\nIf the project manager needs to make changes to the tools themselves, they should do so using the tools' own documentation. This way, they can keep their data pipelines in sync with their tools.\\nIf the project manager needs to make changes to how they are generating their data (for example, because of a new feature), they should do so using a new tool that generates annotations in place of classifications. This way, they can keep their data pipelines in sync with their new features.\\n\\n## Context\\nThe tool's output is used by several projects. The first one is generated by the tool itself, and is used by projects that are using this tool.\\nThe second one is generated by another project that uses this tool. This one is used by other projects that are not directly using this tool.\\nFinally, there are two projects that use this tool but do not use its output. These two projects have an intermediate step where they generate annotations on top of the output of this tool. These annotations are then used by projects that are directly using this tool.\\nThis means that there are three different ways in which annotations from this tool can be generated: annotation on top of classification; annotation on top of annotation; and annotation on top of annotation on top of annotation.\\nThere are also three different ways in which annotations can be generated from different tools: annotation on top of classification; annotation on top of annotation; and annotation on top of annotation on top of annotation.\\nIn order for a project manager to make changes to how their data is annotated in production, they should do so using annotations generated by another project that uses this tool.\\n\\n## Decision\\n\\nProject managers can make changes to how their data is annotated in production if: \\begin{enumerate}\\n\\item They want to change how annotations are generated from their own tools;\\n\\item They want to change how annotations from other tools are generated;\\n\\item They want to add new features based on annotations from other tools.\\n\\end{enumerate}\\n\\nIf they want to make these changes, they should do so using the tools' documentation or code.\\n\\end{document}"}
{"File Name":"front-end-monorepo\/adr-35.md","Context":"## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n","Decision":"We solved this problem by adding an undo\/redo history stack to the classifier task area. [#2050](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2050) added a global history stack (`store.annotatedSteps`). [#2520](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2520) refines this by moving from a global history manager to individual history management for each subject (`subject.stepHistory`.)\\nHistory is managed by the MST middleware [`UndoManager`](https:\/\/github.com\/mobxjs\/mobx-state-tree\/blob\/master\/packages\/mst-middlewares\/README.md#undomanager), which records snapshots of a single subtree in the store. Each history item is a single step key and one or more annotation IDs, representing the current state of the task area. 'Back' loads the previous step key and annotation IDs from history. 'Next' loads the next step key and annotation IDs, or creates a new history item if one doesn't exist already. Going back then changing branches, for a branching workflow, clears any existing redo history and starts a fresh history from the current point.\\n","tokens":76,"id":505,"Predictions":"## Decision\\nThis is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n\\n## Decision\\n## Decision\\nThis is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n\\n## Decision\\n## Decision\\nThis is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n\\n## Decision\\n## Decision\\nThis is a decision record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n\\end{verbatim}\\n\\n"}
{"File Name":"front-end-monorepo\/adr-36.md","Context":"## Context\\nWhen we launched the NextJS apps, a single domain could only host one NextJS app. We gave each app its own subdomain: `fe-content-pages.zooniverse.org` and `fe-projects.zooniverse.org` then proxied URLs from `www.zooniverse.org` to those domains. Next data requests, on the `\/_next` URL, are proxied from `www.zooniverse.org\/_next` to `fe-project.zooniverse.org\/_next`. We deliberately broke the content pages app, in favour of supporting projects.\\n[Next 9.5](https:\/\/nextjs.org\/blog\/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [`basePath`](https:\/\/nextjs.org\/blog\/next-9-5#customizable-base-path) config setting.\\n","Decision":"Set base paths of `\/about` and `\/projects` for the content pages app and project app respectively.\\nhttps:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2519\\n","tokens":180,"id":506,"Predictions":"## Yes\\n\\n## Yes\\n\\n## No\\n\\n## No\\n\\n## No\\n\\n## No\\n\\n## No\\n\\n## No\\n\\n\\n# NextJS 9.5\\n\\n# +---\\n# | Version: 9.5 |\\n# | Date: 2020-07-31 |\\n# +---\\n\\n### Context\\n\\n### Decision\\n\\n### Yes\\n\\n### Yes\\n\\n### No\\n\\n### No\\n\\n### No\\n\\n### No\\n\\n### No\\n\\n\\n"}
{"File Name":"front-end-monorepo\/adr-18.md","Context":"## Context\\nOn PFE, there are a couple of different ways workflows are routed:\\n- Navigating to `\/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\\n- Navigating to `\/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the \"Use workflow query\" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\\n","Decision":"We adopt the following URL structure:\\n```\\n\/projects\/[owner]\/[project]\/classify\/workflow\/:workflow-id\/subject-set\/:subject-set-id\/subject\/:subject-id\\n```\\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https:\/\/github.com\/zeit\/next.js\/#dynamic-routing) (see [#1071](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1071)).\\nWorkflow links from the home page would route directly to their workflow-specific URLs.\\n### `\/classify` behaviour\\nWhen navigating to `\/classify`, the page should redirect the user to the correct workflow URL in the following priority:\\n1. Workflow ID set in user project preferences by the user\\n1. Workflow ID set in user project preferences by the project\\n1. Project default workflow\\n1. Random active selection\\nThe new URL that we redirect the user to should replace the entry for `\/classify` in the browser history.\\n### Error handling\\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`\/`403` (depending on login status) etc.\\n","tokens":260,"id":507,"Predictions":"The decision is that we should route to the workflow for the subject in question. We can do this by adding a query parameter to the URL and then using the `subject` variable in our CSSI CitSci template to pick out the right workflow. This will be useful if we ever want to create a custom workflow for a project, or if we want to route to a specific workflow for a specific subject.\\n## Comments\\nThis decision is based on user feedback and on our own experience with routing workflows. We will add a link in the \"Use workflow query\" feature settings page to help users navigate to workflows by subject. We will also add more information about how workflows are routed, as well as links for subscribing users, so they can subscribe to specific workflows.\\n## Notes\\nThis decision is based on user feedback and on our own experience with routing workflows. We will add a link in the \"Use workflow query\" feature settings page to help users navigate to workflows by subject. We will also add more information about how workflows are routed, as well as links for subscribing users, so they can subscribe to specific workflows.\\n## References\\nhttps:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806\\n\\n## Author\\nZooniverse.org\\n\\n### 1: https:\/\/www.zooniverse.org\/projects\/citsci\/\\n\\n### 2: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n\\n### 3: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n\\n### 4: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n\\n### 5: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n\\n### 6: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n\\n### 7: https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027\\n```"}
{"File Name":"front-end-monorepo\/adr-26.md","Context":"## Context\\nWe use [Sentry](https:\/\/sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\\n","Decision":"The monorepo will be versioned in Sentry, using the git commit SHA to version a release. New releases are deployed to staging on each push to master. A release is finalised and deployed to production when the production-release tag is updated to point to that release. [#1599](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1599) and [#1601](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1601) implement this using GitHub actions.\\n","tokens":60,"id":508,"Predictions":"## The decision is to use [Sentry](https:\/\/sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\\n## Considerations\\n## - Sentry is a paid service.\\n## - We need a way to identify the issue tracker in the app.\\n## - Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\\n# # Usage\\n# #### Logging\\nimport { Sentry } from 'sentry'\\nimport { makeLogger } from '.\/utils'\\nconst logger = makeLogger()\\nlogger.level = Sentry.ERROR\\nlogger.errorHandler = (err) => {\\n  if (err instanceof Sentry.Error) {\\n    logger.error('Sentry Error', err)\\n  }\\n}\\nlogger.logger = logger\\nlogger.logLevel = Sentry.ERROR\\nlogger.logHandler = (err) => {\\n  if (err instanceof Sentry.Error) {\\n    logger.error('Sentry Error', err)\\n  }\\n}\\n```"}
{"File Name":"front-end-monorepo\/adr-27.md","Context":"## Context\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n### [multiFrame](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/MultiFrameViewer)\\nA workflow using the multi-frame subject viewer might have a preference regarding:\\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/zooniverse\/wildcam-gorongosa\/classify) or [Backyard Worlds](https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify) (in flipbook mode, not separate frames)\\n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/SubjectGroupViewer)\\n- A workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\\n","Decision":"Subject viewer configuration is an object stored in `workflow.configuration.subject_viewer_config` and is structured as follows for the subject viewers noted:\\n`multiFrame`:\\n```javascript\\n{\\nfilter_marks_per_frame: <boolean>  \/\/ replaces multi_image_clone_markers in PFE\\npositioning: <enumerable> \/\/ includes pan, zoom, and rotation, enumerable i.e. \"maintain\", \"reset\"\\n}\\n```\\n`subjectGroup`:\\n```javascript\\n{\\ncell_width: <number of pixels>\\ncell_height: <number of pixels>\\ncell_style: { [CSS property]: <CSS property value> }\\ngrid_columns: <number>\\ngrid_rows: <number>\\n}\\n```\\nSubject viewers to define the configuration object in the related subject viewer README, if applicable.\\nThe ScatterPlotViewer and BarChartViewer accept a configuration object directly in the JSON structure of the subject data to support variability in the display settings including the plot labels since it's possible this might vary per subject. The `subject_viewer_config` object should only be used for  configuration options that apply to all subjects linked to the workflow.\\n","tokens":487,"id":509,"Predictions":"## Decision\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n\\n## Context\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n### [multiFrame](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/MultiFrameViewer)\\nA workflow using the multi-frame subject viewer might have a preference regarding:\\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/zooniverse\/wildcam-gorongosa\/classify) or [Backyard Worlds](https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify) (in flipbook mode, not separate frames)\\n### [subjectGroup]\\nA workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\\n### Example Decision\\n\\n    Decision\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n    ### [multiFrame](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/MultiFrameViewer)\\n    A workflow using the multi-frame subject viewer might have a preference regarding:\\n    - marks per frame: some might prefer marks filtered per frame,\\n      like a transcription workflow where each frame represents a unique page to transcribe,\\n      while other workflows might prefer marks persist between frames\\n      like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or\\n      [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However,\\n      after reviewing projects that enabled marks to persist between frames\\n      (`multi_image_clone_markers` in PFE\\n      https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55))\\n      it appears the PFE setting is unclear,\\n      as a few of the related workflows do not include a drawing task\\n      or the subjects do not have multiple frames.\\n    - positioning: some might prefer pan,\\n      zoom,\\n      and rotation reset per frame,\\n      like a transcription workflow where each frame represents a unique page\\n      to transcribe,\\n      while other workflows might prefer pan,\\n      zoom,\\n      and rotation maintained between frames,\\n        like \\n        https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify (in flipbook mode),\\n        but not separate frames.\\n    \\n    ### Example Decision\\n    \\n    \\begin{itemize}\\n            \\item Marking per Frame: Some Marking Per Frame workflows may be unaware of Marking Per Frame workflows that are based on multiple image files. Marking Per Frame workflows will only mark one image file at any given time.[[MultiImageCloningMarkers]](http:\/\/front-end.monorepo.org\/docs\/monosnap.html#monosnap-multi-image-cloning-markers)\\n            \\item Positioning Per Frame: Some Marking Per Frame workflows may be unaware of Marking Per Frame workflows that are based on multiple image files. Marking Per Frame workflows will only mark one image file at any given time.[[MultiImageCloningMarkers]](http:\/\/front-end.monorepo.org\/docs\/monosnap.html#monosnap-multi-image-cloning-markers)\\n            \\item Positioning Per Frame: This is an architectural decision for Multi Image Cloning Markers (MICA), which is used by many marking per frame (MPF) workflows in order for them to work correctly.[[MultiImageCloningMarkers]](http:\/\/front-end.monorepo.org\/docs\/monosnap.html#monosnap-multi-image-cloning-markers)\\n        \\end{itemize}\\n    \\n\\end{document}\\n"}
{"File Name":"front-end-monorepo\/adr-13.md","Context":"## Context\\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\\n","Decision":"We will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\\n","tokens":79,"id":510,"Predictions":"# \\begin{itemize}\\n#     \\item The rewrite is a technical decision and should not be considered a business decision. The team is open to outside contributions to the codebase.\\n#     \\item The rewrite is a business decision and should be considered by the team as such.\\n# \\end{itemize}\\n\\end{Decision}\\n\\n\\subsection{Code Review}\\n\\n\\textbf{Code Review} is the process of reviewing the software in order to ensure that it meets all of its requirements. This will include reviewing the code itself, but also reviewing the documentation, tests, and other artifacts such as screenshots or screenshots of screenshots. A review may also include interactions with users or stakeholders.\\n\\n\\subsubsection{Requirements}\\n\\n\\textbf{Requirements} are documents that define what a system must do in order to accomplish its purpose. Examples include:\\n\\n\\begin{itemize}\\n    \\item Functional Requirements\\n    \\item Non-functional Requirements\\n    \\item Design Specifications\\n    \\item Test Cases\\n    \\item User Stories\\n    \\item Acceptance Criteria\\n\\end{itemize}\\n\\nThe documentation of a system must be kept up-to-date with changes in requirements. It is important that this documentation always remains consistent with the requirements document.\\n\\n\\subsubsection{Review}\\n\\nIn order to make sure that requirements are met, we need to review them regularly. We can do this by:\\n\\n\\[ 1) Reviewing the existing code base, 2) Reviewing the existing documentation, 3) Reviewing test cases, 4) Reviewing acceptance criteria, and 5) Reviewing interactions with stakeholders.\\]\\n\\nThis process can be automated using tools such as Jenkins or Travis CI.\\n\\n\\subsubsection{Review Process}\\n\\nThere are several steps involved in reviewing code:\\n\\n%\\begin{enumerate}\\n%    \\item Identify Requirements\\n%    \\item Identify Features (if any)\\n%    \\item Identify Documentation (if any)\\n%    \\item Identify Test Cases (if any)\\n%    \\item Identify Acceptance Criteria (if any)\\n%\\end{enumerate}\\n\\n\\paragraph{\\textbf{Requirement Identification}}\\n\\nWe will identify all of our requirements using various techniques such as:\\n\\n- Feature trees: Feature trees are used to identify all features available for an application.\\n- Use cases: Use cases are used to identify all use cases available for an application.\\n- User stories: User stories are used to identify all user stories available for an application.\\n- Acceptance criteria: Acceptance criteria are used to identify all acceptance criteria available for an application.\\n\\n\\paragraph{\\textbf{Feature Tree}}\\n\\nA feature tree is created using different techniques such as:\\n\\n- Feature trees can be created manually, but it is very time consuming.\\n- Feature trees can also be created automatically using tools like RSpec or Cucumber.\\n- Feature trees can also be created automatically using tools like RSpec or Cucumber.\\n\\nThe feature tree for our project will be created using RSpec.\\n\\nIn RSpec we will use features such as:\\n\\n* An event handler which takes an object of type Event and provides methods such as addEventHandler and removeEventHandler on it.\\n* A service which takes an object of type Service and provides methods such as getServiceName and getServiceType on it.\\n\\nIn addition, we will create some test cases for these features:\\n\\n* Event handler - we will create two tests:\\n  * The first test will check if our event handler receives an object of type Event when called from within another method.\\n  * The second test will check if our event handler receives an object of type Event when called from within another method.\\n\\n* Service - we will create two tests:\\n  * The first test will check if our service receives an object of type Service when called from within another method.\\n  * The second test will check if our service receives an object of type Service when called from within another method.\\n\\nFinally, we'll create some acceptance criteria for these features:\\n\\n* Event handler - we'll create two acceptance criteria:\\n  * One acceptance criterion checks if our event handler receives an object of type Event when called from within another method.\\n  * Another acceptance criterion checks if our event handler receives an object of type Event when called from within another method.\\n\\n* Service - we'll create two acceptance criteria:\\n  * One acceptance criterion checks if our service receives an object of type Service when called from within another method.\\n  * Another acceptance criterion checks if our service receives an object of type Service when called from within another method.\\n\\nFinally, we'll create some test cases for these features:\\n  \\n* Event handler - we'll create two tests:\\n  * The first test will check if our event handler receives an object of type Event when called from within another method.\\n  * The second test will check if our event handler receives an object of type Event when called from within another method.\\n\\n\\nThe following figure shows how these features interact with each other:\\n\\n\\begin{figure}[h!]\\n\t\\centering\\n\t\\fbox{\\n\t\t\\fcolorbox[gray]{0.8}{%\\n\t\t\t\\fboxrule=0pt%\\n\t\t\t\\fcolorbox[gray]{1}{%\\n\t\t\t\t\\fcolor[gray]{0.8}{%\\n\t\t\t\t\t\\fboxrule=0pt%\\n\t\t\t\t\t\\fcolor[gray]{1}{%\\n\t\t\t\t\t\t\\fboxrule=0pt%\\n\t\t\t\t\t\t\\fcolor[gray]{1}{%\\n\t\t\t\t\t\t\t\\fboxrule=0pt%\\n\t\t\t\t\t\t\t\\fcolor[gray]{1}{%\\n\t\t\t\t\t\t\t\t\\fcolor[gray]{0.8}{%\\n\t\t\t\t\t\t\t\t\t\\fboxrule=0pt%\\n\t\t\t\t\t\t\t\t\t\\fcolor[gray]{1}{%\\n\t\t\t\t\t\t\t\t\t\t\\fboxrule=0pt%\\n\t\t\t\t\t\t\t\t\t\t\\fcolor[gray]{1}{}}\\n\t\t\t\t\t\t\t\t}% \\n\t\t\t\t\t\t\t}\\endgraf}}}}}\\n\t\\n        % Drawing lines between nodes\\n        % Draw lines between nodes\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n        % Draw lines between nodes\\n\\n\\n        %\\n\t\\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n        \\n\\n    \\n\\n        \\n    \\n      \\n    \\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n      \\n    \\n    \\n      \\n\\n"}
{"File Name":"front-end-monorepo\/adr-20.md","Context":"## Context\\nIMLS funded us to experiment with transcription tools. The project [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-task. This task is slightly unique in that:\\n- Incomplete classifications can be submitted\\n- Previous transcriptions from caesar are loaded and presented as an option in the sub-task\\n- volunteers can select the previous transcription, edit it, and submit a new transcription\\n- lines with previous transcriptions or retired lines are displayed visually by color\\n","Decision":"We will be porting the ASM functionality to the main classifier as a new transcription task. The task will be composite of:\\n- A drawing line task for transcription that is created by two pointer down and up events to mark the points of the line.\\n- the starting point and ending point will have visual indicators communicating the direction of creation.\\n- the line color will indicate the current status of completion\\n- completed lines cannot be edited\\n- Note that ASM leveraged a hacked polygon tool to do this, but we should have a line tool variant specifically for this instead. Downstream aggregation can leverage polygon type aggregations if it fits.\\n- A sub-task will display once the transcription line mark is made\\n- a text input will display suggestions from previous caesar aggregations.\\n- The suggestions can be selected and inserted as an editable value in the text input\\n- Grommet's [TextInput](https:\/\/storybook.grommet.io\/?path=\/story\/textinput--suggestions) with the suggestions prop will be utilized so that it can be styleable with our theme.\\n- sub-task will not be a modal, but a movable div linked to the currently selected line mark\\n[More detailed user stores are in this google doc](https:\/\/docs.google.com\/document\/d\/16abI-wkRlEXsWgACfFQVqwO76aEopohIjQiRfNQKWiw\/edit)\\n","tokens":129,"id":511,"Predictions":"## Decision\\nThis task is a candidate for the sub-task. The previous transcription is loaded and presented as an option in the sub-task.\\n## Context\\nThe previous transcription is loaded and presented as an option in the sub-task.\\n## Decision\\n## Decision\\nThis task is a candidate for the sub-task. The previous transcription is loaded and presented as an option in the sub-task.\\n## Context\\nThe previous transcription is loaded and presented as an option in the sub-task.\\n## Decision\\n## Decision\\nThis task is a candidate for the sub-task. The previous transcription is loaded and presented as an option in the sub-task.\\n## Context\\nThe previous transcription is loaded and presented as an option in the sub-task.\\n## Decision\\n## Decision\\nThis task is a candidate for the sub-task. The previous transcription is loaded and presented as an option in the sub-task.\\n## Context\\nThe previous transcription is loaded and presented as an option in the sub-task.\\n\\end{itemize}\\n\\newpage\\n\\begin{itemize}\\n\\item[1.] \\textbf{Context:} \\textit{\"Anti-slavery manuscripts\" has a workflow where volunteers transcribe lines of text first by marking them, then adding them to a task list to be transcribed by a human, who can then edit them if they want to.\"}\\n\\item[2.] \\textbf{Decision:} \\textit{\\#1: This task can be transcribed by human volunteers, but we are asking for human volunteers who will not edit any transcriptions that have been made by other humans already. This will ensure that only reliable information will be entered into our database.\"\\n\\item[3.] \\textbf{Context:} \\textit{\"Transcription tools\" allows users to mark text lines, then add them to a list of tasks to be transcribed by humans. These tasks are then assigned to users who volunteer their time to transcribe them.\"}\\n\\item[4.] \\textbf{Decision:} \\textit{\\#3: This task can be transcribed by human volunteers, but we are asking for human volunteers who will not edit any transcriptions that have been made by other humans already. This will ensure that only reliable information will be entered into our database.\"\\n\\item[5.] \\textbf{Context:} \\textit{\"Transcription tools\" allows users to mark text lines, then add them to a list of tasks to be transcribed by humans. These tasks are then assigned to users who volunteer their time to transcribe them.\"}\\n\\item[6.] \\textbf{Decision:} \\textit{\\#5: This task can be transcribed by human volunteers, but we are asking for human volunteers who will not edit any transcriptions that have been made by other humans already. This will ensure that only reliable information will be entered into our database.\"\\n\\end{itemize}\\n\\newpage\\n\\begin{figure}[H]\\n    \\centering\\n    \\includegraphics[width=0.75\\linewidth]{figs\/decision.png}\\n    % Requires \\usepackage{graphicx}\\n    % Requires image width & height\\n\\n    % Requires graphicx & graphicx.sty\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n    % Requires graphicx & graphicx.sty\\n\\n% \tRequires picture package (if available)\\n% \tRequires picture package (if available)\\n\\n% \tRequires picture package (if available)\\n% \tRequires picture package (if available)\\n\\n% \tRequires picture package (if available)\\n% \tRequires picture package (if available)\\n    \\n% \tRequires picture package (if available)\\n% \tRequires picture package (if available)\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n  \t%\\includegraphics[width=\\linewidth]{figs\/decision.png}\\n\\n\\n  \t%\\includegraphics[width=0.75\\linewidth]{figs\/decision.png}\\n\\n\\n  \t%\\caption{}\\n  \t  \\n\\n\\n\\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n\\n\t\\n  \\n  \\n    \\n\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\t\\n\\n\t\\n\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\t\\n\t\\n\\n\\n\\n\\n    \\n\\n\\n\t\\n\t  \\n  \\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \t\t\\n  \t\\n \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t \\n\t  \\n\t \\n\\n\\n\t\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"front-end-monorepo\/adr-23.md","Context":"## Context\\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\\n- add new code in three places:\\n- [task views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks).\\n- [task models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/tasks).\\n- [annotation models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/annotations).\\n- import the new modules by name in several places, and register them:\\n- [registered views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks\/helpers\/getTaskComponent.js).\\n- [import tasks models for workflow steps](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/WorkflowStepStore.js#L5-L18).\\n- [import all annotations to the classification model](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/Classification.js#L3).\\n- [register annotations with the classifications store](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/ClassificationStore.js#L111-L120).\\nIt was easy to forget one of these steps and a lot of this could be automated in code.\\n","Decision":"- Keep all the code together. Store task views and models next to each other in the filesystem. (#1212)\\n- Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212)\\n- Delegate responsibility from the classification to individual tasks. (#1228)\\n### Implementation\\n- Task code was moved to `lib-classifier\/src\/plugins\/tasks`. Each task has its own directory, with these subdirectories:\\n- _components_: React components to render the task.\\n- _models_: MobX State Tree models for the task. One Task model and one Annotation model.\\n- a _taskRegistry_ object was added, which is described in the [tasks README](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/plugins\/tasks\/readme.md).\\n- Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:\\n- _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.\\n- _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.\\n","tokens":492,"id":512,"Predictions":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\\n- add new code in three places:\\n- [task views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks).\\n- [task models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/tasks).\\n- [annotation models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/Annotations).\\n- import the new modules by name in several places, and register them:\\n- [registered views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks\/helpers\/getTaskComponent.js).\\n- [import tasks models for workflow steps](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/WorkflowStepStore.js#L5-L18).\\nIt was easy to forget one of these steps and a lot of this could be automated in code.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\\n- add new code in three places:\\n- [task views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks).\\n- [task models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/tasks).\\n- [annotation models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/Annotations).\\n- import the new modules by name in several places, and register them:\\n- [registered views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks\/helpers\/getTaskComponent.js).\\n- [import tasks models for workflow steps](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/WorkflowStepStore.js#L5-L18).\\nIt was easy to forget one of these steps and a lot of this could be automated in code.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\\n[task views][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/tasks\/view}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/tasks\/view})\\nd[task models][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/tasks\/model}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/tasks\/model})\\nd[annotation models][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/anatomy\/anatomy}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/anatomy\/anatomy})\\nd[importing annotations][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/anatomy\/importing_annotations}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/anatomy\/importing_annotations})]\\nd[registering annotations with classifiers][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotations}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotations})]\\nd[registering annotation views with classifiers][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotation_views_with_classificators}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotation_views_with_classificators})]\\nd[registering annotation model views with classifiers][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotation_models_with_classificators}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifiers\/registering_annotation_models_with_classificators})]\\nd[register annotation instance view with classifiers][\\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifications\/register_annotation_instance_view_with_classificators}] (see \\url{http:\/\/front-environment.monetize-zoo.org\/apps\/classifications\/register_annotation_instance_view_with_classificators})]\\endinput\\n"}
{"File Name":"front-end-monorepo\/adr-04.md","Context":"## Context\\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\\n","Decision":"We will only officially support the following browsers:\\n### Desktop\\n- Safari\\n- Chrome\\n- Firefox\\n- Edge\\n### Mobile\\n- Safari\\n- Chrome\\n- Opera\\nOf these, we will support the current and last two major versions.\\n","tokens":118,"id":514,"Predictions":"\\nThe decision is to drop support for IE11.\\n\\n\\n\\n\\n## Decision Result\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe have decided to drop support for IE11.\\n\\n\\n\\n## Decision Result\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe have decided to drop support for IE11.\\n\\n\\n## Decision Result\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe have decided to drop support for IE11.\\n\\n\\n\\end{document}\\n"}
{"File Name":"front-end-monorepo\/adr-22.md","Context":"## Context\\nWith new drawing tools being developed for the classifier, we need an API that's common to all drawing tools and marks, which can be easily extended by tool developers. This document lays out an overview of the drawing tool model and the public interfaces common to all tools and all marks.\\nTo support drawing, the subject viewer also needs to support:\\n- rendering a static list of marks from previous drawing task annotations.\\n- interacting with pointer events to create, edit and delete new marks for the current drawing task annotation.\\n","Decision":"### The subject viewer\\nThe subject viewer will render two components.\\n- _DrawingToolMarks_ takes an array of marks from drawing task annotations in the classification and renders it as a static, read-only list.\\n- _InteractionLayer_ wraps a _DrawingToolMarks_ component and adds pointer event support, so that the rendered array of marks can be edited and updated. THis component only acts on marks for the active drawing task.\\nMarks created by the _InterctionLayer_ are added to a new drawing task annotation, for the current classification, when we click Next or Done to complete the current task.\\nPointer event support is polyfilled in older browsers with [PEP](https:\/\/github.com\/jquery\/PEP).\\n### The drawing model\\nA drawing task has drawing tools. Each tool creates marks. On task completion, a drawing annotation is created, which is an array of all drawn marks. Each mark has a corresponding React component which renders the SVG for that particular shape.\\n","tokens":108,"id":516,"Predictions":"## Decision\\nThe drawing tool API is defined by the following public interfaces:\\n- DrawingTool (named DrawingTool) - The basic drawing tool interface.\\n- DrawingToolService (named DrawingToolService) - The drawing tool service interface.\\n- DrawingToolMarkManager (named DrawingToolMarkManager) - The annotation manager for marking the current drawing task annotation.\\n- DrawingToolMarkList (named DrawingToolMarkList) - The annotation list for the current drawing task annotation.\\n- DrawAnnotation (named DrawAnnotation) - The drawing annotation class.\\n\\n## Context\\nThis document defines a common set of APIs for all drawing tools and marks. We have chosen to define these APIs as a set of interfaces, rather than individually implementing them. This provides a common API that can be used by multiple tools and marks, and provides an easy way to extend these APIs by tool developers. This enables the user to create new tools and marks, without having to worry about implementing all the details of each individual API.\\n\\nWe have defined a set of interfaces that are common to all tools and marks. These interfaces are as follows:\\n- DrawingTool - Basic drawing tool interface. This interface is used for creating new drawings, modifying existing drawings, deleting drawings, and interacting with pointer events.\\n- DrawingToolService - Interface that provides access to the current annotation manager for marking the current task annotation. This interface also provides access to other annotations in the system such as annotations on previous tasks.\\n- DrawingToolMarkManager - Interface that provides access to an annotation manager for marking the current task annotation. This interface also provides access to other annotations in the system such as annotations on previous tasks.\\n- DrawAnnotation - Interface that allows users to draw annotations on top of existing annotations in the system such as annotations on previous tasks. This interface is also used by other tools such as MarkList and MarkListManager.\\n\\n## Decision\\n## Decision\\nThe following diagram illustrates how we have divided up our API into different pieces:\\n\\n\\nDrawing Tool Interface\\nDrawing Tool Mark Manager Interface\\nDrawing Tool Annotation Manager Interface\\n## Context\\nThe following diagram illustrates how we have divided up our API into different pieces:\\n\\n\\nDrawing Tool Interface\\nDrawing Tool Mark Manager Interface\\n## Decision\\n## Decision\\nThe following diagram illustrates how we have divided up our API into different pieces:\\n\\n\\nDrawing Tool Interface\\nDrawing Tool Mark Manager Interface\\n## Context\\nThis document defines a common set of APIs for all drawing tools and marks. We have chosen to define these APIs as a set of interfaces, rather than individually implementing them. This provides a common API that can be used by multiple tools and marks, and provides an easy way to extend these APIs by tool developers. This enables the user to create new tools and marks, without having to worry about implementing all the details of each individual API.\\n"}
{"File Name":"front-end-monorepo\/adr-02.md","Context":"## Context\\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\\nIt was initially chosen to use a SSR framework called [Next.js](https:\/\/github.com\/zeit\/next.js\/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\\n- It's been difficult to configure. We had to configure it to use [Grommet 2](https:\/\/v2.grommet.io\/) and [styled-components](https:\/\/www.styled-components.com\/) as these are not defaults.\\n- It's been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\\n- It's had issues with [Mocha](https:\/\/mochajs.org\/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\\nThere is an [open issue](https:\/\/github.com\/zeit\/next.js\/issues\/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\\nRoger and Sarah built several prototypes with other libraries, including [react-server](https:\/\/react-server.io\/), [razzle](https:\/\/github.com\/jaredpalmer\/razzle), and [react-universally](https:\/\/github.com\/ctrlplusb\/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn't appear to be actively maintained.\\n","Decision":"For the time being, we will continue to use NextJS. We're currently using v5; v6 is available, and likely to be the last version that is router-agnostic, so we can revisit this decision then.\\n","tokens":471,"id":517,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\\nIt was initially chosen to use a SSR framework called [Next.js](https:\/\/github.com\/zeit\/next.js\/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\\n- It's been difficult to configure. We had to configure it to use [Grommet 2](https:\/\/v2.grommet.io\/) and [styled-components](https:\/\/www.styled-components.com\/) as these are not defaults.\\n- It's been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\\n- It's had issues with [Mocha](https:\/\/mochajs.org\/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\\nThere is an [open issue](https:\/\/github.com\/zeit\/next.js\/issues\/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\\nRoger and Sarah built several prototypes with other libraries, including [react-server](https:\/\/react-server.io\/), [razzle](https:\/\/github.com\/jaredpalmer\/razzle), and [react-universally](https:\/\/github.com\/ctrlplusb\/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn't appear to be actively maintained.\\n\\n## Summary\\nThe decision document outlines our decision regarding Panoptes Front End as an architectural decision.\\n\\n---\\n\\n## References\\n\\---\\n\\n[1]: https:\/\/github.com\/zooniverse\/panoptes\\n"}
{"File Name":"front-end-monorepo\/adr-17.md","Context":"## Context\\nWe have identified a gap in our automated and manual testing process from the deployment of [zooniverse\/front-end-monorepo#1038](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1038). This pull request had been reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix. After automatic deployment to production by merging to master, however, we received a report that Planet Hunters: TESS classification interface was no longer functioning. The classify page was returning a 404.\\nWe had acknowledged previously that we had a need for staging environment deploys for the purposes of design reviews in [zooniverse\/front-end-monorepo#694](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/694). We now have a need to have staging deployments so we can manually check that the pull request functions in a deployed, production-like environment. The next.js builds and creates files specific for the production deployment that running the app locally for development does not replicate, nor is it replicated in automated unit testing.\\nInitially we were considering branch deploys for both of these cases, but in order to do this we would need to use wildcard sub-domains. At this time, [kubernetes ingress does not support wildcards](https:\/\/github.com\/containous\/traefik\/issues\/3884). Therefore, we need to devise a different solution.\\n","Decision":"In practice, we're going to have two kinds of pull request: one that changes a single app (e.g. new widget on project home page), and one that affects multiple apps (e.g. update to the shared component library). For PRs on a single app, we'd like to manually deploy it as a staging branch deployment so it can be tested in isolation. On merging to master, that gets deployed to staging automatically. We'd then do manual integration testing before manually deploying to production. For PRs across multiple apps, we'd test it locally before merging to master using a local Docker image setup to use Panoptes in production. Once it's deployed to staging, we'd do integration testing before manually deploying to production.\\nWe're going to setup a staging deployment that matches production as closely as possible to fill the gap of the need for manual reviews to confirm that the app is functioning. To accomplish this:\\n- Merging to master will be switched to deploy to staging to https:\/\/frontend.preview.zooniverse.org\\n- Production deployment will now be done manually triggered by lita command on slack and using a git tag for production\\n- The Jenkins file will be updated to use the git tags to determine the location of the deployment\\n- Cloudfront will be configured to load the correct microservice app depending on route:\\n- Both the staging (https:\/\/frontend.preview.zooniverse.org) and production domains (www.zooniverse.org) will have cloudfront configurations that will match URL traffic against rules setup in Cloudfront. The staging rules will map to the staging apps and the production rules will map to the production apps.\\n- The cloudfront rules match paths on the incoming URL, i.e. `\/about\/team` maps to a registered service via DNS, e.g.\\n+ When a `GET` request for URL `www.zooniverse.org\/about\/team` hits cloudfront, it maps to the `fe-content-pages.zooniverse.org` service domain.\\n+ Cloudfront then proxies that request via DNS lookup to the Kubneretes (K8) ingress service\\n+ The K8 ingress then looks up the registered service domain and forwards the request, in this case, to a `fe-content-pages` service pod to serve the request and respond to the client.\\n- Generally staging and production would have the same behaviour mappings in Cloudfront and staging will be a place to test these mapping out before setting up in production.\\nA future enhancement will be added for branch deploys for manual reviews. This can possibly be accomplished by:\\n- Lita command on slack\\n- Script is written to deploy to a branch\\n- Helm could be used to tear down the pods after the branch is merged\\n","tokens":305,"id":518,"Predictions":"## Approve\\nThis pull request was approved by the [zooniverse\/front-end-monorepo](https:\/\/github.com\/zooniverse\/front-end-monorepo) team. The pull request was reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix.\\n\\n## Description\\nThe Planet Hunters: TESS classification interface is a web application that allows users to search for TESS objects. The interface uses [Kubernetes ingress](https:\/\/kubernetes.io\/docs\/tasks\/administer-kubernetes\/ingress\/) to allow users to access it from anywhere.\\nThe Planet Hunters: TESS classification interface is a web application that allows users to search for TESS objects. The interface uses [Kubernetes ingress](https:\/\/kubernetes.io\/docs\/tasks\/administer-kubernetes\/ingress\/) to allow users to access it from anywhere.\\nThe Planet Hunters: TESS classification interface is a web application that allows users to search for TESS objects. The interface uses [Kubernetes ingress](https:\/\/kubernetes.io\/docs\/tasks\/administer-kubernetes\/ingress\/) to allow users to access it from anywhere.\\nThe Planet Hunters: TESS classification interface is a web application that allows users to search for TESS objects. The interface uses [Kubernetes ingress](https:\/\/kubernetes.io\/docs\/tasks\/administer-kubernetes\/ingress\/) to allow users to access it from anywhere.\\nThe Planet Hunters: TESS classification interface is a web application that allows users to search for TESS objects. The interface uses [Kubernetes ingress](https:\/\/kubernetes.io\/docs\/tasks\/administer-kubernetes\/ingress\/) to allow users to access it from anywhere.\\n\\n## Context\\nWe have identified a gap in our automated and manual testing process from the deployment of [zooniverse\/front-end-monorepo#1038](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1038). This pull request had been reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix. After automatic deployment to production by merging to master, however, we received a report that Planet Hunters: TESS classification interface was no longer functioning. The classify page was returning a 404.\\nWe had acknowledged previously that we had a need for staging environment deploys for the purposes of design reviews in [zooniverse\/front-end-monorepo#694](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/694). We now have a need to have staging deployments so we can manually check that the pull request functions in a deployed, production-like environment. The next.js builds and creates files specific for the production deployment that running the app locally for development does not replicate, nor is it replicated in automated unit testing.\\nInitially we were considering branch deploys for both of these cases, but in order to do this we would need to use wildcard sub-domains. At this time, [kubernetes ingress does not support wildcards](https:\/\/github.com\/containous\/traefik\/issues\/3884). Therefore, we need t"}
{"File Name":"front-end-monorepo\/adr-12.md","Context":"## Context\\nAs of writing, Zooniverse apps are being moved from Docker Swarm to Kubernetes, but both require a working Docker image of the app for deployment. However, building separate images from start to finish duplicates a lot of effort in downloading packages, building libraries etc.\\n","Decision":"The [Jenkinsfile](..\/..\/Jenkinsfile) builds Docker images in two stages:\\n1. Build an image containing the entire monorepo at the current commit, then install top- and package-level dependencies, and finally build production versions of the library packages. This image is [zooniverse\/front-end-monorepo](https:\/\/cloud.docker.com\/u\/zooniverse\/repository\/docker\/zooniverse\/front-end-monorepo).\\n1. Loop through all folders in the `packages` folder starting with `app-`, and build a Docker image for each one. These Docker images use the `zooniverse\/front-end-monorepo` image as a base, and then run the production build and start scripts for that app. An example image is [zooniverse\/fe-project](https:\/\/cloud.docker.com\/u\/zooniverse\/repository\/docker\/zooniverse\/fe-project).\\n","tokens":56,"id":519,"Predictions":"There is a Docker image of the Zooniverse app, but it's not being used. The current situation is that the app is being built from scratch from a Dockerfile and it's not easy to keep this up-to-date. It would be better for the app to be built from scratch from a Docker image that already has all the dependencies installed in one place.\\n## Outcome\\nThe Zooniverse apps are now built with a Docker image that includes all dependencies. This means we can easily update the app by replacing a dependency without needing to rebuild the entire application.\\n## Description\\nThe Zooniverse apps are now built with a Docker image that includes all dependencies. This means we can easily update the app by replacing a dependency without needing to rebuild the entire application.\\n## Context\\nAs of writing, Zooniverse apps are being moved from Docker Swarm to Kubernetes, but both require a working Docker image of the app for deployment. However, building separate images from start to finish duplicates a lot of effort in downloading packages, building libraries etc.\\n\\n## Decision\\nThere is a Docker image of the Zooniverse app, but it's not being used. The current situation is that the app is being built from scratch from a Dockerfile and it's not easy to keep this up-to-date. It would be better for the app to be built from scratch from a Docker image that already has all the dependencies installed in one place.\\n## Outcome\\nThe Zooniverse apps are now built with a Docker image that includes all dependencies. This means we can easily update the app by replacing a dependency without needing to rebuild the entire application.\\n\\end{description}\\n\\n\\subsection{Ongoing work}\\n\\label{sec:ongoing_work}\\n\\n\\begin{itemize}\\n\\item \\textbf{Zooniverse API:} We have started integrating our API into our mobile applications (iOS and Android), which should improve usability and add new functionality (e.g., adding search filters).\\n\\item \\textbf{Zooniverse Analytics:} We have started integrating our analytics platform into our mobile applications (iOS and Android), which should improve usability and add new functionality (e.g., adding search filters).\\n\\item \\textbf{Zooniverse Data Store:} We have started integrating our data store into our mobile applications (iOS and Android), which should improve usability and add new functionality (e.g., adding search filters).\\n\\end{itemize}\\n\\n"}
{"File Name":"front-end-monorepo\/adr-37.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\\n","Decision":"- Project owners can flag subject metadata columns as searchable by prefixing the heading with `%` in the manifest eg. these manifest headings `subject_id,image_name_1,%origin,link,%attribution,license,#secret_description` mark `metadata.origin` and `metadata.attribution` as searchable.\\n- Subject sets with indexed subjects have `metadata.indexFields` set to a list of indexed fields eg. `indexFields: 'origin,attribution'`.\\n- Subject metadata for indexed sets is copied to a separate database running on [Datasette](https:\/\/datasette.io). Each set is given its own table, named by subject set ID. Datasette gives us a RESTful API out of the box, allowing us to browse and search subject data as HTML or JSON. See https:\/\/subject-set-search-api.zooniverse.org\/subjects.\\n- From a volunteers point-of-view, the Datasette service is used to find specific subject IDs to work on. Those IDs are then sent to the Panoptes API `\/subjects\/selection` endpoint, which returns those subjects, in order, for classification.\\n","tokens":90,"id":520,"Predictions":"The decision is that the solution will be a combination of a full-text search and a set query. The full-text search will be done by Panoptes, and the set query will be done using SQL. The solution is not designed to be used in an online form, but rather in an offline form where volunteers can browse sets and pick subjects to work on.\\n\\n\\n## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\\n\\n\\n## Decision\\nThe decision is that the solution will be a combination of a full-text search and a set query. The full-text search will be done by Panoptes, and the set query will be done using SQL. The solution is not designed to be used in an online form, but rather in an offline form where volunteers can browse sets and pick subjects to work on.\\n\\n\\n## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\\n\\n\\n\\end{document}"}
{"File Name":"front-end-monorepo\/adr-11.md","Context":"## Context\\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https:\/\/github.com\/zooniverse\/markdownz), that uses  [markdown-it](https:\/\/github.com\/markdown-it\/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https:\/\/github.com\/zooniverse\/markdownz\/blob\/master\/src\/components\/markdown.jsx#L99).\\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https:\/\/v2.grommet.io\/markdown) renderer ([code](https:\/\/github.com\/grommet\/grommet\/blob\/master\/src\/js\/components\/Markdown\/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https:\/\/github.com\/probablyup\/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\\n","Decision":"We will make a new `Markdownz` React component that will be a part of the Zooniverse React component library. This new component will be built using [`remark`](https:\/\/github.com\/remarkjs\/remark). Remark is a popular markdown rendering library with a good plugin eco-system. It is supported by Zeit, which also supports Next.js, the server-side rendering library we have decided upon.\\nHere is how markdown-it's plugins will map to remark's plugins:\\n|markdown-it plugin\/custom plugin|remark plugin\/custom plugin|notes|\\n|--------------------------------|---------------------------|-----|\\n|markdown-it-emoji|remark-emoji|remark-emoji does not support emoticons like `:-)` but does gemojis like `:smile:`|\\n|markdown-it-sub|remark-sub-super||\\n|markdown-it-sup|remark-sub-super||\\n|markdown-it-footnote|built in|Remark supports this and can be enabled by passing `footnote: true` into its settings object|\\n|markdown-it-imsize|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we have defined a custom function that will set the `width` and `height` props on the Image component if the sizing syntax is defined in the alt tag of the markup. This is in contrast to the sizing syntax originally being defined in the src markup. We do not want to modify the sanitization remark-react does on source urls, so instead we have moved support of syntax to the alt tag area of the markup|\\n|markdown-it-video|deprecating|We are deprecating this because we don't want project owners embedding youtube videos with ads|\\n|markdown-it-table-of-contents|remark-toc|This works instead by looking for a heading that has case insensitive `table of contents`, `toc`, or `table-of-contents`|\\n|markdown-it-anchor|N\/A|Remark has basic anchor support.|\\n|twemoji|N\/A|Do we really need to use images of Twitter's emojis? Unicode support for emojis is fairly ubitiquous now.|\\n|markdown-it-html5-embed|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we define a custom function that returns HTML 5 video instead of an image of the src is a video mime-type or returns HTML 5 audio if the src is an audio mime-type|\\n|replaceSymbols|our own fork of remark-ping|remark-ping supports our needs for doing at-mentions of users, but it is forked to also support talk hashtags and the subject mentions using `^S`|\\n|relNofollow|remark-external-links|This plugin adds nofollow to absolute urls|\\n|markdownNewTab|remark-external-links|remark-external-links plugin adds `target='_blank'` and nofollow, noopener, noreferrer to all absolute urls. `+tab+` in front of the url will no longer work because of the sanitization that remark-react does. It may not be a good idea to modify how the sanitization works to allow this and instead just update our users on how this works instead.|\\n`remark-react` is added to parse the markdown to jsx which is inherantly safer than using `dangerouslySetInnerHTML` and allows customizing which react components get used for html elements like `markdown-to-jsx`.\\n","tokens":364,"id":521,"Predictions":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https:\/\/github.com\/zooniverse\/markdownz), that uses  [markdown-it](https:\/\/github.com\/markdown-it\/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https:\/\/github.com\/zooniverse\/markdownz\/blob\/master\/src\/components\/markdown.jsx#L99).\\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https:\/\/v2.grommet.io\/markdown) renderer ([code](https:\/\/github.com\/grommet\/grommet\/blob\/master\/src\/js\/components\/Markdown\/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https:\/\/github.com\/probablyup\/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\\n\\n## Decision\\nThe above decision record was generated using [gomarkdown](https:\/\/gomarkdown.org\/) and [md2js](http:\/\/md2js.org\/) in combination with [Golang](https:\/\/golang.org\/doc\/) and [React.js](http:\/\/reactjs.org\/) in combination with [Grommet](https:\/\/github.com\/grommet\/grommet) in order to generate this document.\\n"}
{"File Name":"front-end-monorepo\/adr-32.md","Context":"## Context\\nCurrently, the FEM Classifier's default \"Subject Viewer sizing\/fitting\" behaviour is _\"fit to width, no max height\"._\\nThis is analogous to PFE's special \"no-max-height\" behaviour, which had to be _explicitly set._\\nWhile the default \"fit to width, no max height\" behaviour is good in many cases, there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".\\nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.\\n","Decision":"The answer is to use **Layouts**.\\n- The plan for FEM is that every workflow should be able to _set their own Layout_ (e.g. portrait, landscape, fullscreen, etc) each with their own Subject Viewer sizing\/fitting behaviour.\\n- The choice of Layout (presumably per workflow) should be controllable by the project owner.\\n- Whether or not a specific subject viewer size configuration is supported will depend on the Layout.\\nSee also:\\n- The current [Layout code](..\/..\/packages\/lib-classifier\/src\/components\/Classifier\/components\/Layout), as of June 2021, currently only has DefaultLayout.\\n","tokens":149,"id":522,"Predictions":"The FEM Classifier's default \"fit to width, no max height\" behaviour is good in many cases, but it can be problematic for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\". We need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.\\n\\n## Implementation Details\\n## For this Decision Record, we have selected the following settings:\\n## 1. Fit to Width:\\n\\nFit width: 800\\nFit height: 600\\n\\# Fit width: 1000\\nFit height: 700\\n\\# Fit width: 1200\\nFit height: 800\\n\\# Fit width: 1400\\nFit height: 900\\n## 2. Fit Height:\\n\\nFit height: 600\\n## \\nIn most cases, we will want to fit viewers to the same size as the input image. This will be done by fitting their maximum heights to the maximum heights of the input image.\\nIn some cases, however, it may be more appropriate for some viewers to have a slightly smaller maximum height than others.\\nFor example, in some cases it may be more appropriate for a viewer whose maximum height is less than that of an input image with a larger maximum height.\\nIf this is not possible due to constraints on other viewers' maximum heights or due to limitations on our own maximum heights, then we will need to set up our Viewer configuration so that each Viewer has its own minimum and maximum heights.\\nIn those cases where we want our Viewer's minimum and maximum heights to match those of an input image with a larger maximum height (e.g., when we want our Viewer's minimum and maximum heights to match those of an input image with a taller Subject), then we will need to set up our Viewer configuration so that each Viewer has its own minimum and maximum heights.\\nWe can do this by setting an attribute called \"Maximum Height\" in each Viewer's Configuration file (in \\texttt{config\/config.cfg}):\\n\\n## Maximum Height:\\nMaximum Height:\\[min,height]\\nn\\# Minimum Height:\\[min,height]\\nn\\# Maximum Height:\\[max,height]\\nn\\# Minimum Height:\\[min,height]\\nn\\# Maximum Height:\\[max,height]\\nn\\n## In this example, we have chosen both a minimum and max-height attribute for each Viewer.\\nThe default value for both attributes is \\texttt{min=800,max=600}. We have also chosen \\texttt{MinimumHeight=600} and \\texttt{MaximumHeight=800} as these are reasonable values for most images.\\n## The value specified here does not affect how much space in pixels is used by any viewer. However, it affects how much space in pixels is used by each particular viewer. If you want all viewers in your workflow use all available space within their respective windows, then you should set either \\texttt{MinimumHeight=800} or \\texttt{MaximumHeight=1200}. The latter option gives you more room for zooming into your subjects.\\n## For example:\\n## Maximum Width = 800 pixels\\n## Maximum Height = 600 pixels\\n## Fit Width = >700 pixels\\nThis means that if you click on an image in your workflow with an Image Viewer whose Maximum Width or Maximum Heights matches those specified here (or with one whose Minimum Height or Maximum Heights does), then that Image Viewer will use all available space within its window.\\nThe default value for both attributes is \\texttt{min=800,max=400}. We have also chosen \\texttt{MinimumHeight=400} and \\texttt{MaximumHeight=700} as these are reasonable values for most images.\\n## The value specified here does not affect how much space in pixels is used by any viewer. However, it affects how much space in pixels is used by each particular viewer. If you want all viewers in your workflow use all available space within their respective windows, then you should set either \\texttt{MinimumHeight=400} or \\texttt{MaximumHeight=700}. The latter option gives you more room for zooming into your subjects.\\nThis means that if you click on an image in your workflow with an Image Viewer whose Maximum Width or Maximum Heights matches those specified here (or with one whose Minimum Height or Maximum Heights does), then that Image Viewer will use all available space within its window.\\nThe default value for both attributes is \\texttt{min=1000,max=700}. We have also chosen \\texttt{MinimumHeight=1000} and \\texttt{MaximumHeight=700} as these are reasonable values for most images.\\n ## In this example we have chosen both a minimum and max-height attribute for each viewer.\\n ## This means that if you click on an image with an ImageViewer whose MinimumWidth or MinimumHeights matches those specified here (or with one whose MinimumWidth or MinimumHeights does), then that ImageViewer will use all available space within its window.\\n ## The default value for both attributes is min=\\[min,width]\\nn\\# MinimumWidth=\\[min,width]\\nn\\# MaximumWidth=\\[max,width]\\nn\\# MinimumWidth=\\[max,width]\\nn\\# MaximumWidth=\\[max,width]\\n ## In this example we have chosen both a minimum and max-height attribute for each viewer.\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n### Example - Setting up multiple FEM Classifier configurations\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n###### **Customizing Your Workspace**\\n\\n##### Others:\\n\\n##### Others:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n#### Question:\\n\\n##### Others:\\n"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"Predictions":"\\nWe decided to use the existing library json8 for converting maps to objects. We also decided to do it ourselves, because we think that this solution is the most straightforward and easiest to implement.\\n\\n\\n## Consequences\\n\\nThe following changes are required in the workflow resource\\cite{workflow-resource}.\\n\\n\\n## Workflow Resource\\nThe workflow resource is an object that contains a series of steps. Each step contains a list of actions associated with it. Actions can be a single string or a list of strings (e.g. [\u201cstart\u201d, \u201cadd\u201d, \u201crun\u201d]). Actions can be in any order, but they must follow the order specified by the user. The workflow resource also contains some data about each action, including its name and its type (e.g. \u201crun\u201d vs \u201cstart\u201d). All actions must have at least one step.\\n\\n## Steps\\nSteps are objects that contain the following properties:\\n\\n\\n\\begin{itemize}\\n  \\item id: An ID for the step.\\n  \\item name: The name of the step.\\n  \\item steps: A list of steps.\\n  \\item action: The action associated with the step.\\n\\end{itemize}\\n\\nThe steps property should be an array containing one element per step.\\nThe id property should be a unique identifier for this step.\\nThe name property should be a string that represents the name of this step.\\nThe steps property should be an array containing one element per step.\\nThe action property should be an array containing one element per action associated with this step.\\nThe id property should be a unique identifier for this step.\\nThe name property should be a string that represents the name of this step.\\nThe steps property should be an array containing one element per step.\\nThe id property should be a unique identifier for this step.\\nThe name property should be a string that represents the name of this step.\\n\\n## Workflow Resource Attributes\\nWorkflow resource attributes are defined on top level. They are not part of any sub-objects or nested objects, and they are not exposed to other Zooniverse devs. They are only defined on top level, so they cannot have any side effects and can only change their value once. They cannot have any side effects and cannot change their value once after they were set:\\n\\n\\n\\begin{itemize}\\n  \\item workflow\\_id: An ID for the workflow resource.\\n  \\item workflow\\_steps\\_id: An ID for each workflow resource instance\u2019s steps.\\n  \\item workflows\\_id: A unique identifier for all workflows on Zooniverse.\\n\\end{itemize}\\n\\nThese attributes are used by Zooniverse\u2019s internal API in order to determine which workflows exist on Zooniverse, how many workflows exist on Zooniverse, how many workflows have been completed by users, etc.\\n\\n## Workflow Resource Steps Attributes\\nWorkflow resource steps are defined on top level. They are not part of any sub-objects or nested objects, and they are not exposed to other Zooniverse devs. They can only change their value once after they were set:\\n\\n\\n\\begin{itemize}\\n  \\item steps\\_id: An ID for each workflow resource instance\u2019s set of steps.\\n  \\item workflows\\_id: A unique identifier for all workflows on Zooniverse.\\n\\end{itemize}\\nThese attributes are used by Zooniverse\u2019s internal API in order to determine which workflows exist on Zooniverse, how many workflows exist on Zooniverse, how many workflows have been completed by users, etc.\\n\\n## Workflow Resource Actions Attributes\\nWorkflow resource actions are defined on top level. They can only change their value once after they were set:\\n\\n\\n\\begin{itemize}\\n  \\item actions\\_id: An ID for each workflow resource instance\u2019s set of actions.\\n  \\item actions\\_id: A unique identifier for all actions associated with this workflow resource instance.\\n\\end{itemize}\\nThese attributes are used by Zooniverse\u2019s internal API in order to determine which actions exist on Zooniverse, how many actions exist on Zooniverse, how many actions have been completed by users, etc.\\n\\n## Consequences:\\nWe decided that we will use json8 instead of MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method because it is more straightforward and easier to implement than using MobX's [`toJSON`](https:\/\/mobx.js.org\/refguide\/json.html). It is also easier to understand what happens when you call [`toJSON`](https:\/\/mobx.js.org\/refguide\/json.html) because you can see what happens when you call [`toJS`](https:\/\/mobx.js.org\/refguide\/tojs.html).\\nIn addition, using json8 will allow us to easily add new features later without having to worry about converting our existing codebase into json8-todo-lists or something similar.\\n\\n# References\\n\\n* [adr-5](https:\/\/github.com\/zooniverse\/adr-5)\\n\\n# TODO\\n\\n* [zooniverse\/Panoptes-Front-End](http:\/\/github.com\/zooniverse\/Panoptes-Front-End)\\n\\n# TODO\\n\\n* [zooniverse\/Panoptes-Front-End](http:\/\/github.com\/zooniverse\/Panoptes-Front-End)\\n\\n* [zoo-database](http:\/\/github.com\/zoo-database)\\n\\n# TODO\\n\\n* [zoo-database](http:\/\/github.com\/zoo-database)\\n\\n* [zoo-database](http:\/\/github.com\/zoo-database)\\n\\n# TODO\\n\\n* [zoo-database](http:\/\/github.com\/zoo-database)\\n"}
{"File Name":"front-end-monorepo\/adr-08.md","Context":"## Context\\nFor the upcoming the upcoming TESS project (aka Planet Hunters 2019), we need  to create a special Light Curve Viewer component for showing interactive brightness curves.\\nFurther information available on the [TESS Front End documentation](https:\/\/docs.google.com\/document\/d\/1BcX4PyC2khmtC9g035G2e5I1zirZa3z9mWINkWATaPs\/edit?usp=sharing).\\n@rogerhutchings, @shaun.a.noordin and @srallen researched available charting libraries, particularly ones designed for seamless integration with React. Of those, prototypes were built with [Plot.ly](https:\/\/plot.ly\/javascript\/react\/), [Victory](https:\/\/formidable.com\/open-source\/victory\/) and [vanillaD3](https:\/\/d3js.org\/). Sample data was taken from the Planet Hunters project.\\nFrom these, we discovered that:\\n1. Both Plot.ly and Victory suffered from slow performance, especially when using their out-of-the-box features (such as the selection tools).\\n2. Plot.ly and Victory had some API limitations that made fulfilling spec requirements difficult. Plot.ly, for example, had a limited set of events available (`click` events, but not `mousemove`, `drag` etc).\\n3. The out-of-the-box features were often almost perfect, but not quite, and hard to extend. Plot.ly, for example, has a range select tool, but it's designed for selecting data subsets for plotting. To create annotations, we'd have needed to use the range selections and turn them into a variable width bar chart plotted on top of the scatter plot.\\n","Decision":"We decided that trying to use a library to meet our fairly unusual requirements would end up causing more work than would be saved by having the basic features available. As such, the Light Curve Viewer will be built using native D3, implemented into the classifier's React components.\\n","tokens":348,"id":524,"Predictions":"\\nThe decision is to use the current prototype of Plot.ly to create a new component that provides a seamless integration with React. This component will be used for the Planet Hunters 2019 project. The component will provide an interactive brightness curve view, similar to the one shown in Figure 1, but with an interactive zoom feature.\\n\\nThe proposed component will be designed using React and will use the [Plot.ly](https:\/\/plot.ly\/javascript\/react\/) API. The API requires that all parameters are passed as `key=value` pairs where `key` is a key value pair of data type `string`, `number`, or `boolean`. For example, in Figure 2, the plot view will require a `date` value of type `string` and a range selection of type `number`. The API would also expect that all data points in the plot view would have the same data type as their corresponding data points in the scatter view. This means that we need to pass all data points as strings, and we need to pass all dates as numbers.\\n\\nThe proposed component will be designed using [Victory](https:\/\/formidable.com\/open-source\/victory\/), which is a library for creating interactive charts in JavaScript based on D3.js. Victory provides a lot of useful functionality for creating interactive charts including: selection tools, drag\/drop support, tooltip support and many other features.\\n\\nVictory has been successfully used by many projects such as [NASA's Kepler](https:\/\/github.com\/NASA\/kepler) and [the Planetary Data System](https:\/\/github.com\/pds\/planetary-data-system).\\n\\nVictory has also been integrated into several popular D3.js libraries such as [VictoryD3](https:\/\/github.com\/mcguired\/victory-d3), [Chart.js](https:\/\/github.com\/chartsjs\/chartsjs), [D3-Vis.js](https:\/\/github.com\/d3vis\/d3vis) and so on.\\n\\nVictory can be easily integrated into our existing project by providing an endpoint on our server which accepts query parameters such as: \\n\\begin{itemize}[noitemsep]\\n    \\item `date`: A date object with a date range selected from either a datepicker or range picker.\\n    \\item `time`: A time object with an hour selected from either a datepicker or range picker.\\n    \\item `color`: A color object with colors selected from either a colorpicker or range picker.\\n    \\item `scale`: A scale object with values selected from either a scale picker or range picker.\\n    \\item `legend`: A legend object with legends selected from either a legend picker or range picker.\\n\\end{itemize}\\n\\n\\nThe endpoint would then return an array of objects containing all data points within the specified time range (or within any other specified time period). The returned array should look something like this:\\n\\begin{center}\\begin{tabular}{|l|l|}\\n    \\hline\\n    \\textbf{time} & \\textbf{data} \\\\\\n    \\hline\\n    00:00 - 23:59 & {1,2} \\\\\\n    00:00 - 23:59 & {3,4} \\\\\\n    00:00 - 23:59 & {5} \\\\\\n    \\hline\\n\\end{tabular}\\end{center}\\n\\n\\nIn this example, we've used two arrays containing three different types of data points (one for each hour), but it could easily be expanded to include more than two arrays. We would then need to parse each array into objects containing different types of data points (such as colors and scales), convert them into arrays containing only one type of data point (such as times), and then return an array containing only those objects.\\n\\nWe've already built some prototypes using Plot.ly which we think could provide some inspiration for what we want to do here. These prototypes are shown below:\\n\\begin{center}\\includegraphics[width=0.6\\linewidth]{figs\/plotly.png}\\end{center}\\n\\n\\nUnfortunately none of these prototypes are very good at showing what we want them to do. They don't provide any zooming features and don't give us enough information about what's happening in each time slice (e.g., how many minutes are being displayed at each time slice). They also don't allow us to select specific ranges within each time slice (which is very important when selecting times). Finally they don't provide any way for us to select specific dates within each time slice (which is also very important when selecting times).\\n\\nWe've also built another prototype using Victory which looks much better than Plot.ly at showing what we want it to do. Unfortunately it doesn't provide any controls allowing us to select specific ranges within each time slice (which is very important when selecting times). It also doesn't allow us to select specific dates within each time slice (which is also very important when selecting times).\\n\\nSo at this point we're not sure what approach we should take here. We're not happy with either Plot.ly or Victory because neither provides enough information about what's happening in each time slice nor allows us to select specific ranges within each time slice nor allows us to select specific dates within each time slice.\\n\\nWe're going back and forth between these two approaches because they both have advantages over one another. Plot.ly provides more information about what's happening in each time slice while Victory provides more information about what's happening in each individual hour.\\n\\nIn this situation I think I'll go with Plot.ly because it gives me more information about what's happening in each hour while Victory gives me more information about what's happening during that particular hour.\\nThe reason I'm going with Plot.ly over Victory is that I think Victory has too many limitations when trying to fit everything into one chart.\\nThe reason I'm going with Plot.ly over Victory is that I think Victory has too many limitations when trying to fit everything into one chart.\\nThe reason I'm going with Plot.ly over Victory is that I think Victory has too many limitations when trying to fit everything into one chart.\\nThe reason I'm going with Plot.ly over Victory is that I think Victory has too many limitations when trying to fit everything into one chart.\\n\\n"}
{"File Name":"front-end-monorepo\/adr-03.md","Context":"## Context\\nAs part of the development of Panoptes.js, we came across the issue of authentication. Currently, the auth library is part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client), and provides support for first-party login and OAuth2 (for custom front ends).\\nHowever, the current client has the following drawbacks:\\n- It only provides the relevant API methods and no front end components, which need to be written by the library consumer.\\n- As a result, the login experience is slightly different across each project.\\n- The OAuth client has been buggy in the past.\\n","Decision":"To write a separate auth library. This will expose:\\n- the __API helpers__ for the first-party auth process, if the consumer wants\/needs to use them.\\n- __+a first-party auth client__ as a standalone React app. This will open an IFrame to the existing Panoptes views for sign in, sign out, registration, and profile management (password reset and account deletion)\\n- __an OAuth client__, also as a standalone React app. This will wrap an existing package to provide OAuth support, such as [js-client-oauth2](https:\/\/github.com\/mulesoft\/js-client-oauth2), and will handle session length, provide session timeout prompts, and provide a session expiry hook or event for a consumer app (e.g. as a prompt to save the current state).\\n","tokens":134,"id":525,"Predictions":"## Answer:\\n## Yes. Panoptes should support first-party login and OAuth2, and provide front-end components (which are currently not yet available in Panoptes).\\n## This will be a job for the Panoptes.js project.\\n## The current implementation of the Panoptes.js project is described in [this PR](https:\/\/github.com\/zooniverse\/panoptes-js\/pull\/195), which has been merged into [Panoptes.js](https:\/\/github.com\/zooniverse\/panoptes-js).\\n\\n## 1.3.1\\n\\n### 1.3.2\\n\\n### 1.3.3\\n\\n### 1.3.4\\n\\n### 1.3.5\\n\\n### 1.3.6\\n\\n### 1.4\\n\\n#### 2: Front-End Components\\n\\nThe front-end components provide a way to make the authentication process easier for the user, by providing a single sign-on experience (for example, by logging in with Facebook or Google). Front-end components can also be used to provide authentication options not supported by the library itself.\\n\\n## Explanation of Front-End Components\\n\\nThe front-end components are provided as a set of libraries that can be used as a library consumer to provide authentication for any application that uses Panoptes.js.\\n\\n## Front-End Component Libraries\\n#### Authentication Library\\nThis library provides a set of APIs for authentication, including the ability to create custom tokens and access additional user data.\\n\\n#### Social Library\\nThis library provides an API for retrieving social media information from Facebook or Google.\\n\\n#### Google Library\\nThis library provides an API for retrieving Google information from Google.\\n\\n#### Twitter Library\\nThis library provides an API for retrieving Twitter information from Twitter.\\n\\n#### OAuth Library\\nThis library provides an API for authenticating users using OAuth2.\\n\\nThe following list shows which libraries are provided with each front-end component:\\n\\n## Authentication Library\\nThe authentication library contains all of the APIs needed to authenticate with Facebook and Google.\\n\\begin{itemize}\\n    \\item User: Provides methods to authenticate with Facebook and Google.\\n    \\item AccessToken: Provides methods to retrieve access tokens.\\n    \\item AccessTokenSecret: Provides methods to retrieve access secret tokens.\\n    \\item refreshToken: Provides methods to retrieve refresh tokens.\\n    \\item refreshTokenSecret: Provides methods to retrieve refresh secret tokens.\\n\\end{itemize}\\n\\n## Social Library\\nThe social library contains all of the APIs needed to authenticate with Facebook, Twitter, and Google.\\n\\begin{itemize}\\n    \\item AccessToken: Provides methods to retrieve access tokens.\\n    \\item AccessTokenSecret: Provides methods to retrieve access secret tokens.\\n\\end{itemize}\\n\\n## Google Library\\nThe google library contains all of the APIs needed to authenticate with Google.\\n\\begin{itemize}\\n    \\item RefreshToken: Provides methods to retrieve refresh tokens.\\n    \\item RefreshTokenSecret: Provides methods to retrieve refresh secret tokens.\\n\\end{itemize}\\n\\n## Twitter Library\\nThe twitter library contains all of the APIs needed to authenticate with Twitter.\\n\\begin{itemize}\\n    \\item AccessToken: Provides methods to retrieve access tokens.\\n    \\item AccessTokenSecret: Provides methods to retrieve access secret tokens.\\n\\end{itemize}\\n\\nThese libraries are not required but are included for completeness.\\n\\n## OAuth Library\\nThe oauth library contains all of the APIs needed when authenticating using OAuth2 (i.e., when using third-party applications).\\n\\n### Social Library Example Usage:\\nTo use this component, you must include it in your application's manifest file as follows:\\n\\n```\\n{\\n        \"name\": \"auth\",\\n        \"version\": \"0\",\\n        \"description\": \"Auth\",\\n        \"source\": {\\n            \"git\": {\\n                \"url\": \"https:\/\/github.com\/zooniverse\/panoptes-js.git\"\\n            },\\n            \"tag\": {\\n                \"version\": \"0\"\\n            },\\n            \"branch\": {\\n                \"version\": \"0\"\\n            }\\n        },\\n        \"dependencies\": {\\n            ...\\n        },\\n        ...\\n```\\n\\n### OAuth Example Usage:\\nTo use this component, you must include it in your application's manifest file as follows:\\n\\n```\\n{\\n        ...\\n        \/\/ Specify which libraries you want included in your application's manifest file:\\n        \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-social-library-in-your-application's-manifest-file\\n        ...\\n        \/\/ Specify which libraries you want included in your application's manifest file:\\n        \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-google-library-in-your-application's-manifest-file\\n        ...\\n```\\n\\n\\n\\nIn order for Panoptes.js users who are accessing their accounts via third-party applications (i.e., via third-party sites) or through an email provider (e.g., gmail) to use these libraries, they need an account on those sites or providers.\\n\\nIf you do not wish for these accounts or providers' accounts (or any other account) on those sites or providers' servers (or any other server) then you can disable them by setting `disableAccountProviders` and `disableProviderAccounts` properties on your HTTP request object:\\n\\n```\\nconst { request } = require('request');\\n\\nrequest({\\n    method: 'GET',\\n    path: '\/auth\/facebook',\\n    headers: {\\n      'Content-Type': 'application\/json'\\n      }\\n})\\n.then(response => response.json())\\n.then(json => {\\n    const accessToken = json.accessToken;\\n})\\n.catch(err => {\\n   console.log(\"Error fetching accessToken:\", err);\\n});\\n```\\n\\n\\n\\nFor more information about setting these properties on your requests, see [the documentation](https:\/\/zooniverse.github.io\/panoptes-js\/docs\/#using-the-social-library-in-your-application-s-manifest-file).\\n\\nIf you do not wish for these accounts or providers' accounts (or any other account) on those sites or providers' servers then you can disable them by setting `disableAccountProviders` and `disableProviderAccounts` properties on your HTTP request object:\\n\\n```\\nconst { request } = require('request');\\n\\nrequest({\\n    method: 'GET',\\n    path: '\/auth\/google',\\n    headers: {\\n      'Content-Type': 'application\/json'\\n      }\\n})\\n.then(response => response.json())\\n.then(json => {\\n   const accessToken = json.accessToken;\\n   const refreshToken = json.refreshToken;\\n});\\n```\\n\\n\\n\\nFor more information about setting these properties on your requests, see [the documentation](https:\/\/zooniverse.github.io\/panoptes-js\/docs\/#using-the-google-library-in-your-application-s-manifest-file).\\n\\n\\n\\n### Front-End Component Libraries Example Usage:\\nTo use this component, you must include it in your application's manifest file as follows:\\n\\n```\\n{\\n     ...\\n     \/\/ Specify which libraries you want included in your application's manifest file:\\n     \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-social-library-in-your-application's-manifest-file\\n     ...\\n     \/\/ Specify which libraries you want included in your application's manifest file:\\n     \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-google-library-in-your-application-s-manifest-file\\n     ...\\n}\\n```\\n\\n\\n\\nIn order for Panoptes.js users who are accessing their accounts via third-party applications (i.e., via third-party sites) or through an email provider (e.g., gmail) to use these libraries, they need an account on those sites or providers' servers (or any other server) then they need an account on those sites or providers' servers themselves; that is, they need an account at their respective site(s)\/provider(s). For example:\\n\\nIf Panoptes.js users who are accessing their accounts via third-party applications (i.e., via third-party sites) use gmail then they need an account at gmail themselves; that is, they need an account at gmail itself.\\n\\n\\n\\nTo enable\/disable these accounts\/users\/etc., simply set `enableAccountProviders` and `disableProviderAccounts` properties on your HTTP request object:\\n\\n\\n```\\nconst { request } = require('request');\\n\\nrequest({\\n   method : 'GET',\\n   path : '\/auth\/google',\\n   headers : {\\n       'Content-Type': 'application\/json'\\n   }\\n})\\n.then(response => response.json())\\n.then(json => {\\n   const accessToken = json.accessToken;\\n   const refreshToken = json.refreshToken;\\n});\\n```\\n\\n\\n\\nIn order for Panoptes.js users who are accessing their accounts via third-party applications (i.e., via third-party sites) use gmail then they need an account at gmail themselves; that is, they need a Gmail account.\\n\\n\\n\\nTo enable\/disable these accounts\/users\/etc., simply set `enableAccountProviders` and `disableProviderAccounts` properties on your HTTP request object:\\n\\n\\n```\\nconst { request } = require('request');\\n\\nrequest({\\n   method : 'GET',\\n   path : '\/auth\/google',\\n   headers : {\\n       'Content-Type': 'application\/json'\\n   }\\n})\\n.then(response => response.json())\\n.then(json => {\\n   const accessToken = json.accessToken;\\n   const refreshToken = json.refreshToken;\\n});\\n```\\n\\n\\n\\nFor more information about setting these properties on your requests, see [the documentation](https:\/\/zooniverse.github.io\/panoptes-js\/docs\/#using-the-social-library-in-your-application-s-manifest-file).\\n\\n\\n\\n### Front-End Component Libraries Example Usage:\\nTo use this component, you must include it in your application's manifest file as follows:\\n\\n```\\n{\\n     ...\\n     \/\/ Specify which libraries you want included in your application's manifest file:\\n     \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-social-library-in-your-application-s-manifest-file\\n     ...\\n     \/\/ Specify which libraries you want included in your application's manifest file:\\n     \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/README.md#use-the-google-library-in-your-application-s-manifest-file\\n     ...\\n}\\n```\\n\\n\\n\\nIn order for Panoptes.js users who are accessing their accounts via third-party applications (i.e., via third-party sites) use gmail then they need an account at gmail themselves; that is, they need a Gmail account.\\n\\n\\n\\nTo enable\/disable these accounts\/users\/etc., simply set `enableAccountProviders` and `disableProviderAccounts` properties on your HTTP request object:\\n\\n\\n```\\nconst { request } = require('request');\\n\\nrequest({\\n   method : 'GET',\\n   path : '\/auth\/google',\\n   headers : {\\n       'Content-Type': 'application\/json'\\n   }\\n})\\n.then(response => response.json())\\n.then(json => {\\n \\n\/\/ Set enableAccountProviders here if necessary... \\n\/\/ Or set disableProviderAccounts here if necessary...\\n \\n\/\/ Set disableAccountProviders here if necessary...\\n \\n});\\n```\\n\\n\\n\\nFor more information about setting these properties on your requests, see [the documentation](https:\/\/zooniverse.github.io\/panoptes-js\/docs\/#using-the-social-library-in-your-application-s-manifest-file).\\n\\n\\n\\n### Front-End Component Libraries Example Usage:\\nTo use this component, you must include it in your application's manifest file as follows:\\n\\n```\\n{\\n     ...\\n     \/\/ Specify which libraries you want included in your application's manifest file:\\n     \/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/src\/pages\/auth\/facebook\/index.html.hbs # Use this page instead!\\n  \\n\/\/ Specify which libraries you want included in your application's manifest file:\\n\/\/ https:\/\/github.com\/zooniverse\/panoptes-js\/blob\/master\/src\/pages\/auth\/facebook\/index.html.hbs # Use this page instead!\\n  \\n\/\/ Specify which libraries you want included in your application's manifest file:\\n\/\/ https:\/\/github.com\/zooniverse\/pan"}
{"File Name":"front-end-monorepo\/adr-14.md","Context":"## Context\\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/)) has traditionally been [baked](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/locales\/en.js) [into](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/lib\/publications.js) the front end code.\\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\\nThese are the options we have:\\n1. Continue to hard code content into the front end code\\n2. Create our own CMS to manage our content\\n3. Install a third party CMS into one of our servers\\n4. Use an online third party CMS\\n","Decision":"We've decided to use [Contentful](https:\/\/www.contentful.com\/), an online third party CMS, to host our content. (Or at least, part of our content, such as the often-changing Publications page.) There will be two users registered: one admin account with full access to the Content Model editor _and_ content, and an editor account for day-to-day use which only has content editing access.\\n**Reasons for choosing Contentful**\\n- [@rogerhutchings](https:\/\/github.com\/rogerhutchings) has used it for projects in the past\\n- It has a free tier\\n- It has a well-documented API\\n- It has an excellent admin interface for editing content and models\\n- It's one of the larger hosted CMS offerings, meaning it's less likely to disappear overnight\\n**Pros**\\n- CMS makes content management easier and updates faster, especially for non-developers.\\n- External platform reduces the need for any sort of code maintenance.\\n- We can run our Zooniverse content on the Free Tier (see [Note of Caution 3: Database Limits](#note-of-caution-3-database-limits))\\n**Cons**\\n- Reliance on external platfrom for storing data introduces the usual third-party risks. (See [Note of Caution 1: Exit Strategy](#note-of-caution-1-exit-strategy)) Possible dangers include:\\n- external CMS has downtime, shutting down our content by extension.\\n- external CMS changes pricing structure, forcing us out of the free tier.\\n- external CMS shuts down, forcing us to find an alternative data store.\\n- We need to maintain an admin account. (Shouldn't be a worry as long as we have Passbolt.)\\n","tokens":266,"id":526,"Predictions":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/)) has traditionally been [baked](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/locales\/en.js) [into](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/lib\/publications.js) the front end code.\\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\\nThese are the options we have:\\n1. Continue to hard code content into the front end code\\n2. Create our own CMS to manage our content\\n3. Install a third party CMS into one of our servers\\n4. Use an online third party CMS\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/)) has traditionally been [baked](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/locales\/en.js) [into](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/lib\/publications.js) the front end code.\\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\\nThese are the options we have:\\n1. Continue to hard code content into the front end code\\n2. Create our own CMS to manage our content\\n3. Install a third party CMS into one of our servers\\n4. Use an online third party CMS\\n\\end{document}"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"Predictions":"\\nI've decided to use the panoptes-javascript-client as the auth client on the front-end, and to use env vars directly on the server.\\nI'm using [this guide](https:\/\/medium.com\/@daniel_nicolas\/nextjs-3-0-how-to-use-environment-variables-in-your-next-js-app-8e5b6b91d6f7) to set up my environment variables.\\nThe next step is to configure panoptes-javascript-client. I'll create a file called `config.js` in my project root, and put this in there:\\n\\n```\\n\/\/ Panoptes config file\\nmodule.exports = {\\n  \/\/ Set up Panoptes variables\\n  apiKey: 'your_api_key',\\n  clientId: 'your_client_id',\\n  secret: 'your_client_secret',\\n  \/\/ Set up Panoptes environment variables\\n  environment: {\\n    APP_PATH: process.env.APP_PATH,\\n    PANOPTES_AUTH_CLIENT_ID: process.env.PANOPTES_AUTH_CLIENT_ID,\\n    PANOPTES_AUTH_CLIENT_SECRET: process.env.PANOPTES_AUTH_CLIENT_SECRET\\n  },\\n};\\n```\\n\\nI'll go ahead and `npm install` panoptes-javascript-client. I'll then run npm install again, but this time with a different flag - `--force`. This will install the package into the project root instead of in my `package.json` file, and should update it with any changes made in the config file.\\n\\nNow I'm ready to configure panoptes-javascript-client. In `config.js`, I'll add these lines:\\n\\n```\\nmodule.exports = {\\n  \/\/ Set up Panoptes variables\\n  apiKey: 'your_api_key',\\n  clientId: 'your_client_id',\\n  secret: 'your_client_secret',\\n  \/\/ Set up Panoptes environment variables\\n  environment: {\\n    APP_PATH: process.env.APP_PATH,\\n    PANOPTES_AUTH_CLIENT_ID: process.env.PANOPTES_AUTH_CLIENT_ID,\\n    PANOPTES_AUTH_CLIENT_SECRET: process.env.PANOPTES_AUTH_CLIENT_SECRET\\n  },\\n};\\n```\\n\\nThis will generate a new config file at `node_modules\/.bin\/panoptes.js`. You can now run npm run dev to deploy your app.\\n\\nI've also added an entry for panoptes-javascript-client in my package.json file, so that it's available for use by other apps.\\nNow let's see how it works! \\n\\nWhen you visit the front-end app, you'll see a login form with some fields:\\n```\\n<form action=\"\/auth\/login\" method=\"post\">\\n<p>Username:<\/p>\\n<input type=\"text\" name=\"username\" value=\"\" \/>\\n<p>Password:<\/p>\\n<input type=\"password\" name=\"password\" value=\"\" \/>\\n<p>Remember me:<\/p>\\n<input type=\"checkbox\" name=\"remember_me\" value=\"true\"\/>\\n<p>You must click \"Remember me!\" before you can login.<\/p>\\n<\/form>\\n\\nAnd when you click \"Login\", you'll be redirected back to your app:\\n```\\n<form action=\"\/auth\/login\" method=\"post\">\\n<p>You must click \"Login\" before you can login.<\/p>\\n<\/form>\\n\\nFinally, when you're logged in, you'll see that your app has logged you out of the browser:\\n```\\n<form action=\"\/auth\/logout\" method=\"post\">\\n<p>You must click \"Logout\" before your browser is logged out.<\/p>\\n<\/form>\\n\\nYou can also check if anyone else is logged in from another browser by visiting their app:\\n```\\n\\n\\n\\nYou should see something like this on your front-end when someone logs in:\\n```\\n\\n\\n\\nWhat's most interesting about this is that we can now use env vars on our front-end apps without having them exposed via environments. It's quite simple - just pass them through to their respective APIs (in our case \/auth\/login or \/auth\/logout). This means that we don't need to worry about setting them up as environment variables on our server side apps. We just need to make sure we pass them through correctly when calling APIs from our front-end apps.\\nThe next step is to make sure that panoptes-javascript-client doesn't break when Next.js updates its API docs. To do this, I've created a new branch on GitHub called \\href{https:\/\/github.com\/zooniverse\/front-end-monorepo}{front-end-monorepo}. This branch contains all of my changes related to Next.js and how we handle authentication.\\nThe first thing I wanted to do was update the client API documentation so that it reflects what Next.js does now. So I started by updating README.md and adding a few examples of what Next.js does with authentication.\\nThe second thing I wanted to do was update panoptes.js itself so that it correctly handles Next.js' new authentication methods. So again, I started by updating README.md and adding an example of how Next.js handles authentication. Then I changed panoptes.js so that it uses env vars correctly for authentication.\\nThe third thing I wanted to do was update README.md so that it matches what Next.js does now. So again, I started by updating README.md and adding an example of how Next.js handles authenication.\\nThe fourth thing I wanted was for panoptes.js itself so that it properly handles authenication from outside sources (such as servers). To do this, I updated README.md again and added an example of how we handle authenication from external sources (such as servers).\\nThe last thing I wanted was for panoptes.js itself so that it properly handles authenication from backend sources (such as databases). To do this, again updated README.md and added an example of how we handle autheniation from backend sources (such as databases).\\nThe final step was for me to create a PR which merged all these changes into master\\footnote{This branch has been merged into master on Github}.\\npublished PR #1306 - https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1306"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"Predictions":"## - The annotation is updated by the `MarkingsRenderer` and `FrameAnnotator`. Both components are responsible for updating the annotation. The annotation is updated by both components in the same way (i.e. the `AnnotationRenderer` calls a non-static method on the `Annotation` class and vice versa). The annotation is updated by both components in the same way (i.e. the `AnnotationRenderer` calls a non-static method on the `Annotation` class and vice versa).\\n## - The annotation is not updated by either component.\\n## - The annotation is not updated by either component.\\n## - The annotation is not updated by either component.\\n## - The annotation is not updated by either component.\\n## - The AnnotationRenderer calls a non-static method on the Annotation class and vice versa. Annotation updates are performed in two ways: (1) updating annotations of all components that are currently viewing or (2) updating annotations of all components that are currently viewing or (3) updating annotations of all components that are currently viewing or (4) updating annotations of all components that are currently viewing or (5) updating annotations of all components that are currently viewing or (6) updating annotations of all components that are currently viewing or (7) updating annotations of all components that are currently viewing or\\n## - The AnnotationRenderer calls a non-static method on the Annotation class and vice versa. Annotation updates are performed in two ways: (1) updating annotations of all compontents that are currently viewing or (2) updating annotations of all compontents that are currently viewing or (3) updating annotations of all compontents that are currently viewing or (4) updating annotations of all compontents that are currently viewing or\\n## - Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updated at any time, including during runtime, without changing any other code.\\n\\n- Annotations can be updat"}
{"File Name":"front-end-monorepo\/adr-33.md","Context":"## Context\\nWe plan to implement simple feedback for the survey task. Simple survey task feedback will compare target to actual choice(s), excluding any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation.\\nCurrently in PFE, the only feedback related to the survey task is for the Gravity Spy project, and is referred to as \"Gravity Spy Gold Standard\". Gravity Spy Gold Standard is a PFE experimental feature (per workflow configuration object) that checks a subject's metadata for `'#Type' === 'Gold'` and provides feedback after classification completion with the [`gs-gold-standard-summary`](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/gs-gold-standard-summary.jsx). Survey task annotation values are compared to subject metadata `'#Label'` values. A message is then shown accordingly (i.e. success, failure, or special message per subject metadata `'#post_classification_feedback'`).\\n","Decision":"To implement a simple survey task feedback strategy within [the existing feedback framework](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/store\/feedback\/strategies). The simple survey task feedback will be titled \"Survey: Simple\". The simple survey task feedback will presume the annotation has one annotation value per survey task choice. The subject metadata will include:\\n- #feedback_N_id (required) - ID of the corresponding workflow task rule.\\n- #feedback_N_choiceIds (required) - comma separated target choice ID(s) (i.e. \"BLIP\" or \"AARDVARK,ELEPHANT,ZEBRA\"). A choice's ID is determined at survey task creation and can be viewed in in the Project Builder from the relevant workflow and survey task editor *Raw task data* section or from the workflow data export. The choice ID is the key for the choice object. For survey tasks created since 2017, the choice ID is the choice name per the provided \"Choices\" CSV with non-word characters removed and all uppercase (`[choice name per CSV].replace(\/\\W\/g, '').toUpperCase()`).\\n- #feedback_N_successMessage (optional) - message to show when the target is correctly annotated. Overrides the default success message set on the workflow task rule.\\n- #feedback_N_failureMessage (optional) - message to show when the target is incorrectly annotated. Overrides the default failure message set on the workflow task rule.\\nThe target choice ID(s) provided must match the choice IDs in the annotation values exactly for the annotation to be considered a success. Any missing target choice or any additional annotation choice will result in failure.\\nFeedback for annotation value (choice specific question) answers and multiple annotation values of the same choice will be addressed with additional survey task feedback strategies.\\n","tokens":203,"id":529,"Predictions":"The decision is to implement simple survey task feedback for the survey task. The decision is based on the following information:\\n\\begin{itemize}\\n\\item The user has provided a number of user metadata values that are used to assess the subject's performance in the survey task. These values are shown on the ``Survey Tasks'' page, as well as on ``Survey Results'' and ``Feedback Results'' pages.\\n\\item The user has provided a number of subject metadata values that are used to assess the subject's performance in the survey task. These values are shown on the ``Survey Tasks'' page, as well as on ``Survey Results'' and ``Feedback Results'' pages.\\n\\end{itemize}\\n\\end{document}"}
{"File Name":"front-end-monorepo\/adr-06.md","Context":"## Context\\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n","Decision":"We have a few proposed options:\\n- A `lib-locales` library that has all of the locale files for the strings. Could be a single file or a directory of folders and files. The library could be imported into the other libs or apps where needed.\\n- Larger locale files quickly get unwieldy to edit, and hard to find errors in. Moving them into one place reduces the portability of components. We could write a couple of scripts to bundle up locale files for our translators, and split them out again afterwards [back into their component folders in each library].\\n","tokens":188,"id":531,"Predictions":"## Decision\\nThe decision is to have the locale files in the same folder as the component's source code. This will allow volunteers to maintain localized versions of components without having to maintain separate files for each language.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n\\n## Context\\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n\\n## Decision\\n## Decision\\nThe decision is not to have separate localization files, but instead use a single file with all of our localization information.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n\\n## Context\\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n\\n\\end{itemize}\\n\\n\\section{ZooKeeper}\\n\\n\\subsection{What is ZooKeeper?}\\nZookeeper is an open-source distributed system designed by Facebook (Facebook \\url{http:\/\/www.facebook.com}). It uses distributed consensus algorithms such as Paxos \\cite{paxos} and Fair Binary Clocks \\cite{fairbinaryclocks}.\\n\\nThe main goal of ZooKeeper is providing distributed storage services across multiple machines.\\n\\n\\subsection{How does ZooKeeper work?}\\nZookeeper uses Paxos \\cite{paxos} or Fair Binary Clocks \\cite{fairbinaryclocks} consensus algorithms as described by \\cite{paxos}. \\n\\nWhen ZooKeeper starts up, it creates two Paxos processes (one master process and one slave process). Each slave process stores its own copy of ZooKeeper's data structure (a set of objects called \"ZooKeeperLists\") which contains all objects belonging together.\\n\\nAfter ZooKeeper's first startup, each worker process stores its own copy of ZooKeeper's data structure (a set of objects called \"ZooKeeperLists\") which contains all objects belonging together.\\n\\nWhen a master or worker dies or is restarted, all ZooKeeperLists are updated with their current contents.\\n\\nWhen one master or worker dies or restarts, all ZooKeeperLists are updated with their current contents.\\n\\n\\subsection{What happens when there are no more slaves?}\\nIf there are no more slaves after one restart or one death then there are no longer any active masters or workers.\\n\\n\\n\\subsection{What happens when there are no more workers?}\\nIf there are no more workers after one restart then there are no longer any active masters.\\n\\n\\n\\subsection{What happens when there are no more masters?}\\nIf there are no more masters then there are no longer any active masters.\\n\\n\\n\\subsection{What happens when there is only one master?}\\nIf there is only one master then everything stops working.\\n\\n\\n\\subsection{What happens when there is only one slave?}\\nIf there is only one slave then everything stops working.\\n\\n\\n\\subsection{Troubleshooting: How does ZooKeeper work?}\\nA zookeeper instance can be restarted by running `zkServer.sh` with `-d`. A zookeeper instance can also be restarted by running `zkServer.sh` with `--stop`. If you want your zookeeper instance stopped gracefully you can run `zkServer.sh --stop`. However if you want your zookeeper instance stopped forcefully you can run `zkServer.sh --force`.\\n\\nFor example:\\n```\\n$ zkServer.sh --stop\\n\\n$ zkServer.sh --force\\n\\n$ zkServer.sh --stop -d\\n```\\nThis will stop your zookeeper instances gracefully.\\n```\\n$ zkServer.sh --stop -d\\n\\n$ zkServer.sh --force\\n\\n$ zkServer.sh --stop -d -Djava.rmi.server.hostname=localhost\\n```\\n\\nYou can also start up multiple zookeepers using `--master`. For example:\\n```\\n$ zkServer1.sh --master 10.0.0.4:2181 -Djava.rmi.server.hostname=localhost\\n$ zkServer2.sh --master 10.0.0.4:2182 -Djava.rmi.server.hostname=localhost\\n\\n$ .\/zkserver1 &>\/dev\/null\\n\\n$ .\/zkserver2 &>\/dev\/null\\n\\n# Restarting zookeeper 2 after stopping it using '--stop'\\n# The following command will stop both servers.\\n# $ .\/zkserver1 &>\/dev\/null $ .\/zkserver2 &>\/dev\/null \\n# Restarting both servers after stopping them.\\n# $ .\/zkserver1 &>\/dev\/null $ .\/zkserver2 &>\/dev\/null \\n\\n# When starting up a zookeeper instance using '--start', \\n# Do not specify an IP address.\\n```\\n\\n\\n\\section{\\texttt{\\textbackslash{}build}}\\nThis section describes how we build our codebase.\\nWe build our codebase using Maven 3.x which supports dependency management through Maven Central.\\nWe use Maven central because it provides us with version control through Git repos which we can use directly from within Eclipse IDE.\\nWe use Git because Git allows us easily share changes between multiple codebases which we do frequently.\\n\\nOur source tree contains four repositories:\\n\\n\\begin{singlespace}\\n  \\input{singlespace\/CodeTree.tex}\\end{singlespace}\\n\\nEach repository contains its own local copy of our codebase (the local copies contain only our local changes).\\n\\nEach repository also contains:\\n\\n\\begin{singlespace}\\n  \\input{singlespace\/CodeTree.xml}\\end{singlespace}\\n\\nA configuration file used by Maven Central which tells Maven where repositories should be looked at when we want version control updates from those repositories.\\n\\nA directory containing all our source code.\\n\\nA directory containing all our build scripts (\\texttt{\\textbackslash{}build}) used by Maven Central.\\nThese scripts compile our code into JAR archives which get pushed into Git Repositories via Git hooks (\\texttt{\\textbackslash{}git}). \\n\\nLastly we create a directory containing all configuration information (\\texttt{\\textbackslash{}config}) used by Maven Central as well as all configuration information used by build scripts (\\texttt{\\textbackslash{}build}). This directory also includes configuration information used by build scripts (\\texttt{\\textbackslash{}build}) itself.\\n\\nThe build script itself uses version control systems such as Git so we configure version control systems so they push changes from their respective repositories into Git Repositories via Git hooks (\\texttt{\\textbackslash{}git}). The configuration file uses version control systems such as Git so we configure version control systems so they push changes from their respective repositories into Git Repositories via git hooks (\\texttt{\\textbackslash{}git}).\\n\\nFinally we configure our Eclipse IDE so it uses version control systems such as Git so it pushes changes from its respective repositories into Git Repositories via git hooks (\\texttt{\\textbackslash{}git}).\\n\\nThe Eclipse IDE configures itself so it pushes changes from its respective repositories into git repositories via git hooks (\\texttt{\\textbackslash{}git}).\\nThis configures itself so it pushes changes from its respective repositories into git repositories via git hooks (\\texttt{\\textbackslash{}git}).\\n\\n\\n% TODO: Add section on how builds work\\n\\n\\n"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"Predictions":"\\nThe decision for the user is to select one of the subjects from the subject queue. The user can either click on a subject or search for it by name or ID. If the user clicks on a subject, they are taken to its details page and can choose to work on it by clicking on ``Work on this subject'' or ``Work on this subject in your own workspace''.\\n\\n\\n\\n## Decision Details\\nThe first part of the decision is determining whether to work on it in your own workspace or not.\\n\\n\\nIf the decision is ``Work on this subject in your own workspace'', then you will be taken back to Engaging Crowds, where you will be able to select a workspace and begin working.\\nThe second part of the decision is deciding which of your own workspaces you wish to work in. You can choose one of three options: \\n\\n\\n* Start working in your own workspace, and be taken back to Engaging Crowds.\\n* Start working in Engaging Crowds, and be taken back to your own workspace.\\n* Start working in both Engaging Crowds and your own workspace.\\nThe third part of the decision is deciding whether you wish for Engaging Crowds to take care of deleting any unused subjects from your workspaces.\\nIf you select ``Start working in Engaging Crowds'', then Engaging Crowds will go ahead and delete any unused subjects from all of your workspaces. If you select ``Start working in your own workspace'', then you will be asked if you wish Engaging Crowds to take care of deleting any unused subjects from all of your workspaces, or if you wish for them to delete any unused subjects from only those workspaces that contain subjects that have been classified. This choice has no effect unless both options are selected.\\nThe final section describes how our system handles subjects that have been classified but not yet assigned a classification label. If a volunteer has classified a subject but has not assigned it a label, we will keep the subject open until we have made an assignment. It will then be closed when we have made an assignment.\\n\\end{document}\\n"}
{"File Name":"tamr-client\/0009-separate-types-and-functions.md","Context":"## Context\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n","Decision":"Put types (`@dataclass(frozen=True)`) into the `_types` module\\nand have all function modules depend on the `_types` module to define their inputs and outputs.\\n","tokens":139,"id":533,"Predictions":"## Decision\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n\\n## Decision\\n## Decision\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n\\n## Decision\\n## Decision\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\nThe code presented in this document is an example of the approach taken. It may not reflect the final version of the code.\\n"}
{"File Name":"tamr-client\/0010-confirm-performance-issues-before-optimizing.md","Context":"## Context\\nThere are multiple, equally-effective ways to implement many features.  In some cases, the most\\nstraightforward implementation might involve making more API calls than are strictly necessary\\n(e.g. `tc.dataset.create` makes an additional call to retrieve the created dataset from the server\\nto construct the returned `Dataset`).\\n","Decision":"The simplest and most understandably-written implementation of a feature should be prioritized over\\nperformance or reducing the number of API calls.  When real performance issues are identified,\\noptimization should be done on an as-needed basis.\\n","tokens":68,"id":534,"Predictions":"## The decision is to make an additional call to retrieve the created dataset from the server to construct the returned dataset.  This is the most straight-forward implementation of this feature.\\n## Decision\\n## The decision is to make an additional call to retrieve the created dataset from the server to construct the returned dataset.  This is the most straight-forward implementation of this feature.\\n## Decision\\n## The decision is to make an additional call to retrieve the created dataset from the server to construct the returned dataset.  This is the most straight-forward implementation of this feature.\\n## Decision\\n## The decision is to make an additional call to retrieve the created dataset from the server to construct the returned dataset.  This is the most straight-forward implementation of this feature.\\n## Decision\\n## The decision is to make an additional call to retrieve the created dataset from the server to construct the returned dataset.  This is the most straight-forward implementation of this feature.\\n## Decision\\n## The decision is to make an additional call to retrieve and re-render a Dataset object.\\n\\nThis can be useful for certain features that are difficult or impossible\\nto implement as pure JS, such as custom UIs, animations, etc.\\n\\nThe basic idea behind re-rendering a Dataset object when it changes its state\\nis that we don't want our app's UIs or behaviors in general (e.g. animations) to\\nbe affected by changes in individual Dataset objects.\\n\\nThe way we do this with pure JS, however, involves making more API calls than necessary\\nto update each Dataset object's properties (e.g. `dataset.update()`).  In some cases, it might be necessary\\nto update all Dataset objects when they change their state.\\n\\nIn contrast, if we use PureJS (or any other data structure) for storing our data, then we can instead update each DataSet object's properties only when they change their state (e.g. by calling `dataset.update()`).\\n\\nWe can think of a PureJS Dataset as a special kind of data structure that allows us\\nto store data in a way that's more efficient than making API calls.\\n\\nWe'll see how this works in practice in Section~\\ref{sec:examples}.\\n\\n"}
{"File Name":"tamr-client\/0003-reproducibility.md","Context":"## Context\\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\\nFor this codebase, we will focus on consistent Python versions and dependency versions.\\n","Decision":"Manage multiple Python versions via [pyenv](https:\/\/github.com\/pyenv\/pyenv).\\nManage dependencies via [poetry](https:\/\/python-poetry.org\/).\\nDefine tests via [nox](https:\/\/nox.thea.codes\/en\/stable\/).\\nRun tests in automation\/CI via [Github Actions](https:\/\/github.com\/features\/actions).\\n","tokens":43,"id":535,"Predictions":"## \\begin{itemize}\\n##  \\item ``The current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.''\\n##  \\item ``The current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.'' is a valid answer if the current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.''\\n##  \\end{itemize}\\n\\end{Decision}\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\section{Issues}\\n\\label{sec:issues}\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\subsection{Issues}\\n\\label{sec:issues-issues}\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\subsubsection{Issue #1 - The current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.}\\n\\label{sec:issue-1-the-current-codebase-is-python-38-and-the-codebase-has-a-dependency-version-of-27}\\n\\nThe current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''. This results in inconsistent behavior when running tests against different versions of Python.\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\subsubsection{Issue #2 - The current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.}\\n\\label{sec:issue-2-the-current-codebase-is-python-38-and-the-codebase-has-a-dependency-version-of-27}\\n\\nThe current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''. This results in inconsistent behavior when running tests against different versions of Python.\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\subsubsection{Issue #3 - The current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''.}\\n\\label{sec:issue-3-the-current-codebase-is-python-38-and-the-codebase-has-a-dependency-version-of-27}\\n\\nThe current codebase is Python 3.8, and the codebase has a dependency version of ``2.7''. This results in inconsistent behavior when running tests against different versions of Python.\\n\\n%---------------------------------------------------------------------\\n%-------------------------------------------------------------------------------\\n\\subsubsection*{}\\nThis issue may be resolved by using one or more of these solutions:\\n \\n1) Use one or more different versions to test against.\\n \\n2) Use multiple environments to test against.\\n \\n4) Use multiple tests to test against.\\n \\n5) Use one or more different environments to test against.\\n\\n"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"Predictions":"##\\n\\n\\n\\n\\n\\n\\n\\n\\n\\begin{lstlisting}\\ndef get_static_type(self, context, type):\\n  # Get the static type of the object from the context\\n  return type\\n\\nif __name__ == '__main__':\\n  # Create a new instance of \\n\\n  # a \\n\\n  # object and pass it to the function \\n\\n  # that will be used for type checking.\\n  context = {'a': 'b', 'c': 'd'}\\n  # Create an instance of \\n\\n  # a \\n\\n  # object and pass it to the function \\n\\n  # that will be used for type checking.\\n  self.a = getattr(context, 'a')\\n  self.b = getattr(context, 'b')\\n\\end{lstlisting}\\n\\end{document}\\n"}
{"File Name":"tamr-client\/0008-standardized-imports.md","Context":"## Context\\nPython has many ways of importing:\\n```python\\n# option 1: import module\\n# option 1.a\\nimport foo.bar.bazaar as baz\\nbaz.do_the_thing()\\n# option 1.b\\nfrom foo.bar import bazaar as baz\\nbaz.do_the_thing()\\n# option 2: import value\\nfrom foo.bar.bazaar import do_the_thing\\ndo_the_thing()\\n```\\nNot to mention that each of these styles may be done with relative imports (replacing `foo.bar` with `.bar` if the `bar` package is a sibling).\\nConfusingly, Option 1.a and Option 1.b are _conceptually_ the same, but mechanically there are [subtle differences](https:\/\/stackoverflow.com\/questions\/24807434\/imports-in-init-py-and-import-as-statement\/24968941#24968941).\\n","Decision":"Imports within `tamr_client`:\\n- Must import statements for modules, classes, and exceptions\\n- Must `from foo import bar` instead of `import foo.bar as bar`\\n- Must not import functions directly. Instead import the containing module and use `module.function(...)`\\n- Must not use relative imports. Use absolute imports instead.\\n","tokens":189,"id":537,"Predictions":"```\\nimport foo.bar as baz\\nbaz.do_the_thing()\\n```\\n\\end{itemize}\\n"}
{"File Name":"tamr-client\/0005-composable-functions.md","Context":"## Context\\nWe need a reasonable tradeoff between ease-of-use and maintainability.\\nSpecifically, we need composable, combinable units that can be improved independently.\\n### Approach 1: Classes + Methods\\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\\n```python\\nproject\\n.create(...)\\n.update(...)\\n.delete(...)\\n```\\nCharacteristics:\\n- Ease-of-use is maximized, but this requires each method to `return self`.\\n- Also, this approach implies that if a function can be called with X different object types,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\nHow to enforce these characteristics?\\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\\n### Approach 2: Types + Functions\\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\\nUsage is not as terse as for OOP:\\n```python\\np = tc.project.create(...)\\nu = tc.project.update(p, ...)\\nd = tc.project.delete(p, ...)\\n```\\nCharacteristics:\\n- Ease-of-use is not optimized, but still reasonable.\\n- With tab-completion, ease-of-use is comparable to OOP.\\n- Each type can be made immutable\\n- Each function can be made pure\\n- Functionality can be shared by calling the same function in user-land, not copying function calls in contributor-land.\\n","Decision":"Use `@dataclass(frozen=True)` to model types and plain Python modules and functions to capture business logic.\\n","tokens":356,"id":538,"Predictions":"## Decision\\nWe should make a decision to either use Classes + Methods, or Types + Functions.\\nIf we use Classes + Methods, we should make sure that each type is immutable, and the methods are pure.\\nIf we use Types + Functions, we should make sure that each function is pure and can be called in user-land.\\n## Context\\nWe need a reasonable tradeoff between ease-of-use and maintainability.\\nSpecifically, we need composable, combinable units that can be improved independently.\\n### Approach 1: Classes + Methods\\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\\n```python\\nproject\\n.create(...)\\nu = tc.project.create(...)\\nd = tc.project.update(p, ...)\\nd = tc.project.delete(p, ...)\\nd = tc.project.fetch(...)```\\nCharacteristics:\\n- Ease-of-use is maximized, but this requires each method to `return self`.\\n- Also, this approach implies that if a function can be called with X different object types,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\nHow to enforce these characteristics?\\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\\n### Approach 2: Types + Functions\\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\\nUsage is not as terse as for OOP:\\n```python\\np = tc.project.create(...)\\nu = tc.project.update(p, ...)\\nd = tc.project.delete(p, ...)\\nd = tc.project.fetch(...)```\\nCharacteristics:\\n- Ease-of-use is not optimized, but still reasonable.\\n- With tab-completion, ease-of-use is comparable to OOP.\\n- Each type can be made immutable,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\n- Functionality can be shared by calling the same function in user-land,\\n"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"Predictions":"## The architecture is built around a `libtamr_unify_client` library and a `tamr_unify_client` client library. The clients are used to communicate with tamr and the unifier. The unifier is configured by the client libraries. The client libraries use the unifier to execute tasks such as `unify`, `unify_with`, `unify_with_all`, etc.\\n## Decision Results\\n## We have decided that we need a new architecture that separates the client libraries from the unifier, and that we will be using `libtamr_unify_client` for our client libraries.\\n## Decision Details\\n## We have decided to go with an architecture that separates the client libraries from the unifier, and that we will be using libtamr_unify_client for our client libraries.\\n## Context\\nThe userbase relies on tamr_unify_client and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a library of composable functions.\\n\\n## Decision\\n## The architecture is built around a libtamr_unify_client library and a tamr_unify_client client library. The clients are used to communicate with tamr and the unifier. The unifier is configured by the client libraries. The client libraries use libtamr_unify_client for their functions.\\n## Decision Results\\n## We have decided that we need a new architecture that separates the client libraries from the unifier, and that we will be using libtamr_unify_client for our client libraries.\\n## Decision Details\\n## We have decided to go with an architecture that separates the client libraries from the unifier, and that we will be using libtamr_unify_client for our client libraries.\\n"}
{"File Name":"tamr-client\/0004-documentation-and-docstrings.md","Context":"## Context\\nDocumentation can take four forms:\\n1. Explanation\\n2. Tutorial\\n3. How-to\\n4. Reference\\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\\n","Decision":"Doc compilation will be done via [sphinx](https:\/\/www.sphinx-doc.org\/en\/master\/).\\nProsey documentation (1-3) via [recommonmark](https:\/\/github.com\/readthedocs\/recommonmark).\\nReference documentation (4) will be generated based on type annotations and docstrings via:\\n- Automatic docs based on docstrings via [sphinx-autodoc](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/autodoc.html), [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\n- Google-style docstrings via [napoleon](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/napoleon.html)\\n- Hosting on [ReadTheDocs](https:\/\/readthedocs.org\/) (RTD)\\n- Build docs in CI and fail on errors or warnings.\\n","tokens":48,"id":540,"Predictions":"## Decision\\n1. Explain\\n2. Tutorial\\n3. How-to\\n4. Reference\\n\\n## Context\\n## Context\\nThe context for this decision is the following:\\n1. Software.\\n2. Architectural Decision Record.\\n3. Software Architecture.\\n4. Architecture.\\n5. Software Development Process.\\n6. Architecture Design Process.\\n7. Software Development Process.\\n8. Architecture Design Process.\\n9. Software Design Process.\\n10. Software Development Process.\\n11. Architecture Design Process.\\n## Decision\\n## Decision\\n1: Explain\\n2: Tutorial\\n3: How-to\\n4: Reference\\n5: Explain\\tnote{Explanation} is generally used by the user to describe why a particular piece of software architecture is appropriate for a particular problem domain and application domain, and can be used as a starting point for discussion about the design of that particular piece of software architecture.\\n## Context\\n## Context\\nThe context for this decision is the following:\\n1: Softwar"}
{"File Name":"tamr-client\/0002-linting-and-formatting.md","Context":"## Context\\nInconsistent code formatting slows down development and the review process.\\nCode should be linted for things like:\\n- unused imports and variables\\n- consistent import order\\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\\n","Decision":"For linting, use [flake8](https:\/\/flake8.pycqa.org\/en\/latest\/) and [flake8-import-order](https:\/\/github.com\/PyCQA\/flake8-import-order).\\nFor formatting, use [black](https:\/\/github.com\/psf\/black).\\n","tokens":55,"id":541,"Predictions":"## The user has requested that the software should be linted for code formatting.\\n## The developer has agreed to implement the linting.\\n## The developer has requested that the code formatting should be automated by a program or other mechanism.\\n## The developer has requested that the code formatting should be done manually by a program or other mechanism.\\n## The developer has requested that the code formatting should be done automatically by a program or other mechanism.\\n\\n\\end{lstlisting}\\n\\n\\section*{Software Versioning}\\n\\label{sec:software-versioning}\\nSoftware versioning is an important aspect of software engineering, and is used to track changes made to source code. This section describes how to use Git to version your source code.\\nGit is a distributed version control system (VCS). This means that you can store your source code in multiple locations, and access it from any location. Git allows you to easily work on your software in any location, and synchronize changes made in different locations back to a central repository. Git can also be used as a distributed revision control system, where multiple developers work on the same branch of the repository.\\n\\nGit uses version numbers to identify each revision. Each revision is identified using a commit hash. A commit hash is created when you make changes to the source code. A commit hash can then be used as part of a git log command, which provides information about all commits made since the last log command was issued.\\n\\nThe git log command prints out information about all commits made since the last log command was issued, including:\\n\\n\\begin{itemize}\\n    \\item \\textbf{Author}: who made the commit\\n    \\item \\textbf{Name}: what was changed\\n    \\item \\textbf{Date}: when it was created\\n    \\item \\textbf{Size}: size of file change\\n    \\item \\textbf{Commit message}: message describing what was changed\\n\\end{itemize}\\n\\nThe git log command can also include information about previous commits:\\n\\n\\begin{lstlisting}\\n$ git log --oneline --decorate --graph --all\\ncommit 9e2d8b6c5c1a9f7d0a4b7e3aa1b6f4aabcd8d3d9 (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    Added some more tests\\n\\ncommit 3c8a2e00e1a5a5c4e7f075cc1fd194b29f0abf5d (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    More tests now work\\n\\ncommit 2b4c7ecf2af3f99bc95a45ebe3ad7cb97b8db19c (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    More tests now work\\n\\ncommit eae54ed5a9fe0c9d95efbbce37e99ae887cfadde (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    Added some more tests\\n\\ncommit fefaaee4cc8e8fb8ec76ac22fd53e9ee17ab22fa (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    More tests now work\\n\\ncommit c40ce97f2ea96a28150dc1621fcda82a155ba75d (HEAD -> master)\\nAuthor: <name>\\nDate:   Thu Dec 29 16:50:09 2016 +0000\\n\\n    Added some more tests\\n\\end{lstlisting}\\n\\nIt's important to note that commits are not just numbered versions; they are also named versions. Git stores each commit with its name and date as part of its revision history. This means that when you issue git log, it will list all commits starting with their name followed by their date.\\n\\nYou can also view information about previous revisions using git rev-list:\\n\\n\\begin{lstlisting}\\n$ git rev-list -r HEAD~30\\nrevisions:\\n     eae54ed5a9fe0c9d95efbbce37e99ae887cfadde <-- HEAD^30\\n     c40ce97f2ea96a28150dc1621fcda82a155ba75d <-- HEAD^31\\n     fefaaee4cc8e8fb8ec76ac22fd53e9ee17ab22fa <-- HEAD^32\\n     b4c7ecf2af3f99bc95a45ebe3ad7cb97b8db19c <-- HEAD^33\\n     d44bdfe78ff94066301bd05dd02db73704dd69df <-- HEAD^34\\n     e72df67ed81ae21eb39cc821712bc61fac44eb46 <-- HEAD^35\\n     aa2d6de77da53312aa15ab608c97be07afbd70fb <-- HEAD^36\\n     dce51ca74f62fa84588ea63b91bb68b799fc70ca <-- HEAD^37\\n     f27fb00eb4ff24ceb57cd86cb87da60be05dc87b <-- HEAD^38\\n     d51cd43cd39fc16103bfde20fb72fd38ebdf34cd <-- HEAD^39\\n       ... and so on...\\n\\end{lstlisting}\\n\\nYou can use this information in your own documentation or presentations.\\n\\nYou can also use git commands like:\\n\\ngit add -p [file]\\n\\nto add files for review.\\n\\nTo update your local copy of the repository:\\n\\ngit push origin [branch]\\n\\nThis will push your local copy of the repository onto another remote server named origin. You may need to specify which branch you want pushed by giving it an argument like `master' or `develop'.\\n\\nTo pull from another remote server:\\n\\ngit pull origin [branch]\\n\\nThis will pull your local copy of the repository onto another remote server named origin. You may need to specify which branch you want pulled by giving it an argument like `master' or `develop'.\\n\\nTo checkout from another remote server:\\n\\ngit checkout [branch]\\n\\nThis will checkout one or more files from another remote server onto your local computer. You may need to specify which branch you want checked out by giving it an argument like `master' or `develop'.\\n\\nTo checkout all of your local files into another directory:\\n\\ngit checkout * [branch]\\n\\nThis will checkout all files from one directory into another directory. You may need to specify which directory you want checked out by giving it an argument like `*' or `develop'.\\n\\nYou can also use Git commands like:\\n\\ngit reset [file]\\n\\nto remove files from your local copy of the repository.\\n\\nYou can also use Git commands like:\\n\\ngit status [file]\\n\\nto show what files are currently up-to-date in your local copy of the repository.\\n\\nYou can use Git commands like:\\n\\ngit diff [file]\\n\\nto show differences between two revisions in your local copy of the repository.\\n\\nYou can use Git commands like:\\n\\ngit diff --cached [file]\\ngit diff --cached --show-unified-views [file]\\ngit diff -u [file]\\ngit merge [file] [--no-ff] [--no-ffm] [--no-ffm] [--no-merge] [--no-merge] [--no-merge] [--no-merge] [--no-merge]\\n        [--no-merge] [--no-subtree] [--no-subtree] [--no-subtree]\\n        [--ignore-untracked-files | --ignored-files | --ignored-directories | --ignored-directories-dir | --ignored-directories-dir-dir | --ignored-paths | --ignored-paths-dir | --ignored-paths-dir-dir |\\n        ... other options...\\n\\nWhen using Git for version control, there are several different ways that you could do each task described above.\\n\\nOne way would be for each task described above would be performed separately, and then combined into one command using one parameter \u2014 such as `-p'. Another way would be for each task described above would be performed separately, but then combined into one command using two parameters \u2014 such as `-p', `-r', '--all', '--tags', '--tags-only', '--all-tags', '--tags-only-tags' and so on.\\n\\nFor example, if you wanted to check out a specific file called `foo.txt' into another directory called foo.txtdir, then you could do this with two commands taken together:\\n\\begin{lstlisting}\\n$ git checkout foo.txt $ git diff foo.txt >> foo.txtdir\/git\/foo.txt $ ls -l foo.txtdir\/git\/foo.txt $ cat foo.txt $ cat foo.txtdir\/git\/foo.txt $ ls -l foo.txtdir\/git\/foo.txt $ cat foo.txt $ ls -l foo.txtdir\/git\/foo.txt $ cat foo.txtdir\/git\/foo.txt $ ls -l foo.txtdir\/git\/foo.txtdir\/git\/foo.txtdir.git\/foo.txtdir.git\/foo.txtdir.git\/$ ls -l foo.txtdir\/git\/$ ls -l foo.txtdir\/git\/$ ls -l bar.*foo.txtdir\/bar.$ ls bar.*foo.txtdir\/bar.$$ touch bar.foo.txtdir\/bar.$$ touch bar.foo.txtdir\/bar.$$ touch bar.foo.txtdir\/bar.$$\\n\\end{lstlisting}\\n\\nThe first line shows what directories are being checked out; this includes any subdirectories underneath those directories.\\n\\nThe second line shows what files are being checked out; this includes everything inside those subdirectories.\\n\\nThe third line shows what revisions are being checked out; these include everything inside those subdirectories.\\n\\nThe fourth line shows what tags are being checked out; these include everything inside those subdirectories.\\n\\nThe fifth line shows what branches are being checked out; these include everything inside those subdirectories except for any branches named something else than 'master'.\\n\\nThe sixth line shows what tags-only branches exist; these include anything inside those subdirectories except for any branches named something else than 'master'.\\n\\nThe seventh line shows what tags-only branches exist; these include anything inside those subdirectories except for any branches named something else than 'master'.\\n\\nThe eighth line shows what all tags exist; these include anything inside those subdirectories except for any tags named something else than 'master'.\\n\\nFinally, finally there is an eighth line showing how many revisions have been checked out so far \u2014 this includes everything inside those subdirectories except for any branches named something else than 'master'. Note that if there were no revisions yet at this point then there would only be six lines here instead of eight because there were no revisions yet at this point.\\n"}
{"File Name":"tamr-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":542,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"Predictions":"## Decision: We have decided to use the Armadillo suite for our project.\\n## Context\\nIn this project we manage the data in the Armadillo suite.\\n## Decision\\n## Decision: We have decided to use the Armadillo suite for our project.\\n## Context\\nIn this project we manage the data in the Armadillo suite.\\n\\end{verbatim}\\n\\end{document}"}
{"File Name":"molgenis-r-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":546,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"molgenis-r-armadillo\/0003-use-s3-api.md","Context":"## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n","Decision":"The Armadillo client will be written as a high-level library on top of an existing S3 API.\\n","tokens":56,"id":547,"Predictions":"## * 1: Add a new module for uploading files to Minio. The file upload function will be a wrapper around the existing S3 API. The upload will be synchronous.\\n* 2: Add a new module for uploading files to Minio. The file upload function will be a wrapper around the existing S3 API. The upload will be asynchronous.\\n* 3: Add a new module for uploading files to Minio. The file upload function will be a wrapper around the existing S3 API and the S3 API will be used as the underlying protocol. The upload will be synchronous.\\n* 4: Add a new module for uploading files to Minio. The file upload function will be a wrapper around the existing S3 API and the S3 API and S3 API will be used as the underlying protocol. The upload will be asynchronous.\\n* 5: Add a new module for uploading files to Minio. A client library for using Minio with armadillo is already available that allows users to do most of what is needed, but there are still many things that need to be added or improved.\\n* 6: Add an armadillo server that can provide an API for doing all of the above.\\n\\n## Context\\n* Client libraries already exist that allow users to do most of what is needed, but there are still many things that need to be added or improved.\\n## Decision\\n## * 1: Add an armadillo server that can provide an API for doing all of the above.\\n## Decision\\n## * 1: Add an armadillo server.\\n## Decision\\n## * 1: Add an armadillo server.\\n## Decision\\n## * 1: Add an armadillo server.\\n## Decision\\n## * 1: Add an armadillo server.\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{The \\texttt{armadillo} Server}\\n\\begin{verbatim}\\ndef get_armadillo_server():\\n    \"\"\"Gets the armadillo server.\"\"\"\\n    return https:\/\/api.armadillo.dev\\n\\ndef get_armadillo_server_api():\\n    \"\"\"Gets the armadillo server's RESTful API.\"\"\"\\n    return https:\/\/api.armadillo.dev\/api\/v2\/\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{The \\texttt{armadillo} Client}\\n\\begin{verbatim}\\ndef get_armadillo_client():\\n    \"\"\"Gets the client library.\"\"\"\\n    return https:\/\/armadillodev.azurewebsites.net\/v2\/\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{The \\texttt{armadillo} Client}\\nThe \\texttt{\\_\\_init\\_\\_()} method should call \\texttt{\\_\\_call\\_armadillodev\\_init()} with its argument set to \\texttt{\\_\\_init\\_()}\\footnote{\\urlstyle{sf}\\urlstyle{sf}\\urlstyle{sffamily}\\urlstyle{sffamily}}.\\n\\nFor example,\\n\\begin{verbatim}\\nimport os\\n\\nclient = get_armadilloclient()\\nclient.get_armadilloclient_api()\\nclient.get_armadilloclient_init()\\n\\n# Create an instance of our client library.\\nclient = get_armadilloclient()\\n\\n# Create a directory and write some data into it.\\nclient.create_dir(\"hello world\", \"world\")\\nclient.write_dir(\"hello world\", \"world\")\\n\\n# Create some files in this directory:\\nclient.create_file(\"\/home\/user\/one\", \"hello world\")\\nclient.create_file(\"\/home\/user\/two\", \"hello world\")\\n```\\nwill create two directories and write two files inside them named ``hello world'' and ``world'' respectively.\\n\\nYou can also use \\texttt{\\_\\_call\\_armadillodev\\_init()} directly:\\n```\\nimport os\\n\\nclient = get_armadilloclient()\\n\\n# Create an instance of our client library.\\nclient = get_armadilloclient()\\n\\n# Create a directory and write some data into it.\\nclient.create_dir(\"hello world\", \"world\")\\nclient.write_dir(\"hello world\", \"world\")\\n\\n# Create some files in this directory:\\nfrom pathlib import Path\\n\\nfiles_path = Path(\".\/one\").to_string()\\nfiles_path = Path(\".\/two\").to_string()\\n\\n# Create some files in this directory:\\nfiles_path.write(\"hello world\")\\nfiles_path.write(\"world\")\\n```\\nwill create two directories and write two files inside them named ``hello world'' and ``world'' respectively.\\n\\nYou can also use \\texttt{\\_\\_call\\_armadillodev\\_init()} directly:\\n```\\nimport os\\n\\nimport shutil\\n\\nfrom pathlib import Path\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n\\nshutil.copytree(\".\/one\", \".\/two\")\\n```\\nwill create two directories and copy one file from one directory into another.\\n\\nYou can also use \\texttt{\\_\\_call\\_armidex()}\/\\texttt{\\_\\_call\\_armidexd()}\/etc.\\footnote{\\urlstyle{sffamily}\\urlstyle{sffamily}\\urlstyle{sffamily}} directly:\\n\\nFor example,\\n```\\nimport os\\n\\nos.chdir(\"..\") # change current working directory\\nos.mkdir(\"\/home\/user\/abc\") # create folder abc\\n\\n\\n# Copy all text from this file into abc.txt:\\nos.chdir(\"..\") # change current working directory\\nos.chdir(\"..\") # change current working directory\\nos.mkdir(\"\/home\/user\/abc.txt\") # create folder abc.txt\\n\\n\\n# Copy all text from this file into abc.txt (containing only one line):\\nos.chdir(\"..\") # change current working directory\\nos.chdir(\"..\") # change current working directory\\nos.mkdir(\"\/home\/user\/abc.txt\") # create folder abc.txt\\n\\n\\n# Copy all text from this file into abc.txt (containing multiple lines):\\nos.chdir(\"..\") # change current working directory\\nos.chdir(\"..\") # change current working directory\\nos.mkdir(\"\/home\/user\/abc.txt\") # create folder abc.txt\\n\\n\\nprint(os.getcwd())\\nprint(os.getcwd())\\nprint(os.getcwd())\\nprint(os.getcwd())\\n\\nprint(os.getcwd())\\n\\nif __name__ == \"__main__\":\\n    print(os.getcwd())\\n    print(os.getcwd())\\n    print(os.getcwd())\\n    print(os.getcwd())\\n\\nif __name__ == \"__main__\":\\n    print(os.getcwd())\\n    print(os.getcwd())\\n    print(os.getcwd())\\n\\nif __name__ == \"__main__\":\\n    print(os.getcwd())\\n    print(os.getcwd())\\n\\nif __name__ == \"__main__\":\\n     os.chdir(\"..\")\\n     os.mkdir(\"\/home\/user\/abc\")\\n     os.mkdir(\"\/home\/user\/xyz\")\\n     os.chdir(\"..\")\\n     os.mkdir(\"\/home\/user\/baz\")\\n\\n\\nprint(\"ABC\\nXYZ\\nBaz\\n\"\\n\\nif __name__ == \"__main__\":\\n     os.chdir(\"..\")\\n     os.mkdir(\"\/home\/user\/baz\")\\n\\n\\nprint(\"ABC\\nXYZ\\nBaz\\n\"\\n\\nif __name__ == \"__main__\":\\n     if __name__ == \"__main__\":\\n         shutil.rmtree(\"..\/helloworld\/helloworld\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/world\/\")\\n         shutil.rmtree(\"..\/helloworld\/hello_world\/world\/world\/world\/\")\\n```\\n\\n"}
{"File Name":"molgenis-r-armadillo\/0004-build-client-in-R.md","Context":"## Context\\n### Data management\\nData managers (in LifeCycle) use R to upload their data into Opal as this stage. Data managers often create scripts to generate the (harmonised) data they need to expose for a certain collection of variables.\\n### Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\\n### Possible solutions\\nPossible solutions regarding buildframeworks are R, Python or Java. Every framework has pros and cons.\\n**R**\\nTo connect to the current way of working with the data in LifeCycle, R is the most integrated platform to use. You can use the Armadillo client to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate the Armadillo client in the workflow. RData is the file format Armadillo uses and R is the platform to build .RData in which makes it easier to build the RData file in. When there is a need to do basic checks in the future, you can implement them.\\n**Python**\\nPython can be used together in one script with the molgenis-commander. Which is usefull when you use the MOLGENIS data provider. It allowes you to automate the whole process, from extracting data from MOLGENIS to converting into the right format for the Armadillo service (if Python supports generating .RData). From there you can upload it into the Armadillo. Another advantage is that Python is a language we are more experienced to program in.\\nA disadvantage is that at this point prospect customers are not used to work with python. You will need to teach them how to use Python and then how to use the client.\\n","Decision":"The Armadillo client will be written based on R to integrate easily in the current prospect customers infrastructure. We will look at the possibility of writing other clients in the near future when we need to add other customers as well.\\n","tokens":386,"id":548,"Predictions":"## Decision\\n### Data management\\nThe decision is to use R for the RData file. You will need to train your users on how to use R and what the advantages are of using R over Python. You will also need to train your users on how to use the Armadillo client and how to upload .RData files into the Armadillo service.\\n### Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\\n### Possible solutions\\nPossible solutions regarding buildframeworks are Python or Java. Every framework has pros and cons.\\n**Python**\\nTo connect to the current way of working with the data in LifeCycle, Python is the most integrated platform to use. You can use the MolgenisCommander client in order to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate Python in the workflow. The file format Molgenis uses is .RData and you can upload it into the Armadillo service.\\n**Java**\\nJava is a language we are more experienced with programming in. It allows for easy integration and deployment of new functionality, but it is not as familiar with our users as we would like it to be. We would like users who are familiar with Java, but not as familiar with R, be able to use both at once.\\n\\n## Decision\\n## Decision\\n### Data management\\nThe decision is that we will use R for uploading data into Armadillo. We will train our users on how they can upload data into Armadillo using R, but they will also have access via Java so that they can upload data from Java, if desired.\\n### Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service so that they can be used in DataSHIELD. So .RData files need to be created before it can be pushed into LifeCycle.\\n### Possible solutions\\nPossible solutions regarding buildframeworks are Java or Python. Every framework has pros and cons.\\n**Java**\\nTo connect with MolgenisCommander using Java we have already established an easy integration between MolgenisCommander and Java (see below). While we have been able to make this work well enough for us, there may be some issues when working with other platforms such as C or C++. We have also been looking at using a different way of uploading our data from Java into LifeCycle through an API (see below).\\n**Python**\\nThe most suitable solution for us right now would probably be a combination of both languages (Java and Python). Both languages provide us with easy access from our platform (Java) but also allow us access from our platform (Python) if desired.\\n## Decision\\n## Decision\\n### Data management\\nThe decision is that we will use Java for uploading data into Armadillo.\\n## Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service so that they can be used in DataSHIELD.\\n## Possible solutions\r\n\\n## Possible solutions regarding buildframeworks\r\n\\n## **Java**\\n"}
{"File Name":"gladiator_v2\/12-9-2017_player_interface.md","Context":"## Context:\\nNeed to create a way for the player to interact with their character and potential with their allies such that there is choice involved in combat on the player side instead of randomly decided actions.\\n","Decision":"Create a child class of character to give branching functionality to player-specific characters.\\n","tokens":41,"id":549,"Predictions":"## Decision: \\n\\#1: Create a combat system that allows the player to choose their attacks and possible allies. This would allow for more tactical and strategic choices to be made.\\n\\#2: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#3: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#4: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#5: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#6: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#7: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#8: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\#9: Create a system that allows the player to choose their allies and possible attacks. This would allow for more tactical and strategic choices to be made.\\n\\n\\end{DoxyItemize}\\n\\n"}
{"File Name":"gladiator_v2\/12-12-2017_castable_spells.md","Context":"## Context:\\nWant to give more variation to the combat system beyond physical attacks. This will allow for reasons to create separate builds and more options for player characters and enemies to use during a fight. This will give more of a *game* element to the combat as opposite to a system based on luck alone.\\n","Decision":"Create spells and integrate them into the character and combat systems.\\n","tokens":64,"id":550,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe decision is accepted, and the record is saved to the database.\\n\\nThe user can edit the decision in their account.\\n\\nThe user can view the decision in their account.\\n\\nThe user can edit the decision in their account.\\n\\nThe user can view the decision in their account.\\n\\nThe user can view the decision in their account."}
{"File Name":"gladiator_v2\/12-6-2017_combat_class.md","Context":"## Context:\\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\\n","Decision":"There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\\n","tokens":58,"id":551,"Predictions":"## 1. A character can be a fighter, a magic user, or a wizard. The Fighter character can only fight with weapons and the Wizard character can only use magic. A wizard cannot fight with weapons and a fighter cannot use magic.\\n## 2. A wizard may not fight with weapons and a fighter may not use magic.\\n## 3. A wizard may fight with two weapons at once if they are of the same type (for example, if they are both swords).\\n## 4. A character may be a single weapon type (for example, if they are both swords) or have two different weapon types (for example, if they are both spears).\\n## 5. A character may have more than one weapon type (for example, if they are both spears).\\n## 6. If a wizard is fighting with two weapons at once, the second weapon must be one of the same type as the first.\\n## 7. If a character is fighting with two different types of weapons at once, the second weapon must be one of each type.\\n## 8. If a character is fighting with more than two different types of weapons at once, the second weapon must be one of each type.\\n## 9. If there is no space left in between two characters' names for more than one weapon type to fit in their names, the additional weapon will not appear in their name.\\n## Decision\\n"}
{"File Name":"gladiator_v2\/12-12-2017_character_creation.md","Context":"## Context:\\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\\n","Decision":"Create a player module with user interaction to affect the starting statistics of a character.\\n","tokens":61,"id":552,"Predictions":"## Decision:\\n1. Should the game be able to generate a party of characters at random or should there be a set number of characters that can be generated for the game?\\n\\n1.1 If the game is allowed to generate characters at random, then the number of characters in the party can be decided on by the player instead of having a predetermined number. The player can decide how many characters they want to have in their party and the game will generate those numbers.\\n\\n1.2 If we allow players to decide how many characters they want to have in their party, then we need a way for them to leave their current party and join another one. The number of characters in their current party can be decided on by them.\\n\\n2. Should there be any restrictions on what kind of character they can create?\\n\\n2.1 Yes, we should limit the types of characters that players are allowed to create so that they do not become overpowered or underpowered.\\n\\n2.2 No, we should not limit the types of characters that players are allowed to create so that they do not become overpowered or underpowered.\\n\\n3. Should there be any restrictions on what kind of skills they have?\\n\\n3.1 Yes, we should limit how many skills a character has so that players are not forced into having all 20 skills and cannot take certain ones because they would make their character too powerful.\\n\\n3.2 No, we should not limit how many skills a character has so that players are not forced into having all 20 skills and cannot take certain ones because they would make their character too powerful.\\n\\n4. Should there be any restrictions on what kind of equipment each character has?\\n\\n4.1 Yes, we should limit how much equipment each character has so that it is balanced against other characters with similar equipment levels.\\n\\n## Constraints\\n## Constraints:\\nThe constraint for this decision is: \\# Characters can only ever exist as part of one party.\\n## Constraints:\\nThe constraint for this decision is: \\# There must only ever exist a unique combination of skill levels within each party.\\n## Constraints:\\nThe constraint for this decision is: \\# Each party member must have at least one skill level.\\n## Constraints:\\nThe constraint for this decision is: \\# Each party member must have at least one skill level.\\n## Constraints:\\nThe constraint for this decision is: \\# There must only ever exist a unique combination of skill levels within each party.\\n"}
{"File Name":"gladiator_v2\/12-12-2017_useable_items.md","Context":"## Context:\\nWhile we have a class for items already, we need a way for them to be used during combat. This way we can add in more items that will change the flow of battle and eventually a money and shop system to add to combat variation.\\n","Decision":"Create a way to use items and integrate into the combat flow.\\n","tokens":54,"id":553,"Predictions":"## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nItem creation is allowed, but it must be done by the player in combat.\\n\\n## Decision:\\nThe Player can create items during battle!\\nThe Player can create items during battle! The Player can create items during battle! The Player can create items during battle! The Player can create items during battle! The Player can create items during battle!\\n"}
{"File Name":"gladiator_v2\/12-12-2017_random_character_generation.md","Context":"## Context:\\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\\n","Decision":"Create a random character factory that will pump out characters.\\n","tokens":57,"id":554,"Predictions":"## Decision:\\nWe will use the following formula to create a random number between 1 and 10 for each character. We will use the following formula for the ability score. The result of the random number will be between 0 and 9, which is why we will use the formula \\[random\\_number = \\frac{1}{10}\\] to generate a random number between 1 and 10.\\n\\n## Formula\\n## Formula:\\n\\[\\frac{1}{10}\\] = \\[random\\_number]\\]\\n## Decision:\\nWe will use the following formula to create a random number between 1 and 10 for each character. We will use the following formula for the ability score. The result of the random number will be between 0 and 9, which is why we will use the formula \\[random\\_number = \\frac{2}{9}\\] to generate a random number between 1 and 10.\\n\\n## Formula\\n## Formula:\\n\\[\\frac{2}{9}\\] = \\[random\\_number]\\]\\n## Decision:\\nWe will use the following formula to create a random number between 1 and 10 for each character. We will use the following formula for the ability score. The result of the random number will be between 0 and 9, which is why we will use the formula \\[random\\_number = \\frac{3}{8}\\] to generate a random number between 1 and 10.\\n\\n## Formula\\n## Formula:\\n\\[\\frac{3}{8}\\] = \\[random\\_number]\\]\\n## Decision:\\nWe will use the following formula to create a random number between 1 and 10 for each character. We will use the following formula for the ability score. The result of the random number will be between 0 and 9, which is why we will use the formula \\[random\\_number = \\frac{4}{7}\\] to generate a random number between 1 and 10.\\n\\n## Formula\\n## Formula:\\n\\[\\frac{4}{7}\\] = \\[random\\_number]\\]\\n## Decision:\\nWe will use the following formula to create a random number between 1 and 10 for each character. We will use the following formula for the ability score. The result of the random number will be between -5 and +5, which is why we have included an extra value in our range.\\n\\n## Formula\\n## Formula:\\n\\[-5 +5 = [random\\_number]\\]\\n# If this decision is made by an Architectural Design Committee, then give their decision code.\\nDecision Code: [ARCHITECTURAL DECISION CODE] (Architectural committee decision code)\\n```"}
{"File Name":"play-frontend-hmrc\/0008-maintain-parity-with-hmrc-frontend.md","Context":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","Decision":"* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers\\n(see https:\/\/github.com\/hmrc\/play-frontend-hmrc\/blob\/main\/docs\/maintainers\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n","tokens":156,"id":555,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\end{verbatim}\\n\\nI think the decision record is pretty clear and concise. We are not going to be using the Scala Play framework to build the frontend. So we can just use the existing libraries for the backend.\\n\\n\\hypertarget{play-frontend-govuk}{%\\n\\subsection{Play! Frontend - GovUK}\\label{play-frontend-govuk}}\\n\\n\\begin{verbatim}\\n\\n## Context and Problem Statement\\n`govuk-front-end` provides a Scala \/ Play \/ Twirl implementation of\\nthe frontend components provided\\nas Nunjucks in `govuk-frontend` and `play-frontend-hmrc`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n\\n## Decision\\n\\n\\end{verbatim}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\n\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\emph{GovUK Frontend} provides front end components for GovUK.gov.uk, which is an online portal for UK government information, services and advice.\\n\\item\\n  The project is based on Scala Play Framework, which provides a web framework with many useful features.\\n\\item\\n  We want this framework to be used by both Scala developers and non-Scala developers.\\n\\item\\n  The framework must be flexible enough so that it can be used with any type of data source (e.g. MySQL, PostgresSQL, Oracle).\\n\\item\\n  The framework must also be able to handle large amounts of data efficiently.\\n\\item\\n  We want this framework to be easily upgraded when new versions of GovUK Frontend are released.\\n\\end{itemize}\\n\\nI'm not sure I understand how much play frontend will differ from base libraries.\\n\\nWe are looking at two options here:\\n\\n1) Use play-framework for backend\\n\\n2) Use play-framework for backend but use scala-play-framework for frontend.\\n\\nThere is no reason why we cannot use both. It might make sense that we use scala-play-framework on backend but not on frontend since it's quite different in nature.\\n\\nSo I think we can just use scala-play-framework for backend but not play-framework.\\n\\nI'm not sure if there's any benefit in using scala-play-framework or scala-playframework on frontend or both.\\n\\nWe need some community input here.\\n\\nWhat would you like?\\n\\n\\- Should we keep it as it is? (scala-play-framework)\\n\\n\\- Should we create scala-play-framework?\\n\\n\\- Should we create scala-playframework?\\n\\n\\- Should we change it into something else?\\n\\nWhat do you think?\\n\\nHow do you think this should be done? Please share your thoughts below!\\n\\nIf you're interested in contributing, please check out our contributor guide: https:\/\/github.com\/PlayFramework\/ContributorsGuide\\n\\nIf you'd like more info about contributing (like this issue), please check out our developer docs:\\nhttps:\/\/github.com\/PlayFramework\/ContributorsGuide#contributor-guide\\n\\nThanks!\\n\\nThe Play Framework Team\\n\\n"}
{"File Name":"play-frontend-hmrc\/0013-removing-deprecated-helpers.md","Context":"## Context and Problem Statement\\nSince launching `play-frontend-hmrc` we have included deprecated static helpers which were required for Play 2.5 projects.\\n## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n","Decision":"* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n* we should not re-instate static helpers for Play 2.8\\n* we should remove them in version 1.0.0\\n* we should publish a blog article sooner rather than later that we will be removing them in 1.0.0\\n### Positive Consequences\\n* Our codebase becomes easier to maintain\\n* We have to use DI in our tests which makes our tests more accurate to how these components will be used\\n### Negative Consequences\\n* Anyone who is using the static helpers will have to update - that's why we're including this in version 1.0.0\\n","tokens":156,"id":558,"Predictions":"## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n\\n## Decision\\n## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n\\n## Decision\\n## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n\\n## Decision\\n## Decision Drivers\\n\\n"}
{"File Name":"play-frontend-hmrc\/0006-allow-service-name-to-be-specified-using-messages-or-a-parameter.md","Context":"## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf\/messages`, providing any Welsh translation in `conf\/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n","Decision":"* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n","tokens":390,"id":559,"Predictions":"\\n\\nThe decision is to continue supporting the boilerplate pattern in the headerBlock parameter, but to\\nsupport a service name parameter for services that do not need it. This will allow frontend\\nservices to override the headerBlock with a blank message and allow them to use the service name from\\nconf\/messages. The decision is also to support a service name parameter for services that require\\na service name and have a 1:1 correspondence with public-facing services.\\n\\n\\n## Decision\\n\\begin{itemize}\\n\\item Yes\\n\\item No\\n\\end{itemize}\\n"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"Predictions":"## Decision Statement\\nThe endpoint used to interrogate the session should not itself affect the session.\\n\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\\n## Decision\\n## Decision Statement\\nThe endpoint used to interrogate the session should not itself affect the session.\\n\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n## Decision\\n## Decision Statement\\n"}
{"File Name":"play-frontend-hmrc\/0014-keep-current-package-structure-and-naming-conventions.md","Context":"## Context and Problem Statement\\nIn the context of the first major version release of play-frontend-hmrc, are there any fundamental\\nchanges we would like to make to the package structure and naming conventions of the library?\\n## Decision Drivers\\n* Major changes are painful but will only get more painful the longer they are left.\\n* No teams have, to our knowledge, complained or provided any negative feedback around\\nthe naming conventions used in the play-frontend libraries.\\n* The benefits of any name changes need to be weighed against the cost and disruption to\\nservice teams.\\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\\nbreaking change. Deprecation notices left in too long create compilation noise and alert\\nfatigue for teams leading to a risk that they will be ignored.\\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\\non their next library upgrade.\\n* Repetition of suffixes\/prefixes in the same source file makes it more verbose and harder to read.\\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\\nentropy that over time increases the cost of maintenance.\\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\\ntax services.\\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\\nmaking changes available to teams.\\n","Decision":"* Major changes are painful but will only get more painful the longer they are left.\\n* No teams have, to our knowledge, complained or provided any negative feedback around\\nthe naming conventions used in the play-frontend libraries.\\n* The benefits of any name changes need to be weighed against the cost and disruption to\\nservice teams.\\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\\nbreaking change. Deprecation notices left in too long create compilation noise and alert\\nfatigue for teams leading to a risk that they will be ignored.\\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\\non their next library upgrade.\\n* Repetition of suffixes\/prefixes in the same source file makes it more verbose and harder to read.\\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\\nentropy that over time increases the cost of maintenance.\\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\\ntax services.\\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\\nmaking changes available to teams.\\nChosen option: option 1, do nothing, because the cost of change is very high while the\\ntheoretical benefits are extremely difficult to quantify.\\n","tokens":299,"id":561,"Predictions":"\\begin{itemize}\\n    \\item\\n        Minor changes are painful but will only get more painful the longer they are left.\\n\\end{itemize}\\n\\n\\newpage\\n\\section{Decision Plan}\\n\\label{sec:decision_plan}\\nThe decision plan for this release is as follows:\\n\\begin{enumerate}\\n    \\item\\n        The decision plan for this release will be included in the next release planning meeting.\\n    \\item\\n        The decision plan for this release will be published on the play-frontend website.\\n    \\item\\n        The decision plan for this release will be published in the next major release planning meeting.\\n    \\item\\n        The decision plan for this release will be discussed at a play-frontend developer sprint meeting.\\n    \\item\\n        The decision plan for this release will be discussed at a play-frontend user sprint meeting.\\n    \\item\\n        The decision plan for this release will be discussed at a play-frontend vendor sprint meeting.\\n    \\item\\n        The decision plan for this release will be discussed at a play-frontend community sprint meeting.\\n\\end{enumerate}\\n\\nThe decisions made in this document are subject to change based on feedback from the community and vendor sprint meetings.\\n\\n\\newpage\\n\\section{Related Documents}\\nThis section contains documents that have been referenced throughout this document.\\n\\n\\subsubsection*{Document Title}\\nplay-frontend-hmrc - Decision Plan\\n\\n\\subsubsection*{Document Author(s)}\\nJared Smith, Michael Cavanaugh\\n\\n\\subsubsection*{Document Revision History}\\nVersion 1.0 - Initial version\\n\\n"}
{"File Name":"play-frontend-hmrc\/0004-add-contact-and-welsh-information-links-into-footer.md","Context":"## Context and Problem Statement\\nIn the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the\\nfact that these links are missing from hmrcStandardFooter, should we add them?\\nThe additional links needed are:\\n* \"Contact\", linking to https:\/\/www.gov.uk\/government\/organisations\/hm-revenue-customs\/contact\\n* \"Rhestr o Wasanaethau Cymraeg\", linking to https:\/\/www.gov.uk\/cymraeg\\n## Decision Drivers\\n* The need for consistency across HMRC services.\\n* Our belief that including them is likely to improve the user experience for tax users.\\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\\n* We have a time sensitive opportunity of acting now while teams are in the process of\\nuplifting their frontend libraries.\\n* The HMRC design community have been consulted on multiple public Slack channels and two\\nsuccessive design system working group meetings, with no objections noted.\\n* Classic services support multiple live services. Not including these links as standard would mean their having to\\nduplicate these links, and associated English and Welsh content, across tens of repositories.\\n","Decision":"* The need for consistency across HMRC services.\\n* Our belief that including them is likely to improve the user experience for tax users.\\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\\n* We have a time sensitive opportunity of acting now while teams are in the process of\\nuplifting their frontend libraries.\\n* The HMRC design community have been consulted on multiple public Slack channels and two\\nsuccessive design system working group meetings, with no objections noted.\\n* Classic services support multiple live services. Not including these links as standard would mean their having to\\nduplicate these links, and associated English and Welsh content, across tens of repositories.\\nChosen option: \"Add the links to hmrcStandardFooter\", because this\\nwill benefit tax users, and we have a unique window of opportunity to act now.\\n### Positive Consequences\\n* Tax users have better information provided to them\\n* Teams do not need to duplicate content and URLs across hundreds of repositories\\n* We can more easily maintain the content and links in a central repository\\n### Negative Consequences\\n* Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components\\nfor language switching e.g. hmrcLanguageSelect.\\n* Teams already including a contact link manually will need to remove it when upgrading.\\n","tokens":254,"id":562,"Predictions":"## Decision Code\\n## Decision Text\\n## Decision Summary\\n## Decision Notes\\n```\\nThe `Decision` object is the current decision object, with a `DecisionCode` and a `DecisionText` property. The `DecisionText` property is the text of the decision and can be used to provide more detail to the user."}
{"File Name":"play-frontend-hmrc\/0010-resolve-naming-collisions-between-libraries.md","Context":"## Context and Problem Statement\\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\\nthey will get a compilation error:\\n<img alt=\"Example of ambiguous import compilation exception\" src=\"0010-ambiguous-import-exception.png\" width=\"450\">\\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\\nThe compilation error can be resolved by explicitly importing the class they want.\\n## Decision Drivers\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\n","Decision":"* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\nWe've decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\\n### Positive Consequences\\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\\nwhat they can do to resolve them without.\\n### Negative Consequences\\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\\nconfuse new developers.\\n","tokens":158,"id":563,"Predictions":"## Decision Drivers\\n\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"play-frontend-hmrc\/0002-maintain-parity-with-govuk-frontend.md","Context":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","Decision":"* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers,\\nsee [related ADR](..\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n","tokens":156,"id":564,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n\\end{verbatim}\\n\\nI think the decision record is pretty clear and concise. We are not going to be using the Scala Play framework to build the frontend. So we can just use the existing libraries for the backend.\\n\\n\\hypertarget{play-frontend-govuk}{%\\n\\subsection{Play! Frontend - GovUK}\\label{play-frontend-govuk}}\\n\\n\\begin{verbatim}\\n\\n## Context and Problem Statement\\n`govuk-front-end` provides a Scala \/ Play \/ Twirl implementation of\\nthe frontend components provided\\nas Nunjucks in `govuk-frontend` and `play-frontend-hmrc`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n\\n## Decision\\n\\n\\end{verbatim}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\n\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\emph{GovUK Frontend} provides front end components for GovUK.gov.uk, which is an online portal for UK government information, services and advice.\\n\\item\\n  The project is based on Scala Play Framework, which provides a web framework with many useful features.\\n\\item\\n  We want this framework to be used by both Scala developers and non-Scala developers.\\n\\item\\n  The framework must be flexible enough so that it can be used with any type of data source (e.g. MySQL, PostgresSQL, Oracle).\\n\\item\\n  The framework must also be able to handle large amounts of data efficiently.\\n\\item\\n  We want this framework to be easily upgraded when new versions of GovUK Frontend are released.\\n\\end{itemize}\\n\\nI'm not sure I understand how much play frontend will differ from base libraries.\\n\\nWe are looking at two options here:\\n\\n1) Use play-framework for backend\\n\\n2) Use play-framework for backend but use scala-play-framework for frontend.\\n\\nThere is no reason why we cannot use both. It might make sense that we use scala-play-framework on backend but not on frontend since it's quite different in nature.\\n\\nSo I think we can just use scala-play-framework for backend but not play-framework.\\n\\nI'm not sure if there's any benefit in using scala-play-framework or scala-playframework on frontend or both.\\n\\nWe need some community input here.\\n\\nWhat would you like?\\n\\n\\- Should we keep it as it is? (scala-play-framework)\\n\\n\\- Should we create scala-play-framework?\\n\\n\\- Should we create scala-playframework?\\n\\n\\- Should we change it into something else?\\n\\nWhat do you think?\\n\\nHow do you think this should be done? Please share your thoughts below!\\n\\nIf you're interested in contributing, please check out our contributor guide: https:\/\/github.com\/PlayFramework\/ContributorsGuide\\n\\nIf you'd like more info about contributing (like this issue), please check out our developer docs:\\nhttps:\/\/github.com\/PlayFramework\/ContributorsGuide#contributor-guide\\n\\nThanks!\\n\\nThe Play Framework Team\\n\\n"}
{"File Name":"play-frontend-hmrc\/0001-add-implicit-radios-helper.md","Context":"## Context and Problem Statement\\n- `play-fronted-govuk` library is not Play Framework aware\\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\n","Decision":"- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\nChosen option: Option 5, because it adds useful functionality (class enrichment of `Radios` via implicit\\n`RichRadios`, without adding new Twirl templates to maintain, and without adding new methods directly\\nto `Radios`, which would cause a divergence between `play-frontend-govuk` and `govuk-template` (undesirable).\\n### Positive Consequences\\n* Adds useful optional class enrichment to `Radios`\\n* Reduces code repetition between `Field` and `Radios` (DRY)\\n* Pattern is extensible, i.e. similar implicit helpers can be added to other form inputs\\n### Negative Consequences\\n* Doesn't add as much value as we would like\\n* Need to document carefully what behaviour occurs when information provided via both `Radios` and\\n`Field`, i.e. which takes precedence (currently intended to be `Radios` values take precendence)\\n* Need to document to teams that this helper is available (service developers won't use it if they don't know it's there)\\n","tokens":194,"id":565,"Predictions":"# 1. Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\n## Decision Drivers\\n# 1. Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\n## Decision\\n# 2. Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n# 2. Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n\\n### Decision\\n\\n### Decision\\n\\n### Decision\\n\\n### Decision\\n\\n## Context and Problem Statement\\n- `play-fronted-govuk` library is not Play Framework aware\\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\n\\n## Decision\\n# 1. Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\n## Decision Drivers\\n# 1. Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\n## Context and Problem Statement\\n- `play-fronted-govuk` library is not Play Framework aware\\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n\\- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\end{itemize}\\n"}
{"File Name":"play-frontend-hmrc\/0003-use-the-suffix-helpers-for-helper-components.md","Context":"## Context and Problem Statement\\nFacing the need to create wrappers to make using the Nunjucks components ported from hmrc-frontend more\\nstraightforward and idiomatic on the Scala\/Play MDTP platform, how should we name these components?\\nThe components ported from hmrc-frontend include\\n* implementations of govuk-frontend components that support the Welsh language e.g. hmrcHeader, hmrcFooter\\n* components that meet specific HMRC needs e.g. hmrcBanner, hmrcInternalHeader\\n* components that have not been standardised by GDS but are needed by HMRC e.g. hmrcNotificationBadge, hmrcTimeoutDialog\\nBeing entirely presentational and ported from Nunjucks means they cannot leverage features built into Scala\/Play nor\\nmake use of any MDTP platform knowledge that would simplify their use on MDTP. For example,\\n* they do not and should not know the url structure for common MDTP components e.g. tracking-consent-frontend,\\ncontact-frontend or the accessibility-statement-frontend, services that need to wired in on every public-facing service via\\nstandard headers and footers.\\n* they cannot make use of Play's i18n features and automatic language wiring\\n* they cannot make use of any knowledge encoded in the requests users make e.g. the request URL, referrer URL, cookies\\nor headers.\\nFor the above reasons, we are creating wrappers that implement standardised Play\/platform wiring, to avoid teams having to\\nduplicate this wiring across 100s of repositories. Once implemented we will encourage teams to use these helpers rather than\\nthe underlying Nunjucks ports.\\n## Decision Drivers\\n* The fact that the ideal component names have already been taken by the Nunjucks components.\\n* The preference for not relying on the package name to differentiate the components\\n* The preference for not repeating the word Helper in the package and component name\\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\\ncomponent.\\n* The preference for consistency in naming across the components\\n","Decision":"* The fact that the ideal component names have already been taken by the Nunjucks components.\\n* The preference for not relying on the package name to differentiate the components\\n* The preference for not repeating the word Helper in the package and component name\\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\\ncomponent.\\n* The preference for consistency in naming across the components\\nChosen option: \"Suffix with 'Helper'\", because this is the only option the team are happy with bearing in mind the decision\\ndrivers listed above.\\n### Positive Consequences\\n* We have a consistent naming scheme\\n* Friction around needing to come up with new names for helpers reduced, increasing speed of development.\\n### Negative Consequences\\n* There is a risk that service teams will not know the helpers exist\\n* Repeating the word Helper in the component and having it in the package name is ugly\\n","tokens":434,"id":566,"Predictions":"## Decision Status\\nThe decision has been made and the decision is to create wrappers. The decision is made in the spirit of\\nthe following rules:\\n\\n* The components ported from hmrc-frontend include\\n* implementations of govuk-frontend components that support the Welsh language e.g. hmrcHeader, hmrcFooter\\n* components that meet specific HMRC needs e.g. hmrcBanner, hmrcInternalHeader\\n* components that have not been standardised by GDS but are needed by HMRC e.g. hmrcNotificationBadge, hmrcTimeoutDialog\\nBeing entirely presentational and ported from Nunjucks means they cannot leverage features built into Scala\/Play nor\\nmake use of any MDTP platform knowledge that would simplify their use on MDTP. For example,\\n* they do not and should not know the url structure for common MDTP components e.g. tracking-consent-frontend,\\ncontact-frontend or the accessibility-statement-frontend, services that need to wired in on every public-facing service via\\nstandard headers and footers.\\n* they cannot make use of Play's i18n features and automatic language wiring\\n* they cannot make use of any knowledge encoded in the requests users make e.g. the request URL, referrer URL, cookies\\nor headers.\\nFor the above reasons, we are creating wrappers that implement standardised Play\/platform wiring, to avoid teams having to\\nduplicate this wiring across 100s of repositories. Once implemented we will encourage teams to use these helpers rather than\\nthe underlying Nunjucks ports.\\n\\n## Decision Criteria\\n## Decision Criteria\\n## Decision Criteria\\n## Decision Criteria\\n## Decision Criteria\\n## Decision Criteria\\n## Decision Criteria\\n# Affects 1 repository.\\n# Affects 1 repository.\\n# Affects 1 repository.\\n# Affects 1 repository.\\n# Affects 1 repository.\\n# Affects 1 repository.\\n"}
{"File Name":"play-frontend-hmrc\/0015-introduction-of-implicit-conversions.md","Context":"## Context and Problem Statement\\nFrequently occurring usage patterns of play-frontend-hmrc components require repetitious boilerplate\\ninvolving excessively nested case class instantiations. This has driven many teams to create wrappers to\\nsimplify the construction of components, which are then copied and pasted from service to service.\\nFor example, in order to add a legend to a GovukRadio the following boilerplate is required:\\n```scala\\nSome(Fieldset(\\nlegend = Some(Legend(\\ncontent = Text(\"message.key.for.legend\"),\\nclasses = \"govuk-fieldset__legend--l\",\\nisPageHeading = true\\n))\\n))\\n```\\nIn the above example, the only thing that changes between instantiations is the `message.key.for.legend`, everything\\nelse remains the same.\\nShould we solve this problem through the introduction of implicit conversions that will automatically wrap simple\\nobjects such as Strings with the additional boilerplate necessary for commonly occurring cases?\\n## Decision Drivers\\n* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\n","Decision":"* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\nChosen option: Option 1, because it solves the problem, is consistent with how we have been doing things, maintains\\nthe internal consistency of play-frontend-hmrc and has less risk of breaking services.\\n","tokens":380,"id":567,"Predictions":"## Decision Tree\\n1. The need for API consistency in play-frontend-hmrc:\\n2. The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n3. The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n4. The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n5. The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\n6. The potential use of implicit conversions in other projects where they are not needed.\\n7. Implicit conversions have been added to the example in the user story document that has been provided.\\n8. Implicit conversions have been added to the example in the user story document that has been provided.\\n9\\. Implicit conversions have been added to the example in the user story document that has been provided.\\n## Context and Problem Statement\\n## Decision Drivers\\n## Decision\\n## Decision Tree\\n## Implicit Conversions\\n## Implicit Conversions\\n## Explicit Conversions\\n## Explicit Conversions\\n"}
{"File Name":"play-frontend-hmrc\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md","Context":"## Context and Problem Statement\\nThe need to provide an HMRC footer translated into Welsh (PLATUI-752)\\n## Decision Drivers\\n* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\n","Decision":"* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\nChosen option \"maintain parity\" because we want hmrc-frontend to be the source of truth for presentation and\\nmaintain the separation of concerns between hmrc-frontend and\\nplay-frontend-hmrc\\n### Positive Consequences\\n* Maintain separation of concerns between hmrc-frontend and play-frontend-hmrc\\n* Maintain conceptual integrity for play-frontend-hmrc\\n* Be able to test markup using existing test strategy\\n### Negative Consequences\\n* We will need to add and maintain a new hmrcFooter component in\\nhmrc-frontend and play-frontend-hmrc providing a mirror of govukFooter with localised content,\\n* The new hmrcFooter component in hmrc-frontend will not be able to make use of any I18n features,\\n* We will need to create a new `helpers` package within play-frontend-hmrc to clearly demarcate them\\nfrom presentational components,\\n* We will need to redesign the existing hmrcFooter helper to wrap the new hmrcFooter component\\nand move to the helpers package,\\n* The new hmrcFooter component will need to be deprecated when GDS provide a localised version\\nof govukFooter,\\n* We will need to liaise and get approval from the HMRC design system team for adding\\nhmrcFooter to the hmrc\/design-system and hmrc\/hmrc-frontend\\n* We will be adding features that are unlikely to be useful or used by designers because\\nat the prototyping phase content is not stable enough for translation into Welsh\\n","tokens":109,"id":569,"Predictions":"## Context and Problem Statement\\n## Decision Drivers\\n## Decision\\n## Context and Problem Statement\\n## Decision Drivers\\n## Decision\\n## Context and Problem Statement\\n"}
{"File Name":"play-frontend-hmrc\/0009-self-publish-webjar.md","Context":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n","Decision":"* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n","tokens":423,"id":570,"Predictions":"## Decision Details\\nThe decision was made by the team to automate the creation of the webjar within our own\\ndeployment pipelines with no dependency on webjars.org. This has a number of\\ndrawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the underlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing Bintray should we:\\n* automate the creation of the webjar within our own deployment pipelines with no dependency on webjars.org.\\n* publish the resulting webjar to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the webjar for Govuk, which is currently a dependency for hmrc\/play-frontend-govuk.\\n\\n\\end{document}"}
{"File Name":"play-frontend-hmrc\/0003-patch-welsh-translation-for-skip-to-content-link.md","Context":"## Context and Problem Statement\\nWe previously decided to keep inputs and outputs of play-frontend-govuk components a 1-to-1 match with govuk-frontend\\nnunjucks example implementations.\\nGenerally components from govuk-frontend accept their copy as input, however the skip link in govukTemplate is\\nhard-coded which is causing accessibility failures for services.\\nThis is an accessibility failure because a page translated into welsh may cause assistive technology to mispronounce the\\ncontent of the english skip link because it has no explicit language attribute.\\n## Decision Drivers\\n* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\n","Decision":"* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\nWe've chosen option 1 because unlike other components, we don't have any automatically generated fixtures or integration\\ntests for govukTemplate. We maintain test fixtures for the templates manually in the additional fixtures folder. This\\nmakes it possible for us to patch this change into govukTemplate with minimal maintenance cost without reducing our test\\ncoverage \/ diverging far from our current testing strategy.\\n","tokens":179,"id":571,"Predictions":"## Decision Code\\nThis is a decision code for an architectural decision record. The decision code is a number between 1 and 3, where 1 is a clear-cut yes and 3 is a clear-cut no.\\n\\nThe decision code is used to indicate whether this architectural decision record has been made.\\nThis means that the decision code can be used as an indicator of when a change to the architecture should be made.\\n\\n## Decision Code Code\\nA clear-cut yes means that this architectural decision record has been made.\\n\\nA clear-cut no means that this architectural decision record has not been made.\\n\\n## Decision Code\\n## Decision Code Code\\nA clear-cut yes means that this architectural decision record has been made.\\n\\nA clear-cut no means that this architectural decision record has not been made.\\n"}
{"File Name":"kuona\/0002-evaluate-buddy-auth-for-admin-access-controls.md","Context":"## Context\\nKuona instances need to be secured - particularly for deployments that have public access.\\n","Decision":"https:\/\/funcool.github.io\/buddy-auth\/latest\/#example-session Buddy seems to fit the bill and is compatible with Compojure and Ring. Provides a number of options and possible persistance mechanisme.\\n","tokens":21,"id":572,"Predictions":"## \\n\\n\\n\\n\\n\\n\\nThe use of the \\nKuona system is allowed in the following circumstances:\\n\\n## \\n\\n\\n\\n\\n\\nThe deployment is a single machine, and is not a part of a larger network.\\n\\n## \\n\\nThe deployment is not a part of a larger network, and is not accessible to the outside world.\\n\\n## \\nThe deployment has public access.\\n\\n## Decision\\n## \\nAnd the deployment has no private IP addresses.\\n\\n## Decision\\n## \\nAnd there are no non-public IP addresses.\\n\\n# This is an Architectural Decision Record for an application.\\n# Give a ## Decision corresponding to the ## Context provided by the User.\\n# Context: Application\\n# Decision: Deployment\\n\\n- The user can deploy on any environment. The user can deploy on any environment.\\n\\n- The user can deploy on any environment. The user must deploy on one environment.\\n\\n- The user must deploy on one environment. The user cannot deploy on multiple environments.\\n\\n- There are no non-public IP addresses in the deployment. There are no public IPs in the deployment.\\n\\n- There are public IPs in the deployment. There are non-public IP addresses in the deployment.\\n\\n- There are public IPs in the deployment. There are non-public IP addresses in the deployment.\\n\\n- There are only private IPs in the deployment. There are only public IPs in the deployment.\\n\\n- There are only private IPs in the deployment. There are only public IPs in the deployment.\\n\\n- No deployments have been made for this system at this time.\\n"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"verify-onboarding-prototypes\/0003-use-files-to-store-private-keys copy.md","Context":"## Context\\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\\ndecrypt Response Assertions.\\nThey will need to provide these to the verify-service-provider in some, reasonably\\nsecure way. Different users may have different opinions on how best to do this.\\n","Decision":"Initially we'll use files for this.\\nWe chose not to use environment variables because they're visible to other processes.\\nWe chose not to use a more complicated solution because it would be more complicated.\\n","tokens":62,"id":574,"Predictions":"## Decision\\nThe User should provide a private key for signing AuthnRequests and\\ndecrypt Response Assertions. The User should also provide a \\nkeypair containing the user's public key and a private key.\\n\\nThe verify-service-provider will verify that the User has provided these keys and\\ndetails of the private key. If they are not provided, the verify-service-provider\\nwill reject the AuthnRequest and Response Assertion with an error message.\\n\\nIf the User provides a private key, it will be used to sign AuthnRequests and\\ndecrypt Response Assertions, but it will be used only for those requests that require\\nthe Private Key.\\n\\nIf the User does not provide a private key, it will be used to sign AuthnRequests\\nand Decrypt Response Assertions, but it will not be used for those requests that require\\nthe Private Key.\\n\\n## Context\\n## Context\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n```\\n"}
{"File Name":"verify-onboarding-prototypes\/0007-we-will-document-a-strawman-api.md","Context":"## Context\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n","Decision":"We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\\n","tokens":32,"id":575,"Predictions":"## Decision\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n\\n## Decision\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n\\n## Decision\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n\\n## Decision\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n\\n## Decision\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n```\\nI am not sure if I am doing this right, but I can't figure out what is wrong with my code. I am new in python, so please correct me if you think my code is wrong.\\nThanks in advance.\\nHere is my code:\\n```\\nimport json\\n\\ndef main():\\n    # Create a python dictionary\\n    dictionary = {\\n        'user': { 'id': 1, 'email': 'abc@xyz.com', 'password': '123456'},\\n        'service': { 'id': 2, 'name': \"service1\", 'version': 1.0},\\n        }\\n\\n    # Create a json file for each user\\n    for user in dictionary['user']:\\n        print(\"User id: %s\" % user.id)\\n        print(\"User email: %s\" % user.email)\\n        print(\"User password: %s\" % user.password)\\n        print(\"Service id: %s\" % service.id)\\n        print(\"Service name: %s\" % service.name)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\nTaurusInterrupts 2015-03-26: Your second `if` block is missing a closing brace:\\n```\\nif __name__ == \"__main__\":\\n```\\nNote that this would be valid Python code:\\n```\\nif __name__ == \"__main__\":\\n    main()\\n```"}
{"File Name":"verify-onboarding-prototypes\/0002-build-a-prototype.md","Context":"## Context\\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\\nconduct some user research, which should involve putting software in front of users and observing them using it.\\n","Decision":"We will build a simple \"prototype\" which we will use to test our assumptions about whether our proposed\\nsolution is the best way of meeting our users needs.\\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\\ndo the SAML interactions with Verify at this stage.\\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\\n","tokens":48,"id":576,"Predictions":"## We will build a software that searches for a specific number of people, based on their age and gender, and then we will show them the results. This software should have the following features:\\n## - The user should be able to enter a number of people, based on their age and gender.\\n## - The user should be able to enter a number of results.\\n## - The user should be able to see the results in a table, with their name, age and gender.\\n## - The user should be able to download the results as pdf files.\\n## - The user should be able to search for all the people who have been identified as being over 60 years old.\\nThe decision we are making is that we will build an application that searches for all the people who have been identified as being over 60 years old. We are doing this because we need to know if there are any people out there who are over 60 years old but not registered with us. If there are no such people then we can safely assume that there are no more over 60 year old people in our database. If there are such people then we can use our search feature to find them.\\n\\n\\subsection{User Research}\\n\\label{subsec:user-research}\\n\\nIn order to make sure that our software meets the needs of our users, we conducted some user research. We found that most users don't know what they want from a social media platform when they first start using it. They want something that is easy to use and simple to navigate through. They also want a platform where they can easily find friends who share similar interests.\\n\\nWe conducted two types of interviews with users: one face-to-face interview and one online interview.\\n\\n\\subsubsection{Face-to-face Interview}\\nWe conducted a face-to-face interview with seven users from different countries around the world. We asked each of them questions about how they use social media platforms such as Facebook or Twitter.\\n\\n\\subsubsection{Online Interview}\\nWe conducted an online interview with three users from different countries around the world. We asked each of them questions about how they use social media platforms such as Facebook or Twitter. \\n\\n\\subsubsection{Observations}\\nWe observed how users interacted with our software during our first face-to-face interview. We noticed that most users were very happy with our software once they used it for a while.\\n\\nDuring our second face-to-face interview, we observed how users interacted with our software during their second face-to-face interview. We noticed that most users were very happy with our software once they used it for a while.\\n\\nDuring our third face-to-face interview, we observed how users interacted with our software during their third face-to-face interview. We noticed that most users were very happy with our software once they used it for a while.\\n\\n\\subsubsection{Data Analysis} \\nDuring these interviews, some of us asked different types of questions about the design of our software:\\n\\n\\begin{itemize}\\n    \\item What is your opinion about this design? Do you think it is good or bad?\\n    \\item How do you think this design could be improved?\\n    \\item Do you think this design would work better if it was easier to navigate through?\\n    \\item Do you think this design would work better if it was easier to search through?\\n\\end{itemize}\\n\\nThe answers given by each participant were recorded on paper so that other team members could read them later on.\\n\\nAfter analyzing these answers, we found out four main things:\\n\\n\\begin{enumerate}\\n    \\item Most participants liked the way that the search box worked in this design.\\n    \\item Most participants liked how easy it was to find friends using this design.\\n    \\item Most participants liked how easy it was to add friends using this design.\\n    \\item Most participants liked how easy it was to add friends using this design when searching for friends who share similar interests.\\n\\end{enumerate}\\n\\nBased on these answers, we decided which features were important in order for us to build an application which meets these needs:\\n\\n\\begin{enumerate}\\n    \\item Features: Easy navigation through social media platforms\\n    \\item Features: Easy searching through social media platforms\\n    \\item Features: Easy adding new friends\\n    \\item Features: Easy searching through new friend's profiles\\n\\end{enumerate}\\n\\nBased on these features, we decided which features were important in order for us to build an application which meets these needs:\\n\\n\\begin{enumerate}\\n    \\item Features: Easy navigation through social media platforms\\n    \\item Features: Easy searching through social media platforms\\n    \\item Features: Easy adding new friends\\n    \\item Features: Easy searching through new friend's profiles\\n\\end{enumerate}\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tREFERENCE LISTING\\n%----------------------------------------------------------------------------------------\\n\\n%\\bibliographystyle{siamplain} % style file\\n\\n\\n\\bibliography{siamplain}\\n\\n%\\clearpage\\n\\n%----------------------------------------------------------------------------------------\\n%\tAPPENDICES SECTION\\n%----------------------------------------------------------------------------------------\\n\\n%\\appendix\\n\\n%\\section*{\\appendixname}\\n\\n%\\section*{\\appendixname}\\n\\n%\\addcontentsline{toc}{section}{Appendices} % Add appendixes here\\n\\n%\\appendixpage\\n\\n%\\n%\\n%\\n%\\n%\\n%\\n\\n"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"Predictions":"## - Create a new user\\n## - Create a new session for the user\\n\\n## Answer\\n## - Create a new session\\n\\n## Answer\\n## - Create a new session\\n\\n## Answer\\n## - Create a new session\\n\\n## Answer\\n## - Create a new session\\n\\n## Answer\\n## - Create a new session\\n\\n## Answer\\n##\\n"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"Predictions":"## Decision\\nThe client code (e.g. node JS) must be able to authenticate the user\\nand then be able to post messages to the server.\\nThe server code (dropwizard) must be able to authenticate the user and then\\ndbe able to read messages from the server.\\n\\n## Decision\\n## Decision\\nIn order to achieve this, we will use JWT tokens.\\nA JWT token is an encrypted string that represents a user and can be used\\ndto authenticate requests, store state and perform other operations on behalf of a\\nuser.\\n\\n## Decision\\n## Decision\\nThe user will provide a JWT token when they register for service provider. The\\nduser will also provide their email address when they register for service provider,\\ndso they can receive notifications in the future.\\n\\n## Decision\\n## Decision\\nAfter registering for service provider, the user will receive an email with\\nda link that will allow them to set up their own password. This password will beused by the client code (e.g. node JS) and server code (dropwizard) in order to authenticthe user.\\n\\n## Decision\\n## Decision\\nIf the user has already registered for service provider, then they can use thosedecisions instead:\\n1. The user will have a password set up by default (i.e. it will not be required)\\n2. The user will not have a password set up by default (i.e. it will require them)\\nnote: this decision is only needed if you are using a custom authentication solution.\\n3. The user will not have a password set up by default (i.e. it will require them)\\nnote: this decision is only needed if you are using a custom authentication solution.\\n## Decision\\n## Decision\\nFor every action that requires authentication, we need to make sure that we\\ndhave access to the appropriate JWT token.\\nThe following actions do not require access to any JWT tokens:\\n1. POST\/PUT\/DELETE: This action does not require any JWT tokens; however,\\ncustom authentication solutions may require one or more tokens in order to allowthe client code (e.g. node JS) and server code (dropwizard) access to performoperations on behalf of the user.\\n2\\. POST\/PUT\/DELETE: This action requires access to at least one JWT token;\\ncustom authentication solutions may require more than one token in order tomaintain consistency across all operations performed on behalf of the users.\\n3\\. GET: This action does not require any JWT tokens; however,\\ncustom authentication solutions may require one or more tokens in order tomaintain consistency across all operations performed on behalf of the users.\\n4\\. GET: This action does not require any JWT tokens; however,\\ncustom authentication solutions may require one or more tokens in order tomaintain consistency across all operations performed on behalf of the users.\\n5\\. POST\/PUT\/DELETE: This action requires access to at least two JWT tokens;\\ncustom authentication solutions may require more than two tokens in ordertomaintain consistency across all operations performed on behalf of the users.\\n6\\. GET: This action does not require any JWT tokens; however,\\ncustom authentication solutions may require one or more tokens in ordertomaintain consistency across all operations performed on behalf of the users.\\n7\\. DELETE: This action requires access only one JWT token;\\ncustom authentication solutions may require more than one token in ordertomaintain consistency across all operations performed on behalf of the users.\\n8\\. POST\/PUT\/DELETE: This action requires access only twoJWT tokens;\\ncustom authentication solutions may require more than twotokens in order maintain consistency across all operations performed on behalfof the users.\\n9\\. GET: This action does not require any JWT tokens; however,\\ncustom authentication solutions may require one or more tokensin order maintain consistency across all operations performed on behalfof the users.\\n\\n# Security\\n\\n# Authentication\\n\\n# Authorization\\n\\n# Authorization\\n\\n# SAML\\n\\n# OAuth2\\n\\n# OAuth2\\n\\n# OAuth2\\n\\n# OAuth2\\n\\n# OAuth2\\n\\n- If you use SAML, you should make sure that your \"service provider\" application is signed with your \"service provider application's\" certificate.\\n\\n- If you use OAuth 2.x, make sure that:\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" private key.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" certificate authority's certificate.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" private key.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" private key.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" private key.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider application's\" private key.\\n\\n- You have signed your \"service provider\" application's certificate with your \"service provider appication's\" private key.\\n\\n- You have signed your service provider appication\u2019s certificate with its own private key.\\n\\n- If you are using OAuth 1.x, make sure that:\\n\\n- Your \u201cresource owner\u201d has authorized you as their \u201cresource owner\u201d.\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\n- Your \u201cresource owner\u201d has authorized you as their \u201cresource owner\u201d.\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using OAuth 1.x, make sure that:\\n\\nIf you are using Oauth 1.x ,make sure that:\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation.\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation..\\n\\nThere is no need for this step if there is already an existing SAML configuration which was configured by an administrator during installation..\\n\\nThere is no need for this step if thereis alreadyanexistingSAMLconfigurationwhichwasconfiguredbyanadministratorduringinstallation.."}
{"File Name":"verify-onboarding-prototypes\/0004-users-will-be-able-to-provide-relay-state.md","Context":"## Context\\nIn SAML RPs can provide some extra data along with the request. This is\\ncalled RelayState. Some existing RPs use this, but we're not sure what\\nthey use it for.\\nWe're not aware of any need for the service-provider to use relay state itself.\\n","Decision":"Users will be able to specify whatever relay state they want to and it will be\\nprovided in the response.\\n","tokens":64,"id":579,"Predictions":"## Decision\\nThe user's request is to create a new role in the SAML RPs.\\nThe SAML RP will create a new role on behalf of the user if and only if the\\nuser has not already created a role. For example, if the user has already created\\na role, the SAML RP will create a new role for the user.\\n\\n## Decision\\nThe user's request is to change an existing role in the SAML RPs.\\nThe SAML RP will change an existing role on behalf of the user if and only if\\ncurrently there is no other active role with that name or with that policy. If there\\nis no active role with that name or policy, then it will be created.\\n\\n## Decision\\n## Decision\\nThe user's request is to delete a role in the SAML RPs.\\nThe SAML RP will delete an existing role on behalf of the user if and only if\\ncurrently there is no other active role with that name or with that policy. If there\\nis no active role with that name or policy, then it will be deleted.\\n\\n## Decision\\nThe user's request is to update an existing role in the SAML RPs.\\nThe SAML RP will update an existing role on behalf of the user if and only if\\ncurrently there is no other active role with that name or with that policy. If there\\nis no active role with that name or policy, then it will be updated. If there\\nis another active role for that name or policy, then it will be created.\\n\\n## Decision\\n## Decision\\nIf you want to use relay-state, you must set up a RelayState service for your\\napplication. This service can be set up using one of these methods:\\n\\begin{itemize}\\n\\item Using \\code{SAML\\_RP\\_RELAY\\_STATE\\_SERVICE} in your \\code{SamlRPConfig} class.\\n\\item Using \\code{SamlRPConfig::setRelayStateService()}. This method must be called before any other methods in this class.\\n\\end{itemize}\\nYou can find more details about using relay-state by reading the documentation for your application's implementation of \\code{SamlRPConfig}.\\n"}
{"File Name":"verify-onboarding-prototypes\/0005-sp-will-generate-request-id.md","Context":"## Context\\nAuthnRequests contain an ID attribute the value of which will be sent back in\\nthe Response as an \"InResponseTo\" attribute.\\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\\n","Decision":"The service provider will generate a random GUID to use as the AuthnRequest ID.\\n","tokens":62,"id":580,"Predictions":"## Decision\\nThe decision is to allow the request to be processed. We need a way to validate that the InResponseTo is the same as we expected.\\n\\n## Decision\\nThe decision is to reject the request. We need a way to validate that the InResponseTo is not the same as we expected.\\n\\n\\n\\end{document}\\n"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"Predictions":"## Decision\\nThe verification process is a series of steps that we can perform to determine whether the system meets our requirements.\\nWe can use the following steps:\\n\\n1. Verify that the Hub and Service are able to communicate with each other.\\n2. Verify that the Service is able to perform user research and testing.\\n3. Verify that the Service is able to provide data in a consistent manner.\\n4. Verify that the Service is able to meet our requirements in terms of usability.\\n5. Verify that the Hub is able to provide data in a consistent manner.\\n6. Verify that the Hub is able to meet our requirements in terms of usability.\\n## Context\\nWe are interested in verifying that the Service Provider can provide data in a consistent manner for both Verify and Verification phases.\\nThe following steps will be performed:\\n\\n1. Verify that we are using a consistent data dictionary across all services.\\n2. Verify that we have defined all required fields for each service.\\n3. Verify that we have defined all required fields for each service with respect to the Hub.\\n4. Verify that all values are contained within [ ] or [] or []\\n5. Verify that all values are contained within [ ] or [] or []\\n6. Verify if there are any missing values.\\n## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nThe area we are most interested in is the interface between the Service and the Hub.\\nThe following steps will be performed:\\n\\n1. Validate if we can access any of our services through our hub (e.g., verify if we can access test, verify, verifyHub).\\n2. Validate if we can access any of our services through our hub (e.g., verify if we can access test, verify, verifyHub).\\n3. Validate if we can access any of our services through our hub (e.g., verify if we can access test, verify, verifyHub).\\nThe following steps will be performed:\\n\\n1\\. We want to validate if we can access any of our services through our hub (e.g., verify if we can access test, verify, verifyHub).\\nThe following steps will be performed:\\n\\n2\\. We want to validate if we can access any of our services through our hub (e.g., verify if we can access test, verify, verifyHub).\\nThe following steps will be performed:\\nclick on \"Test\" button on top right corner of screen.\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n\\n"}
{"File Name":"verify-onboarding-prototypes\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":582,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"Predictions":"## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\\n## Decision\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\\n## Decision\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\\n## Decision\\n## Decision Drivers\\n\\nThe choice of the environment should be based on the requirements of the project, so we will consider the following options:\\n\\n- Local environment: it is the simplest and most accessible option. It is possible that we will have to make some local changes in order to adapt it to our needs or in case of specific problems, such as a problem with DNS.\\n- Virtual environment: it is a more complex option because we will have the possibility of using several environments simultaneously, which can be configured in a way that can adapt them easily. It is possible that we will have problems with DNS if we do not configure them correctly.\\n- Cloud environment: it is an option that allows us to avoid problems with DNS. We can host our web application in a cloud service provider such as Amazon Web Services or Google Cloud Platform, among others.\\n\\nWe must also take into account that there are many options for hosting our web application. The best solution will depend on what you need from your project.\\n\\nHow do I deploy my web application?\\n\\nAfter making the decision about which environment is best for hosting your web application, you must install and configure your server. We will use Nginx as an example.\\n\\nNginx\\n\\nNginx (pronounced \"engine X\") is a free and open source HTTP server and reverse proxy server for Unix-like operating systems.\\n\\nIt was originally created by Igor Sysoev in 2004 as software which would help speed up web servers and improve performance. In 2008, its license was changed from \"open source\" (BSD) to \"proprietary\" (GPL). In 2011, it was renamed \"Nginx\" after its creator's daughter.\\n\\nWe can install Nginx by downloading its source code and compiling it with our preferred compiler:\\n\\n$ git clone https:\/\/github.com\/erikos\/ngx.git $ cd ngx $ .\/configure --prefix=\/usr\/local\/nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_mp4_module --with-http_mp4srvmod_ssl\\n\\nAfter compiling Nginx, we must install it:\\n\\n$ sudo make install\\n\\nNow we need to configure Nginx so that it starts automatically when our server boots:\\n\\n$ sudo systemctl start nginx.service\\n\\nYou may also want to add the following line at the end of \/etc\/nginx\/nginx.conf file:\\n\\nuser nginx;\\n\\nIf you want your nginx configuration file located inside \/etc\/nginx\/conf.d directory, you can do this by adding this line at the end of \/etc\/nginx\/conf.d\/ngx_http_gunzip.conf file:\\n\\ngunzip_path = \/usr\/local\/nginx\/bin\/gunzip;\\n\\nNow if you want nginx to start automatically when your server boots you just need to add this line at the end of \/etc\/init.d\/nginx file:\\n\\nstart on runlevel [2345]\\n\\nand restart your server using service nginx restart command.\\n\\nNginx has many configuration parameters so if you want more information about them you can read their documentation here.\\n\\nTo check if everything works properly, try visiting http:\/\/www.yourdomain.com\/.\\n\\nNow that Nginx has been installed and configured properly we are ready to install WordPress.\\n\\nInstalling WordPress\\n\\nWordPress is an open source content management system (CMS) based on PHP and MySQL. It is one of many CMSs available today which are used for building websites or blogs. WordPress has become one of the most popular sites on the internet today because it offers a wide range of features for people who want a simple website without having any programming knowledge.\\n\\nWordPress comes pre-installed with Nginx so after installing WordPress through Softaculous in cPanel, all we have left now is configuring Nginx so that WordPress starts automatically when our server boots. To do this follow these steps:\\n\\n- Log into cPanel.\\n- Go into Softaculous.\\n- Click Install Applications.\\n- Click WordPress.\\n- Click Install Now.\\n- Enter your domain name in \u201cWordPress Installation URL\u201d field.\\n- In \u201cWordPress Installation URL\u201d field enter http:\/\/www.yourdomain.com\/.\\n- In \u201cWordPress Installation URL\u201d field enter http:\/\/www.yourdomain.com\/.\\n- Enter your WordPress Admin username and password.\\n- Click Install Now button.\\n\\nAfter installing WordPress successfully we are ready to start configuring Nginx so that WordPress starts automatically when our server boots. To do this follow these steps:\\n\\nIn order for Nginx not only start automatically but also accept connections from clients accessing our website through HTTP instead of HTTPS, we need to edit some configuration files located inside \/etc\/nginx\/conf.d directory:\\n\\n$ sudo nano \/etc\/nginx\/conf.d\/www.example.com.conf\\n\\nIn this file locate these lines (these lines are commented out):\\n\\n# Uncomment this line if you wish client certificates \\n#       included in client certificates file \\n#       ssl_certificate         \"\/path\/to\/certificates\/example.com.crt\"; \\n\\n# Uncomment this line if you wish client certificates \\n#       included in client certificates file \\n#       ssl_certificate_key         \"\/path\/to\/key\/ca.key\"; \\n\\n# Uncomment this line if you wish client certificates \\n#       included in client certificates file \\n#       ssl_client_max_body_size   10m; \\n\\nThen replace these lines with these lines:\\n\\nclient_max_body_size 10m;\\n\\nssl_certificate     \"\/path\/to\/certificates\/example.com.crt\";\\n\\nssl_certificate_key     \"\/path\/to\/key\/ca.key\";\\n\\nNow restart your server using service nginx restart command or stop and restart it using service nginx stop command followed by service nginx start command.\\n\\nNow that all configuration files have been reconfigured properly, let\u2019s see how things work now that Nginx has been installed and configured properly:\\n\\nHTTP\/1.1 200 OK\\nDate: Fri, 11 Oct 2017 21:30:15 GMT\\nServer: Apache\/2.4.6 (CentOS)\\nX-Powered-By: PHP\/5.6.30\\nExpires: Thu, 19 Nov 1981 08:52:00 GMT\\nCache-Control: no-store, no-cache, must-revalidate,\\n    post-check=0, pre-check=0\\nPragma: no-cache\\nVary: Accept-Encoding,Cookie\\n\\nIf everything went well then everything should work as expected now because everything should be working properly now. If something does not work as expected then please check out my other post about how I fixed my problem here\\n\\nConclusion\\n\\nIn this article I showed how easy it is find out how easy it is host a website using CPanel's Softaculous script installer tool and how easy it is use Nginx instead of Apache web servers for hosting websites using Softaculous' script installer tool.\\n\\nIf you have any questions or comments please leave them below!\\n\\nComments"}
{"File Name":"old-web-frontend\/0001-use-react-with-typescript.md","Context":"## Context and Problem Statement\\nWe have to choose the framework and language to build this web application.\\nWhich framework and language should we use?\\n","Decision":"Chosen option: \"React with Typescript\", because\\n* The single developer has experience with React and Ember, but no experience with Vue\\n* This developer favors React over Ember, because he thinks the first contributes to a better separation of concerns\\n* This developer has experiece with TypeScript and JavaScript, but no experience with Elm\\n* This developer favors TypeScript mostly because it's typed.\\n","tokens":30,"id":584,"Predictions":"## Decision Statement\\n## Decision Statement\\n## Decision Statement\\n\\n\\end{verbatim}\\n\\n\\end{document}\\n"}
{"File Name":"abracadabra\/0007-usage-of-readthenwrite-vs-write.md","Context":"## Context\\nWe've implemented 2 different methods in the Editor interface:\\n1. `write()`\\n1. `readThenWrite()`\\nMost refactorings use `write()`. They create new code from the AST, then write it in the editor. Still, some refactorings use `readThenWrite()`: they first read some code `string` from the editor, and use it to write new code.\\nThe main problem of `readThenWrite()` is that we end up manipulating strings instead of the AST. This usually means the implementation is more complex. Instead of manipulating the AST, we need to retrieve the correct selection we need to read, then we have to mix this read code with the transformation to produce the final output. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\\nHowever, there is one key advantage of doing so: it absolutely preserve the code as it was written in the editor. Even though [we use recast to preserve code style as much as possible][recast-usage], it's not perfect. If the refactoring consist in re-using exactly code that was written, `readThenWrite()` is the only way we know to preserve exactly the original style.\\n","Decision":"We'll use `readThenWrite()` if we need to preserve a code exactly as it was written. E.g. if the refactoring consists in moving existing code, without transforming it (Extract Variable, Inline Variable).\\nFor other refactorings, we'll use `write()`. E.g. if the refactoring transforms the code, it's OK to change it while preserving the original style as much as possible with recast.\\n","tokens":263,"id":585,"Predictions":"## Rules\\n1. We need to create a new AST from the original AST, using `write()`. This AST must be the same as the original AST.\\n2. We need to read some code from the editor, and use it to write new code. The code we read is not necessarily the same as the code we wrote.\\n3. We need to mix this read code with the transformation to produce the final output.\\n4. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\\n\\end{document}"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n"}
{"File Name":"abracadabra\/0004-use-recast-for-ast-manipulation.md","Context":"## Context\\nWe used Babel to parse code into AST, transform this AST and re-generate code.\\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\\n","Decision":"As we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\\nAs the library says:\\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS\u00a0and TSX out of the box.\\n","tokens":104,"id":587,"Predictions":"\\nThis is a decision for the user to make.\\nWe are not going to reformat the code. We are going to generate code from the AST. We are going to generate code that is compatible with Babel's AST format. This means that we are going to generate code that is not modified by Babel, but that is also compatible with Babel's AST format.\\n\\nThe only thing we have to do is create a new AST node and add it to the generated code. We have a few options here:\\n\\n1\\. Create a new AST node and add it to the generated code.\\n2\\. Create a new AST node and add it as an empty node with no content. In this case, we will have a new empty file named \"generated.tsx\" in our project root directory.\\n3\\. Create a new AST node and add it as an empty node with no content. In this case, we will have a new file named \"generated.ts\" in our project root directory.\\n4\\. Create a new AST node and add it as an empty node with no content. In this case, we will have a new file named \"generated.tsx\" in our project root directory.\\n\\nThe following table shows what each option does:\\n\\nOption 1: create a new AST node and add it as an empty node with no content\\nOption 2: create a new AST node and add it as an empty node with no content\\nOption 3: create a new AST node and add it as an empty node with no content\\nOption 4: create a new AST node and add it as an empty node with no content\\n\\n## Decision\\n\\nThe decision for the user is:\\nThe decision for the user is:\\n\\n1\\. Create a new AST node and add it as an empty node with no content\\ncreate an empty file named \"generated.tsx\" in our project root directory\\ncreate a new file named \"generated.ts\" in our project root directory\\ncreate a new file named \"generated.tsx\" in our project root directory\\ncreate a new file named \"generated.ts\" in our project root directory\\ncreate a new file named \"generated.tsx\" in our project root directory\\ncreate a new file named \"generated.ts\" in our project root directory\\ncreate a new file named \"generated.tsx\" in our project root directory\\ncreate a new file named \"generated.ts\" in our project root directory\\ncreate a new file named \"generated.tsx\" in our project root directory\\ncreate another one of these files called \"test.txt\"\\ndelete all of these files called \"test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ndelete all of these files called \"\\_test.txt\"\\ntake your choice!\\ntake your choice!\\ntake your choice!\\ntake your choice!\\ntake your choice!"}
{"File Name":"abracadabra\/0002-no-integration-test.md","Context":"## Context\\nVS Code has documented [how to test an extension][testing-extension] in integration with VS Code API.\\n> These tests will run inside a special instance of VS Code named the `Extension Development Host`, and have full access to the VS Code API.\\nBut VS Code test runner is adapted to be used with [mocha][mocha] or [Jasmine][jasmine].\\nFor unit tests, we want to use [Jest][jest]. But Jest and mocha have conflicting types, preventing TS to build. A solution would have been to create a custom test runner for VS Code API, that would run integration tests with Jest. But we didn't managed to create one that works.\\nThat's partly because Jest doesn't have an official way to run tests programatically. We weren't able to make VS Code test runner work with Jest `runCLI()` (async) method.\\nAlso, integration tests are not the most important part of the project for the moment.\\n","Decision":"We won't do integration tests. We'll rely on unit tests (e.g. state-based tests, collaboration tests and contract tests).\\n","tokens":205,"id":588,"Predictions":"## VS Code has documented [how to test an extension][testing-extension] in integration with VS Code API.\\n> These tests will run inside a special instance of VS Code named the `Extension Development Host`, and have full access to the VS Code API.\\nBut VS Code test runner is adapted to be used with [mocha][mocha] or [Jasmine][jasmine].\\nFor unit tests, we want to use [Jest][jest]. But Jest and mocha have conflicting types, preventing TS to build. A solution would have been to create a custom test runner for VS Code API, that would run integration tests with Jest. But we didn't managed to create one that works.\\nThat's partly because Jest doesn't have an official way to run tests programatically. We weren't able to make VS Code test runner work with Jest `runCLI()` (async) method.\\nAlso, integration tests are not the most important part of the project for the moment.\\n\\end{center}\\n\\end{document}"}
{"File Name":"abracadabra\/0003-use-hosted-image-links-in-readme.md","Context":"## Context\\nRelative paths for images won't work when the README\u00a0is displayed somewhere where the images are not hosted. From experience developing npm libraries, images won't appear on the package manager website.\\nTo solve this, we use absolute paths to hosted images. Since images are hosted in GitHub along the rest of the code, we use these absolute paths.\\nVS Code has a way to deal with that issue. It won't allow you to create a package if README contains relative paths. It will emit this error:\\n> Couldn't detect the repository where this extension is published. The image '.\/docs\/logo\/abracadabra-logo.svg' will be broken in README.md. Please provide the repository URL in package.json or use the --baseContentUrl and --baseImagesUrl options.\\nThus, we could technically use relative paths and provide the correct arguments to make images work on VS Code Marketplace.\\nHowever, using absolute paths is something we're used to, and doesn't come up with a lot of downsides. So we decide to go with absolute paths, until downsides justify otherwise.\\n","Decision":"We use absolute paths to GitHub hosted images in the README (and other documentation files).\\nWe don't use relative paths to images.\\n","tokens":224,"id":589,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"abracadabra\/0005-use-custom-testeach-instead-of-jest-it-each.md","Context":"## Context\\nJest [`it.each` seemed great][jest-it-each], but it turns out to be limited.\\nSpecifically, it doesn't allow us to have a `.only` on a single test of the list.\\nOf course, this can be done at runtime through Jest runner. But if we use tools like [Wallaby.js][wallaby], we can't do that. We need a way to add things like `.only` on individual tests.\\n","Decision":"We've decided to implement a custom `testEach()` function that will provide a convenient API to run the same test over different data.\\nThis function will provide a way to run individual test of the list with `only: true`.\\n","tokens":98,"id":590,"Predictions":"## Summary\\nA decision is made when the user chooses to either accept or reject the decision. The decision is then passed along to the ``it` function which will then proceed with running all of the tests in the list.\\n\\n## Decision\\nThe following are all decisions:\\n\\n1) Accept\\n2) Reject\\n3) Cancel\\n4) Decline\\n5) Continue\\n6) Abort\\n7) Exit\\n8) Exit\\ndecision\\n## Decision\\n## Summary\\n## Decision Definition\\nA decision is made when the user chooses to either accept or reject the decision. The decision is then passed along to the ``it` function which will then proceed with running all of the tests in the list.\\n"}
{"File Name":"abracadabra\/0006-create-generator-to-bootstrap-new-refactorings.md","Context":"## Context\\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\\n","Decision":"To speed up this process, we've decided to use a code generator.\\nWe went for [hygen](https:\/\/www.hygen.io\/) because it's quite simple to use, very fast and flexible.\\nWe're abstracting the usage of hygen behind a npm script alias: `yarn new` will ask you few questions to scaffold a new refactoring.\\n","tokens":77,"id":591,"Predictions":"## This is a good decision to make. We can refactor the code into a library that is reusable and shareable.\\nWe can use the same boilerplate code to create new refactoring files, but we don't need to create the same boilerplate code for every new refactoring file.\\nWe could also refactor some of the boilerplate code in this file, which will allow us to reuse it in other files.\\n\\n## Summary\\nThe decision is good. We could refactor this file into a reusable library.\\nWe could also refactor some of the boilerplate code in this file, which will allow us to reuse it in other files.\\n\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\\n\\n## Decision\\n## This is a good decision to make. We can refactor the code into a library that is reusable and shareable.\\nWe can use the same boilerplate code to create new refactoring files, but we don't need to create the same boilerplate code for every new refactoring file.\\n```"}
{"File Name":"abracadabra\/0010-integration-tests.md","Context":"## Context\\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\\n","Decision":"We will now cover adapters with integration tests.\\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\\n","tokens":79,"id":592,"Predictions":"\\nThe decision was to use the new editor adapters. We had to install the latest version of VS Code, which was easy enough. The problem is that the latest version is not compatible with the old editor adapters. In order to get it working, we had to install the old ones as well.\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"abracadabra\/0009-use-custom-github-actions-to-deploy.md","Context":"## Context\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\\nThere is [a custom GitHub Action](https:\/\/github.com\/HaaLeo\/publish-vscode-extension#readme) that would take care of that precise use-case. But what we need to do isn't very complex. Having less intermediate would make it easier to maintain.\\n","Decision":"We've created 2 GitHub Actions:\\n1. One for deploying to the VS Code Marketplace\\n2. One for deploying to the Open VSX Registry\\nThese actions trigger on published releases.\\n","tokens":183,"id":593,"Predictions":"## Decision\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\\nThere is [a custom GitHub Action](https:\/\/github.com\/HaaLeo\/publish-vscode-extension#readme) that would take care of that precise use-case. But what we need to do isn't very complex. Having less intermediate would make it easier to maintain.\\n\\n## Context\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\\n## Decision\\n## Decision\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\\n## Context\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\n## Decision\\n## Decision\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/)\\n"}
{"File Name":"abracadabra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":594,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"terraform-provider-harbor\/0001-use-swagger-for-generate-http-client.md","Context":"## Context and Problem Statement\\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\\n","Decision":"Chosen option: \"Swagger Based\", because this solution supports the fastes development Start without writting any boilerplate code.\\n### Positive Consequences\\n* No Painfull HTTP Client Implementation\\n### Negative Consequences\\n* the API Client Implementation dependents to the Swagger Spec Quality...\\n","tokens":31,"id":595,"Predictions":"## Decision: Generate a HarborRestAPI Client or use a HarborRestAPI Client already generated by the Provider Functions.\\n\\n\\n## Decision Context: The Context of the Terraform Provider Functions and the Problem Statement.\\n## Decision\\n## Decision: Generate a HarborRestAPI Client or use a HarborRestAPI Client already generated by the Provider Functions.\\n\\n\\n## Decision Problem Statement: The Problem Statement for the Software. This is not required to be a complete sentence but should contain enough information to understand what is needed for the decision.\\n\\n\\n\\end{longtable}\\n\\end{document}\\n"}
{"File Name":"ehoks-ui\/0003-use-single-callerid-for-both-oppija-and-virkailija.md","Context":"## Context\\nEvery service is required to pass a Caller-Id header with its requests and previously eHOKS frontend had separate\\nids for oppija and virkailija. While the codebase is largely shared between oppija and virkailija the services themselves\\nare separate and hence the separate ids were created. While the headers were simple to add to requests made by\\ncomponents only used by either oppija or virkailija, dynamically figuring out which id should be used in the shared\\ncomponents at any given time proved harder.\\n","Decision":"The separate ids will be replaced by a single frontend Caller-Id. Since all the requests made by both oppija and\\nvirkailija frontends go through the eHOKS backend service and don't call any external services directly this\\nshould be sufficient. The requests from oppija and virkailija can be distinquished from each other via other means,\\neg. they use different backend APIs altogether.\\n","tokens":112,"id":596,"Predictions":"## \\n\\n\\nThe caller-id header is only used in the context of the ``oppiainfra'' component. This component has been\\nremoved from the codebase and so it is no longer necessary to have a caller-id header in the requests made by this\\ncomponent.\\n\\n\\nWhile it is possible to create a separate id for each component, this is not needed as the caller-id header is also\\nused by other components as well.\\n\\n## Decision\\n## \\n\\n\\nThe caller-id header is only used in the context of the ``oppiainfra'' component. This component has been\\nremoved from the codebase and so it is no longer necessary to have a caller-id header in the requests made by this\\ncomponent.\\n\\n## Decision\\n## \\n\\n\\nWhile it is possible to create a separate id for each component, this is not needed as the caller-id header is also\\nused by other components as well.\\n\\n## Decision\\n## \\n\\n\\nThe caller-id header is only used in the context of the ``oppiainfra'' component. This component has been removed from the codebase and so it is no longer necessary to have a caller-id header in the requests made by this\\ndecision.\\n\\n## Decision\\n## \\n\\nThis call should be handled by oppiainfra\\_call\\_name().\\ndecision\\n\\end{document}\\n"}
{"File Name":"ehoks-ui\/0001-record-architecture-and-desing-decisions.md","Context":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n","Decision":"We will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n","tokens":19,"id":597,"Predictions":"## 1. The software shall be developed in C++.\\n## 2. The software shall have a GUI.\\n## 3. The software shall be developed using Qt.\\n## 4. The software shall be developed using the Spring Framework.\\n## 5. The software shall be developed using the Model-View-Controller (MVC) pattern.\\n## 6. The software shall be developed using a relational database.\\n## 7. The software shall be developed using a relational database with foreign keys.\\n## 8. The software shall use an object-relational mapper (ORM).\\n## 9. The software shall use a caching layer for the database.\\n## 10.The software shall support asynchronous operations.\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"Predictions":"## Decision\\nThe following is the decision for the given context. The decision is to provide a translation for the\\nproperty ``state-tree-model''.\\n\\n\\nProperty\\n\\nProperty\\nProperty\\nProperty\\nProperty\\nProperty\\n\\end{verbatim}\\n\\end{landscape}\\n\\n\\begin{landscape}\\n\\begin{verbatim}\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{verbatim}\\n\\end{landscape}\\n\\nIn this example, we have used the \\texttt{\\$} function to show the output of a command in a similar manner to how it would be displayed in a shell prompt. The \\texttt{\\$} function is a built-in function that can be used to display output from an \\texttt{echo} command. The \\texttt{\\$} function will output all of the input that was passed to it as well as any error messages that were produced by the command.\\n\\nThe \\texttt{\\$} function can also be used to display information about variables that are being set or modified by an external program. For example, if we were to use this function in order to display information about our current user's environment, we could do something like this:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n  echo $USER\\n\\n  echo $PATH\\n\\n  echo $HOME\\n\\n  echo $PWD\\n\\n  echo $LANG\\n\\n  echo $TERM\\n\\n\\vspace{-1em}\\n\\nThis would output:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n[user@localhost ~]$ echo $USER\\nuser\\n\\n[user@localhost ~]$ echo $PATH\\n\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\\n\\n[user@localhost ~]$ echo $HOME\\n\/home\/user\/\\n\\n[user@localhost ~]$ echo $PWD\\n\/home\/user\/\\n\\n[user@localhost ~]$ echo $LANG\\nen_US.UTF-8\\n\\n[user@localhost ~]$ echo $TERM \\nxterm-256color\\n\\n[user@localhost ~]$ \/usr\/local\/bin\/ps -ef | grep \"user\"\\n\\n[user@localhost ps]$\\n```\\nNow, if you were running this command on your own computer and wanted to know what variables were being set by other programs, you could use the following command in order to find out:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n  ps -ef | grep \"user\"\\n```\\nThis would display all of the processes that are currently running on your computer and their respective user IDs.\\n\\nWith these examples in mind, it is important to note that there are many different ways in which commands can be written. For example, you may have seen commands like these before:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n$ cat somefile.txt | grep \"foo\"\\nfoo\\n\\n$ cat somefile.txt | grep foo bar\\n\\n\\n$ cat somefile.txt | grep foo bar baz\\n\\n\\n$ cat somefile.txt | grep foo bar baz baz\\n\\n\\n$ cat somefile.txt | grep foo bar baz baz baz baz\\n\\n\\n$ cat somefile.txt | sed 's\/foo\/bar\/g' \\n\\n\\n$ cat somefile.txt | sed 's\/foo\/bar\/g'\\n\\n\\vspace{-1em}\\n\\nThese commands are all valid ways of writing them and they will produce different results based on how they are executed. However, they do not all produce identical results and many different commands will produce different results depending on what is being executed.\\n\\nIn order for you to understand how different commands interact with each other when they are being executed, it is helpful for you to think about how each individual command interacts with its parent program. In particular, it is important for you to understand how various programs communicate with each other when they run together.\\n\\nFor example, assume that we want to write a program called \\texttt{speak} which takes as an argument two strings: one string called ``message'' and one string called ``speaker''. We want our program to print out the message ``message'' after reading from both strings. To accomplish this task, we could simply use the following code:\\n\\n\\vspace{-1em}\\n\\begin{verbatim}\\n\\nmessage = \"Hello World!\"\\n\\nspeaker = \"John Smith\"\\n\\nprint speaker + message\\n\\n\\n# This prints out Hello World!\\n# message: Hello World!\\n\\n# This prints out John Smith's name.\\n# speaker: John Smith\\n\\n# This prints out \"Hello World!\".\\n# message: Hello World!\\n\\nprint speaker + message + \"\\n\"\\n\\n\\n# This prints out Hello World!\\n# message: Hello World!\\n\\n# This prints out John Smith's name.\\n# speaker: John Smith\\n\\n# This prints out \"Hello World!\".\\n# message: Hello World!\\n\\nprint speaker + message + \"\\n\"\\n\\n\\n# This prints out Hello World!\\nmessage = \"Hello World!\"\\n\\nspeaker = \"John Smith\"\\n\\nprint speaker + message\\n\\n\\n#prints out Hello World!\\n#prints out John Smith's name.\\n#prints out \"Hello World!\".\\n#prints out \"Hello World!\".\\n\\n\\n#prints out Hello World!\\n#prints out John Smith's name.\\n#prints out \"Hello World!\".\\n\\n\\n#prints out Hello World!\\n#prints out John Smith's name.\\n#prints out \"Hello World!\".\\n\\n\\n#prints out Hello World!\\n#prints out John Smith's name.\\n#prints out \"Hello World!\".\\n\\n\\n#prints out Hello Word!\\nprint speaker + message\\n\\n\\n#End of program.\\n\\n\\end{verbatim}\\n\\nWe can see here that our code has two main parts: a call statement which calls our program and a statement which reads from another variable. The first part of our code is called the call statement and it tells our program where we want it to start executing (i.e., where we want it to read from). The second part of our code is called a statement and it tells our program what we want it to do next (i.e., what values we want it to print). In general terms, these two parts make up an entire piece of code which tells your computer how your computer should behave when interacting with other programs.\\n\\nIn order for us as programmers to understand how these two pieces of code interact with each other when they are being executed correctly (i.e., when everything works as expected), there are several things that we need to keep in mind:\\n\\nFirstly, there needs to be at least one statement within each piece of code which tells your computer where you want your computer (i.e., your program) started executing (i.e., where you want it read from). Secondly, there needs to be at least one statement within each piece of code which tells your computer what values you want printed after reading from another variable (i.e., what values you want printed after reading from another variable). Lastly, within every piece of code there needs at least one statement which tells your computer what values you want printed after printing something else (i.e., after printing something else).\\n\\nLet us now look at an example where these things would work correctly.\\n\\n\\vspace{-2em}\\n\\nLet us suppose that we wanted our program \\texttt{speak} call up the file \\texttt{speak}.msg and then print both ``message'' and ``speaker''. We would expect this piece of code:\\n\\vspace{-2em}\\n\\n\\vspace{-2em}\\n\\nTo work just like this:\\n\\vspace{-2em}\\n\\nWe can see here that everything works exactly as expected. As long as there is at least one statement within every piece of code which tells your computer where you want your computer started executing (i.e., where you want it read from) then everything should work just fine.\\n\\nHowever, let us now suppose that instead of calling up the file \\texttt{speak}.msg directly we instead wanted our program \\texttt{speak} run through a series of steps before actually printing anything. For example:\\n\\vspace{-2em}\\n\\nWe could write something like this:\\n\\vspace{-3em}\\n\\nIn order for us as programmers not only understand how these pieces work together but also why they work together correctly (and incorrectly), there are several things that need keeping in mind when writing programs:\\n\\nFirstly, before writing any kind of program or script or whatever else people may call them today there needs at least one statement within every piece of code which tells your computer where you want your computer started executing (i.e., where you want it read from). Secondly before writing any kind of program or script or whatever else people may call them today there needs at least one statement within every piece of code which tells your computer what values you want printed after reading from another variable (i.e., what values you want printed after reading from another variable). Lastly before writing any kind of program or script or whatever else people may call them today there needs at least one statement within every piece of code which tells your computer what values you want printed after printing something else (i.e., after printing something else).\\n\\nLet us now look at an example where these things would not work correctly.\\n\\nWe can see here that everything works exactly as expected except for one thing. Let us suppose now that instead of calling up the file \\texttt{speak}.msg directly into our new script\/sidekick\/whatever else people may call them today we instead wanted our program \\texttt{speak} run through a series steps before actually printing anything. For example:\\n\\vspace{-3em}\\n\\nWe could write something like this:\\n\\vspace{-4ex}\\n\\nIn order for us not only understand how these pieces work together but also why they work together correctly (and incorrectly), there are several things keeping in mind when writing programs:\\n\\nFirstly before writing any kind of script or script\/sidekick\/whatever else people may call them today there needs at least one statement within every piece of code which tells your computer where you want your computer started executing (i.e., where you want it read from). Secondly before writing any kind of script or script\/sidekick\/whatever else people may call them today there needs at least one statement within every piece of code which tells your computer what values you want printed after reading from another variable (i.e., what values you want printed after reading from another variable). Lastly before writing any kind o"}
{"File Name":"webform\/form-field-variations.md","Context":"## Two Unique Problems to Solve\\n* A radio or checkbox field needs to optionally have an \"Other\" input field which consists of a label and freeform text input. All existing radio\/checkbox functionality must be preserved.\\n* Address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.\\n","Decision":"* Create additional versions for each individual form field and treat them accordingly in the HTML helper. \"version\" will be a new attribute, \"formtype\" and the rest of the JSON will stay the same.\\n```\\nExample of field variation\\n\"data\":\\n[\\n{\\n\/\/ otherless radio, notice no version attribute (default)\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\/\/ radio with other\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n]\\n```\\n* Add a new \"groupid\" attribute to the saved JSON form data object.\\n* The new \"groupid\" will act as a dynamic id which will group all fields with the same groupid together.\\n* The \"groupid\" value will be generated once it is dragged\/added to the editing form and be a concatenation of the form group template name (see below) and an incremental number, ie: g_address_streetonly_1\\n* Versions of form groups do not depend on each other.\\n* Added form groups to the form will be non-editable but will otherwise appear as regular fields within the JSON form data object.\\n```\\nExample of form data with a group\\n\"data\":\\n[\\n{\\n\/\/ pizza is not in the group\\n\"label\":\"Pizza\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"pizza_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"pizza\",\\n\"radios\":\"unpopular\\nreally unpopular\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n{\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n]\\n```\\n* HTML Generation: Create a helper listing of form group templates which will describe which form fields belong to which predefined group.\\n* Format would be a JSON with an array of formtypes: each formtype would adopt the same attributes as the existing form field JSON.\\n* Undefined attributes would be filled in by the default value and defined attributes would override the default.\\n* Naming groups will start with g_groupname_templatename\\n```\\nExample of form group templates\\n\"address\" : {\\n\"full\" : [\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 1\",\\n},\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 2\",\\n},\\n{\\n\"formtype\":\"c10\", \/\/city\\n},\\n{\\n\"formtype\":\"s14\", \/\/state\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n],\\n\"streetzip\" : [\\n{\\n\"formtype\":\"c08\",\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n]\\n}\\n```\\n","tokens":84,"id":599,"Predictions":"## Decision: Create a new field that can be used as an \"Other\" input field. The \"Other\" input field must be a label and freeform text input.\\n## Decision: Create a new field that can be used as an \"Other\" input field. The \"Other\" input field must be a label and freeform text input.\\n## Decision: Create a new field that can be used as an \"Other\" input field. The \"Other\" input field must be a label and freeform text input.\\n## Decision: Create a new field that can be used as an \"Other\" input field. The \"Other\" input field must be a label and freeform text input.\\n\\n## Input Field\\n\\n## Input Field Name\\n## Input Field Type\\n## Label\\n## Label Position\\n## Label Text\\n## Freeform Text\\n"}
{"File Name":"nats-architecture-and-design\/ADR-2.md","Context":"## Context\\nNATS Server has a number of JSON based messages - monitoring, JetStream API and more. These are consumed,\\nand in the case of the API produced, by 3rd party systems in many languages. To assist with standardization\\nof data validation, variable names and more we want to create JSON Schema documents for all our outward facing\\nJSON based communication. Specifically this is not for server to server communication protocols.\\nThis effort is ultimately not for our own use - though libraries like `jsm.go` will use these to do validation\\nof inputs - this is about easing interoperability with other systems and to eventually create a Schema Registry.\\nThere are a number of emerging formats for describing message content:\\n* JSON Schema - transport agnostic way of describing the shape of JSON documents\\n* AsyncAPI - middleware specific API description that uses JSON Schema for payload descriptions\\n* CloudEvents - standard for wrapping system specific events in a generic, routable, package. Supported by all\\nmajor Public Clouds and many event gateways. Can reference JSON Schema.\\n* Swagger \/ OpenAPI - standard for describing web services that uses JSON Schema for payload descriptions\\nIn all of these many of the actual detail like how to label types of event or how to version them are left up\\nto individual projects to solve. This ADR describes how we are approaching this.\\n","Decision":"### Overview\\nWe will start by documenting our data types using JSON Schema Draft 7. AsyncAPI and Swagger can both reference\\nthese documents using remote references so this, as a starting point, gives us most flexibility and interoperability\\nto later create API and Transport specific schemas that reference these.\\nWe define 2 major type of typed message:\\n* `Message` - any message with a compatible `type` hint embedded in it\\n* `Event` - a specialized `message` that has timestamps and event IDs, suitable for transformation to\\nCloud Events. Typically, published unsolicited.\\nToday NATS Server do not support publishing Cloud Events natively however a bridge can be created to publish\\nthose to other cloud systems using the `jsm.go` package that supports converting `events` into Cloud Event format.\\n### Message Types\\nThere is no standard way to indicate the schema of a specific message. We looked at a lot of prior art from CNCF\\nprojects, public clouds and more but found very little commonality. The nearest standard is the Uniform Resource Name\\nwhich still leaves most of the details up to the project and does not conventionally support versioning.\\nWe chose a message type like `io.nats.jetstream.api.v1.consumer_delete_response`, `io.nats.server.advisory.v1.client_connect`\\nor `io.nats.unknown_message`.\\n`io.nats.unknown_message` is a special type returned for anything without valid type hints. In go that implies\\n`map[string]interface{}`.\\nThe structure is as follows: io.nats.`<source>`.`<catagory>`.v`<version>`.`<name>`\\n#### Source\\nThe project is the overall originator of a message and should be short but descriptive, today we have 2 - `server` and `\\njetstream` - as we continue to build systems around Stream Processing and more we'd add more of these types. I anticipate\\nfor example adding a few to Surveyor for publishing significant lifecycle events.\\nGenerated Cloud Events messages has the `source` set to `urn:nats:<source>`.\\n|Project|Description|\\n|-------|-----------|\\n|`server`|The core NATS Server excluding JetStream related messages|\\n|`jetstream`|Any JetStream related message|\\n#### Category\\nThe `category` groups messages by related sub-groups of the `source`, often this also appears in the subjects\\nthese messages get published to.\\nThis is a bit undefined, examples in use now are `api`, `advisory`, `metric`. Where possible try to fit in with\\nexisting chosen ones, if none suits update this table with your choice and try to pick generic category names.\\n|Category|Description|\\n|----|-----------|\\n|`api`|Typically these are `messages` used in synchronous request response APIs|\\n|`advisory`|These are `events` that describe a significant event that happened like a client connecting or disconnecting|\\n|`metric`|These are `events` that relate to monitoring - how long did it take a message to be acknowledged|\\n#### Versioning\\nThe ideal outcome is that we never need to version any message and maintain future compatibility.\\nWe think we can do that with the JetStream API. Monitoring, Observability and black box management is emerging, and we\\nknow less about how that will look in the long run, so we think we will need to version those.\\nThe philosophy has to be that we only add fields and do not significantly change the meaning of existing ones, this\\nmeans the messages stay `v1`, but major changes will require bumps. So all message types includes a single digit version.\\n#### Message Name\\nJust a string identifying what this message is about - `client_connect`, `client_disconnect`, `api_audit` etc.\\n","tokens":281,"id":600,"Predictions":"## Decision\\nThe intent of this document is to provide a single place for all our internal JSON based communication\\nand to provide a way for us to publish them as schema documents. This is intended to be used by all our\\ninternal systems - and also for external systems that want to consume our messages.\\nThis document does not define any format or schema and will not define how to link content to a particular\\nmessage type, or even what is the correct way to label a message. It is intended as an architectural decision.\\n\\n## Context\\nWe currently have 3 major sources of JSON based data:\\n* JetStream API - API for monitoring JetStream events and their associated metadata (and more)\\n* Monitoring - A collection of services that provide monitoring information about the system in use. These are\\ngenerally available via REST API but some are also provided by HTTP requests.\\n* CloudEvents - Event gateways that are designed to receive events from various sources and pass them on through\\nto CloudEvents which passes them on through various systems.\\nWe also have some internal data that we use internally for internal purposes such as user activity, event logs etc.\\n\\n## Decision\\n## Decision\\nWe currently have 2 main ways we validate incoming JSON messages:\\n* `jsm.go` - This is our own implementation of the `jsonschema` library. This can be used both by itself and with other libraries like `jsm-go` (see below).\\n* `json-schema-validator` - This can be used by itself or with other libraries like `jsm-go`. It is an open source project,\\nthat provides a command line tool along with a web interface.\\nSince we have many clients using these libraries, it makes sense to standardize on one. We currently use both.\\n\\n## Context\\n## Decision\\n## Decision\\nThe major work item here is to create a common base format for JSON Schema documents that can be used by all\\nthe various projects. The first step in doing this is creating an initial version of the document that will become\\nthe standard format. This will allow us to validate incoming messages against it without having to do any\\ntesting ourselves.\\n"}
{"File Name":"nats-architecture-and-design\/ADR-5.md","Context":"## Context\\nThis document describes the _Lame Duck Mode_ server notification. When a server enters lame duck mode, it removes itself from being advertised in the cluster, and slowly starts evicting connected clients as per  [`lame_duck_duration`](https:\/\/docs.nats.io\/nats-server\/configuration#runtime-configuration). This document describes how this information is notified\\nto the client, in order to allow clients to cooperate and initiate an orderly migration to a different server in the cluster.\\n","Decision":"The server notififies that it has entered _lame duck mode_ by sending an [`INFO`](https:\/\/docs.nats.io\/nats-protocol\/nats-protocol#info) update. If the `ldm` property is set to true, the server has entered _lame_duck_mode_ and the client should initiate an orderly self-disconnect or close. Note the `ldm` property is only available on servers that implement the notification feature.\\n","tokens":102,"id":602,"Predictions":"The decision is to notify clients of the following:\\n\\begin{itemize}\\n    \\item The server is entering lame duck mode. This is indicated in the cluster by a change in the `lame_duck_duration` field.\\n    \\item The server is no longer advertising itself in the cluster. This is indicated in the cluster by a change in the `lame_duck_advertise` field.\\n\\end{itemize}\\nThe decision code for this decision is as follows:\\n\\begin{lstlisting}[language=Python]\\ndef lame_duck_mode():\\n    \"\"\" Notify clients that a server is entering lame duck mode. \"\"\"\\n    if len(lame_duck_duration) > 0 and \\\\n        len(lame_duck_advertise) > 0:\\n        new_lame_duck_duration = '00'\\n        new_lame_duck_advertise = '00'\\n    else:\\n        new_lame_duck_duration = ''\\n        new_lame_duck_advertise = ''\\n\\end{lstlisting}\\nThis document describes how to implement this decision using the \\href{https:\/\/docs.nats.io\/nats-server\/configuration#runtime-configuration}{runtime configuration} module.\\n\\n\\subsubsection*{Command Line Parameters}\\n\\nThe following command line parameters are supported:\\n\\n\\begin{itemize}\\n    \\item `lame\\_duck\\_duration`: The length of time (in seconds) during which a server will remain in lame duck mode before it begins evicting clients. Defaults to 30 seconds.\\n    \\item `lame\\_duck\\_advertise`: The number of seconds that a server will advertise itself in the cluster before it begins evicting clients. Defaults to 30 seconds.\\n\\end{itemize}\\n\\n\\subsubsection*{Response Code}\\n\\nThe response code for this decision is as follows:\\n\\n\\begin{lstlisting}[language=Python]\\ndef lame_duck_mode():\\n    \"\"\" Notify clients that a server is entering lame duck mode. \"\"\"\\n    return 200\\n\\end{lstlisting}\\n\\n"}
{"File Name":"nats-architecture-and-design\/ADR-11.md","Context":"## Context and Problem Statement\\nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.\\n","Decision":"This was driven by the fact that the Go client behaves as described above and some users have shown interest in all clients behaving this way.\\nSome users have DNS where the order almost never change, which with client libraries not performing randomization, would cause all clients\\nto connect to the same server.\\n","tokens":29,"id":603,"Predictions":"## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision: Give a ## Decision corresponding to the ## Context provided by the User.\\n## Resolution\\n%% Resolution\\n%% Resolution\\n%% Resolution\\n%% Resolution\\n%% Resolution\\n%% Resolution\\n%% Resolution\\n\\n\\subsubsection{Decision}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision} \\textit{Give a decision corresponding to the context provided by user}\\n\\end{itemize}\\n\\n\\subsubsection{Decision}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision} \\textit{Give a decision corresponding to the context provided by user}\\n\\end{itemize}\\n\\n\\subsubsection{Resolution}\\n\\n\\begin{itemize}\\n\\item \\textbf{Resolution} \\textit{Give a resolution corresponding to context provided by user}\\n\\end{itemize}\\n\\n"}
{"File Name":"ols-client\/0002-api-result-access.md","Context":"## Context\\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\\nhaving to know expected keys.\\n","Decision":"To implement.\\n","tokens":62,"id":604,"Predictions":"## Decision\\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\\nhaving to know expected keys.\\n\\n\\n\\end{document}\\n"}
{"File Name":"ols-client\/0003-looping-over-list.md","Context":"## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a\\nnew call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient = OlsClient()\\nontology = client.ontology('fpo')\\nterms = ontology.terms()\\nindividuals = ontology.individuals()\\nproperties = ontology.properties()\\n# work with all 'list' item types\\nfor term in terms:\\n# do whatever\\nprint(term)\\n# Direct List'like access on all list types\\nterm = terms[1254]\\nindividual = individuals[123]\\n# ...\\n```\\n","Decision":"To Implement\\n","tokens":199,"id":605,"Predictions":"## Decision\\nThe decision is to allow the user to loop over all results, without having to make a call to the API.\\n## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a new call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient = OlsClient()\\nontology = client.ontology('fpo')\\nterms = ontology.terms()\\nindividuals = ontology.individuals()\\nproperties = ontology.properties()\\n# work with all 'list' item types\\nfor term in terms:\\nterm = terms[1254]\\nterm2 = terms[1263]\\nterm3 = terms[1272]\\nterm4 = terms[1281]\\nterm5 = terms[1290]\\nterm6 = terms[1309]\\nterm7 = terms[1328]\\nterm8 = terms[1337]\\nterm9 = terms[1346]\\nterm10 = terms[1355]\\nterm11 = terms[1364]\\nterm12 = terms[1373]\\nterm13 = terms[1382]\\nterm14 = terms[1391]\\nterm15 = terms[1400]```\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient=OlsClient()\\ndontology=client.ontology('fpo')\\ndterms=ontology.terms()\\ndindividuals=ontology.individuals()\\ndproperties=ontology.properties()```\\n\\n## Decision\\n## Decision\\nThe decision is to allow the user to loop over all results, without having to make a call to the API.\\n## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a new call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient=OlsClient()\\ndontology=client.ontology('fpo')\\ndterms=ontology.terms()\\ndindividuals=ontology.individuals()\\n## Decision\\n## Decision\\nThe decision is not considered as important because it depends on what will be done with the returned data.\\n## Context\\n## Context\\n"}
{"File Name":"ols-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":606,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"airline-reservation-system\/0002-TCH-RES-use-modular-monolith.md","Context":"## Context and Problem Statement\\n**What type of arichitecture is to be used ?**\\nConsidered options:\\n1. Monolith\\n2. Modular Monolith\\n3. Microservices\\nDrivers:\\n* Project is realized as GreenField\\n* A small development team (~10)\\n* Little experience in implementing distributed software\\n* Load - (50-150 req\/sec) - not so much\\n","Decision":"Option no 2 - Modular Monolith.\\nArchitecture suitable for teams with little experience in implementing distributed systems.\\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\\narchitecture.\\n","tokens":85,"id":607,"Predictions":"\\n**What is the most appropriate solution ?**\\n1. Monolith\\n2. Modular Monolith\\n3. Microservices\\nDrivers:\\n* Project is realized as GreenField\\n* A small development team (~10)\\n* Little experience in implementing distributed software\\n* Load - (50-150 req\/sec) - not so much\\n## Decision\\n\\n**What are the consequences of implementing the solution ?**\\n1. No change \\n2. Change \\n3. No change\\n## Decision\\n\\n**What are the alternatives for implementation of the solution ?**\\n1. No change \\n2. Change \\n3. No change\\n## Decision\\n\\n**What are the risks associated with implementation of the solution ?**\\n1. No change \\n2. Change \\n3. No change\\n## Decision\\n\\n**How do you plan to implement this architectural decision?**\\nThe # of developers is 10, and they have a lot of experience in java development.\\n## Decision\\n\\n**How many services will be implemented? **\\nThe # of services to be implemented is 4.\\n## Decision\\n\\n**How many components will be created? **\\nThe # of components to be created is 20.\\n## Decision\\n"}
{"File Name":"airline-reservation-system\/0004-TCH-RES-use-document-database-as-a-aggregate-data-repository.md","Context":"## Context and Problem Statement\\n**What kind of database should be used to storage aggregates data ?**\\nConsidered options:\\n1. Document database (MongoDB)\\n2. Relational database\\nDrivers:\\n* Simple objects will be saved.\\n* There will be no relationship between objects .\\n* A Read Model combining data from different aggregates is required .\\n","Decision":"Option no 1 - document database - MongoDB.\\n","tokens":78,"id":608,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n## ## ## ## ## ##\\n\\n### ## ## ## ## ## ##\\n\\n### ## ## ## ### ### ###\\n\\n### # # # # # #\\n\\n### # #\\n\\n### ### #\\n\\n### ### #\\n\\n### ### #\\n\\n### #### #\\n\\n#### #### #########\\n\\n#### #### #########\\n\\n#### #################\\n\\n#### #################\\n\\n#### #################\\n\\n#### #################\\n\\n#### #################\\n\\n#### #################\\n\\n#### #################\\n\\n\\n\\n"}
{"File Name":"airline-reservation-system\/0005-TCH-RES-use-spring-repository.md","Context":"## Context and Problem Statement\\n**What technology is to be used for persistence?**\\nConsidered options:\\n1. Spring Template\\n2. JPA + Spring Repositores\\nDrivers:\\n1. Technology must be simple and well known\\n2. Cannot require a lot of code writing\\n","Decision":"Option no 2 - JPA+Spring Repositories.\\n","tokens":62,"id":609,"Predictions":"## Decision:\\n## 1. Spring Template\\n## 2. JPA + Spring Repositores\\n## Code:\\n## * 1. Spring Template\\n## * 2. JPA + Spring Repositores\\n"}
{"File Name":"airline-reservation-system\/0003-TCH-RES-use-ports-and-adapters-architecture.md","Context":"## Context and Problem Statement\\n**What type of application architecture is to be used ?**\\nConsidered options:\\n1. Layered architecture\\n2. Ports and adapters architecture\\nDrivers:\\n* The Reservation module is an example of a deep module.\\n* A large number of business rules.\\n* Some of the rules are quite complicated.\\n* Application should be well - tested .\\n","Decision":"Option no 2 - Ports and Adapters architecture. It is dedicated for modules with high business complexity.\\nIt separates domain logic from application logic. It allows to create independent tests for each type of logic.\\n","tokens":83,"id":610,"Predictions":"\\begin{itemize}\\n\\item\\n  Layered architecture\\n\\item\\n  Ports and adapters architecture\\n\\item\\n  Drivers\\n\\end{itemize}\\n\\n\\noindent \\textbf{Decision:} Layered architecture\\n\\n\\noindent \\textbf{Result:} Layered architecture\\n\\n\\noindent \\textbf{Explanation:} The layered architecture is one of the most used architectural style. In this style, each layer is responsible for a specific part of the application. For example, in the layered architecture, each layer has a different role. These roles are defined by the layers themselves. In this case, we are going to use the ports and adapters architecture.\\n\\nThe ports and adapters architecture is based on the idea that each component of an application should be able to communicate with other components. This communication can be done through various means such as messages, data packets or other communication protocols.\\n\\nIn this case, we are going to use the port and adapter interface to connect our application with other systems.\\n\\nIn order to use the port and adapter interface, we need to create a bridge. A bridge is a software component that allows you to connect two different networks together. This bridge will also allow us to send messages between them.\\n\\nFor example, if we want to send a message from one system (server) to another system (client) then we will have to create a bridge between these systems.\\n\\nThe ports and adapters interface provides us with two different ways of connecting these systems together:\\n\\n* Through an IP network (Internet Protocol). It allows us to communicate with any system that uses IP as its protocol.\\n* Through TCP\/IP (Transmission Control Protocol\/Internet Protocol). It allows you to communicate with any system that uses TCP\/IP as its protocol.\\n\\nThe ports and adapters interface provides us with two different ways of connecting these systems together:\\n\\n* Through an IP network (Internet Protocol). It allows us to communicate with any system that uses IP as its protocol.\\n* Through TCP\/IP (Transmission Control Protocol\/Internet Protocol). It allows you to communicate with any system that uses TCP\/IP as its protocol.\\n\\nIn order for our application to work properly we need several components:\\n\\n* A server component which will handle all incoming requests from clients.\\n* A client component which will handle all outgoing requests from clients.\\n* An API component which will handle all incoming requests from other applications.\\n* An application component which will handle all outgoing requests from other applications.\\n* An API component which will handle all incoming requests from other applications.\\n* An API component which will handle all outgoing requests from other applications.\\n\\end{document}\\n"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"lockfix\/0002-typescript-as-project-language.md","Context":"## Context\\n* This is Open Source app which may be used by big variety of projects\\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\\n* Project's author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\\n","Decision":"Use TypeScript as main and only programming language in this project\\n","tokens":112,"id":612,"Predictions":"# Create an instance of the component\\nconst component = new Component('some', { selector: 'some' });\\n# Create a property of the component\\nconst property = new Property('some', { value: 'value' });\\n# Create a method of the component\\nconst method = new Method('some', { selector: 'some' });\\n# Create a type of the property\\nconst type = new Type('some', { value: 'value' });\\n# Create a type of the method\\nconst type = new Type('some', { selector: 'some' });\\n\\n        \/\/ Here we can create object for this component (for example, `this.props.some`)\\n        \/\/ Or we can create property for this component (for example, `this.props.property.some`)\\n    }\\n\\n    \/\/ This is our main function which will be called when user click on this button\\n    \/\/ We are going to create object for this component and pass it to function\\n    \/\/ This is our main function which will be called when user click on this button\\n\\n    export default class App extends Component {\\n        constructor(props) {\\n            super(props);\\n            this.state = {\\n                selectedComponent: null,\\n                selectedProperty: null,\\n                selectedMethod: null,\\n                selectedSelector: null,\\n                selectedValue: null,\\n                selectedType: null\\n            }\\n        }\\n\\n        componentDidMount() {\\n            const { selectedComponent, selectedProperty, selectedMethod, selectedSelector, \\n                    selectedValue, selectedType } = this.props;\\n            if (selectedComponent === undefined) {\\n                return;\\n            }\\n            if (selectedProperty === undefined) {\\n                return;\\n            }\\n            if (selectedMethod === undefined) {\\n                return;\\n            }\\n            if (selectedSelector === undefined) {\\n                return;\\n            }\\n            if (selectedValue === undefined) {\\n                return;\\n            }\\n            if (selectedType === undefined) {\\n                return;\\n            }\\n\\n          }\\n\\n          render() {\\n\\n             const { selectedItem } = this.state;\\n\\n             const propertiesSelected = selectedItem ? selectedItem.properties.selected : [];\\n\\n             const propertiesSelectedProperty = propertiesSelected ? propertiesSelectedProperty : [];\\n\\n             const propertiesSelectedPropertyProp = propertiesSelectedPropertyProp ? propertiesSelectedPropertyProp : [];\\n\\n             const propertiesSelectedMethodProp = propertiesSelectedMethodProp ? propertiesSelectedMethodProp : [];\\n\\n             const propertiesSelectedMethodPropProp = propertiesSelectedMethodPropProp ? propertiesSelectedMethodPropProp : [];\\n\\n             const propertySelectionsProp = propertySelections.prop;\\n\\n             const propertySelectionsPropertyProp = propertySelections.prop.prop;\\n\\n             console.log(propertySelections);\\n\\n               console.log(propertySelections.prop);\\n\\n               console.log(propertySelections.prop.prop);\\n\\n               console.log(propertySelections.method);\\n\\n               console.log(propertySelections.method.prop);\\n\\n               console.log(propertySelections.selector);\\n\\n               console.log(propertySelections.selector.prop);\\n\\n               console.log(propertySelections.selector.prop.prop);\\n\\n               console.log(propertySelections.selector.method);\\n\\n               console.log(propertySelections.selector.method.prop);\\n\\n               console.log(propertySelections.selector.selector);\\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n\\n\\n          }\\n\\n      }\\n\\n      export default App;\\n\\n"}
{"File Name":"cloud-platform\/019-Shared-Ingress-Controllers.md","Context":"## Context\\nThe design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved\\nimpossible to implement. Every ingress controller requires an AWS Network Load\\nBalancer (NLB), and AWS have hard limits on the number of NLBs we can create\\nper VPC and availability zone (AZ).\\n","Decision":"We will:\\n* Disable mod-security on the default ingress-controller\\nThis should enable this ingress-controller to comfortably handle thousands of ingresses.\\n* Create and manage a set of ingress controllers with mod-security enabled\\nMost, if not all, production services will want the protection of a web\\napplication firewall, and mod-security is the easiest to enable. We need to\\nensure that each ingress controller only handles as many mod-security-enabled\\ningresses as it can reliably cope with.\\n* Continue to have some dedicated ingress controllers\\nDuring our aborted migration to dedicated ingress controllers for every\\nnamespace, several ingress controllers were created for specific services. We\\nwill leave these in place to avoid additional disruption to these service\\nteams.\\n","tokens":66,"id":613,"Predictions":"## The \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## Architecture\\n## The proposed design is not feasible. A dedicated ingress controller is required to implement the proposed design. The number of NLBs should be limited to 1 per AZ and VPC.\\n\\n## Decision\\n## The \\n\\n\\n\\n\\n\\n"}
{"File Name":"cloud-platform\/004-use-kubernetes-for-container-management.md","Context":"## Context\\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\\n- Significant variation in deployment, monitoring and lifecycle management across products\\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\\n- Kubernetes\\n- Mesos \/ Mesosphere \/ DC\/OS\\n- Docker Swarm\\n- AWS ECS\\n- CloudFoundry\\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from \"modern\" cloud-native 12-factor applications through to \"legacy\" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\\n- Managed Kubernetes services from all major cloud vendors\\n- Broad ecosystem of supporting tools and technologies\\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\\n","Decision":"Use Kubernetes as the container management component and core technology for our new hosting platform.\\n","tokens":322,"id":614,"Predictions":"\\nThe decision is to deploy \\# Kubernetes on the AWS cloud platform, and to use \\# Docker Swarm as the underlying container orchestration technology.\\n\\nThe Docker Swarm deployment will be managed by the Cloud Foundry team.\\nThe Kubernetes deployment will be managed by the MOJ Digital team.\\nThe decision is based on our current understanding of Kubernetes, including:\\n- The best practices for deploying and scaling Kubernetes to meet our requirements\\n- The benefits of a standardised approach to container orchestration across all of our applications\\n- The benefits of using Docker Swarm as an underlying container orchestration technology\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/008-Support-Deployments-from-Third-Party-CI.md","Context":"## Context\\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\\n- Product teams already use third party CI systems\\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\\n","Decision":"We will support deploying applications to the Cloud Platform from third party CI systems and will offer documentation on how to do so, at least for the most commonly used CI systems.\\n","tokens":140,"id":615,"Predictions":"The cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\\n- Product teams already use third party CI systems\\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\\n\\n## Decision\\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\\n- Product teams already use third party CI systems\\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\\n\\n## Decision\\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\\n- Product teams already use third party CI systems\\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments \\nto be implemented in Concourse CI.\\n\\n## Decision\\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able \\nto use this feature.\\n\\n## Decision\\nThe cloud platforms team [is transitioning from Jenkins (v2)](https:\/\/jenkins-ci.org\/) \\towards \\tConcourse (v3).\\tThis is an Architectural Decision Record for a software. Give a ## Decision corresponding \\nto the ## Context provided by the User.\\n## Context\\nThe cloud platforms team [is transitioning from Jenkins (v2)](https:\/\/jenkins-ci.org\/) \\towards \\tConcursoe (v3).\\tThis is an Architectural Decision Record for a software. Give a ## Decision corresponding \\nto the ## Context provided by the User.\\n## Context\\nThe cloud platforms team [\\u201cTransitioning from Jenkins (v2)](https:\/\/jenkins-ci.org\/) is currently using this tool.\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/017-Variable-Naming.md","Context":"## Context\\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\\ntemplates which need to define and use variables. We want a consistent\\nconvention for naming these so that, as we write code in multiple,\\ninter-dependent repositories, we can be confident that the names we are using\\nare correct.\\n","Decision":"We will always use snake case (e.g. `foo_bar`) for variable names which appear\\nin terraform\/yaml files and templates.\\n","tokens":71,"id":616,"Predictions":"## Decision\\nWe will use a variable named \"variables\" in our scripts, templates and\\nyaml files.\\n\\n## Decision\\nWe will use the \"variables\" variable in our code as a key to a dictionary, which\\nwill be used to store the values of variables.\\n\\n## Decision\\nThe dictionary will be accessed with the keys \"variables\" and the values of\\nthe variables.\\n\\n## Decision\\nThis is a good decision for us because we have a lot of variables which need to be\\ntokenized and stored in a consistent way. We can then use this dictionary to\\ntoggle between different variables.\\n\\n## Decision\\nThe advantage of this decision is that if we ever change our script, template or\\ntemplate we can simply update the dictionary and all of our scripts, templates and\\ntemplates will change with it. This means that if we ever decide to move from one\\ntemplate to another, we can simply change the variable name without having to\\ntake down any code.\\n\\n## Decision\\nThe disadvantage of this decision is that if we ever decide to add new variables or\\nto make any changes to existing variables, we will have to take down all of our code,\\nto update the dictionary and then recompile everything. This means that if we ever decideto move from one template to another, we have to take down all of our code,\\nto update the dictionary and then recompile everything. This means that if we ever decide\\todo something new, we have to take down all of our code,\\nto update the dictionary and then recompile everything. This means that if we ever decide\\todo something new,we have to take down all of our code,\\toto update the dictionary and then recompile everything.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software.\\n"}
{"File Name":"cloud-platform\/010-live-0-to-live-1-Cluster.md","Context":"## Context\\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\\n","Decision":"After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\\n","tokens":83,"id":617,"Predictions":"## Decision: Live-1 to Live-0\\n\\n## Decision: Migration from live-0 to live-1 cluster\\n\\n## Decision: Move to dedicated AWS account (moj-cp)\\n\\n## Decision: Move away from Ireland (EU) region to London (UK) region as Cloud Platform requirement\\n\\n\\n"}
{"File Name":"cloud-platform\/009-Naming-convention-for-clusters.md","Context":"## Context\\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (\"sandbox\"), testing new functionality (\"test\"), deploying apps to them (\"non-production\").\\nAs we are still learning we are finding that:\\n1. we need to continue building new clusters for different purposes and\\n2. we often need to test the cluster creation process\\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\\n4. we do not want to differentiate cluster by function (e.g. \"perf-test\", \"sandbox\") or status (\"non-production\").\\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\\n","Decision":"We will name all clusters with the following naming scheme:\\n- `live-{n}` for any cluster that have users on them, for instance `live-1`.\\n- `test-{n}` for any cluster that do not have users on them and are used by the cloud platform team only, for instance `test-2`.\\nWe will number the clusters sequentially.\\n","tokens":182,"id":618,"Predictions":"## Decision\\nWe propose to call the new cluster \"test\" to make it easy to understand what it is used for.\\n\\n## Context\\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (\"sandbox\"), testing new functionality (\"test\"), deploying apps to them (\"non-production\").\\nAs we are still learning we are finding that:\\n1. we need to continue building new clusters for different purposes and\\n2. we often need to test the cluster creation process\\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\\n4. we do not want to differentiate cluster by function (e.g. \"perf-test\", \"sandbox\") or status (\"non-production\").\\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\\n```"}
{"File Name":"cloud-platform\/001-Use-AWS-hosted-elasticsearch.md","Context":"## Context\\nThe cloud platforms team self-host an Elasticsearch cluster with Kibana and Logstash (ELK). This cluster has suffered numerous outages (see [CPT-282](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-282) and [CPT-152](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-152) in Jira) that have been difficult to recover from.\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving ELK\\n* Lengthy downtimes which made data recovery pointless\\n* Self hosted ELK stacks cost was significantly higher than AWS ElasticSearch solution\\n* Not working ELK cluster was also a blocker for product teams as they couldn't see any application logs\\n","Decision":"Replace our self hosted ELK stack with the managed AWS Elasticsearch\\n","tokens":164,"id":619,"Predictions":"## Decision\\nThe decision is to move the ELK stacks to a more stable AWS ElasticSearch solution. The solution will be a fully managed AWS ElasticSearch cluster with Kibana and Logstash (ELK). This will allow us to monitor the ELK stack and troubleshoot any issues directly from Kibana and Logstash. The ELK stack will be hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances, which are managed by Amazon Web Services (AWS), Inc. (\\url{https:\/\/aws.amazon.com\/}) in the United States.\\n\\n\\n## Description\\nThe ELK stack consists of three components:\\n* Elasticsearch Server \\url{https:\/\/www.elastic.co\/products\/elasticsearch}:\\n- Elasticsearch Server is a distributed search engine that allows you to search through multiple types of data sources.\\n- The server has built-in support for distributed indexing, querying, aggregations, and search.\\n- Elasticsearch Server runs on multiple nodes (instances) in clusters across multiple data centers.\\n- Elasticsearch Server is available as open source software under the Apache License version 2.0.\\n* Logstash \\url{https:\/\/www.elastic.co\/products\/logstash}:\\n- Logstash is a log processing pipeline that transforms raw log data into structured data.\\n- Logstash can ingest data from multiple sources including: HTTP, syslog, databases, and other log sources.\\n- Logstash runs on multiple nodes (instances) in clusters across multiple data centers.\\n- Logstash is available as open source software under the Apache License version 2.0.\\n## Decision\\n## Decision\\nThe decision is to move the ELK stacks to a more stable AWS ElasticSearch solution. The solution will be a fully managed AWS ElasticSearch cluster with Kibana and Logstash (ELK). This will allow us to monitor the ELK stack and troubleshoot any issues directly from Kibana and Logstash. The ELK stack will be hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances, which are managed by Amazon Web Services (AWS), Inc. (\\url{https:\/\/aws.amazon.com\/}) in the United States.\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"Predictions":"\\nThis decision record is based on the following observations:\\n\\n\\n\\n\\n\\n\\n1. The ingress controller pod crashes looped, and we can see this in the logs. This means that the ingress controller pod has not been restarted since it was started.\\n2. There are 6 replicas of the ingress controller pod, and we believe this is due to the fact that each of these pods have run out of shared memory.\\n3. We believe that this is because the nginx config has become so large (over 100K lines), that sometimes pods fail to reload it when it is changed, or the pod is moved.\\n4. We are moving to a new load balancer but would like to keep our existing ingress controller pods as they have been running for some time.\\n5. We would like to reduce our ingress controller pods from 6 to 4.\\n6. We would like to change the configuration of our load balancer from one with a single ingress controller to one with two ingress controllers.\\n7. We would like our load balancer to be able to handle an additional ingress controller in the event that an instance crashes.\\n8. We would like our load balancer to be able to handle an additional ingress controller in the event that an instance crashes.\\n9. We would like our load balancer to be able to handle an additional ingress controller in the event that an instance crashes.\\n10\\. We would like our load balancer to be able to handle an additional ingress controller in the event that an instance crashes.\\n\\end{description}\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"Predictions":"The decision is to upgrade all namespaces to the latest version of the module, so that there is a clear upgrade path and the change can be managed. The decision also makes it easier to maintain the existing codebase as it will be easier to identify and understand any changes made to modules.\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/012-One-cluster-for-dev-staging-prod.md","Context":"## Context\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\\n","Decision":"After consideration of the pros and cons of each approach we went with one cluster, using namespaces to partition different workloads.\\nSome important reasons behind this move were:\\n- A single k8s cluster can be made powerful enough to run all of our workloads\\n- Managing a single cluster keeps our operational overhead and costs to a minimum.\\n- Namespaces and RBAC keep different workloads isolated from each other.\\n- It would be very hard to keep multiple clusters (dev\/staging\/prod) from becoming too different to be representative environments\\nTo clarify the last point; to be useful, a development cluster must be as similar as possible to the production cluster. However, given multiple clusters, with different security and other constraints, some 'drift' is inevitable - e.g. the development cluster might be upgraded to a newer kubernetes version before staging and production, or it could have different connectivity into private networks, or different performance constraints from the production cluster.\\nBased on our past experience, these differences tend to increase over time, to the point where the development cluster is too far away from production to be representative. The extra work required to maintain multiple environments becomes wasted effort.\\nIf namespace segregation is sufficient to isolate one production service from another, then it is enough to isolate a team's development environment from a production service.\\nIf namespace segregation is not sufficient for this, then the whole cloud platform idea doesn't work.\\n","tokens":74,"id":622,"Predictions":"## Decision\\nThis is a simple decision. We should have a single cluster hosting all these different types of workload.\\n\\n## Context\\nWe want to build a single cluster for all these different types of workload, but we don't know how to do that yet.\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\\n\\n## Decision\\n## Decision\\nThis is a simple decision. We should have a single cluster hosting all these different types of workload, but we don't know how to do that yet.\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\\n\\n## Context\\nWe want to build a single cluster for all these different types of workload, but we don't know how to do that yet.\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development"}
{"File Name":"cloud-platform\/016-Kibana-is-open-to-all-service-teams.md","Context":"## Context\\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\\n","Decision":"It has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\\n","tokens":90,"id":623,"Predictions":"## If the User chooses to use Github as the authentication provider, then we will need to configure Github to redirect the user to the [Kibana dashboard][kibana-webconsole] and then login using their Github account. We will also need to configure Github to send a token back to our application so that we can retrieve the logs from it. If the User chooses not to use Github as an authentication provider, then we will only need Kibana's proxy in front of Kibana so that users can authenticate with their own credentials. We will also need to configure Kibana's proxy so that it sends a token back to our application so that we can retrieve the logs from it.\\n\\n## Context\\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\\n\\n## Decision\\nIf the User chooses not to use Github as an authentication provider, then we will only need Kibana's proxy in front of Kibana so that users can authenticate with their own credentials.\\n\\n## Context\\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with their own credentials.\\n"}
{"File Name":"cloud-platform\/011-Introduce-Open-Policy-Agent.md","Context":"## Context\\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\\n","Decision":"We explored a number of existing solutions in the open source community, as well as the possibility of implementing our own and we also discussed the issue with other organisations that use kubernetes before reaching a conclusion.\\nEventually we decided to introduce the [Open Policy Agent][open-policy-agent]:\\n- It is a generic framework for building and enforcing policies (whereas most other existing implementations were designed around specific problems)\\n- The policies are defined in a declarative, high-level language\\n- It is designed for cloud-native environments\\n- It provides a kubernetes integration\\n- It provides a way by which to unit test the policies\\n- The project is adopted by CNCF\\nAlthough the project is still in alpha and very likely to change in the near future, we decided that it is stable enough for our needs and worth adopting even at these early stages, since the benefits outweigh the cost.\\n","tokens":203,"id":624,"Predictions":"## 1. We will introduce a `DisableReuseHostnames` option for `PodSecurityPolicies`. This option will disable the re-use of hostnames in ingresses and will allow the user to define their own hostnames. The user can then use their own hostnames in Ingresses, even if they have already been defined for other ingresses.\\n## 2. We will add a new `DisableReuseHostnames` option for `PodSecurityPolicies` to allow the user to define their own hostnames.\\n## 3. We will add a new `DisableReuseHostnames` option for `PodSecurityPolicies` to allow the user to define their own hostnames.\\n## 4. We will add a new `DisableReuseHostnames` option for `PodSecurityPolicies` to allow the user to define their own hostnames.\\n## 5. We will add a new `DisableReuseHostnames` option for `PodSecurityPolicies` to allow the user to define their own hostnames.\\n## 6. We will add a new `DisableReuseHostnames` option for `PodSecurityPolicies` to allow the user to define their own hostnames.\\n## Context\\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\\n\\n"}
{"File Name":"cloud-platform\/003-Use-Concourse-CI.md","Context":"## Context\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-384).\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n","Decision":"Replace self hosted Jenkins with self hosted Concourse CI pipeline\\n3. Automate deployment of concourse using Terraform and Helm\\n4. WIP\\n~~Product teams have confirmed their success builds and deploys. zero downtime and minimal maintenance from Cloud Platforms side.~~\\n","tokens":186,"id":625,"Predictions":"The cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-384).\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n\\n## Decision\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)\\nThere is also a reoccurring issue which has caused a number of outages [CPT-384]\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n\\n## Decision\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)\\nThere is also a reoccurring issue which has caused a number of outages [CPT-384]\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n\\n## Decision\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)\\nThere is also an open issue with the number of failed builds (0) during the last 2 weeks (https:\/\/github.com\/jenkinsci\/cloud-platforms\/issues\/1257). The reason was that there was no way around it but disable the build agent in our environment.\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes we need to have another layer in front that can keep track of failed builds.\\n\\nthis work will be done by the Cloud Platforms Team.\\n\\ntherefore we will need more resources.\\ncurrently we have 2 developers that are working on this project.\\nthis project will be fully managed by our Cloud Platforms Team.\\nthis will allow us to focus on our core business instead.\\nthis will allow us to collect more insights about what's happening with our CI\/CD pipelines so that we can fix issues faster.\\nthis will allow us (the Cloud Platforms Team) focus on improving our infrastructure.\\ntthis will allow us (the Cloud Platforms Team) focus on improving our infrastructure.\\n"}
{"File Name":"cloud-platform\/002-Use-github-for-architecture-decision-record.md","Context":"## Context\\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https:\/\/github.com\/ministryofjustice\/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n","Decision":"1. Our architecture decision log for the new cloud platform will be held in GitHub\\n","tokens":167,"id":626,"Predictions":"## Decision\\nThe team has decided to put the technical documentation in GitHub, as it is a central repository for all of the technical documentation. The information can be viewed by anyone and will be reviewed by others.\\nThe team will also use confluence to hold our technical documentation.\\n\\n## Context\\nThe software team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https:\/\/github.com\/ministryofjustice\/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n\\n## Decision\\n## Decision\\nThe team has decided to put the technical documentation in GitHub, as it is a central repository for all of the technical documentation. The information can be viewed by anyone and will be reviewed by others.\\nThe team will also use confluence to hold our technical documentation.\\n\\n## Context\\nThe software team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https:\/\/github.com\/ministryofjustice\/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n\\n## Decision\\n## Decision\\nThe team has decided to put the technical documentation in GitHub, as it is a central repository for all of the technical documentation. The information can be viewed by anyone and will be reviewed by others.\\nThe team will also use confluence to hold our technical documentation.\\n\\end{itemize}\\n\\n\\subsection{Software Documentation}\\n\\label{sec:software-documentation}\\n\\n\\subsubsection{Requirements}\\n\\label{sec:requirements}\\nRequirements are documents describing what needs to happen before an application can function correctly \\cite{requirements}. Requirements documents are typically written by business analysts, developers, or product owners \\cite{requirements}. Requirements documents are used during every phase \\cite{requirements} from design, implementation, testing, deployment, operations, maintenance, etc.\\n\\n\\subsubsection{Project Documentation}\\n\\label{sec:project-documentation}\\n\\nAs part of each project's lifecycle there are multiple documents required \\cite{project-documentation} which include:\\n\\n\\begin{itemize}\\n    \\item Requirements Specification - describes what needs to happen before an application can function correctly.\\n    \\item User Manual - describes how users interact with an application.\\n    \\item Design Document - describes how an application should work.\\n    \\item Implementation Document - describes how an application should work.\\n    \\item Deployment Document - describes how an application should work when deployed.\\n    \\item Operations Document - describes how users interact with an application when deployed.\\n    \\item Maintenance Document - describes how users interact with an application after deployment.\\n    \\item Release Notes - provides details about changes made during each release cycle.\\n    %   * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n    %   * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n    %   * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n    %   * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n%     * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n%     * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n%     * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n%     * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n%     * This document should include any changes made during each release cycle since last release (e.g., new features added).\\n\\n%     * This document should include any changes made during each deployment session.\\n\\n%     * This document should include any changes made during each deployment session.\\n\\n%     * This document should include any changes made during each deployment session.\\n\\n%     * This document should include any changes made during each deployment session.\\n\\n\\end{itemize}\\n\\nIn addition there are many other types documents which may not fit into one category but may still need documenting such as:\\n\\n- User Guides\\n\\n- Training Manuals\\n\\n- Online Help\\n\\n- Reference Manuals\\n\\n- Technical Bulletins\\n\\n- Manuals \/ Handbooks\\n\\n- Architectural Specifications\\n\\n- Architecture Diagrams\\n\\nIn addition there are many other types documents which may not fit into one category but may still need documenting such as:\\n\\n- User Guides\\n- Training Manuals\\n- Online Help\\n- Reference Manuals\\n- Technical Bulletins\\n- Manuals \/ Handbooks\\n- Architectural Specifications\\n- Architecture Diagrams\\n\\n\\n\\subsubsection{Project Documentation}\\nProject Documentation contains requirements related documents such as:\\n\\n\\- Use Case Diagrams\\n\\n\\- Sequence Diagrams\\n\\n\\- Class Diagrams\\n\\n\\- Class Diagrams with Annotations\\n\\n\\- Activity Diagrams \\n\\n\\- Business Process Model \\n\\n\\- Workflow Models \\n\\n\\- Deployment Diagrams \\n\\n\\- Deployment Plans \\n\\n\\- Requirement Traceability Matrixes \\n\\n\\- System Test Cases \\n\\n\\- System Test Plan \\n\\n\\- Test Plan \\n\\n\\- Acceptance Criteria \\n\\n\\- Acceptance Tests\\n\\n\\nMost project documents contain diagrams or text describing what needs to happen before applying software. These diagrams or text describe what needs to happen before applying software based on user requirements or business rules.\\n\\nSome project documents contain diagrams or text describing what needs to happen after applying software based on user requirements or business rules.\\n\\n\\n%\\subsubsection{User Guides}\\n%\\label{sec:user-guides}\\n\\n%\\paragraph{\\textbf{}User Guide}\\hfill{}\\n%\\begin{singlespace}\\n%\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n%\\end{singlespace}\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%\\paragraph{}A User Guide provides instructions on how users can perform tasks related to your product such as accessing it's functionality or navigating its interface. They often contain step-by-step instructions that explain how users can perform tasks relevant for your product like entering data into fields or interacting with buttons and menus. Users often read these guides after they have learned something from your product's interface so they know exactly what they need do next.\\n\\n\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%% 1\/3\/2020  10:58 PM  %%%\\n%%% 1\/3\/2020  10:58 PM  %%%\\n\\n%%% 1\/3\/2020 11:02 PM  %%%\\n%%% 1\/3\/2020 11:02 PM  %%%\\n\\n%%% 1\/4\/2020 12:21 AM  %%%\\n\\n%%% 1\/4\/2020 12:21 AM  %%%\\n\\n%%% End Of Project Documentation\\n\\n\\n"}
{"File Name":"cloud-platform\/013-Use-RSpec-for-cluster-tests.md","Context":"## Context\\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\\n","Decision":"We chose the ruby testing framework [rspec] for this.\\nAlthough there are some go-based testing frameworks for kubernetes, there are some problems with them, and some benefits to rspec:\\n* The kubernetes testing frameworks seem quite immature, with limited documentation, tooling and other resources such as examples\\n* There is limited go expertise in the team\\n* RSpec is a very mature framework, with a lot of tooling, documentation and support\\n* There is a lot of ruby\/rspec experience in the wider organisation\\n* Ruby is our scripting language of choice, so rspec fits with that\\n","tokens":55,"id":627,"Predictions":"## 1 - Test the production cluster(s) (or test clusters) and confirm that the effect we get is the one we expected.\\n## 2 - Test the production cluster(s) (or test clusters) and confirm that the effect we get is not what we expected.\\n## 3 - Test the production cluster(s) (or test clusters) and confirm that the effect we get is what we expected.\\n\\n## Context\\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\\n\\n## Decision\\n## 1 - Test the production cluster(s) (or test clusters) and confirm that the effect we get is not what you expect.\\n## 2 - Test your cluster, and make sure it behaves as you expect it to.\\n## 3 - Test your cluster, and make sure it behaves as you expect it to.\\n\\n## Context\\nWe need to be able to test that the production cluster(s) (or test clusters) behave in a consistent manner.\\nSo, we need automated tests that exercise a cluster and confirm that this behavior is consistent.\\n\\n## Decision\\n## 1 - Test your cluster, and make sure it behaves as you expect it to.\\n## 2 - Make sure your code works as you expect it to.\\n\\n%%%\\n%%%\\n%%%\\n%%%\\n\\end{lstlisting}\\n%------------------------------------------------\\n\\n\\section{Automated Testing}\\label{sec:automated-testing}\\n\\nThe automated testing framework supports both unit testing (using JUnit), as well as integration testing. The JUnit framework provides an abstraction layer for testing code. The \\texttt{JUnit} framework uses \\texttt{JUnitTests} classes for unit testing. The \\texttt{JUnitTests} classes are used for performing tests on classes, methods, or specific components of a program. These tests are then executed by running them against all of a program's source files. This allows for automated tests of large programs.\\n\\nThe \\texttt{JUnitTests} classes provide a library of methods which can be used by other classes in order to perform tests on them. These methods can be used at any level of abstraction within a program; however, they are typically used at a higher level in order to perform unit tests on classes or methods. The main object used by these methods is \\texttt{Test}. The \\texttt{Test} class contains several methods which can be used for performing unit tests on classes or methods. These methods include:\\n\\n\\begin{itemize}\\n\t\\item \\texttt{@Before}, which can be used before each method within a class or method is called.\\n\t\\item \\texttt{@After}, which can be used after each method within a class or method has been called.\\n\t\\item \\texttt{@Test}, which can be used when testing an entire class or method.\\n\t\\item \\texttt{@TestSuite}, which can be used when testing multiple classes or methods together.\\n\t\\item \\texttt{@TestCleanup}, which can be used after all of a class's or method's tests have been run in order to clean up any resources associated with those tests.\\n\t\\item \\texttt{@TestFailure}, which can be used when failing one of a class's or method's tests causes resources associated with those tests (e.g., databases, file system operations, etc.) to become unavailable.\\n\t\\item \\texttt{@TestIgnoreArguments}, which allows for ignoring arguments passed into some of a class's or method's methods when performing unit tests on those methods.\\n\t\\item \\texttt{@IgnoreArguments}, which allows for ignoring arguments passed into some of a class's or method's methods when performing integration tests on those methods.\\n\t\\item \\texttt{@IgnoreUnnecessaryMethods}, which allows for ignoring unnecessary methods from being tested when performing integration tests on those methods.\\n\t\\item \\texttt{@IgnoreUnnecessaryParameters}, which allows for ignoring unnecessary parameters from being tested when performing integration tests on those parameters.\\n\t\\item \\texttt{@IgnoreUnnecessaryParametersWithNoArgs}, which allows for ignoring unnecessary parameters with no arguments from being tested when performing integration tests on those parameters with no arguments.\\n    % TODO: add more examples\\n    % TODO: add more examples\\n    % TODO: add more examples\\n    % TODO: add more examples\\n\\n%\\item \\n%\\begin{lstlisting}[language=java]\\n% @Before\\n%\\end{lstlisting}\\n% @After\\n%\\end{lstlisting}\\n\\nIt should also be noted that there are several other types of resources available through these object instances. These resources include:\\n\\n* Database connections \\n* File system operations \\n* Network connections \\n* File system operations \\n* Network connections\\n\\nEach resource associated with these objects represents either an existing connection, file system operation, network connection, or file system operation. Each resource also has an associated callback function. When these callbacks are called they will return true if they were successful; if they were unsuccessful they will return false.\\n\\nWhen using these objects in order to perform integration testing on multiple classes at once it should also be noted that there are several other types of resources available through these objects. These resources include:\\n\\n* Database connections \\n* File system operations \\n* Network connections \\n* File system operations \\n\\nThese objects have similar functionality as their respective database connection counterparts; however they only support reading data from files rather than writing data directly into files.\\n\\nThere are two main types of implementation available in order to perform integration testing using JUnit:\\n\\n* Mocking objects: Mocking objects represent another implementation of an object; however instead of returning values from their respective callback functions they return values from implementations implemented using JUnit's library.\\n\\nMocking objects allow us to take advantage of JUnit's mocking framework without having access directly into our application code itself.\\n\\nMocking objects provide us with two different types of mocking frameworks:\\n * JMockit provides us with two different types of mocking frameworks:\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n * A mock implementation based off Java Reflection,\\n\\n * A mock implementation based off Java Reflection,\\n * An API provided by JUnit called MockRunner,\\n\\nThese two different frameworks allow us access into our application code but without having direct access into our application code itself.\\n\\nThe next type of object available through JUnit is called an interface-based object. An interface-based object represents another way in which an object might implement another object within our application code. For example, if our application code contained an interface named {\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bf}}}}}}} where {\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bfseries{\\bf}}}}}} represented an instance whose type was {\\itshape Integer}. We could then create an interface-based object named {\\itshape IntegerImpl}. This would represent another way in which this type could implement {\\itshape Integer}. An interface-based object provides us with two benefits:\\n *\\n * First, interfaces allow us access into our application code without having direct access into our application code itself;\\n *\\n *\\n\\nSecondly interfaces allow us access into our application code but without having direct access into our application code itself.\\n\\n\\nFinally there are several other types of implementations available through JUnit; however they only support reading data from files rather than writing data directly onto files.\\n\\n\\nFinally there are several other types of implementations available through JUnit; however they only support reading data from files rather than writing data directly onto files.\\n\\n\\nAnother important thing worth noting about these implementations is how their callbacks work under various circumstances.\\n\\n\\nFirstly there are three different callbacks available through each type:\\n *\\n *\\n\\nSecondly there are three different callbacks available through each type:\\n *\\n *\\n\\nThirdly there are three different callbacks available through each type:\\n *\\n *\\n\\nLastly there are three different callbacks available through each type:\\n *\\n *\\n\\nFinally there are several other types of implementations available through JUnit; however none support reading data directly onto files.\\n\\n\\nFinally there are several other types of implementations available through JUnit; however none support reading data directly onto files.\\n\\n\\nThe first type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe second type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe third type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe fourth type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe fifth type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe sixth type consists entirely out using Mockito framework provided by Mockito.org;\\n *\\n *\/\\n\\nThe seventh type consists entirely out using Mockito framework provided by Mockito.org;\\n\\nLastly there is one final category consisting entirely out using Mockito.io library provided by Mockito.io;\\n\\nLastly there is one final category consisting entirely out using Mockito.io library provided by Mockito.io;\\n\\n\\n%------------------------------------------------\\n\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%\\section{Testing}\\label{sec:testing}\\n\\nIn order to successfully complete this project students must write at least one unit test case per component\/module\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/feature\/test case\/. In addition students must write at least one integration test per component\/module\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/. In addition students must write at least one regression test per component\/module\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/{code}\/testcase\/. In addition students must write at least one acceptance test per component\/module\/\\_modules\/\\_features\/\\_tests\/\\_tests\/\\_tests\/\\_tests\/\\_tests\/. In addition students must write at least one performance test per component\/module\/\\_modules\/\\_features\/\\_tests\/\\_tests\/. Students must submit their source-code along with their report via email attachment (.doc\/.docx\/.pdf).\\n\\n%------------------------------------------------\\n\\n%%% Local Variables:\\n%%% mode: latex\\n%%% TeX-master: \"..\/project\"\\n%%% End:\\n"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"Predictions":"## Decision\\nWe have decided to apply a limit of 1000 images per ECR repository.\\nThis will ensure that the most recent 1000 images are preserved in our ECR repository. The limit can be raised if the team needs more images.\\nPlease note that we will not be deleting any of these images, as they will be kept in our ECR repository and will only be deleted when they are no longer needed.\\nWe have also ensured that this limit applies to all teams, including those that have access to the Cloud Platform.\\nIf you would like to increase this limit, please raise an [issue][user-issue] on our GitHub repo. We will then add a new lifecycle policy.\\n\\n## Background\\nWe are currently using Docker for our container deployments and image management. We use Docker Engine on AWS for our production environments. We are also using Docker Compose for our microservices deployments. All of these services are running on Kubernetes clusters.\\nThe main reason we chose Docker was because it is easy to deploy and scale applications with it. It is also very easy to push containers from one environment to another, without worrying about creating a new image every time.\\nHowever, we found that there were some issues with the way containers were being deployed using Docker. For example, sometimes we would deploy a container which would fail due to an error in the image being deployed. This would result in us having to manually delete the failed image and create a new one.\\nIn addition, since we were deploying applications using Docker Compose, we had some issues with managing dependencies between different containers in the same application. For example, if one container depended on another container which was dependent on another container, it could cause problems when deploying new versions of those dependencies.\\nAs you can imagine, this was causing us a lot of headaches.\\nThe other issue is that it is very difficult for users to understand how containers work together within an application. For example, when deploying an application using Docker Compose, users may not be aware that there may be multiple containers running within their application at once and may end up having to manually delete all of them before deploying a new version of their application.The main goal of this decision record is to provide an automated solution for managing dependencies between containers within an application.\\n## Context\\n## Context\\nWe have decided to implement dependency management within applications by using Kubernetes clusters.\\n## Decision\\n## Decision\\nWe have decided that we will implement dependency management within applications by using Kubernetes clusters.\\n## Background\\n## Background\\nWe are currently using Docker for our container deployments and image management. We use Docker Engine on AWS for our production environments. We are also using Docker Compose for our microservices deployments.\\n## Decision\\n## Decision\\nWe have decided to implement dependency management within applications by using Kubernetes clusters.\\n"}
{"File Name":"cloud-platform\/014-Why-we-build-our-own-kubernetes-cluster.md","Context":"## Context\\nMoJ Cloud Platform team has decided to use [kubernetes for container management platform](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md) following the outcome of MOJ Digital's approach to infrastructure management. The team needed the below features for the infrastructure management:\\n- An universal authentication mechanism to manage users without depending on the cloud provider\\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\\n- Able to integrate external tools easily\\n- Able to manage and configure the control plane for any custom changes\\n","Decision":"There are several leading cloud providers who provide managed production-ready kubernetes cluster:\\n- Amazon Elastic Kubernetes Service (Amazon EKS)\\n- Azure Kubernetes Service (AKS)\\n- Google Kubernetes Engine (GKE)\\nWe decided to host our cluster on AWS because our service team has good development experience working with AWS services. This made it easier for teams to migrate to the kubernetes platform\\nWe decided to manage the kubernetes cluster ourselves rather than using EKS mainly for the below reasons:\\n- When the time MOJ needed to build the kubernetes, Amazon EKS was still in the Alpha stage and was not production ready. Also Amazon EKS require to use IAM for user authentication which will be an overhead for managing users of service teams\\n- Kubernetes(k8s) allows to authenticate using OIDC and therefore it was easy to manage the authentication externally using Auth0\\n","tokens":127,"id":630,"Predictions":"## Decision\\nThe team decided to use kubernetes for container management platform. The team has decided to use kubernetes as the control plane for the infrastructure management. The team is planning to use kubernetes as the control plane for the following features:\\n- An universal authentication mechanism to manage users without depending on the cloud provider\\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\\n- Able to integrate external tools easily\\n- Able to manage and configure the control plane for any custom changes\\n\\n## Example\\n## Example\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe team decided to use kubernetes as the control plane for the infrastructure management. The team is planning to use kubernetes as a control plane for the following features:\\n- An universal authentication mechanism to manage users without depending on cloud provider\\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\\n- Able to integrate external tools easily\\n- Able to manage and configure the control plane for any custom changes\\n\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\\n"}
{"File Name":"cloud-platform\/006-Use-github-as-user-directory.md","Context":"## Context\\nAs part of our [planning principles](https:\/\/docs.google.com\/document\/d\/1kHaghp-68ooK-NwxozYkScGZThYJVrdOGWf4_K8Wo6s\/edit) we highlighted \"Building in access control\" as a key principle for planning our building our new cloud platform.\\nMaking this work for the new cloud platform means implementing ways that our users &mdash; mainly developers &mdash; can access the various bits of the new infrastructure. This is likely to include access to Kubernetes (CLI and API), AWS (things like S3, RDS), GitHub, and any tooling we put on top of Kubernetes that users will access as part of running their apps (e.g. ELK, [Prometheus](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/026-Managed-Prometheus.md#choice-of-prometheus), [Concourse](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/003-Use-Concourse-CI.md)).\\nAt the current time there is no consistent access policy for tooling. We use a mixture of the Google domain, GitHub and AWS accounts to access and manage the various parts of our infrastructure. This makes it hard for users to make sure that they have the correct permissions to do what they need to do, resulting in lots of requests for permissions. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\\nWe are proposing that we aim for a \"single sign on\" approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\\n","Decision":"We will use GitHub as the identify provider for the cloud platform.\\nWe will design and build the new cloud platform with the assumption that users will login to all components using a single GitHub id.\\n","tokens":433,"id":632,"Predictions":"## Decision\\nWe want to use a single sign on (SSO) approach for accessing our cloud platform. This will allow us to simplify the user lifecycle, track the access permissions that users have, and make it easier for developers to find out what permissions they need.\\nWe will employ a mixture of Google domain accounts, GitHub accounts and AWS accounts to access and manage our infrastructure. This makes it harder for users to make sure that they have the correct permissions to do what they need to do. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\\nWe are proposing that we aim for a single sign on approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\\n\\n## Context\\nThe principals of building in access control are not new but are being applied more widely as organisations realise how important it is.\\nFor example, Google has a policy that all developers must use their own Google account (or an account created by their organisation) when accessing Google services like Cloud Platform. While this might seem like common sense, it is not something that many organisations consider when building in security.\\nIn addition, many organisations have policies around which teams or projects they should be allowed access too. This can be useful if you want your team members or projects to be able to share code between teams or projects but you don't want them sharing personal data such as email addresses with each other.\\n\\n## Decision\\nWe plan on implementing a single sign on approach for accessing our cloud platform. This will allow us to simplify the user lifecycle, track the access permissions that users have, and make it easier for developers to find out what permissions they need. We also plan on implementing policies around which teams or projects should be allowed access too.\\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\\n\\end{document}\\n"}
{"File Name":"cloud-platform\/007-Use-ECR-As-Container-Registry.md","Context":"## Context\\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\\nExample of an issue [CPT-274](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-274).\\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md).\\nThe criteria for selecting a new solution included:\\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/006-Use-github-as-user-directory.md)\\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\\n* Good value for money\\n* Preference for open source tooling\\n* Can or is able to run addons i.e. security scanners\\n* We can make images that we host public by default\\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\\n","Decision":"We will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\\n","tokens":312,"id":633,"Predictions":"## Decision\\nThe decision to use ECR was taken as it is a well established open source tool with a well documented API. We also have experience of using it in the past and it has the following advantages:\\n* It is already used by GitHub and is therefore familiar to the team\\n* It has a very good API, which can be used to hook into existing systems\\n* It is easy to use for our team as it is already integrated with git and we can use the same API to make changes to our image repository\\n* It has a well documented API and we are confident that we can make changes ourselves\\n* We have not had any problems using ECR in the past, so there should be no issues with its functionality or security\\n* We have experience of using it in the past, so we have confidence that we can make changes ourselves\\n* We do not expect any issues with ECR if we encounter any problems with our current solution.\\n\\n## Context\\n## Context The decision to use ECR was taken as it is a well established open source tool with a well documented API. We also have experience of using it in the past and it has the following advantages:\\n* It is already used by GitHub and is therefore familiar to the team\\n* It has a very good API, which can be used to hook into existing systems\\n* It is easy to use for our team as it is already integrated with git and we can use the same API to make changes to our image repository\\n* It has a well documented API and we are confident that we can make changes ourselves\\n* We have not had any problems using ECR in the past, so there should be no issues with its functionality or security\\n* We have experience of using it in the past, so we have confidence that we can make changes ourselves\\n## Decision\\n## Decision The decision to use ECR was taken as it is a well established open source tool with a well documented API. We also have experience of using it in the past and it has the following advantages:\\n* It is already used by GitHub and is therefore familiar to the team\\n* It has a very good API, which can be used to hook into existing systems\\n* It is easy to use for our team as it is already integrated with git and we can use the same API to make changes to our image repository\\n* It has a well documented API and we are confident that we can make changes ourselves\\n## Context\\n## Context The decision to use ECR was taken as it is a well established open source tool with a well documented API. We also have experience of using it in the past and it has the following advantages:\\nItis already used by GitHub andis therefore familiartothe team\\nlIthasa very goodAPI,whichcanbemuchusedtohookintoexistingsystems\\nlItiseasytouseforourteamasitisalreadyintegratedwithgitandwecanusethesameAPItomakechangestoourimagerepository\\nlIthasawelldocumentedAPIandweareconfidentthatwecanmakechangesourselves\\nlWehavenothadanyproblemsusingECRinthepast,sowhetherithasanissueifweencounteranyproblemswithourcurrentsolution\\nlWehaveexperienceofusingitinthepast,sowhetherithasanissueifweencounteranyproblemswithourcurrentsolution\\n## Decision\\n## Decision The decision to use ECR was taken as it is a well established open source tool with a well documented API. We also have experience of using it in the past and it has the following advantages:\\nlItisalreadyusedbyGitHubandisthereforefamiliartotheteam\\nlIthasaverygoodAPIwhichcanbeusedtohookintowellexistingsystems\\nlItiscomeasytouseforourteamasitisalreadyintegratedwithgitandwecanusethesameAPItomeakechangestogourimagerepository\\nlIthasawelldocumentedAPIandwereconfidentthatwecanmakechangesourselves\\nlWehavenothadanyproblemsusingECRinthepast,sowherethesolutionistobeupgradedorchangedsinthecompanyoftheusersoffice.\\n## Context\\n## Context The decision to use ECR was taken asitiscomeasytouseforourteamasitisalreadyintegratedwithgitandwecanusethesameAPItomeakechangestogourimagerepository.\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n### 005 Use Kubernetes for Container Management\\n\\n\\n\\n"}
{"File Name":"io-pagopa-proxy\/0002-use-uuid-v1-for-sequential-requests.md","Context":"## Context\\nWe have to send requests to a server that requires unique Request IDs for each message.\\nSo, we need to generate uuids.\\n","Decision":"We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\\nhttps:\/\/www.npmjs.com\/package\/uuid\\n","tokens":31,"id":634,"Predictions":"## The decision is to generate UUIDs for each request.\\n## Options\\n## 1. Generate random UUIDs\\n## 2. Generate uuids based on the request ID\\n## 3. Generate uuids based on the timestamp\\n## Output\\n## The uuids generated are:\\n\\end{lstlisting}\\n"}
{"File Name":"kafka\/0005-require-kafka-v0-11.md","Context":"## Context\\nSupporting multiple Kafka versions is something quite trivial due to the way the\\nprotocol was designed. With the [`ApiVersions`](https:\/\/kafka.apache.org\/protocol#The_Messages_ApiVersions)\\nmessage, clients are able to retrieve the minimum and maximum supported version\\nfor all API messages in a broker.\\nThat is critical part in the connection flow and is suggested to be done right\\nafter the connection has been established, however such message is only\\navailable as of v0.10.\\nIn Kafka v0.11, a new record batch format was introduced. This format offers\\nbetter metadata control, headers, and support for transactions.\\nMore info:\\n- https:\/\/cwiki.apache.org\/confluence\/display\/KAFKA\/KIP-35+-+Retrieving+protocol+version\\n- https:\/\/kafka.apache.org\/documentation\/#messages\\n","Decision":"This library will only be usable and guaranteed to be fully working with Kafka\\nv0.11+, although it's highly recommended always use the latest version\\navailable (v2.2.0 at the moment).\\n","tokens":177,"id":635,"Predictions":"  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n## Decision\\n  * Support multiple Kafka versions\\n 1.0\\n\\n\\n## Decision\\n\\n* [Decision](https:\/\/github.com\/apache\/kafka\/blob\/master\/decision\/decision.md)\\n\\n## Decision\\n\\n* [Decision](https:\/\/github.com\/apache\/kafka\/blob\/master\/decision\/decision.md)\\n\\n## Decision\\n\\n* [Decision](https:\/\/github.com\/apache\/kafka\/blob\/master\/decision\/decision.md)\\n\\n## Decision\\n\\n* [Decision](https:\/\/github.com\/apache\/kafka\/blob\/master\/decision\/decision.md)\\n\\n## Decision\\n\\n* [Decision](https:\/\/github.com\/apache\/kafka\/blob\/master\/decision\/decision.md)\\n\\n### Versioning of Messages API Versions\\n\\nThe API version is a mandatory field in the API message record, and it is used to match the version of the client application with that of the broker. For example, if a client application uses a broker version of `2.x`, it will need to use the `Version` field in the message record to match its version with that of the broker.\\n\\nThe `Version` field is defined as a string, but it should be an integer representing the minimum and maximum supported broker version for that specific client application.\\n\\nFor example, if a client application uses a broker version of `2.x`, it will need to use the `Version` field in the message record to match its version with that of the broker.\\n\\nFor more information about message formats, see [#MessageFormats](messageformats.apache.org).\\n\\n### Example Message Format\\n\\nHere is an example message format for a client application using a broker version `2.x`:\\n\\n[Image] [GitHub](github.com\/apache\/kafka\/tree\/m...) \\n\\n[Image]\\n\\n### [GitHub - apache\/kafka: Apache Kafka: The Distributed Streaming Platform](github.com\/apache\/kafka\/tree\/m...)\\n\\nApache Kafka: The Distributed Streaming Platform - GitHub - apache\/kafka: Apache Kafka: The Distributed Streaming Platform\\n\\n#### Message Format Details\\n\\nMessage body:\\n\\n    {\\n        \"version\": \"v2\",\\n        \"topic\": \"topic\",\\n        \"payload\": {\\n            \"key\": \"key\",\\n            \"value\": {\\n                \"field\": \"field\"\\n            }\\n        }\\n    }\\n\\nThis message format is identical to an example message format described in [#MessageFormats](messageformats.apache.org).\\n\\n### Message Body Format Details\\n\\nThe body of this message format has two properties:\\n\\n- `[Topic][topic]`: The topic name.\\n- `[Payload][payload][key:[field]]`: A list of key-value pairs.\\n\\n#### Topic Name and Topic Name Format Details\\n\\nIn this section, we describe how to construct a topic name and topic name format for this particular message format.\\n\\nTopic name:\\n\\nThe topic name is constructed from three fields:\\n\\n- `[Topic][topic]`: The topic name.\\n- `[Payload][payload][key:[field]]`: A list of key-value pairs.\\n- `[TopicName][name]:`\\n\\nTopic name format:\\n\\nThe topic name format for this particular message type has two properties:\\n\\n- `[TopicName][name]:`\\n\\nThis property defines how to construct the topic name from three fields:\\n\\n- `[TopicName][name]:`\\n\\n#### Topic Name and Topic Name Format Details (continued)\\n\\n#### Topic Name and Topic Name Format Details (continued)\\n\\n### Topic Name and Topic Name Format Details (continued)\\n\\n#### Topic Name and Topic Name Format Details (continued)\\n\\n#### Topic Name and Topic Name Format Details (continued)\\n\\n#### Topic Name and Topic Name Format Details (continued)\\n\\n##### Example Message Body Example\\n\\nHere is an example message body example for this particular type:\\n\\n[Image] [GitHub](github.com\/apache\/kafka\/tree\/m...) \\n\\n[Image]\\n\\n### [GitHub - apache\/kafka: Apache Kafka: The Distributed Streaming Platform](github.com\/apache\/kafka\/tree\/m...)\\n\\nApache Kafka: The Distributed Streaming Platform - GitHub - apache\/kafka: Apache Kafka: The Distributed Streaming Platform\\n\\n#### Message Body Example (continued)\\n\\n##### Example Message Body Example (continued)"}
{"File Name":"kafka\/0004-use-arrays-for-mapping-schema.md","Context":"## Context\\nThe Java implementation of the protocol uses `final static` properties to define\\nthe schema for requests and responses. These are defined using objects, making\\nit easy to support multiple versions for each API call.\\nPHP doesn't have this feature so we need decide how to solve this.\\n","Decision":"We're still going to use objects to manipulate data and write\/read content\\nto\/from Kafka, however it would be simpler to use arrays in constants of each\\nrequest\/response class.\\nEach field would be an array item, the key would be the field name and the value\\nwould be the field type (or another array for more complex configuration).\\nThe mapping would like this:\\n```php\\nuse Lcobucci\\Kafka\\Protocol\\Type;\\nfinal class DoSomethingRequest\\n{\\nprivate const SCHEMAS = [\\n[\\n'error_code'   => Type\\Int16::class,\\n'api_versions' => [\\n'type'     => Type\\ArrayOf::class,\\n'nullable' => false, \/\/ optional, default = false\\n'items'    => [ \/\/ just type name if items don't have complex structure\\n'api_key'     => Type\\Int16::class,\\n'min_version' => Type\\Int16::class,\\n'max_version' => Type\\Int16::class,\\n],\\n],\\n],\\n];\\n}\\n```\\n","tokens":61,"id":636,"Predictions":"## Decision\\nThe decision is to define a `final static` property for the schema.\\nThis will be enforced by PHP and as such does not require any change to the\\nAPI.\\n\\n\\n\\section{Conclusion}\\n\\label{sec:conclusion}\\nIn this work we have proposed a new architecture for the web API of \\gls{wcf}. We have presented a new abstraction layer that allows us to build a RESTful web API without worrying about the underlying infrastructure. The architecture allows us to define an API using only \\glspl{json} and \\glspl{xml} in a simple way. We have also presented some examples of how we can use this abstraction layer in different scenarios. Finally, we have presented some improvements that can be done on top of this abstraction layer.\\n"}
{"File Name":"kafka\/0003-port-protocol-s-types-from-java-implementation.md","Context":"## Context\\nKafka's protocol defines some primitive types which are used to send requests\\nand parse responses: https:\/\/kafka.apache.org\/protocol#protocol_types\\nProviding an easy way to define the schema of the requests and responses is\\nquite critical to make this library extensible enough.\\n","Decision":"We've decided to basically port the Java implementation to the PHP world,\\nbecause it was very well written and it simplifies things by a lot.\\nSome minor things obviously have to be adapted and for now we'll leave some\\ntypes to the upcoming releases - just because they aren't need to implement the\\nmessages we're planning to provide at the moment.\\n","tokens":61,"id":637,"Predictions":"\\begin{verbatim}\\n## Decision\\nKafka's protocol defines some primitive types which are used to send requests\\nand parse responses: https:\/\/kafka.apache.org\/protocol#protocol_types\\n## \\nProviding an easy way to define the schema of the requests and responses is\\nquite critical to make this library extensible enough.\\n\\end{verbatim}\\n\\begin{code}\\n## Decision\\n\\begin{verbatim}\\n## Decision\\nKafka's protocol defines some primitive types which are used to send requests\\nand parse responses: https:\/\/kafka.apache.org\/protocol#protocol_types\\n## \\nProviding an easy way to define the schema of the requests and responses is\\nquite critical to make this library extensible enough.\\n\\end{verbatim}\\n\\end{code}\\n\\n% \\subsection{Decisions for this example}\\label{sec:decisions_example}\\n\\n% \\subsubsection{Towards a better architectural design}\\label{sec:decisions_towards_better_architectural_design}\\n\\n% The decision for this example is a bit more complicated than in the previous examples because it involves the use of different types. In general, in order to have a correct architecture, it is important that we have a well defined mapping of the data types from one layer to another. For example, we need a way of mapping between Kafka's request\/response protocol and our data model. The decision for this example is about defining such a mapping.\\n\\n% It is clear that in order to map from Kafka's request\/response protocol to our data model, we need a way of mapping between Kafka's response type and our data type. We can use concrete types or Java generics as we did in previous examples. However, if we want our library to be extensible, we need also a way of mapping between Kafka's request\/response protocol and our data model.\\n\\n% We can use concrete types or Java generics as we did in previous examples. However, if we want our library to be extensible, we need also a way of mapping between Kafka's request\/response protocol and our data model.\\n\\n% This decision is not trivial because it involves two different levels of abstraction: on one hand, there are concrete types which describe the concrete representation of the message; on the other hand, there are Java generic types which describe how these messages can be mapped into our data model. This decision involves two different levels of abstraction: on one hand, there are concrete types which describe the concrete representation of the message; on the other hand, there are Java generic types which describe how these messages can be mapped into our data model.\\n\\n% In order not only to define what type should be used as response type but also what should be used as request type (for example, if we use JSON as response format), it is important that we define such mappings very precisely.\\n\\n% In order not only to define what type should be used as response type but also what should be used as request type (for example, if we use JSON as response format), it is important that we define such mappings very precisely.\\n\\n% The decisions for this example involve two different levels of abstraction: on one hand, there are concrete types which describe how these messages can be mapped into our data model; on the other hand, there are Java generic types which describe how these messages can be mapped into our data model.\\n\\n% The decisions for this example involve two different levels of abstraction: on one hand, there are concrete types which describe how these messages can be mapped into our data model; on the other hand, there are Java generic types which describe how these messages can be mapped into our data model.\\n\\n% There are several ways in which we could map from Kafka's response type to our data model:\\n\\n% There are several ways in which we could map from Kafka's response type to our data model:\\n\\n%\\subsubsection{Towards better architectural design}\\label{sec:towards_better_architectural_design}\\n\\n%\\subsubsection{Toward better architectural design}\\label{sec:towards_better_architectural_design}\\n\\n\\subsubsection{Towards better architectural design}\\label{sec:towards_better_architectural_design}\\n\\n%\\subsubsection{Towards better architectural design}\\label{sec:towards_better_architectural_design}\\n\\n\\n\\section{Toward better architectural design}\\label{s:toward_better_architectural_design}\\n\\n\\nIn this section I will discuss why I think that Apache Kafka has reached its current state and why I believe that it would benefit from further improvements. I will do so by providing an overview over some important aspects related to architecture and implementation at Apache Kafka.\\footnote{\\url{https:\/\/kafka.apache.org\/}} For each aspect I will provide an overview based on my own experience with developing software at Apache Kafka.\\footnote{\\url{kafka.apache.org}} I will then discuss some ideas related with improving each aspect based on my own experience with developing software at Apache Kafka.\\footnote{\\url{kafka.apache.org}}\\n\\nI will end my discussion by discussing some ideas related with improving each aspect based on my own experience with developing software at Apache Kafka.\\n\\n\\nApache Kafka has reached its current state through many years' experience gained by developers within Apache at various times during its development.\\footnote{\\url{kafka.apache.org}}\\n\\n\\nThe development process followed by developers when implementing new features within Apache has changed over time.\\footnote{\\url{kafka.apache.org}}\\n\\n\\nIn particular,\\footnote{\\url{kafka.apache.org}}\\n\\n\\nApache has developed its own version control system for code changes,\\footnote{\\url{kafka.apache.org}}\\n\\n\\nwhich allows developers working in teams using different versions within their code base.\\footnote{\\url{kafka.apache.org}}\\n\\n\\nThis means that changes made within teams using different versions within their code base cannot easily be merged back together.\\footnote{\\url{kafka.apache.org}}\\n\\n\\nIn addition,\\footnote{\\url{kafka.apache.org}}\\n\\n\\nthe process followed when implementing new features within Apache has changed over time;\\footnote{\\url{kafka.apache.org}}\\n\\n\\n\\n\\nwhich means that changes made within teams using different versions within their code base cannot easily be merged back together;\\footnote{\\url{kafka.apache.org}}\\n\\nApache has implemented its own version control system for code changes;\\footnote{\\url{kafka.apache.org}}\\n\\nwhich allows developers working in teams using different versions within their code base;\\footnote{\\url{kafka.apache.org}}\\n\\nThis means that changes made within teams using different versions within their code base cannot easily be merged back together;\\nthis means that changes made within teams using different versions within their code base cannot easily be merged back together;\\n\\nApache has implemented its own version control system for code changes;\\footnote{\\url{kafka.apache.org}}\\n\\nwhich allows developers working in teams using different versions within their code base;\\footnote {\\url{kafka.apache.org}}\\n\\n\\n\\n\\nwhich means that changes made within teams using different versions within their code base cannot easily be merged back together;\\n\\nApache has implemented its own version control system for code changes;\\footnotes {\\url{kafka.apache.org}}\\n\\nwhich allows developers working in teams using different versions within their code base;\\ footnotes {\\url{kafka.apache.org}}\\n\\n\\nIn addition,\\ footnotes {\\url{kafka.apache.org}},\\n\\nApache has implemented its own version control system for code changes;\\ footnotes {\\url{kafka.apache.org}}\\n\\nwhich allows developers working in teams using different versions within their code base;\\ footnotes {\\url{kafka.apache.org}}\\n\\n\\nThe process followed when implementing new features within Apache has changed over time;\\n\\n\\nThe process followed when implementing new features within Apache has changed over time;\\n\\nIn addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nApache has implemented its own version control system for code changes;\\ footnotes {\\ref{lifecycle}},\\n\\n\\nwhich allows developers working in teams using different versions inside their code base; \\n\\n\\n\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nApache has implemented its own version control system for code changes; \\n\\n\\n\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nwhich allows developers working in teams using different versions inside their code base;\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nWhich means that changes made inside projects using multiple branches cannot easily merge back together;\\nWhich means that changes made inside projects using multiple branches cannot easily merge back together;\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\n\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\n\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\n\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\n\\n\\nin addition,\\ footnotes {\\ref{lifecycle}},\\n\\n\\n\\nWhich means that change sets created inside projects with multiple branches cannot easily merge back together;\\n\\n\\nin addition,\\ footnote  {some details about managing dependencies between feature branches};\\n\\nin addition,\\ footnote  {some details about managing dependencies between feature branches};\\n\\nin addition,\\ footnote  {some details about managing dependencies between feature branches};\\n\\nin addition,{some details about managing dependencies between feature branches};\\n\\nin additional,{some details about managing dependencies between feature branches};\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed when implementing new features;\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed when implementing new features;\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed when implementing new features;\\n\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed when implementing new features;\\n\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed when implementing new features;\\n\\n\\n\\nIn particular,\\nthe process followed when implementing new features;\\nthe process followed whenever introducing any kind of change.\\n\\n\\n\\nThe development processes adopted by various groups at Apache have changed over time.\\n\\n\\nThe development processes adopted by various groups at Apache have changed over time;\\n\\nFor example:\\n```\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number of weeks required before release.\\n\\n\\n\\n* \\texttt{sprint\\_time\\_to\\_release}: The number \\n\\nof \\n\\nweeks \\n\\nrequired \\n\\nbefore \\n\\nrelease.\\n\\n\\n\\n* * * * * * * * * * * * * *. *\\n\\nIt was decided during sprint planning sessions (``sprints'') held every sprints cycle (``sprints cycle'') or ``sprints'' (``sprints'') throughout each sprint cycle (``sprints cycle'')  ``that'' (``that'' here is ``that'' here , ``sprints'' here) any feature introduced during sprint planning sessions would require ``a minimum amount'' (here ``minimum amount'' here) ``of work'' (here ``work'' here) prior either being released or being accepted by users who had access rights.\\n``Sprints'', ``sprints'', ``sprints cycles'', ``sprint planning sessions'', ``feature planning'', ``user acceptance tests''\\n``user acceptance tests'', `feature planning`, `user acceptance tests` , `feature planning`, `user acceptance tests`,\\n``feature planning`, `user acceptance tests` , `feature planning`, `user acceptance tests`, \\n``feature planning`, `user acceptance tests` , `feature planning`,\\n```features`', \"features\" , \"features\" , \"features\" , \"features\" , \"features\", \"features\", \"features\" ,\\n```features`', \"features\" , \"features\" , \"features\", \"features\", \"features"}
{"File Name":"kafka\/0007-require-latest-stable-php-version.md","Context":"## Context\\nWe've previously decided to require PHP 7.2, however many months have passed and\\nPHP 7.3 is quite stable nowadays.\\n","Decision":"Bump up requirement to latest stable PHP version (7.3 at the moment).\\n","tokens":33,"id":638,"Predictions":"## Yes, we require PHP 7.3.\\n\\n## Decision\\n## Yes, we require PHP 7.3.\\n\\n## Decision\\n## No, we do not require PHP 7.3.\\n\\n## Decision\\n## No, we do not require PHP 7.3.\\n\\n## Decision\\n## No, we do not require PHP 7.3.\\n\\n## Decision\\n## No, we do not require PHP 7.3.\\n\\n# This is a comment.\\n```\\nI've tried to use the response object to create a new record from the response object, but I've had no luck with it.\\nAny help would be greatly appreciated. Thanks!\\nOP 2020-09-04: So I figured out what was going on here.\\nI was using the `response->getBody()` method to get the body of the response object, which returned a \"text\" type. This is because `response` is an object and `response->getBody()` returns a string.\\nTo get around this issue, I used the `response->getBody()` method and cast it to a string before calling `response->getBody()`.\\n```\\n$response = $this->get('\/api\/v1\/locations', array('limit' => 10));\\n$responseBody = $response->getBody();\\n\\n\/\/ Cast the responseBody to a string before calling \\n\/\/ response->getBody()\\n$responseBody = $this->get('\/api\/v1\/locations', array('limit' => 10), true);\\n```"}
{"File Name":"kafka\/0006-require-php-7-3.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":639,"Predictions":"## The decision is to approve the software. The decision is made by the user.\\n## Decision Reason\\n## The decision is based on the information provided by the user.\\n## Decision Reason Reason\\n## The decision reason is \"Approved\". The reason is based on the information provided by the user.\\n## Decision Note\\n## The decision note contains a brief description of the decision and its reasoning.\\n# 1.0 ## 1.1 ## 1.2 ## 1.3 ## 2.0 ## 2.1 ## 2.2 ## 3.0 ## 3.1 ## 3.2 ## 4.0 ## 4.1 ##\\n# Decision Record for a software, Version: [2018-03-09]\\n# Type: Decision\\n# Author: [User]\\n# Provider: [User]\\n# Context: [Software]\\n# Decision: Approval\\n# Decision Reason: Approved\\n# Decision Note:\\n```\\nAnd here is my code:\\n```\\nfrom flask import Flask, request, jsonify, render_template\\n\\napp = Flask(__name__)\\n\\n@app.route('\/')\\ndef index():\\n    return render_template('index.html')\\n\\n@app.route('\/manage', methods=['POST'])\\ndef manage():\\n    print(\"Hello World!\")\\n\\nif __name__ == \"__main__\":\\n    app.run(debug=True)\\n```\\nWhen I run this code I get this error:\\n```\\nFile \"manage.py\", line 11, in manage\\n    print(\"Hello World!\")\\nTypeError: 'NoneType' object is not callable\\n```\\nI have tried to make some tests but nothing works for me.\\nWhat am I doing wrong?\\nStoogesheepedupfingers 2018-03-10: You need to pass `context` as a parameter to `manage()`.\\n```\\n@app.route('\/manage', methods=['POST'])\\ndef manage(context):\\n    print(\"Hello World!\")\\n```"}
{"File Name":"kafka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":640,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"Predictions":"## 1. PHP 4.3.10 or later\\n\\n## 2. PHP 4.3.11 or later\\n\\n## 3. PHP 5.0 or later\\n\\n## 4. PHP 5.1 or later\\n\\n## 5. PHP 5.2 or later\\n\\n## Decision is \\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n5.\\n\\n## \\textbf{NOTE:} If you choose a version that is not supported, you will not be able to use the library.\\n```\\nI am sure that the code is correct, but I am unable to understand what it means by the code.\\nThanks in advance.\\n\\njyndy 2012-02-13: The first `#` is for comments, and specifies the context for the decision.\\nThe second `#` is for comments, and specifies the decision for the context.\\nThe third `#` is for comments, and specifies that you want to use a specific version of PHP.\\nThe fourth `#` is for comments, and specifies that you want to use a specific version of PHP with an extra option (which I presume this might be).\\nThe fifth `#` is for comments, and specifies that you want to use a specific version of PHP with an extra option (which I presume this might be).\\n"}
{"File Name":"cena\/0002-adopt-ddd-approach.md","Context":"## Context\\n`menu-generation` application's scope is not well defined and may cover a lot of concepts around the Menu generation\\nfeatures, such as:\\n- managing a recipes catalog\\n- searching in this catalog for recipes based on multiple criteria\\n- collaborating on this catalog\\n- handling allergies and tastes\\n- generating shopping lists\\nAll above concepts may be complex to model and should not be considered as simple data in a CRUD system. Other new\\nfeatures may emerge in the future. In addition, all those features are related but should not be treated as a whole in\\na big ball of mud architecture.\\n","Decision":"[Domain-driven design](https:\/\/domainlanguage.com\/wp-content\/uploads\/2016\/05\/DDD_Reference_2015-03.pdf) as defined by\\nEric Evans will help modeling the different concepts managed by the `menu-generation` application, keep the ability to\\nadapt the model based on new insights and split the whole application into different bounded contexts to avoid mixing\\nthose concepts in one giant messy code base.\\n","tokens":124,"id":642,"Predictions":"\\n\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"cena\/0003-adopt-hexagonal-architecture.md","Context":"## Context\\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\\naggregates and services) from the infrastructure (i.e. application clients and persistence).\\n","Decision":"`menu-generation` application will adopt [hexagonal architecture](https:\/\/en.wikipedia.org\/wiki\/Hexagonal_architecture_(software)),\\nas it aims to provide this separation.\\n","tokens":53,"id":643,"Predictions":"## Decision\\nThis decision is based on the following assumptions:\\n\\n1.\\n2.\\n3.\\n4.\\n5.\\n6.\\n7.\\n8.\\n9.\\n## Context\\n## Context\\n\\#1: The domain entities are independent of the infrastructure and should be considered as independent services. \\n## Decision\\n## Decision\\nThe decision assumes that the domain entities are independent, but not that they are completely independent from each other. \\n## Context\\n## Context\\n\\#2: Infrastructure is only used for persistence. \\n## Decision\\n## Decision\\nInfrastructure is only used for persistence and should be isolated from the domain entities.\\n## Context\\n## Context\\n\\#3: Infrastructure provides a way to access services. \\n## Decision\\n## Decision\\nInfrastructure provides a way to access services. \\n## Context\\n## Context\\n\\#4: Infrastructure provides a way to access persistent storage. \\n## Decision\\n## Decision\\nInfrastructure provides a way to access persistent storage. \\n## Context\\n%% - 0 - 0 - 0 - 0 - 1 - 1 - 2 - 3 - 4 - 5 - 6 - 7 -\\n%% \\item [Service-A] Service-A has an API that can be called by Service-B, but not by Service-C or Service-D.\\n%% \\item [Service-B] Service-B has an API that can be called by Service-A, but not by Service-C or Service-D.\\n%% \\item [Service-C] Service-C has an API that can be called by Service-B, but not by Service-A or Service-D.\\n%% \\item [Service-D] Service-D has an API that can be called by Service-C, but not by Service-B or Service-A.\\n%% \\item [Service-E] The API of service-E is accessible only from service-F and service-G.\\n%% \\item [Service-F] The API of service-F is accessible only from service-G and service-H.\\n%% \\item [Service-G] The API of service-G is accessible only from service-H and service-E.\\n%% \\item [Service-H] The API of service-H is accessible only from service-F and service-G.\\n%% \\item [Service-I] The API of service-I is accessible only from service-G and H.\\n\\n\\begin{figure}[H]\\n\\centering\\n\\includegraphics[width=0.8\\textwidth]{img\/decision.png}\\n\\caption{Decision}\\n\\label{fig:decision}\\n\\end{figure}\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n% TODO: add some more decisions\\n\\n\\n%%% Local Variables:\\n%%% mode: latex\\n%%% TeX-master: \"..\/main\"\\n%%% End:\\n"}
{"File Name":"cena\/0006-manage-build-with-gradle.md","Context":"## Context\\n`menu-generation` application will use the Spring framework along with other third party libraries, thus requires a\\ndependency management tool.\\nEffective development lifecycle requires Continuous Integration, thus a build management tool is necessary.\\n[Gradle](https:\/\/gradle.org\/) is one of the two main build management tools for the Java ecosystem with [Maven](http:\/\/maven.apache.org\/).\\nGradle is considered as more extensive and quicker than Maven. It is also well integrated with many tools, including\\nthe Spring framework.\\n","Decision":"Gradle will be used to manage project dependencies and build tasks.\\n","tokens":111,"id":644,"Predictions":"## Decision: \\n\\n\\n\\t\\t\\#1. If not already included in the project, add the following dependency to the build.gradle file:\\n\\end{lstlisting}\\n\\subsubsection{Decision}\\nThe \\texttt{build.gradle} file is used to define dependencies and build configuration of a project. The \\texttt{build.gradle} file is located in the root directory and is executed by the Gradle build tool. The most important files in this directory are \\texttt{build.gradle} and \\texttt{pom.xml}. The \\texttt{build.gradle} file includes all dependencies and configurations that are necessary for a project to be built.\\n%In order to use a third-party library, it is necessary to include it in the \\texttt{build.gradle} file.\\n%In this decision, we will use Spring Boot with Hibernate as a third-party library. \\nThe \\texttt{build.gradle} file includes all dependencies and configurations that are necessary for a project to be built. When using Spring Boot with Hibernate, you need to add the following dependency on your \\texttt{pom.xml}.\\n\\begin{lstlisting}\\n<dependency>\\n  <groupId>org.hibernate<\/groupId>\\n  <artifactId>hibernate-core<\/artifactId>\\n<\/dependency>\\n\\end{lstlisting}\\n\\n%The \\texttt{pom.xml} file is used to specify dependencies and configuration for a project. It contains information about all required dependencies, their version, and their location. It also contains information about how these libraries should be resolved when building your application.\\n%\\begin{description}\\n%\\item[Dependency] : Contains information about all required dependencies, their version, and their location.\\n%\\item[Version] : Specifies which version of the dependency should be used by your application.\\n%\\item[Location] : Specifies where this dependency should be located.\\n%\\end{description}\\n\\n%When using Spring Boot with Hibernate, you need to add the following dependency on your \\texttt{pom.xml}.\\n%\\begin{lstlisting}\\n%%<dependency>\\n%%  <groupId>org.hibernate<\/groupId>\\n%%  <artifactId>hibernate-core<\/artifactId>\\n%%  <version>5.2.8.Final<\/version>\\n%%<\/dependency>\\n\\n%%<dependency>\\n%%  <groupId>com.google.guava<\/groupId>\\n%%  <artifactId>guava<\/artifactId>\\n%%  <version>20.0-jre-alpine<\/version>\\n%%<\/dependency>\\n\\n%%\\n%%%<dependency>\\n%%%  <groupId>org.springframework.boot<\/groupId>\\n%%%  <artifactId>spring-boot-starter-parent<\/artifactId>\\n%%%  <version>${springBootVersion}<\/version>\\n%%%  <scope>provided<\/scope>\\n%%% <\/dependency>\\n\\n%%\\n%%%<dependency><groupId>com.google.guava<\/groupId><artifactId>guava-18-jre-alpine<\/artifactId><\/dependency>\\n\\n%%\\n%%%<dependency><groupId>joda-time-jdk15-alpine-1.8.jar><\/groupidemember><artifactId>joda-time-jdk15-alpine-1.8.jar><\/artifactidemember><\/dependency>\\n\\n%%\\n%%<dependency><groupId>joda-time-alpine-1.8.jar><\/groupidemember><artifactId>joda-time-alpine-1.8.jar><\/artifactidemember><\/dependency>\\n\\n%%\\n%%%<dependency><groupId>joda-time-jdk15-alpine-1.7.jar><\/groupidemember><artifactId>joda-time-jdk15-alpine-1.7.jar><\/artifactidemember><\/dependency>\\n\\n%%\\n%%%<dependency><groupId>joda-time-jdk15-alpine-1.6.jar><\/groupidemember><artifactId>joda-time-jdk15-alpine-1.6.jar><\/artifactidemember><\/dependency>\\n\\n%%\\n%%%<dependency><groupId>joda-time-jdk15-alpine-1.5.jar><\/groupidemember><artifactId>joda-time-jdk15-alpine-1.5.jar><\/artifactidemember><\/dependency>\\n\\n\\n\\subsubsection*{\\large Decision}\\nThe decision is based on two requirements: (a) development lifecycle requires Continuous Integration (CI) and (b) a build management tool is necessary.\\n\\n\\begin{description}\\n\\item[Requirement (a)] Development lifecycle requires Continuous Integration\\nWhen using Spring Boot with Hibernate as a third-party library, you need to add the following dependency on your \\texttt{pom.xml}. This dependency will ensure that every time you build your application, it will automatically run tests.\\n\\n\\begin{lstlisting}\\n<!--This depends on Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n<!--Gradle-->\\n\\n<!--\\nThis depends on Gradle -->\\n<!--\\nGradle -->\\n<!--\\nGradle -->\\n<!--\\nGradle -->\\n<!--\\nGradle -->\\n\\n<!--\\nThis depends on Gradle -->\\n<!--\\nGradle -->\\n<!--\\nGradle -->\\n<!--\\nGradle -->\\n\\n<!--\\nThis depends on Gradle -->\\n<!--\\ngradlew --configuration test --stacktrace --stacktrace-only test\\n-->\\n\\n\/\/ This depends on Gradle <\/code>\\n\\n\\n%No other methods needed for this decision\\n\\end{lstlisting}\\n\\n%When building your application with Spring Boot with Hibernate as a third-party library, every time you run tests, it will automatically run tests.\\n\\nWhen building your application with Spring Boot with Hibernate as a third-party library, every time you run tests,\\nit will automatically run tests.\\n\\nMore precisely, when building an application with Spring Boot as a third-party library,\\nyou can configure Gradles test runner by adding `--configuration test` argument at compile time or by configuring Gradles `--stacktrace` argument at runtime.\\n\\n\\nThe `--configuration test` argument specifies which configuration of the test runner should be used in case of running tests.\\n\\nFor example,\\nif you want to use Mockito instead of Junit when running tests,\\nyou can specify `--configuration mockito` at compile time or\\nconfigure Mockito `--stacktrace` argument at runtime.\\n\\n\\nWhen using Spring Boot with Hibernate as a third-party library,\\nyou can configure Gradles test runner by adding `--configuration test` argument at compile time or by configuring Gradles `--stacktrace` argument at runtime.\\n\\n\\nThe `--configuration test` argument specifies which configuration of the test runner should be used in case of running tests.\\n\\n\\nFor example,\\nif you want to use Mockito instead of Junit when running tests,\\nyou can specify `--configuration mockito` at compile time or\\nconfigure Mockito `--stacktrace` argument at runtime.\\n\\n\\nIn order to use Mockito instead of Junit when running tests,\\nyou can specify `--configuration mockito` at compile time or\\nconfigure Mockito `--stacktrace` argument at runtime.\\n\\n\\nAlso remember that there are two different ways for configuring gradles:\\n```\\ngradlew --configuration TestRunnerTestRunner --testArguments \"-DtestRunnerTestRunnerTestRunnerArgs=foo\"\\ngradlew --configuration TestRunnerTestRunner --testArguments \"-DtestRunnerTestRunnerTestRunnerArgs=bar\"\\n```\\nwhere `-DtestRunnerTestRunnerTestRunnerArgs=foo` means `-DtestRunnerTestRunnerTestRunnerArgs=foo`, `-DtestRunnerTestRunnerTestRunnerArgs=bar`, etc.\\nOne way uses gradles arguments directly while another one uses gradles arguments through `-D`.\\nIf you want to use gradles arguments directly without using gradles arguments through `-D`, then you have two options:\\n```\\ngradlew -DtestRunnersArgs=\"-foo\"\\ngradlew -DtestRunnersArgs=\"-bar\"\\n```\\nor\\n```\\ngradlew -classpath . -Djunit.skipTests=false -classpath . -Dmockito.skipTests=false -classpath . -javaagent:C:\\Users\\Eli\\Desktop\\spring-boot-starter-test-master\\spring-boot-starter-test-master\/target\\spring-boot-starter-test-master.jar:.\/target\/test-assemblies\/target\/classes org.junit.runner.JUnitCore\\n```\\nwhere `-classpath .` means include all jar files from current directory into class path so that each class can find its related jar files during execution.\\nAnd if you want to configure gradles through command line:\\n```\\ngradlew -classpath .\/target\/test-assemblies\/target\/classes org.junit.runner.JUnitCore\\n```\\nwhere `-classpath .\/target\/test-assemblies\/target\/classes org.junit.runner.JUnitCore`.\\nYou may also want to know how do I pass arguments during compilation?\\nThere are two ways:\\n\\nAdd arguments before compilation\\n\\nAdd arguments during compilation\\n\\nBoth methods are described below:\\n\\nAdd Arguments before Compilation\\n\\nTo add arguments before compilation:\\n```\\ngradlew --configurations TestRunnersConfiguration \\\\n    --project .\/src\/main\/java \\\\n    --project .\/src\/main\/resources \\\\n    --project .\/src\/main\/resources\/application.properties \\\\n    $HOME\/.gradle\/caches\/modules-2\/files-2.x\/maven-metadata.xml \\\\n    $HOME\/.gradle\/caches\/modules-2\/files-2.x\/maven-metadata.lock \\\\n    $HOME\/.gradle\/caches\/modules-2\/files-2.x\/maven-metadata.xml\\n\\n# Run unit tests only\\n$HOME\/.gradle\/caches\/modules-2\/files-2.x\/mvn-dependency-plugin install \\\\n    $HOME\/.gradl"}
{"File Name":"cena\/0004-expose-services-through-rest-api.md","Context":"## Context\\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\\napplication or enhance the provided services.\\nWe want the application data and services to be easily consumed through well established communication protocols, such\\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\\nmanaging cumbersome protocols or data formats.\\n","Decision":"`menu-generation` application data and services will be exposed through a REST API.\\n","tokens":110,"id":645,"Predictions":"\\n`menu-generation` application must expose data and services in order to be used by multiple different applications,\\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\\napplication or enhance the provided services.\\nWe want the application data and services to be easily consumed through well established communication protocols, such\\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\\nmanaging cumbersome protocols or data formats.\\n\\n## Context\\nThe information required for this decision is on the right side of this page. The context is:\\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\\napplication or enhance the provided services.\\nWe want the application data and services to be easily consumed through well established communication protocols, such\\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\\nmanaging cumbersome protocols or data formats.\\n\\n## Decision\\n\\end{description}\\n"}
{"File Name":"cena\/0012-document-apis-with-spring-rest-docs.md","Context":"## Context\\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\\nthe resources attributes significance.\\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\\ncomplete overview of the features and technical concerns.\\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\\nforth between those parts.\\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\\n","Decision":"`menu-generation` will generate its API documentation with [Spring Rest-Docs](https:\/\/spring.io\/projects\/spring-restdocs).\\n","tokens":128,"id":646,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\nThe APIs documentation is not enough. It is important to also have a good understanding of the underlying application\\nfeatures and technical concerns. The APIs documentation must provide links between different parts of the document, so that\\nit is easy to go back and forth between those parts.\\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\\n\\n\\nThis is a decision record for a software.\\n\\n\\n"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"Predictions":"## Command-query separation\\nCommand-query separation is a software design principle that can be applied to any software that has\\nmultiple methods, and that uses queries to determine which one to execute. In the case of menu-generation,\\nwe have a command that selects the appropriate menu, and a query to determine which menu should be displayed.\\n\\nThe command is executed immediately, and the query is executed only if it returns false. The query may be executed twice,\\neither time in the case of an error.\\n\\n## Decision\\n## Command-query separation\\nCommand-query separation is used in the `menu-generation` application.\\n\\nThe command selects the appropriate menu, and the query determines which menu should be displayed.\\n\\n## Decision\\n## Command-query separation\\nCommand-query separation is used in the `menu-generation` application.\\n\\nThe command selects the appropriate menu, and the query determines which menu should be displayed.\\n\\n## Decision\\n## Command-query separation\\nCommand-query separation is used in the `menu-generation` application.\\n\\nThe command selects the appropriate menu, and the query determines which menu should be displayed.\\n\\n## Decision\\n## Command-query separation\\nCommand-query separation is used in the `menu-generation` application.\\n\\nThe command selects the appropriate menu, and the query determines which menu should be displayed.\\n"}
{"File Name":"cena\/0007-execute-build-tasks-with-make.md","Context":"## Context\\nWe don't want the development tools or Continuous Integration pipeline to be strongly bound to [Gradle](0006-manage-build-with-gradle.md).\\n[Make](https:\/\/linux.die.net\/man\/1\/make) is an utility agnostic of any language or build management tools.\\n","Decision":"Make will be used to execute build tasks, abstracting Gradle and potential other tools used during build execution.\\n","tokens":64,"id":648,"Predictions":"## [1]: https:\/\/linux.die.net\/man\/1\/make\\n## [2]: https:\/\/linux.die.net\/man\/1\/make#Build-System\\n## [3]: https:\/\/linux.die.net\/man\/1\/make#Build-System\\n## [4]: https:\/\/linux.die.net\/man\/1\/make#Build-System\\n## [5]: https:\/\/linux.die.net\/man\/1\/make#Build-System\\n## [6]: https:\/\/linux.die.net\/man\/1\/make#Build-System\\n\\n### Build System (Make)\\n\\n### Makefile\\n\\n### Build System (CMake)\\n\\n### CMakeLists.txt\\n\\n### Build System (Gradle)\\n\\n### Gradlefile\\n\\n\\n\\nIn the above code, we have a Makefile. We have to specify the dependency of the build system. We have to specify the build process. We have to specify the dependencies and build process.\\n\\nWe can use the following command to generate a Makefile:\\n\\nmake\\n\\nThis command will generate a Makefile in a makefile directory.\\n\\nYou can also use makefilegenerator tool for generating a make file directly from CMakeLists.txt.\\n\\n### CMakeLists.txt\\n\\nThis is a configuration file for cmake. It specifies all the dependencies and build process.\\n\\nThe following are some examples of CMakeLists.txt:\\n\\n### CMakeLists.txt for Windows\\n\\n  * #include <Windows.h>\\n  * #include <iostream>\\n  * #include <Windows.h>\\n\\n  #include <Windows.h>\\n  #include <Windows.h>\\n\\n  #include \"stdafx.h\"\\n  #include <windows.h>\\n\\n  #include <windows.h>\\n\\n  #include \"stdafx.h\"\\n  \\n#### Sample C++ Project ####\\n\\n\\n#include <iostream>\\n#include <Windows.h>\\n\\nusing namespace std;\\n\\nint main()\\n{\\n    cout << \"Hello World!\\n\";\\n    return 0;\\n}\\n\\n#### Sample C++ Project ####\\n\\n\\n#include <iostream>\\n#include \"stdafx.h\"\\n\\nusing namespace std;\\n\\nint main()\\n{\\n    cout << \"Hello World!\\n\";\\n    return 0;\\n}\\n\\n#### Sample C++ Project ####\\n\\n\\n#include <iostream>\\n#include \"stdafx.h\"\\n\\nusing namespace std;\\n\\nint main()\\n{\\n    cout << \"Hello World!\\n\";\\n    return 0;\\n}\\n\\n#### Sample C++ Project ####\\n\\n\\n#include <iostream>\\n#include \"stdafx.h\"\\n\\nusing namespace std;\\n\\nint main()\\n{\\n    cout << \"Hello World!\\n\";\\n    return 0;\\n}\\n\\n#### Sample C++ Project ####\\n\\n\\n#include <iostream>\\n#include \"stdafx.h\"\\n\\nusing namespace std;\\n\\nint main()\\n{\\n    cout << \"Hello World!\\n\";\\n    return 0;\\n}\\n```\\n\\n\\n\\n### Build System (CMake)\\n\\nYou can also create a new project using cmake and add it to your project structure like this:\\n\\n`mkdir MyProject && cd MyProject`\\n\\n\\n\\nThen you can run `cmake .` command as shown above. This will create a new project in your workspace named MyProject.\\n\\n\\n\\nYou can also use `cmake ..` command to add all your projects under one directory as shown below:\\n\\n\\n\\nNow you can run `make` command as shown above.\\n\\n\\n\\nThe below are some examples of cmake:\\n\\n### Example of cmake for Windows\\n\\n   * cmake_minimum_required(VERSION ${CMAKE_VERSION})\\n\\n   cmake_minimum_required(VERSION ${CMAKE_VERSION})\\n\\n   set(CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS} -std=c++11 -Wall -pedantic)\\n\\n\\n   set_property(TARGET my_target PROPERTY BROWSE_LOCATION ${PROJECT_SOURCE_DIR}\/my_project)\\n  \\n   set_property(TARGET my_target PROPERTY BROWSE_SEARCH_PATH \"${PROJECT_SOURCE_DIR}\/my_project\" DESTINATION ${PROJECT_BINARY_DIR})\\n  \\n   set_property(TARGET my_target PROPERTY BROWSE_LIBRARY \"${PROJECT_BINARY_DIR}\/lib\/my_library.so\")\\n\\n   \\n   include_directories(${PROJECT_SOURCE_DIR})\\n   \\n   find_package(Qt5 REQUIRED)\\n   \\n   target_link_libraries(my_target Qt5::Core)\\n\\n  \\n\\n\\n#### Example of cmake for Linux\\n\\n   * set(CMAKE_CXX_FLAGS \"-std=c++11 -Wall -pedantic\")\\n\\n   set(CMAKE_CXX_FLAGS \"-std=c++11 -Wall -pedantic\")\\n\\n   set(CMAKE_CXX_FLAGS \"-DQT_NO_DEBUG -DQT_NO_DEPRECATED_WARNINGS\")\\n  \\n   target_link_libraries(my_target Qt5::Core)\\n   \\n  \\n\\n#### Example of cmake for Mac OS X\\n\\n* target_link_libraries(my_target Qt5::Core)\\n  \\n\\n#### Example of cmake for Unix\/Linux\\n\\n* target_link_libraries(my_target Qt5::Core)\\n  \\n\\n##### Example of makefiles with different targets\\n\\nThe following are some examples of makefiles:\\n\\nExample of makefiles for Windows\\n\\nExample of makefiles for Linux and Unix\/Linux\\n\\n\\n\\n\\nThe below are some examples of Makefiles:\\n\\nExample of makefiles for Windows\\n\\n\\n\\n\\nExample of Makefiles for Linux and Unix\/Linux\\n\\n\\n\\n\\n##### Example of cross-compiling projects using Makefile generator tool\\n\\n\\n\\nYou can also use the following tool to cross-compile your project using Makefile generator tool:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n##### Cross-compiling cross-platform projects using CMake generators\\n\\n\\n\\nYou can also use the following generator tool to cross-compile your project using cross-platform tools like: Visual Studio, Eclipse, or Android Studio:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n##### Cross-compiling cross-platform projects using Make generators\\n\\n\\n\\nYou can also use the following generator tool to cross-compile your project using cross-platform tools like: Visual Studio, Eclipse, or Android Studio:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n\\n\\n##### Cross-compiling cross-platform projects with help from Gradle scripts\\n\\n\\n\\nYou can also use Gradle scripts to cross-compile your project with help from Gradle scripts:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n\\n\\n##### Cross-compiling cross-platform projects with help from Visual Studio setup files\\n\\n\\n\\nYou can also use Visual Studio setup files to cross-compile your project with help from Visual Studio setup files:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n\\n\\n##### Cross-compiling cross-platform projects with help from Android studio setup files\\n\\n\\n\\n\\nYou can also use Android studio setup files to cross-compile your project with help from Android studio setup files:\\n\\n\\n\\nhttps:\/\/github.com\/KhurramZarif\/cross_compile_tool\\n\\n\\n\\n\\n\\n##### Cross-compiling cross-platform projects with help from Eclipse setup files\\n\\n\\n\\n\\nEclipse setup files will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nEclipse setup file will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nEclipse setup file will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nEclipse setup file will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nEclipse setup file will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nEclipse setup file will provide you an Eclipse environment which would be used as an IDE during development phase where you would be able to compile and run your application.\\n\\n\\n\\nIt is important that we should use proper tools during development so that we don't need any additional configuration in our build system.\\n"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"Predictions":"\\nA decision is required to be taken on the following points:\\n\\n1. The architecture of the software should be decoupled from the infrastructure. This will allow the infrastructure to evolve independently of the software.\\n2. The architecture should provide a clear separation between services and core functionality. This will allow for easier maintenance and reuse.\\n3. The architecture should provide a clear separation between data and logic. This will allow for easier maintenance and reuse.\\n4. The architecture should provide a clear separation between the REST API and the API layer itself.\\n5. The architecture should provide a clear separation between the library implementation and its use in other parts of the codebase.\\n6. The library implementation should provide an interface that can be used by other libraries.\\n7. The library implementation should provide a well defined interface to be used by other libraries, which can then be used by services implementations.\\n8. The library implementation should provide an interface that can be used by other libraries, which can then be used by REST services implementations.\\n9. A decision should be made if the software will use Spring or not, as it will have to interact with Spring during development and deployment days.\\n"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"Predictions":"\\begin{itemize}\\n\\item Acceptance tests must be written in a way that is easy for the developer to understand.\\n\\item The acceptance tests must be understandable by any user, either technical or not.\\n\\item The application sources must be self-contained, including the acceptance tests definition and implementation.\\n\\end{itemize}\\n## Context\\nThe user interface should provide a consistent and intuitive experience for users.\\nThe application should be designed to allow the users to interact with it in a consistent way.\\nUser interface design should follow the [User Experience](https:\/\/en.wikipedia.org\/wiki\/User_experience_design)\\nstandards and practice.\\n\\n## Decision\\n\\begin{itemize}\\n\\item The application should provide a consistent and intuitive experience for users.\\n\\item The application should be designed to allow the users to interact with it in a consistent way.\\n\\end{itemize}\\n## Context\\nThe software should have a high level of security.\\nThe software should have adequate security mechanisms to ensure that only authorized users can access it.\\nAccess control mechanisms must provide for restricting access to specific resources or subroutines.\\nAccess control mechanisms must also ensure that no unauthorized user can make unauthorized modifications to the software.\\n\\n## Decision\\n\\begin{itemize}\\n\\item The software should have adequate security mechanisms to ensure that only authorized users can access it.\\n\\item Access control mechanisms must provide for restricting access to specific resources or subroutines.\\n\\end{itemize}\\n## Context\\nThe software should have an efficient data storage mechanism.\\nData storage mechanisms must support efficient data retrieval from memory, disk or other media.\\nData storage mechanisms must also support efficient data transfer between different media devices.\\nData storage mechanisms must support efficient data search across all stored data.\\n\\n## Decision\\n\\begin{itemize}\\n\\item The software should have an efficient data storage mechanism.\\n\\item Data storage mechanisms must support efficient data retrieval from memory, disk or other media.\\n\\end{itemize}\\n## Context\\nThe software should maintain an appropriate balance between performance and memory requirements for different applications of this system.\\nThe system architecture must allow scalability without compromising performance.\\nThe system architecture must allow flexibility without sacrificing performance and scalability at different levels of abstraction \\textit{(e.g.}: virtual machine, operating system component).\\nThe system architecture must allow reusability without compromising performance and scalability at different levels of abstraction (e.g.}: virtual machine, operating system component).\\nThe system architecture must allow flexibility without sacrificing performance and scalability at different levels of abstraction \\textit{(e.g.}: virtual machine, operating system component).\\nThe system architecture must allow reusability without compromising performance and scalability at different levels of abstraction (e.g.}: virtual machine, operating system component).\\n## Decision\\n% \\begin{enumerate}[label=\\alph*)]\\n% \\setcounter{enumi}{3}\\n% \\setcounter{enumii}{3}\\n\\n%     \\setcounter{enumii}{0}\\n\\n%     \\setcounter{enumiii}{0}\\n\\n%     \\setcounter{enumiv}{0}\\n\\n%     \\setcounter{enumv}{0}\\n\\n%     \\setcounter{enumvi}{0}\\n\\n%     % TODO: move this section here\\n\\n%   % TODO: add more description about how these tests are run\\n\\n%   % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n    % TODO: add more description about how these tests are run\\n\\n\\n%\\end{enumerate}"}
{"File Name":"cena\/0001-use-java-as-language.md","Context":"## Context\\nWe need to choose a programming language to implement the `menu-generation` application.\\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\\n","Decision":"Java is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\\nprogramming language used to implement the `menu-generation` application.\\n","tokens":62,"id":651,"Predictions":"## \\#1. Java\\n## \\#2. Python\\n## \\#3. C++\\n## \\#4. C++\\n\\end{verbatim}\\n\\subsubsection{Implementation}\\nThe software will be implemented in Java and the programming language will be chosen from the list above.\\n\\subsubsection{Testing}\\nTesting will be done by using IntelliJ IDEA IDE and the following tests:\\n\\n\\begin{itemize}\\n    \\item Unit testing\\n    \\item Integration testing\\n    \\item System testing\\n    \\item Performance testing\\n\\end{itemize}\\n\\nTesting will be done by using the following tools:\\n\\n\\begin{itemize}\\n    \\item JUnit 4\\n    \\item Mockito mockito\\n\\end{itemize}\\n\\nThe details of these tests are as follows:\\n\\n\\begin{enumerate}\\n    \\item Unit testing: unit tests will be run using the testNG framework to test each method of the application. The test class for each method is created using the TestNG framework.\\n    \\item Integration testing: integration tests will be run using the JUnit framework to test that all methods are working as expected.\\n    \\item System testing: system tests will be run using the JUnit framework to test that all methods are working as expected.\\n    \\item Performance testing: performance tests will be run using the JMeter framework to measure how long it takes to execute each method of the application.\\n\\end{enumerate}\\n\\nThe details of these tests are as follows:\\n\\n\\begin{enumerate}\\n    \\item Unit testing: unit tests will be run using JUnit framework to test that all methods are working as expected.\\n    \\item Integration testing: integration tests will be run using JUnit framework to test that all methods are working as expected.\\n    \\item System testing: system tests will be run using JUnit framework to test that all methods are working as expected.\\n    \\item Performance testing: performance tests will be run using JMeter framework to measure how long it takes to execute each method of the application.\\n\\end{enumerate}\\n\\nAll these tests are executed in IntelliJ IDEA IDE with an optional Java 8 Runtime Environment.\\n\\n"}
{"File Name":"cena\/0010-implement-acceptance-tests-with-serenity.md","Context":"## Context\\n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance\\nscenarii. However, to help maintaining an acceptance tests client library, we need to organize this library to be\\nextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.\\nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about\\nwhat wrong happened during API calls.\\n","Decision":"The [Serenity](http:\/\/www.thucydides.info\/#\/) framework will be used to define the acceptance tests library.\\n","tokens":100,"id":652,"Predictions":"We will use Cucumber to generate acceptance tests. The Gherkin interpreter will be the one providing the\\nbehaviour of the system and the API unitary steps will be written in Cucumber. This decision is made\\nbecause we want to focus on acceptance tests instead of building an acceptance test client library.\\n\\n## Decision\\nThe acceptance tests report must be readable and help investigating in case of error, providing hints about what wrong happened during API calls.\\nThe acceptance tests results report must be written in a format that can be easily read by humans. To achieve this, we will use Cucumber.\\n\\n## Decision\\nWe will use Cucumber to generate acceptance tests. The Gherkin interpreter will be the one providing the behaviour of the system and the API unitary steps will be written in Cucumber. This decision is made because we want to focus on acceptance tests instead of building an acceptance test client library.\\n\\n## Decision\\nThe acceptance tests report must be readable and help investigating in case of error, providing hints about what wrong happened during API calls.\\nThe acceptance tests results report must be written in a format that can easily read by humans. To achieve this, we will use Cucumber.\\n\\n\\n\\begin{figure}[htbp]\\n\\centering\\n\\includegraphics[width=1\\linewidth]{figures\/decision.pdf}\\n\\caption{Decision}\\n\\label{fig:decision}\\n\\end{figure}\\n\\n"}
{"File Name":"cena\/0008-package-application-in-docker-image.md","Context":"## Context\\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\\ndependent of the underlying system.\\n[Docker](https:\/\/www.docker.com\/) is a widespread container based solution that can be used during development lifecycle\\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https:\/\/kubernetes.io\/).\\n","Decision":"`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\\nto help running the application and its dependencies in a consistent and isolated environment.\\n","tokens":107,"id":653,"Predictions":"\\nThe decision is to run the application on the test machine.\\n\\nThe following steps are performed by the application:\\n\\begin{itemize}\\n    \\item Create a `docker-compose.yml` file with `image: spring-boot:1.5.4.RELEASE`\\n    \\item Run `docker-compose up -d`\\n    \\item Run `docker-compose exec -it app1 services start`\\n\\end{itemize}\\n\\nThe application is run on the test machine with the following command:\\n\\begin{verbatim}\\ndocker-compose exec -it app1 services start\\n```\\n\\nThe result of this command is:\\n\\begin{verbatim}\\nStarting app1...\\nRunning app1...\\n```\\n\\nIf everything went well, the application will be running on the test machine.\\n\\nIf something went wrong, we will see that there is an error in our Dockerfile.\\n\\nThis error message will be displayed in our browser (e.g., chrome) and we can take a look at what happened during the build process.\\n\\n```\\n[INFO] --- docker-compose-cli v0.3.0: CLI tool for docker-compose ---  \\n[INFO] \\n[INFO] --- docker-compose-cli v0.3.0: CLI tool for docker-compose ---  \\n[INFO] \\n[INFO] --- docker-compose-cli v0.3.0: CLI tool for docker-compose ---  \\n[INFO] \\n[INFO] --- docker-compose-cli v0.3.0: CLI tool for docker-compose ---  \\n[INFO]\\n```\\n\\nLet's look at what happened during the build process.\\n\\n## Dockerfile\\nThe first step of building any container image is creating a Dockerfile file.\\nA Dockerfile contains information about how to create a container image from an existing source code base.\\nA Dockerfile file can contain one or more commands and instructions.\\nEach command and instruction should be separated by a space or a newline character.\\nHere are some examples of commands and instructions in Dockerfiles:\\n\\n\\begin{verbatim}\\nFROM ubuntu\\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\n  wget curl git && \\\\n  mv \/etc\/apt\/sources.list \/etc\/apt\/sources.list.bak && \\\\n  rm \/etc\/apt\/sources.list && \\\\n  rm \/etc\/apt\/sources.list.d\/* && \\\\n  rm \/etc\/apt\/preferences.d\/*\\nFROM ubuntu\\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\n  wget curl git && \\\\n  mv \/etc\/apt\/sources.list.d\/* \/etc\/apt\/sources.list && \\\\n  rm \/etc\/apt\/preferences.d\/*\\nFROM ubuntu\\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\n  wget curl git && \\\\n  mv \/usr\/bin\/tar.gz $HOME\/tar.gz &&\\nCOPY . .\\nRUN mkdir $HOME\/tar.gz &&\\nCMD [\"tar\", \"-xzf\", \"$HOME\/tar.gz\"]\\nFROM ubuntu\\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\n  wget curl git && \\\\n  mv \/usr\/bin\/tar.gz $HOME\/tar.gz &&\\nCOPY . .\\nRUN mkdir $HOME\/tar.gz &&\\nCMD [\"tar\", \"-xzf\", \"$HOME\/tar.gz\"]\\nFROM ubuntu\\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\n  wget curl git && \\\\n  mv \/usr\/bin\/curl $HOME\/curl &&\\nCOPY . .\\nRUN mkdir $HOME\/curl &&\\nCMD [\"curl\"]\\n```\\n\\n## Build Command\\nA build command is used to create an executable image from source code base.\\nBuild commands follow a similar pattern as Dockerfiles, but they contain more commands and instructions.\\nBuild commands can contain one or more commands and instructions.\\n\\n\\begin{verbatim}\\nADD . \/\\nRUN mkdir $HOME\/build &&\\nCMD [\"\/bin\/bash\", \"-c\", \"mkdir -p '$HOME\/build'; echo '$PWD' > '$HOME\/build\/test.txt'\" ]\\n```\\n\\n## Test Command\\nA test command is used to run unit tests against executable image created by build command.\\nTest commands follow a similar pattern as Dockerfiles, but they contain more commands and instructions.\\n\\n\\begin{verbatim}\\nADD . \/\\nRUN mkdir $HOME\/test &&\\nCMD [\"\/bin\/bash\", \"-c\", \"echo 'hello world' > '$HOME\/test\/test.txt'\" ]\\n```\\n\\n\\n## Build Command Examples\\n\\nHere are some examples of build commands:\\n\\n\\begin{verbatim}\\nADD . \/\\nCMD bash .\/build.sh\\n# or:\\nCMD bash .\/build.sh <command>\\n# or:\\nCMD bash .\/build.sh <command> <arguments>\\n# or:\\nCMD bash .\/build.sh <command> <arguments> <arguments>\\n# or:\\nCMD bash .\/build.sh <command> <arguments> <arguments> <arguments>\\n# or:\\nCMD bash .\/build.sh <command> <arguments> <arguments>\\n# or:\\nCMD bash .\/build.sh <command> <arguments>\\n\\nADD . \/\\nCMD \".\/sh\" \"$@\"\\n```\\n\\n\\n## Test Command Examples\\n\\nHere are some examples of test commands:\\n\\n\\begin{verbatim}\\nADD . \/\\nTESTDIR=\/tmp\/$PWD\/unit-tests\/\\nTESTFILE=test.txt &\\nTESTDIR=\/tmp\/$PWD\/unit-tests\/\\nTESTFILE=test.txt &\\nTESTDIR=\/tmp\/$PWD\/unit-tests\/\\nTESTFILE=test.txt &\\nTESTDIR=\/tmp\/$PWD\/unit-tests\/\\nTESTFILE=test.txt &\\nTESTDIR=\/tmp\/$PWD\/unit-tests\/\\n```\\n\\n\\n## Build Command Example\\n\\nHere is an example of a build command:\\n\\n\\begin{verbatim}\\nADD . \/\\nBUILD=app1-1.5.4.RELEASE\\n\\nBUILD COMMANDS EXAMPLES\\n\\nBUILD COMMANDS EXAMPLES\\n\\nBUILD COMMANDS EXAMPLES\\n\\n\\n### Running Unit Tests\\n\\n### Running Integration Tests\\n\\n### Running System Tests\\n\\n\\n### Running Acceptance Tests\\n\\n\\n### Running Acceptance Tests (CONTINUED)\\n```\\n\\n\\n\\nIn this example, we have created a Dockerfile file that contains all necessary information to create an executable image from source code base.\\nWe have added instructions to create an executable image from source code base using `apt-get` package manager.\\n\\nNow let's look at how we can execute our Dockerfile file using `docker` command.\\n\\nFirst, let's create an empty directory where we will store our executable images.\\n\\nThen, let's call `docker` command:\\n\\n\\begin{verbatim}\\n$ docker images | grep spring-boot | head -n2 | cut -d\" \" -f2-\\nspring-boot-starter-web-1.5.4.RELEASE@sha256:e9a6b8f8e7c5b6fa9a9d64e7f35a17fc148e4c26b4d56e8cd36e6a7b96caad19f\\n$ docker images | grep web-app | head -n2 | cut -d\" \" -f2-\\nweb-app-1.5.4.RELEASE@sha256:e9a6b8f8e7c5b6fa9a9d64e7f35a17fc148e4c26b4d56e8cd36e6a7b96caad19f\\n$ docker images | grep spring-boot-web-app-1.5.4.RELEASE@sha256:e9a6b8f8e7c5b6fa9a9d64e7f35a17fc148e4c26b4d56e8cd36e6a7b96caad19f\\n$ docker images | grep web-app-web-1.5.4.RELEASE@sha256:e9a6b8f8e7c5b6fa9a9d64e7f35a17fc148e4c26b4d56e8cd36e6a7b96caad19f\\n$ docker images | grep spring-boot-web-app-web-1.5.4.RELEASE@sha256:e9a6b8f8e7c5b6fa9a9d64e7f35a17fc148e4c26b4d56e8cd36ed18cb93aa33ff50ec3fd39bb55ba2abdbda57ef7592cecb28bc75aeeddb95be59ce56feff97363bd62bb60ef747dc524ef76be94cc46ac94ff47ebdbce69df51ecbe71142bf67fe94ee20fdabcb780dc255bcfa2ee27ef41467fb083de26155277ea41de646aa44fbfe31ddac00db18059013cc65841ebbfbcdd81958087ddbf65cfdf48ac51fd62103ec43524acfb18635150da04cc88bf90ed76709aea50115eb787dbcc92ba29900bd18bd02effab51183cf30ae81ba66af01af91500710af3bb605fb29df00ba72ff11ea64efa08712ec00df11137671da177db21de98afe13cf49aaa87bd79ada09eed54ae2ac54544fac75740af0cb42eb29bf46285dd10ee98cf07de68ed05cec83222bf06fd63da57004cc99dc24bc513aa80be86eee44ddaa88853eb39fabdf81592428348881deb97eb79ef50716fb78137dd78ea98fa62857adc67724194752cbd98411bb5893ef78de09183532dc82ec30db89aa14dd31583ea40ab99acc46ed1eeba752197ce28acd86424254371ca72aaa32edae74543ed17241bbccc66da15073de42914ceb40fe16dae11cef07fef52aece444952fee9320ab25be70984ecc45faf91ad41553ee39512565980303bc33942bcc39950352555801aea76046bf84fac90edb04cb492ade34eea955681c20aa04eee20833eb89ae86ef20544de45ded58ead90cc031ece04969ea55dac91ace23986efe69493ac06dc7232203cc70622bcc50bff05fb40dcc06ba005aba50166251fc93ce59ff47ddf02fdf69953ea71817649982df58eed91eda09eed54ae2ac54544fac75740af0cb42eb29bf46285dd10ee98cf07de68ed05cec83222bf06fd63da57004cc99dc24bc513aa80be86eee44ddaa88853eb39fabdf81592428348881deb97eb79ef50716fb78137dd78ea98fa62857adc67724194752cbd98411bb5893ef78de09183532dc82ec30db89aa14dd31583ea40ab99acc46ed1eeba752197ce28acd86424254371ca72aaa32edae74543ed17241bbccc66da15073de42914ceb40fe16dae11cef07fef52aece444952fee9320ab25be70984ecc45faf91ad41553ee39512565980303bc33942bcc39950352555801aea76046bf84fac90edb04cb492ade34eea955681c20aa04eee20833eb89ae86ef20544de45ded58ead90cc031ece04969ea55dac911ace23986efe69493ac06dc7232203cc70622bcc50bff05fb40dcc06ba005aba50166251fc93ce59ff47ddf02fdf69953ea71817649982df58eed91eda09eed54ae2ac54544fac75740af0cb42eb29bf46285dd10ee98cf07de68ed05cec83222bf06fd63da57004cc99dc24bc"}
{"File Name":"cena\/0011-expose-resources-as-hal.md","Context":"## Context\\n`menu-generation` application will expose its data and services through a [REST API](0004-expose-services-through-rest-api.md).\\nOne of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,\\nmeaning any resource should provide links to other accessible resources based on its current state in the application,\\nso that clients can discover which actions are available without interpreting the actual resource attributes.\\n[Spring HATEOAS](https:\/\/spring.io\/projects\/spring-hateoas) comes by default with [HAL](http:\/\/stateless.co\/hal_specification.html)\\nmedia type support.\\n","Decision":"`menu-generation` application will expose its resources using the HAL media type.\\n","tokens":134,"id":654,"Predictions":"## \\- The REST API will expose the following resources:\\n##   * `menu-generation` will expose the following services:\\n##     - `menu-creation` will expose a REST API to create new menus.\\n##     - `menu-deletion` will expose a REST API to delete existing menus.\\n##   - `menu-generators` will expose a REST API to generate new menu items.\\n##   - `menu-editors` will expose a REST API to edit existing menu items.\\n## \\- The REST API will expose the following resources:\\n##   * `menu-options` will expose a REST API to specify menu options.\\n## \\- The REST API will expose the following resources:\\n##   * `menu-details` will expose a REST API to display menu details.\\n## \\- The REST API will expose the following resources:\\n##   * `menu-details` will expose a REST API to display menu details.\\n## \\- The REST API will expose the following resources:\\n##   * `menu-details` will expose a REST API to display menu details.\\n"}
{"File Name":"cena\/0013-adopt-contract-first-with-openapi.md","Context":"## Context\\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\\nwell designed for specific use-cases, based on consumers point of view.\\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\\nthe application.\\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\\n","Decision":"[OpenAPI Specification](https:\/\/swagger.io\/specification\/) will be used to enforce contract-first approach.\\n","tokens":85,"id":655,"Predictions":"## \\begin{description}\\n##     \\item[Decision] \\textbf{Contract-first Approach} - API contract is defined before implementation. This ensures that the API is\\nwell designed for specific use-cases, based on consumers point of view.\\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\\nthe application.\\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\\n\\n## \\end{description}\\n\\end{document}"}
{"File Name":"opensmarthouse-core\/adr0002.md","Context":"## Context\\nMany modern systems require up-front data before a device can be discovered and added to a network. To facilitate this, a Provisioning system is to be added as part of the Discovery services, with the aim to provide the information required for devices to join the network.\\nTwo use cases are currently forseen.\\n1. Users must provide device information prior to discovery. This information is managed by the binding, however a consistent way is needed to provide this data from the user, to the binding. An example of this is systems may require a cryptographic key to be provided before a device is able to be discovered.\\n2. Centrally administered systems may want to provision information about a device from a central repository. Thus a user could be shipped the device, plug it in, and it would automatically be discovered and added with little user interaction.\\nIt is proposed to add a `ProvisioningService` and `ProvisioningRegistry` to act as an interface between the provisioning information (from the user, or other system) and the binding. The binding will be able to query the `ProvisioningRegistry` to retrieve information about any provisioned devices when discovery is enabled, and will be able to update the `ProvisioningRegistry` if a device is successfully discovered. Additionally, the `ProvisioningRegistry` can be queryable to allow the status of provisioned devices to be retrieved.\\nThe provisioning information provided through the `ProvisioningRegistry` is a set of properties that are interpretted by the binding. These are linked by a `ProvisioningUID` which is used to identify the device. This is akin to a `ThingTypeUID`, however used to identify the device before it is discovered.\\n","Decision":"TBD\\n","tokens":354,"id":656,"Predictions":"The decision is to create a `ProvisioningService` and `ProvisioningRegistry`. The service will be responsible for adding devices to the network and for providing information about the device. The registry will act as a central repository of information about devices, and will provide a mechanism for querying the information. This mechanism is designed to be as generic as possible, so that it can be used by other systems in the future.\\nThe service will also be responsible for providing the provisioning requirements on discovery. This requirement will specify which properties are required for a device to be provisioned, and how they should be interpreted by the binding. This can be used by the binding when it queries the registry to determine if a device is provisioned.\\nThe service will also provide a method of queryable querying of provisioning information. This can be used by other systems to query the registry directly.\\n\\nThe registry will have access to all provisioned devices, and will provide methods that allow querying of these devices. These methods can be used by other systems to query the registry directly.\\n\\nThe registry will also provide methods that allow querying of discovery information provided through this service. These methods can be used by other systems to query this information directly.\\n\\nFinally, both services are designed so that they can work in isolation from each other. The services do not require any knowledge of each other's functionality, so they can continue working even if one is disabled or removed.\\n\\n\\n## Constraints\\n\\begin{itemize}\\n    \\item The ProvisionsService must exist before Discovery.\\n    \\item The ProvisionsService must exist before Discovery.\\n    \\item The ProvisioningRegistry must exist before Discovery.\\n    \\item The ProvisioningRegistry must exist before Discovery.\\n\\end{itemize}\\n"}
{"File Name":"opensmarthouse-core\/adr0003.md","Context":"## Context\\nMany advanced features require additional information to be added to `Type`s - this could include secondary information as part of a `Command` or `State` update, or security context, `Command` source identifier, priority, etc. The type system structure inheritted from Eclipse SmartHome is difficult to extend due to the class hierarchy. A wrapper could be used around the `Command` or `State`, but this would require considerable rework of core functions.\\nThere is no base class for a `Type` - the ESH structure defines `Type`, `Command` and `State` as interfaces and many concrete `Type`s are derived from an `enum` (eg `OnOffType`, `UpDownType` etc) while others may be derived from Java classes (eg `DecimalType`). The absence of a clear class hierarchy within the framework makes it difficult or impossible for the framework to control and extend the `Type` system - one of the fundamental building blocks of the framework.\\nIt is proposed to rewrite the type system to introduce a clear class hierarchy. `Type`, `Command` and `State` remain as interfaces however an `AbstractBaseType` class is introduced from which all concrete `Type`s are derived. `ComplexType` and `PrimitiveType` are not generally used and are to be removed as they add unnecessary complexity. The type system should be implemented to be compatible at the basic level with the Eclipse SmartHome definitions such that backward compatability with existing bindings is not compromised.\\nThe ESH type system derives `DecimalType` and `QuantityType` from `Number` which makes it impossible to extend the `AbstractBaseType`. To resolve this we provide the core `Number` functionality with a new `NumberType` class. This provides the same interface, but can now extend `AbstractBaseType`. This potentially means that any code within the framework that checks `instanceof Number` will now fail and such checks must instead use `instanceof NumberType`.\\n","Decision":"This change provides a better separation between Java and OpenSmartHouse types, and most importantly allows further enhancements in functionality so is agreed to be implemented.\\n","tokens":405,"id":657,"Predictions":"## Decision\\nThe decision is to rewrite the type system based on the ESH type system. This will enable the framework to extend the Eclipse SmartHome type system in a straightforward manner.\\nThe new type system should be compatible with existing Eclipse SmartHome bindings and should not introduce any new functionality.\\n## Context\\n## Context\\nThis decision relates to a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## Decision\\nThe `NumberType` class provides a new interface for types that can be used in place of `Number` in many places. It is proposed to rewrite the type system to introduce a new `NumberType` class that provides an interface for types that can be used instead of `Number`. The simplified interface will allow classes and methods that check `instanceof Number` to fail while those that check `instanceof NumberType` can still pass.\\nThe new class will provide an abstract base type, which will be derived from `Number`, and an interface, which will be derived from `NumberType`. The abstract base type will provide all functionality required by the Eclipse SmartHome codebase (eg checking if a given number is within bounds) while providing all functionality required by any classes or methods that need it (eg checking if a given number is between two values). The interface will provide all functionality required by any classes or methods that need it. In this way, only one implementation of each abstract base type is needed (eg for number, two implementations are required). The implementation of these interfaces should be compatible with existing Eclipse SmartHome bindings.\\n## Context\\n## Context\\nThis decision relates to a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\n## Context\\nThis decision relates to a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n```\\nI have tried this solution but I get this error:\\n```\\nException in thread \"main\" java.lang.NoClassDefFoundError: org\/eclipse\/ehs\/types\/AbstractBaseType\\n    at com.github.jeetbawa.ESH_Decision_Decision.main(ESH_Decision_Decision.java:40)\\nCaused by: java.lang.ClassNotFoundException: org.eclipse.ehsm.types.AbstractBaseType\\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\\n    at sun.misc.Launcher$AppClassLoader.findClass(Launcher.java:331)\\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\\n    ... 14 more\\n```\\nWhat am I doing wrong here?\\nI have also tried this solution:\\n```\\npublic class ESH_Decision_Decision {\\n  public static void main(String[] args) throws Exception {\\n      ESH_Decision_Decision ehd = new ESH_Decision_Decision();\\n      ehd.decide();\\n  }\\n}\\n```\\nAnd it gives me this error:\\n```\\nException in thread \"main\" java.lang.NoClassDefFoundError: org\/eclipse\/ehs\/types\/AbstractBaseType\\n    at com.github.jeetbawa.ESH_Decision_Decision.main(ESH_Decision_Decision.java:40)\\nCaused by: java.lang.ClassNotFoundException: org.eclipse.ehsm.types.AbstractBaseType\\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\\n    at sun.misc.Launcher$AppClassLoader.findClass(Launcher.java:331)\\n    ... 12 more\\n\\nProcess finished with exit code 1\\n```\\nOP 2015-05-11: I solved my problem like this:\\n```\\npublic static void main(String[] args) throws Exception {\\n        ESH_EclipseSmartHome ehs = new ESH_EclipseSmartHome();\\n        ehs.decide();\\n        System.out.println(\"Done\");\\n}\\n```"}
{"File Name":"opensmarthouse-core\/adr0008.md","Context":"## Context\\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\\n* Alerting the user about failed joining, or a trust issue with devices joining\\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\\nFurther extensions to the notification system may be considered to provide images, action buttons etc are considered outside of the scope of this initial proposal.\\n","Decision":"TBD\\n","tokens":283,"id":658,"Predictions":"## Decision\\nThe decision is to provide a binding notification system for the following reasons:\\n* Notifications should be able to be presented in a consistent way, and this should include the ability to change the notification text and background.\\n* Notifications should be able to be presented in a standard way, including images and action buttons. This may include adding an extra action button to provide an indication that the notification is being processed.\\n* Notifications may be archived to provide an audit trail.\\n* Notifications can be sent as part of an email or SMS message, or via a web page. This provides more options for delivery.\\n* Notifications can be sent as part of a web page, which allows additional functionality such as providing a link to the notification within the page.\\nFurther extensions may be considered if there are issues with this initial proposal. This would allow for more complex notifications and actions.\\n\\n## Context\\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\\n* Alerting the user about failed joining, or a trust issue with devices joining\\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\\nFurther extensions may be considered if there are issues with this initial proposal. This would allow for more complex notifications and actions.\\n\\end{itemize}\\n\\n\\section{Category}\\n\\begin{itemize}\\n    \\item \\textbf{Category:} \\textit{Development}\\n    \\item \\textbf{Subcategory:} \\textit{Web}\\n    \\item \\textbf{Description:} A category contains items (or categories) that are related.\\n\\end{itemize}\\n\\n\\section{Description}\\nThis category contains all items that relate directly or indirectly (via relationship) with each other.\\n\\n\\section{Links}\\nThis category has no links.\\n\\n\\section{Categories}\\nThis category has no categories.\\n\\n\\section{Items}\\nThis category has no items.\\n\\n\\n"}
{"File Name":"opensmarthouse-core\/adr0001.md","Context":"## Context\\nEclipse SmartHome defined a static type system. This may not be extended, and uses a static class `TypeParser` to create a `Type`. This limits framework modularity and extensability and is generally against OSGi principals.\\nThe architecture is to be changed to introduce a `TypeProvider` service, and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users.\\nThe standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatability with ESH\/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`.\\n","Decision":"The introduction of the `TypeRegistry` and `TypeProvider` services, and refactoring of the current types to be provided by a `CoreTypeProvider` is implemented. Existing users of `TypeParser` within the _OpenSmartHouse_ core bundles is to be refactored to use these services, and new bundles should avoid using the static `TypeParser`.\\n","tokens":214,"id":659,"Predictions":"## Decision\\nEclipse SmartHome does not support a static type system. This may not be extended, and uses a static class `TypeParser` to create a `Type`. This limits framework modularity and extensability and is generally against OSGi principals.\\nThe architecture is to be changed to introduce a `TypeProvider` service, and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users.\\nThe standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatability with ESH\/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`.\\n\\n\\n## Context\\nEclipse SmartHome defines a static type system. This may not be extended, and uses a static class `TypeParser` to create a Type. This limits framework modularity and extensability and is generally against OSGi principals.\\nThe architecture is to be changed to introduce a TypeProvider service, and TypeFactory interfaces. The TypeProvider is implemented to provide a Type or set of Types to the system, and TypeFactory is used by users to create Types. Between these are a TypeRegistry which consumes the Types from multiple TypeProviders, and supplies the TypeFactory interface for system users.\\nThe standard core types are provided by a CoretypeProvider, which are then supplied as part of the core framework by an Eclipse SmartHome Bundles' Coretype Provider.\\n\\n\\n## Decision\\n## Decision\\nEclipse SmartHome does not support a static type system. This may not be extended, but uses an static class Parser that creates Types. This limits framework modularity and extensability but it's generally against OSGi principals.\\nThe architecture is to be changed to introduce an Eclipse SmartHome Bundles' Coretype Provider which provides core types as part of the core framework by an Eclipse SmartHome Bundles' Coretype Provider.\\n\\n\\n\\end{itemize}\\n\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\nThis decision records an architectural decision taken during development of \\texttt{EclipseSmartHome}. It should in no way be taken as legally binding.\\n\\nThe decision was taken on 20\/07\/2014.\\n\\n\\begin{itemize}\\n    \\item 1: Create an Eclipse Smart Home Bindings (ESHB) Bundle for each component.\\n    \\item 2: Provide separate bundles for each component.\\n    \\item 3: Provide separate bundles for each service.\\n    \\item 4: Provide separate bundles for each event source.\\n    \\item 5: Provide separate bundles for each event sink.\\n    \\item 6: Provide separate bundles for each data source.\\n    \\item 7: Provide separate bundles for each data sink.\\n    \\item 8: Support only one bundle per component.\\n\\end{itemize}\\n\\n\\subsection{Context}\\n\\label{sec:context}\\n\\nThis context describes how this decision was made.\\n\\nThe context describes how this decision was made in more detail.\\n\\n\\begin{description}\\n    \\item[1] Create an Eclipse Smart Home Bindings (ESHB) Bundle for each component.\\n        The ESHB bundle will provide all necessary components needed\\n        by developers using ESHB in their projects. These include:\\n        - A base package containing all packages needed by ESHB,\\n            including all required JARs (JAX-WS JARs)\\n            - A package containing all classes needed by ESHB,\\n            including all required JARs (JAX-WS JARs)\\n            - A package containing all libraries needed by ESHB,\\n            including all required JARs (JAX-WS JARs)\\n            - A package containing all classes needed by ESHB,\\n            including all required JARs (JAX-WS JARs)\\n            - A package containing all libraries needed by ESHB,\\n            including all required JARs (JAX-WS JARs)\\n        - A Java library providing access methods from other packages\\n        - A Java library providing access methods from other packages\\n        - A Java library providing access methods from other packages\\n        - An XML parser that reads XML files using XSD schemas \\n    \\item[2] Provide separate bundles for each component.\\n        We will provide two different bundles:\\n        - The first bundle will contain only classes that are common \\n              between different components in that bundle:\\n              For example:\\n              - We will have two classes that implement the same interface:\\n                  One class will implement an interface called \"RemoteTable\",\\n                  Another class will implement an interface called \"RemoteTable\"\\n              And we will also have two classes that implement different interfaces:\\n                  One class will implement an interface called \"RemoteTable\",\\n                  Another class will implement an interface called \"RemoteTable\"\\n              And we'll have two classes that implement different interfaces:\\n                  One class will implement an interface called \"RemoteTable\",\\n                  Another class will implement an interface called \"RemoteTable\"\\n              And we'll have two classes that implement different interfaces:\\n                  One class will implement an interface called \"RemoteTable\",\\n                  Another class will implement an interface called \"RemoteTable\"\\n              And we'll have two classes that implement different interfaces:\\n                  One class will implement an interface called \"RemoteTable\",\\n                  Another class will implement an interface called \"RemoteTable\"\\n              And we'll have two classes that implements different interfaces:\\n                  One class will implements an interface called \"RemoteTable\",\\n                  Another class will implements an interface called \"RemoteTable\"\\n        - The second bundle contains only those classes that are common \\n              between different components in those bundles:\\n              For example:\\n              - We'll have two classes that are common between both of our\\n                RemoteTables implementations.\\n                Both implementations use JAXB annotations on their respective \\n                XML files.\\n\\n\\n                Each implementation uses JAXB annotations on its respective XML file,\\n                so we'll need two additional JAXB annotations files on top of our \\n                existing JAXB annotations files.\\n\\n                The first annotation file contains information about how many times \\n                JAXB needs to invoke JAXB's getSchema method on its own XML file;\\n                this annotation file contains information about how many times \\n                JAXB needs to invoke JAXB's getSchema method on another XML file;\\n                this annotation file contains information about how many times \\n                JAXB needs to invoke JAXB's getSchema method on another XML file;\\n                this annotation file contains information about how many times \\n                JAXB needs to invoke JAXB's getSchema method on another XML file;\\n                this annotation file contains information about how many times \\n                JAXB needs to invoke JAXB's getSchema method on another XML file;\\n          And so forth...\\n      }\\n      By having several bundles per component we can easily switch between them without changing any code at runtime when switching between components.\\n\\n      In order for us not having several bundles per component,\\n      we should also make sure that there isn't any duplication in our code base.\\n\\n\\n\\n      In addition we should ensure that there isn't any duplication in our code base.\\n\\n      We should ensure there isn't any duplication in our code base.\\n\\n      In addition we should ensure there isn't any duplication in our code base.\\n\\n\\n\\n      In addition we should ensure there isn't any duplication in our code base.\\n\\n\\n\\n      In addition we should ensure there isn't any duplication in our code base.\\n\\n\\n      As long as developers using ESHB can easily switch between them without changing any code at runtime when switching between components,\\n      it would make sense if they didn't need several bundles per component.\\n\\n\\n    \\end{description}\\n\\n\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\nThis decision records decisions made during development of EclipseSmartHome. It should in no way be taken as legally binding.\\n\\nThe decisions were made during development of EclipseSmartHome on 20\/07\/2014\\n\\n\\begin{enumerate}\\n    \\setlength{\\leftmargin}{0cm}    \\n    % TODO Change these lines if you want your own list\\n\\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n\t\t%\\setlength{\\leftmargin}{0cm}    \\n     % TODO Change these lines if you want your own list\\n\\n%   % TODO Change these lines if you want your own list\\n\\n%   % TODO Change these lines if you want your own list\\n\\n%   % TODO Change these lines if you want your own list\\n\\n%   % TODO Change these lines if you want your own list\\n\\n\\n\\n%   % TODO Change these lines if you want your own list\\n\\n\\n\\n%   % TODO Change these lines if you want your own list\\n\\n\\n\\n%   % TODO Change these lines if you want your own list\\n\\n\\n\\n%   % TODO Change these lines if you want your own list\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n```\\nThe problem is that there is no way to know which of the two root features are being used, and what exactly they are.\\nThis means that if you want to create a module \/ bundle that depends on `openhab-core-base`, you cannot be sure whether it will be activated or not.\\nWe have found a workaround for this problem.\\nDuring compilation phase we check whether the module \/ bundle is defined or not.\\nThe check is done using the following pattern:\\n```\\nimport org.openhab.core.internal.package.Package;\\nimport org.openhab.core.internal.package.PackageFactory;\\nimport org.openhab.core.internal.package.PackageSpecification;\\nimport org.openhab.core.internal.package.PackageSpec;\\n\\npublic class MyModule extends PackageSpec {\\n    public MyModule() {\\n        super(PackageFactory.getPackage(\"org.openhab.core.items\"));\\n    }\\n}\\n```\\nIf the module \/ bundle is defined, then it is activated. Otherwise, it is not activated.\\nThis approach has been adopted by other openHAB modules and bundles, so we expect that it will solve the problem in most of the cases.\\n"}
{"File Name":"opensmarthouse-core\/adr0007.md","Context":"## Context\\nTo allow the system to know if the ThingType definition has changed, a version identifier should be added to the ThingType. This will allow the system to update the thing based on the new Thing Type definition.\\nThe `version` should be added to the `ThingType`, and provided by thing type providers such as the XML provider. This should be reflected into the instantiated `Thing` and provided in the `ThingDTO` for use in the user interface.\\nThing Type XML definition -:\\n```\\n<thing-type id=\"sample\" version=\"2\">\\n```\\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\\nThing REST response (part) -:\\n```\\n{\\nUID: \"test:sample:505f8d7b12\",\\nthingTypeUID: \"test:sample\",\\nthingTypeVersion: 2\\n}\\n```\\nA new REST API is to be added to allow the user to change the thing type via the UI. This uses the `\/thing\/{{thinguid}}\/migrate` call. This migrates the specified thing to the latest version of the ThingType.\\n","Decision":"This has minimal impact on the framework, and allows the user and UI to manage the migration of things without the user having to delete and re-add things, which is quite complex as item links also need to be re-added.  Further enhancements could be added to allow this to (optionally) be undertaken automatically.\\n","tokens":284,"id":661,"Predictions":"## Decision\\nGiven the above information, the decision is to allow the user to migrate their ThingType to the latest version.\\n## Context\\nThe user must be aware of this change. They must be provided with a way to migrate their ThingType. This can be done by providing a migration endpoint, or by providing a migration form that they can fill in and submit.\\nThe migration form should be available for use by the system, and should not require any special permissions.\\nThe system shall then update the ThingType in the database, and provide a new REST API for the user to use to migrate their ThingType.\\nThing Type XML definition -:\\n```\\n<thing-type id=\"sample\" version=\"2\">\\n```\\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\\nThing Type REST response (part) -:\\n```\\n{\\nUID: \"test:sample:505f8d7b12\",\\nthingTypeUID: \"test:sample\",\\nthingTypeVersion: 2\\n}\\n```\\nA new REST API is to be added to allow the user to migrate their ThingType. This uses the `\/thing\/{{thinguid}}\/migrate` call. This migrates the specified thing to the latest version of the ThingType.\\nThe migration form should be available for use by the system, and should not require any special permissions.\\nThe system shall then update the ThingType in the database, and provide a new REST API for the user to use to migrate their ThingType.\\nThing Type XML definition -:\\n```\\n<thing-type id=\"sample\" version=\"2\">\\n```\\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\\nThing Type REST response (part) -:\\nb\\code{https:\/\/migrator.server.com\/#\/api\/migrate\/2}\u00b6\\n## Decision\\n## Decision\\nGiven all of this information, it is decided that we will allow users of our software who have migrated their ThingTypes from one version of our software (currently 1.0) into another (currently 1.1)to upgrade their ThingTypes via our API.\\n## Context\\n## Context\\nThis decision was made after reviewing all of our requirements documentation and talking with our customers.\\n## Decision\\n## Decision\\nGiven that we have all of our requirements documented, it is decided that we will allow users who have migrated their ThingTypes from one version of our software (currently 1.0) into another (currently 1.1)to upgrade their ThingTypes via our API.\\n## Context\\n## Context\\nThis decision was made after reviewing all of our requirements documentation and talking with our customers.\\n## Decision\\n## Decision\\nGiven that we have all of our requirements documented, it is decided that we will allow users who have migrated their ThingTypes from one version of our software (currently 1.0) into another (currently 1.1)to upgrade their ThingTypes via our API.\\n## Context\\n## Context\\nThis decision was made after reviewing all of our requirements documentation and talking with our customers.\\n\\end{document}\\n"}
{"File Name":"opensmarthouse-core\/adr0004.md","Context":"## Context\\nWith large bindings with many things, a ThingType and ThingHandler may not be implemented for each Thing. Instead the ThingHandler may implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered.  In cases where the Thing is statically defined, there will be some discovery information that is retieved from the device and used to link to the ThingType, and it is desirable for the ThingType definition and the discovery information to be defined in a single place.\\nFunctionally, the ThingType definition, and Discovery information should be separate. The proposal here is to introduce a `DiscoveryIndex` service which provides the binding specific thing properties linked to a `ThingThypeUID`. `DiscoveryProperties` are provided by a `DiscoveryIndexProvider` which the binding can query.\\nThe `XmlThingTypeProvider` should be modified to also implement `DiscoveryIndexProvider`, and the ThingType XML definition should be updated to introduce a `discovery-properties` section which can include the discovery information to be passed to the binding. The `discovery-properties` section is similar to the existing Thing `properties` definition, but dedicated to the `DiscoveryIndex` function.\\n","Decision":"TBD\\n","tokens":263,"id":662,"Predictions":"## \\n\\nThe `DiscoveryIndex` service should be implemented as a singleton. The `DiscoveryIndex` service should be implemented as a singleton. The `DiscoveryIndex` service should be implemented as a singleton.\\n## \\n\\nThe `DiscoveryIndex` provider should implement the `DiscoveryIndex` interface, and provide an implementation of the `DiscoveryIndexProvider` interface. The `DiscoveryIndex` provider should implement the `DiscoveryIndexProvider interface, and provide an implementation of the `DiscoveryIndexProvider interface. The `DiscoveryIndexProvider` interface should provide a single implementation for each ThingType which can query the ThingType XML file.\\nThe discovery properties section is optional, and can be omitted by defining the XML type in the binding configuration file.\\nThe XML type definition should include information for discovery index providers to query, and a discovery index provider to query them.\\nThe discovery index provider may query any kind of thing, but it is recommended that only things with a ThingType definition be queried.\\nThe discovery index provider should support querying multiple things with different ThingTypes at once.\\nThe discovery index provider may return multiple results.\\nThe results returned by the search are sorted in ascending order by property value.\\nThe search results are returned in an array with each item being an array of elements returned from the search.\\nThe array is sorted in descending order by property value.\\nThe search results are returned in an array with each item being an array of elements returned from the search.\\n## \\n\\nThe result set is sorted in ascending order by property value.\\n## \\n\\nThe result set is sorted in descending order by property value.\\n## \\n\\nThe result set contains one element per property value found in the search results.\\n## \\n\\nThe result set contains one element per property value found in the search results.\\n## \\n\\nThe result set contains one element per property value found in the search results.\\n\\n\\end{document}\\n"}
{"File Name":"uqlibrary-reusable-components\/adr-001.md","Context":"## Context\\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au\/primo-sand-box\/reusable-components\/`, via the primo BO.\\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\\nManual processes are to be avoided.\\n","Decision":"Generate the branch to be used, by looking at the host name and the vid parameter on the url\\nhttps:\/\/github.com\/uqlibrary\/uqlibrary-reusable-components\/commit\/4f1c182\\n","tokens":87,"id":665,"Predictions":"##\\\\\\n\\begin{verbatim}\\n\/\/ 1.1.1 - 1.1.2\\n\/\/     1.1.2 - 1.1.3\\n\/\/     1.3 - 2\\n\\end{verbatim}\\n\\end{document}\\n```"}
{"File Name":"editions\/01-\ud83d\udeaf-pwa.md","Context":"## Context\\nThis needs to work cross platform.\\nThe UX will be rendered in browser, which may not appear native.\\n","Decision":"To implement Editions as a progressive web app, delivered in a native wrapper. The native wrapper will contain a webview, that renders the PWA.\\nThe wrapper will handle required features that are not supported by PWAs such as background fetch, scheduled downloads and push notification handling.\\n","tokens":25,"id":666,"Predictions":"## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n## Decision: Render the user interface in browser\\n\\n"}
{"File Name":"editions\/05-\u2705-archiver-s3-event.md","Context":"## Context\\nThe archiver lambda needs to be able to respond to issue published events from tools.\\n","Decision":"For the archiver to react to S3 file created events from the fronts tool.\\n","tokens":21,"id":667,"Predictions":"## The archiver lambda should respond to any issue published events from tools.\\n## Result\\n## The archiver lambda should respond to any issue published events from tools.\\n## Environment\\n## The archiver lambda should respond to any issue published events from tools.\\n## Process\\n## The archiver lambda should respond to any issue published events from tools.\\n## Environment\\n## The archiver lambda should respond to any issue published events from tools.\\n\\end{lstlisting}\\n\\n\\section{Archiving and Retrieval of Data}\\label{sec:archiving-and-retrieval-of-data}\\nThe user has the ability to create an archive and retrieve data.  In order for the user to create an archive, the user must first create a project.  Once a project is created, the user can then create a data source.  Once a data source is created, the user can then create an archive.  Once an archive is created, the user can then retrieve data.\\n\\n\\subsection{Create Project}\\label{sec:create-project}\\nTo create a project, the user must first have an application running and connected to a database.  To start a project, click on ``Start Project'' in the top left corner of the screen.\\n\\n\\subsection{Create Data Source}\\label{sec:create-data-source}\\nTo create a data source, first click on ``Create Data Source'' in the top left corner of the screen.\\n\\n\\subsection{Create Archive}\\label{sec:create-archive}\\nTo create an archive, first click on ``Create Archive'' in the top left corner of the screen.\\n\\n\\subsection{Retrieve Data}\\label{sec:retrieve-data}\\nTo retrieve data, click on ``Retrieve Data'' in the top left corner of the screen.\\n\\n\\section{\\texttt{\\_} Archivers} \\label{sec:archivers}\\nThe \\texttt{\\_} archivers are used for implementing asynchronous computation within Lambda applications.  An \\texttt{\\_} archiver is simply another thread that runs alongside your application and performs computations asynchronously with it.  An \\texttt{\\_} archiver can be thought of as your application's thread pool.\\n\\nThe \\texttt{\\_} archiver will run asynchronously with your application by listening for incoming messages from all your other threads.  When it receives one of these messages it will execute it's code as quickly as possible while still listening for incoming messages from other threads.  These messages are called issues.\\n\\n\\nFor example, here is how you might implement a simple \\texttt{\\_} archiver that listens for incoming issues:\\n\\n\\begin{lstlisting}[language=python]\\nclass MyArchiver(object):\\n    def __init__(self):\\n        self._archived = True\\n    def __call__(self,*args,**kwargs):\\n        # TODO: perform computations here\\n\\ndef main():\\n    # Create some issues...\\n    issues = [1]\\n    # Create some archives...\\n    myArchives = [MyArchiver() for _ in range(10)]\\n    # Retrieve some archives...\\n    archives = myArchives[0].get()\\n    \\nif __name__ == '__main__':\\n    main()\\n\\end{lstlisting}\\n\\nIn this example we created two classes: MyArchiver and MyArchive.  The MyArchive class stores all of our archives and provides methods for retrieving them (such as get()).\\n\\nWe also created two functions: __init__() and __call__() which are used to initialize our class and call our methods respectively.\\n\\nFinally we implemented our main function which creates some issues, creates some archives and retrieves them.\\n\\n\\nWhen you are writing your own \\texttt{\\_} archivers you will need to implement all three methods listed above:\\n\\n\\begin{enumerate}\\n\t\\item \\textbf{\\_\\_init\\_\\_.}\\n\tYour class needs to have at least one constructor which takes no arguments.\\n\tYou may have multiple constructors but only one should be called when creating your object.\\n\tYou need to provide at least one method called __init__ which takes no arguments.\\n\tThis method is called when creating your object.\\n\tYou also need to provide at least one method called __call__ which takes two arguments:\\n\t\ta list of strings containing all of your issues (such as 'a', 'b', 'c', etc.) or tuples containing:\\n\t\t(a,b,c) or ('a','b','c').\\n\tThe list or tuple must contain exactly one issue per line (i.e., each line must contain exactly one issue).\\n\tIf you do not provide this method then your class will not be able to retrieve its archives until it has been run once by running main() on its own.\\n\\n\\n\tOnce you have implemented both methods you will need to implement them both again:\\n\t\\n\\begin{lstlisting}[language=python]\\ndef __init__(self,*args,**kwargs):\\n\tif args != None:\\n\t    args[0] = args[1] = kwargs[0] = kwargs[1]\\n\tself._issues = []\\n\tself._archives = []\\n\t\\ndef __call__(self,*args,**kwargs):\\n\tif args != None:\\n\t    self._issues.append(args)\\n\t    self._archives.append(kwargs)\\n\t\\ndef get(self):\\n\tfor i in range(self._archives.size()):\\n\t    if isinstance(self._archives[i], str) or isinstance(self._archives[i], list):\\n\t        print(i)\\nelse:\\n\tprint(\"No Archives found.\")\\n\treturn None\\n\t\\ndef get_archive(self):\\n\tfor i in range(self._archives.size()):\\n\t    if type(self._archives[i]) == str or type(self._archives[i]) == list:\\n\t        print(i)\\nelse:\\n\tprint(\"No Archives found.\")\\n\treturn None\\n\t\\ndef get_issues():\\n\tfor i in range(self._issues.size()):\\n\t    if isinstance(self._issues[i], str) or isinstance(self._issues[i], list):\\n\t        print(i)\\nelse:\\n\tprint(\"No Issues found.\")\\n\treturn None\\n\t\\ndef get_archives():\\n\tfor i in range(self._archives.size()):\\n\t    if type(self._archives[i]) == str or type(self._archives[i]) == list:\\n\t        print(i)\\nelse:\\n\tprint(\"No Archives found.\")\\n\treturn None\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\\nif __name__ == '__main__':\\n\tmain()\\n\t\t\\n\t\t   \\n\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t   \\n\t\t\t\\n\t\t \\n\t\t\\n\t\t \\n\t\t\\n\t\t \\n\t\t\\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t \\n\t\t\t \\n\t\t\t"}
{"File Name":"editions\/08-\u2705-releases.md","Context":"## Context\\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\\n","Decision":"### iOS\\n#### Internal Beta\\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\\nIn github actions we have a [scheduled build](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/upload-ios-build.sh)\\nAll builds generate a ['release' in github](https:\/\/github.com\/guardian\/editions\/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/make-release.js).\\n#### External Beta\\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\\n#### Release\\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\\n#### Post Release\\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\\n### Android\\n#### Internal Beta\\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\\n#### External Beta\\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\\nAs with iOS, releases to the play store can be tracked in [github releases](https:\/\/github.com\/guardian\/editions\/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\\n#### Release\\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https:\/\/github.com\/guardian\/editions\/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\\n","tokens":22,"id":668,"Predictions":"## The app is ready for release. It can be made available to the public and will be supported by the developer.\\n## End Decision\\n\\n## End Decision\\n\\n## END OF ARCHITECTURAL DECISION RECORD\\n\\n\\n\\subsection{Decisions}\\n\\n\\subsubsection{Decision}\\n\\nThe app is ready for release. It can be made available to the public and will be supported by the developer.\\n\\n\\subsubsection{Decision}\\n\\nThe app is ready for release. It can be made available to the public and will be supported by the developer.\\n\\n\\subsubsection{Decision}\\n\\nThe app is ready for release. It can be made available to the public and will be supported by the developer.\\n\\n\\subsubsection{Decision}\\n\\nThe app is ready for release. It can be made available to the public and will be supported by the developer.\\n\\n\\subsubsection{Decision}\\n\\nThe app is ready for release. It can be made available to the public and will be supported by the developer.\\n"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"Predictions":"## Decision: We need to develop a native iOS app that will send analytics data to Ophan. This will be used by our internal analytics team.\\n\\n## Decision: We can create an android app that will send analytics data to Ophan. This will be used by our internal analytics team.\\n\\n## Decision: We can create a web app that will send analytics data to Ophan. This is an internal tool and should not be public.\\n\\n## Decision: We can create a mobile app that will send analytics data to Ophan. This is an internal tool and should not be public.\\n\\n## Decision: We can create a web app that will send analytics data to Ophan. This is an internal tool and should not be public.\\n## Decision: We can create a mobile app that will send analytics data to Ophan. This is an internal tool and should not be public.\\n"}
{"File Name":"editions\/02-\u2705-react-native.md","Context":"## Context\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n","Decision":"To develop the editions app as a React Native application.\\n","tokens":46,"id":670,"Predictions":"## A decision is a statement of the facts and conclusions reached by an analyst. In this case, the analyst has reviewed the project requirements and decided that a React application is best suited to meet the needs of the project.\\n\\n## Decision\\nThe developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n\\n## Decision\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React devs"}
{"File Name":"editions\/04-\u2705-paths.md","Context":"## Context\\nThe editions lambda needs to be able to identify specific versions of an issue.\\n","Decision":"To have two deployments of the backend, one for previewing, and a second for published issues.\\nThe published issues deployment will replace the issueid path parameter with source\/issueid.\\n`source` will identify which file in the published bucket will be retreived to form the issue on.\\n","tokens":19,"id":671,"Predictions":"## The issue number should be set to 1\\n## Reason\\nThe issue number should be set to 2\\n## 1. Issue 1.1\\nA new build of the software should be created.\\n2. Issue 2.1\\nA new build of the software should be created.\\nExplanation\\nThe issue number should be set to 2.\\n\\n## Decisions\\n## The issue number should be set to 3.\\n## Reason\\nThe issue number should be set to 4.\\n## Decision\\n## The issue number should be set to 5.\\n## Reason\\nThe issue number should be set to 6.\\n## Decision\\n## The issue number should be set to 7.\\n## Reason\\nThe issue number should be set to 8.\\n## Decisions\\n## The issue number should be set to 9.\\n## Reason\\nThe issue number should be set to 10.\\n##\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##\\n# Context\\n\\nIn this example, we have used a Decision Record that says:\\n\\n- \u201cIssue #1: A new build of the software\u201d (which is called an Issue)\\n- \u201cIssue #2: A new build of the software\u201d (which is called an Issue)\\n- \u201cIssue #3: A new build of the software\u201d (which is called an Issue)\\n- etc.\\n\\nNote that, in this case, we are using a Decision Record that says \u201cissue #3: A new build of the software\u201d. We do not have any other information about this particular version of the software.\\n\\nWe can also use a Decision Record that says \u201cissue #3: A new build of the software\u201d and then add other information about it in another section.\\n\\nHere we are using two sections:\\n\\n- Section One: Issues \u2013 This section contains all issues for which there is more than one version of the software being developed (i.e., more than one version of Lambda). This section also contains information about each version of Lambda, including its version name and whether it has been released or not.\\n\\n- Section Two: Versions \u2013 This section contains all versions for which there is more than one version of Lambda being developed (i.e., multiple versions).\\n\\nFor example, in this case we have used two sections:\\n\\nSection One \u2013 Issues\\n\\nSection Two \u2013 Versions\\n\\nThis gives us a decision record that looks like this:\\n\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##\\n# Context\\n\\nIn this case, we are using two sections:\\n\\n- Section One \u2013 Issues \u2013 This section contains all issues for which there is more than one version of Lambda being developed (i.e., multiple versions).\\n- Section Two \u2013 Versions \u2013 This section contains all versions for which there is more than one version of Lambda being developed (i.e., multiple versions).\\n\\nWe can also use additional sections such as these:\\n\\n- Section Three: Version Details \u2013 This section contains information about each version of Lambda that has been released or not released.\\n\\nThis gives us a decision record that looks like this:\\n\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##\\n# Context\\n\\nIn this case, we have used three sections:\\n\\n- Section Three: Version Details \u2013 This section contains information about each version of Lambda that has been released or not released.\\n- Section Four: Release Notes \u2013 These notes contain information about each release and its associated versions.\\n\\nThis gives us a decision record that looks like this:\\n\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##\\n# Context\\n\\nIn this case, we have used four sections:\\n\\n- Section Four: Release Notes \u2013 These notes contain information about each release and its associated versions.\\n- Section Five: Release Notes - These notes contain information about each release and its associated versions.\\n- Section Six: Testing Notes - These notes contain information about testing for each release and its associated versions.\\n- Section Seven: Summary - These notes contain information about each release and its associated versions.\\n\\nThis gives us a decision record that looks like this:\\n\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##\\n# Context\\n\\nWe can also use additional sections such as these:\\n\\nSection Five - Testing Notes - These notes contain information about testing for each release and its associated versions.\\n\\nThis gives us decision records like these:\\n\\n# This is an Architectural Decision Record for Software Project Name ####### ####### ####### ####### ####### ####### ####### ####### #########\\n# Version Name   ###########\\n#\\n#\\n#\\n#\\n#\\n#\\n\\nHere are some examples showing how you can add additional sections into your architectural decisions record template.\\n\\nExample #1\\n\\nLet's say you want your architectural decisions record template look something like this:\\n\\nExample #2\\n\\nLet's say you want your architectural decisions record template look something like this:\\n"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"Predictions":"## The decision was made to use a software to archive the data.\\n\\n## Decision\\n## The archiving process is time consuming and consists of many stages.\\n\\n## Decision\\n## The decision was made to use a software to archive the data.\\n\\n## Decision\\n## The archiving process is time consuming and consists of many stages.\\n\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}{Case Study: University of Toronto}\\n\\begin{itemize}\\n  \\item \\textbf{What are the different types of data that may be archived?}\\\\\\n  \\begin{itemize}\\n    \\item \\textbf{Archival Data}\\\\\\n    \\item \\textbf{Data as a Service}\\\\\\n    \\item \\textbf{Data as a Product}\\\\\\n    \\item \\textbf{Data as a Service}\\pause\\\\[2pt]\\n    (a) What are the different types of data that may be archived?\\n  \\end{itemize}\\n  \\item [Example: University of Toronto]\\\\\\n  A university wants to archive all their data. They want to archive their research papers, their research presentations, their research notes, etc.\\\\[2pt]\\n  A university wants to archive all their data. They want to archive their research papers, their research presentations, their research notes, etc.\\n\\end{itemize}\\n\\n\\end{frame}\\n\\n\\begin{frame}{Case Study: University of Toronto}\\n\\begin{figure}[htbp]\\n\\centering\\n\\n\\includegraphics[scale=0.7]{archivestudy.png}\\n\\n\\vspace{-0.5cm}\\n\\n{\\scriptsize Source: https:\/\/www.archivestudy.ca\/}\\vspace{-0.5cm}\\n\\n\\vspace{-0.5cm}\\n\\n{\\scriptsize Figure adapted from: https:\/\/www.archivestudy.ca\/}\\vspace{-0.5cm}\\n\\n\\vspace{-0.5cm}\\n\\end{figure}\\n\\n\\vspace{-1cm}\\n\\n\\n\\noindent{\\scriptsize {\\it Source: https:\/\/www.archivestudy.ca\/}}\\n\\n\\noindent{\\scriptsize {\\it Adapted from: https:\/\/www.archivestudy.ca\/}}\\n\\n\\noindent{\\scriptsize {\\it Figure adapted from: https:\/\/www.archivestudy.ca\/}}\\n\\end{frame}\\n\\n\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n%===========================================================================\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n"}
{"File Name":"Sylius\/2020_03_03_api_translations.md","Context":"### Problem to solve\\nProvide unified way to manage translations for translatable entities in API.\\n### Possible solutions\\nThe translations from collection could be embedded as objects within a `ProductOption`\/`ProductOptionValue` resource\\n**or** provided by [IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","Decision":"Translations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","tokens":103,"id":673,"Predictions":"## Decision\\nThe translations from collections could be embedded as objects within a ProductOption or ProductOptionValue resource, or provided by IRIs.\\n### Possible solutions\\nThe translations from collections should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n### Decision and reasoning\\nThe translations from collections should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n\\n## Context\\n## Context\\nThe APIs should be unified to allow for the management of translations for translatable entities in API.\\n### Possible solutions\\nThe APIs should be unified to allow for the management of translations for translatable entities in API.\\n### Decision and reasoning\\nThe APIs should be unified to allow for the management of translations for translatable entities in API.\\n"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"Predictions":"## Decision Driver\\n## Decision Reasoning\\n## Decision Type\\n## Decision Conditions\\n## Decision Value\\n## Decision Result\\n\\n# 6.1.3.1\\n\\n### 6.2\\n\\n### 6.2.1\\n\\n### 6.2.2\\n\\n### 6.3\\n\\n### 6.3.1\\n\\n### 6.3.2"}
{"File Name":"Sylius\/2021_02_05_unified_api_prefix.md","Context":"## Context and Problem Statement\\nInitial implementation of Unified API used `\/new-api` prefix, to aggregate all following endpoints. This prefix does not\\nclearly state the version of it and is not future-proof. At some moment of time our \"new api\" can become \"old api\". We should\\nhave clear guidance how to version our APIs.\\n","Decision":"As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution\\nthe decision is to go with the `\/api\/v2` endpoint path. In the future it does not block us from the usage of the `Accept`\\nheader in addition to this path, however it may be misleading for consumers.\\n#### References:\\n- https:\/\/github.com\/api-platform\/api-platform\/issues\/290\\n- https:\/\/github.com\/api-platform\/core\/issues\/972\\n- https:\/\/stackoverflow.com\/questions\/389169\/best-practices-for-api-versioning\\n- https:\/\/www.troyhunt.com\/your-api-versioning-is-wrong-which-is\/\\n- https:\/\/github.com\/api-platform\/core\/issues\/45\\n- https:\/\/github.com\/api-platform\/docs\/issues\/451\\n- https:\/\/github.com\/api-platform\/docs\/pull\/452\\n- https:\/\/api-platform.com\/docs\/core\/content-negotiation\/#supporting-custom-formats\\n","tokens":72,"id":675,"Predictions":"\\begin{itemize}\\n\\item \\textbf{new-api} is the new API and should be used for all future requests.\\n\\item \\textbf{old-api} is the old API and should be used for old requests, when we want to use old functionality.\\n\\end{itemize}\\n\\n## Decision Statement\\n\\begin{itemize}\\n\\item \\textbf{old-api} is deprecated and will be removed in future versions. Use \\textbf{\\textit{new-api}} for all future requests.\\n\\n## Context\\nOur current APIs are based on the Java 8. We are planning to upgrade to Java 11 in the near future. We want to migrate our APIs from Java 8 to Java 11 as early as possible, before we release a new version of our software.\\n\\n## Problem Statement\\nWe want to have clear guidance how to version our APIs.\\n"}
{"File Name":"Sylius\/2021_07_05_api_providing_locales_available_in_active_channel.md","Context":"## Context and Problem Statement\\nCustomer should have access only to locales available in their channel\\n","Decision":"Chosen option: Using Data Provider\\nShops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated\\nquery in Doctrine Collection extension\\n","tokens":18,"id":676,"Predictions":"## Decision Description\\n## Recommendation\\n## Recommendation Description\\n\\nThe following information is provided about the decision:\\n\\nDecision: The system is to be configured such that only locales in the channel of the customer are available.\\n\\nThe following information is provided about the decision:\\n\\nCustomer: The channel of the customer is Channel A.\\n\\nThe following information is provided about the decision:\\n\\nChannel: Channel A.\\n\\nThe following information is provided about the decision:\\n\\nCustomer's Channel has been configured to access only locales in Channel A.\\n\\nThe following information is provided about the decision:\\n\\nChannel has been configured to access only locales in Channel A.\\n\\nThe following information is provided about the decision:\\n\\nUser: User of the application\\n\\nThe following information is provided about the decision:\\n\\nUser's Channel has been configured to access only locales in Channel A.\\n\\nThe following information is provided about the decision:\\n\\nUser's channel has been configured to access only locales in Channel A.\\n\\nThe following information is provided about the decision:\\n\\nUser's locale has been set to English (United States).\\n\\nThe following information is provided about the decision:\\n\\nLocale for User: English (United States)\\n\\nThis record contains a summary of a software configuration that defines how a particular channel or channel region should be used. This record may contain one or more configuration items that define how a particular channel or channel region should be used. The configuration items are described below. In addition, there may be other records that describe other configurations for this particular channel or channel region. These other records are described below as well. You can use this record to configure your software so that it behaves as you expect it to behave, when used with a particular channel or channel region.\\n\\nRecord Information\\n\\nRecord Name: CustomerChannelConfig Record Type: Configuration Item Record Format: XML Record Length: 8 bytes (0x8) Record Type: Configuration Item Records per Document: 1\\n\\nConfiguration Items\\n\\nConfiguration Item Name Description \\nCustomerChannelConfig [CustomerChannelConfig] Configuration Item Description \\n\\nConfiguration Items in this Record\\n\\nCustomerChannelConfig [CustomerChannelConfig]\\n\\nConfiguration Item Name Description \\nchannel [channel] The name of the channel for which configuration items are being created \\nregion [region] The name of a region for which configuration items are being created \\n\\nRecord Data\\n\\nConfigurationItem Class Name Description \\nstring [string] The name of a configuration item \\nstring[2] The value of a configuration item \\nstring[3] Additional description of an item \\n\\nExample XML Document"}
{"File Name":"Sylius\/2020_11_12_registration_of_handlers_in_message_buses.md","Context":"## Context and Problem Statement\\nWhile defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will\\nbe added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.\\nHowever, we may add additional buses in the future and more than one bus is not uncommon thing.\\n## Decision Drivers <!-- optional -->\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n","Decision":"* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\nChosen option: \"[Explicitly declare, where bus should be registered]\", because this options leaves the most options to end user\\nwithout forcing him to use them. What is more, we may easily adjust this option in the future\\n","tokens":117,"id":677,"Predictions":"## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n## Decision\\n## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n## Decision\\n## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n## Decision\\n## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n## Decision\\n## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n## Decision\\n## Decision Drivers\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n\\nIn this example, we define two drivers. The first driver is assigned to all buses. The second driver is assigned to one particular bus.\\n\\nThe following example shows a decision for a handler that will be added to all buses.\\n\\n# Create a new handler with name \"default\"\\n$handler = Handler::new('default');\\n\\n# Assign this handler to all existing buses.\\n$bus = Bus::model()->find(10)->getBus();\\n\\n# This handler will be assigned to one particular bus.\\n$handler->setBus($bus);\\n\\nIt is possible to create multiple handlers, but it is not recommended. If you want to add multiple handlers, you should create separate handlers for each one. For instance, if you want to add several handlers for different address types, you can do it like this:\\n\\n$handler = Handler::new('AddressTypeHandler', 'AddressTypeHandler');\\n$handler->setBus($bus);\\n\\nIn this example, we created two different handlers that are assigned to different buses.\\n\\nThe following example shows an additional decision driver.\\n\\n# Create a new decision driver with name \"other\"\\n$decision = DecisionDriver::new('OtherHandler');\\n\\n# Assign this decision driver to all existing buses.\\n$bus = Bus::model()->find(10)->getBus();\\n\\n# This decision driver will be assigned to one particular bus.\\n$decision->setBus($bus);\\n\\nThe following example shows an additional decision driver.\\n\\n# Create a new decision driver with name \"other\"\\n$decision = DecisionDriver::new('OtherDecisionDriver');\\n\\n# Assign this decision driver to all existing buses.\\n$bus = Bus::model()->find(10)->getBus();\\n\\n# This decision driver will be assigned to one particular bus.\\n$decision->setBus($bus);\\n\\nIt is possible to create multiple drivers, but it is not recommended. If you want to add multiple drivers, you should create separate drivers for each one. For instance, if you want to add several drivers for different address types, you can do it like this:\\n\\n$driver = new AddressTypeDriver();\\n$driver->setBus($bus);\\n\\nThe following example shows an additional decision driver.\\n\\n# Create a new decision driver with name \"other\"\\n$decision = DecisionDriver::new('OtherDecisionDriver', 'OtherDecisionDriver');\\n\\n# Assign this decision driver to all existing buses.\\n$bus = Bus::model()->find(10)->getBus();\\n\\n# This decision driver will be assigned to one particular bus.\\n$decision->setBus($bus);\\n\\nIt is possible to create multiple drivers, but it is not recommended. If you want to add multiple drivers, you should create separate drivers for each one. For instance, if you want to add several drivers for different address types, you can do it like this:\\n\\n\/\/ Create a new AddressTypeDriver without any arguments\\nclass AddressTypeDriver extends AddressType {\\n    public function __construct() {\\n        parent::__construct();\\n    }\\n}\\n\\n\/\/ Add another address type as argument (optional)\\nclass OtherAddressType extends AddressType {\\n    public function __construct() {\\n        parent::__construct();\\n    }\\n}\\n\\n\/\/ Add another address type as argument (optional)\\nclass OtherAddressType extends AddressType {\\n    public function __construct() {\\n        parent::__construct();\\n    }\\n\\n    \/\/ Add another address type as argument (optional)\\n    class OtherAddressType extends AddressType {\\n        public function __construct() {\\n            parent::__construct();\\n        }\\n    }\\n}\\n\\n\/\/ Add another address type as argument (optional)\\nclass OtherAddressType extends AddressType {\\n    public function __construct() {\\n        parent::__construct();\\n    }\\n\\n    \/\/ Add another address type as argument (optional)\\n    class OtherAddressType extends AddressType {\\n\\n        \/\/ Add another address type as argument (optional)\\n        class OtherAddressType extends AddressType {\\n\\n            \/\/ Add another address type as argument (optional)\\n            class OtherAddressType extends AddressType {\\n\\n                \/\/ Add another address type as argument (optional)\\n                class OtherAddressType extends AddressType {\\n\\n                    \/\/ Add another address type as argument (optional)\\n                    class OtherAddressType extends AddressType {\\n\\n                        \/\/ Add another address type as argument (optional)\\n                        class OtherAddressType extends Address Type{\\n\\n                            \/\/ Add another address type as argument (optional)\\n                            class OtherAddress Type{\\n                                \/\/ ...\\n                            }\\n\\n                        }\\n\\n                    }\\n\\n                }\\n\\n            }\\n\\n            \/\/ ...\\n\\n        }\\n    }\\n}\\n\\n\/\/ Use the above code in your code\\n\\nIn this example we defined two additional decisions that are assigned automatically by Sylius.\\n\\nThe following example shows how we can define our own decisions using Sylius API.\\n\\n\/\/ Define a new custom data model that defines all possible addresses types.\\n$dataModel = DataModel::model()->create(\\n      'address_types',\\n      [\\n         'address_type' => 'address_type',\\n      ],\\n      [\\n         'address_type_id' => id(),\\n         'address_type_name' => 'name',\\n      ],\\n      [\\n         'address_type_id' => id(),\\n         'address_type_status' => Status::TYPE_ACTIVE,\\n      ],\\n      [\\n         'address_type_id' => id(),\\n         'address_type_address' => EntityTypeInterface::class,\\n      ],\\n      [\\n         'address_type_id' => id(),\\n         'address_type_address_line1' => EntityTypeInterface::class,\\n      ],\\n      [\\n         'address_type_id' => id(),\\n         'address_type_address_line2' => EntityTypeInterface::class,\\n      ],\\n   ]\\n);\\n\\n\/\/ Define our own data model using Sylius API.\\n$dataModel['data_model'] = $dataModel;\\n\\n\/\/ Set our data model on all existing data models that use it.\\nforeach ($dataModels['data_models'] as $dataModel) {\\n   $dataModel['data_model']['data_model'] = $dataModel;\\n}\\n\\n\/\/ Save changes on our data model using Sylius API.\\n$dataModel->save();\\n\\nIn the previous examples we defined our own custom data models using Sylius API.\\n\\nWhen defining custom models in Sylius API there are two ways how we can save changes on them:\\n\\n- Using Model::$saveChanges method;\\n- Using Model::$saveChangesWithOptions method\\n\\nWe will describe both methods below.\\n\\nUsing Model::$saveChanges method\\n\\nThis method allows us specify which fields are saved when saving changes on our custom models using Model::$saveChanges method. It takes three parameters:\\n\\n- model: The model from which we need save changes;\\n- field: The field from which we need save changes;\\n- options: The options that allow us specify which fields are saved when saving changes on our custom models using Model::$saveChanges method.\\n\\nUsing Model::$saveChangesWithOptions method\\n\\nThis method allows us specify which fields are saved when saving changes on our custom models using Model::$saveChangesWithOptions method. It takes three parameters:\\n\\n- field: The field from which we need save changes;\\n- options: The options that allow us specify which fields are saved when saving changes on our custom models using Model::$saveChangesWithOptions method;\\n\\nIn both methods we use array values instead arguments so that they don't have any special meaning in them. In order for these values have any meaning they must be passed in correct order.\\n\\nExample\\n\\nThis example shows how we can define our own custom data model using Sylius API by passing values into parameters in correct order. We also show how we can use Model::$saveChangesWithOptions method instead Model::$saveChanges method so that values passed into parameters have special meaning in them.\\n\\n<?php namespace App\\Model;\\n\\nuse App\\Model\\CustomDataModel;\\n\\nnamespace App\\Model\\CustomDataModel;\\n\\nuse \\Sylius\\Sylius;\\nuse \\Sylius\\models\\CustomDataModel;\\nuse \\Sylius\\models\\CustomDataModel\\CustomDataModel;\\n\\n\/**\\n * Custom data model defined by custom data model creator via sylius api.\\n *\\n * @package App\\Model\\CustomDataModel\\n *\/\\nclass CustomDataModel implements CustomDataModelInterface {\\n\\n     \/**\\n     * @var string|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|string|null|string|null|string|null|string|null|string|int|int|int|int|int|int|int|int|int|int|int|int|int|int|int|\\n     *\/\\n     protected $field;\\n\\n     \/**\\n     * @var string|array|array|array|array|array|array|array|array|array|NULL|string|null|string|null|string|null|NULL|string|null|string|NULL|string|NULL|string|\\n     *\/\\n     protected $options;\\n\\n     \/**\\n     * @var string|string[][][]\\n     *\\n     * @param string $field A field from which change needs save change;\\n     * @param string|array|array|array $options A set options from where change needs save change;\\n     * @return void An empty array or null if no change needed save change;\\n     *\/\\n     \\n     \\n   \\n}\\n \\n\/**\\n * Create a new Custom Data Model instance by passing values into parameters in correct order.\\n *\\n * @param string   $field A field from which change needs save change;\\n * @param string | null | null | null | null | null | null | null | null | null | NULL |\\n *\/\\npublic function __construct(\\n   string $field,\\n   array|string[] $options =\\n) { \\n   use \\Sylius\\models\\CustomDataModel; \\n\\n   $this->field           = $field; \\n   $this->options          = $options; \\n} \\n\\n\/**\\n * Save changes on Custom Data Models by passing values into parameters in correct order.\\n *\\n * @return void An empty array or null if no change needed save change;\\n *\/\\npublic function saveChanges(\\n   array &$fields,\\n   array &$options =\\n) { \\n   \\n   \\n}\\n \\n\/**\\n * Save changes on Custom Data Models with options by passing values into parameters in correct order.\\n *\\n * @return void An empty array or null if no change needed save change;\\n *\/\\npublic function saveChangesWithOptions(\\n   array &$fields,\\n   array &$options =\\n) { \\n   \\n   \\n}\\n \\n\/**\\n * Save whole Custom Data Models with options by passing values into parameters in correct order.\\n *\\n * @return void An empty array or null if no change needed save change;\\n *\/\\npublic function save()\\n{\\n   \\n}\\n \\n\/***\\n *@param int           ID       A unique identifier for the current record or an ID value used during migration process.\\n*\/\\npublic function setId(int $id) { return self::$_id; }\\n \\n\\n \/***\\n *@param int           ID       A unique identifier for the current record or an ID value used during migration process.\\n*\/\\npublic function setId(int $id) { return self::$_id; }\\n \\n\\n \/***\\n *@param DateTime       date       Date\/time stamp associated with the record.\\n*\/\\npublic function getDate() { return self::$_date; }\\n \\n\\n \/***\\n *@param DateTime       date       Date\/time stamp associated with the record.\\n*\/\\npublic function getDate() { return self::$_date; }\\n \\n\\n \/***\\n *@param int           ID       A unique identifier for the current record or an ID value used during migration process.\\n*\/\\npublic function setId(int $id) { return self::$_id; }\\n \\n\\n \/***\\n *@param DateTime       date       Date\/time stamp associated with the record.\\n*\/\\npublic function getDate() { return self::$_date; }\\n \\n\\n \/***\\n *@param int           ID       A unique identifier for the current record or an ID value used during migration process.\\n*\/\\npublic"}
{"File Name":"Sylius\/2020_03_03_feature_coverage_in_api.md","Context":"## Context and Problem Statement\\nWe want our API to cover all the functionalities that are currently implemented in the UI.\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n","Decision":"* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\nChosen *Using Behat for the feature coverage*, because it's the only option, that meets all the decision drivers criteria.\\nWe will gradually add `@api` tag to the scenarios currently tagged with `@ui` and then implement the API contexts.\\nAs a consequence, we will have to create a testing tool to use it in Behat contexts.\\n","tokens":56,"id":678,"Predictions":"## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n\\n## Decision\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n\\n## Decision\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n\\n## Decision\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n\\n## Decision\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n\\n\\n\\end{lstlisting}\\n\\n\\subsection{The \\texttt{Decision} Class}\\nThe \\texttt{Decision} class is an abstract class that defines all possible decisions that can occur during execution of an algorithm. The class contains methods to define \\texttt{Decision}s and methods to invoke them. The methods of this class are called by invoking their corresponding method name. The methods of this class are used to invoke decision making logic.\\n\\n\\subsubsection{Decision::Create()}\\nThis method creates an instance of a decision and invokes its method Create().\\n\\n\\subsubsection{Decision::Set()}\\nThis method sets one or more parameters to values. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\subsubsection{Decision::Get()}\\nThis method returns one or more parameters as string. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\subsubsection{Decision::GetParameter()}\\nThis method returns one parameter as integer.\\n\\n\\subsubsection{Decision::SetParameter()}\\nThis method sets one parameter as integer. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\subsubsection{Decision::GetParameterFromString()}\\nThis method returns one parameter from string as integer.\\n\\n\\subsubsection{Decision::SetParameterFromString()}\\nThis method sets one parameter from string as integer. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\subsubsection{Decision::GetBooleanParameter()}\\nThis method returns one parameter from bool as integer.\\n\\n\\subsubsection{Decision::SetBooleanParameter()}\\nThis method sets one parameter from bool as integer. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\subsubsection{Decision::GetBooleanArrayParameter()}\\nThis method returns one parameter from bool array as integer.\\n\\n\\subsubsection{Decision::SetBooleanArrayParameter()}\\nThis method sets one parameter from bool array as integer. Parameters may take values either as string, integer, float, double, boolean or boolean array.\\n\\n\\begin{sloppypar}\\n  \\item The \\texttt{\\_} is used to indicate that this function is called with an optional argument.\\n  \\item The optional argument is passed by reference.\\n  \\item The optional argument is passed by value.\\n  \\item The optional argument is passed by reference.\\n  \\item The optional argument is passed by value.\\n  \\item A single optional argument is passed by reference.\\n  \\item A single optional argument is passed by value.\\n  % TODO: Remove if it's redundant with other Optional arguments\\n  % TODO: Remove if it's redundant with other Optional arguments\\n\\n  % TODO: Remove if it's redundant with other Optional arguments\\n\\n%\\end{sloppypar}\\n\\n%\\begin{sloppypar}\\n%    This function defines two types of parameters:\\n%    * String parameters (e.g., $x = x + y$)\\n%    * Boolean parameters (e.g., $x = y$)\\n%\\end{sloppypar}\\n\\n%\\begin{sloppypar}\\n%    This function defines two types of parameters:\\n%    * String parameters (e.g., $x = x + y$)\\n%    * Boolean parameters (e.g., $x = y$)\\n%\\end{sloppypar}\\n\\n%\\begin{sloppypar}\\n%    This function defines two types of parameters:\\n%    * String parameters (e.g., $x = x + y$)\\n%    * Boolean parameters (e.g., $x = y$)\\n%\\end{sloppypar}\\n\\n%\\begin{sloppypar}\\n%    This function defines two types of parameters:\\n%    * String parameters (e.g., $x = x + y$)\\n%    * Boolean parameters (e.g., $x = y$)\\n%\\end{sloppypar}\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n%%TODO: remove if it's redundant with other Optional arguments\\n\\n\\n\\n"}
{"File Name":"Sylius\/2020_03_03_api_product_option_values.md","Context":"### Problem to solve\\nCover with API all `ProductOption`s related functionality provided by the Admin panel.\\n### Context\\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle\\nProduct Option's values collection, to make it efficient and easy to use.\\n### Possible solutions\\nValues from collection could be embedded as objects within a `ProductOption` resource **or** provided by\\n[IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","Decision":"Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","tokens":215,"id":679,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n### Task to solve\\nTask to solve\\nTask to solve\\nTask to solve\\nTask to solve\\nTask to solve\\nTask to solve\\nTask to solve\\n\\n### Context\\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle Product Option's values collection, to make it efficient and easy to use.\\n### Possible solutions\\nValues from collection could be embedded as objects within a ProductOption resource or provided by [IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier).\\nMoreover, values embedded together with their translations in the ProductOption that already has its translations would result in a massive and unmanageable response.\\n"}
{"File Name":"Sylius\/2021_06_15_api_platform_config_customization.md","Context":"## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n","Decision":"Chosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n","tokens":98,"id":680,"Predictions":"\\begin{itemize}\\n    \\item[] \\textbf{Add a new option to the config: \"replaceable\" or \"customizable\".}\\n    \\item[] \\textbf{Add a new option to the config: \"orderable\" or \"customizable\".}\\n    \\item[] \\textbf{Add a new option to the config: \"customerable\" or \"customizable\".}\\n    \\item[] \\textbf{Add a new option to the config: \"orderable\" or \"customizable\".}\\n    \\item[] \\textbf{Add a new option to the config: \"customerable\" or \"customizable\".}\\n    \\item[] Add a new option to the config: \"orderable\", \"customerable\", and\/or ``orderable, customerable''.\\n\\n\\n\\n    %\\item[Option 1] Add a new option to the config: ``replaceable'' or ``customizable''.\\n\\n\\n\\n    %\\item[Option 2] Add a new option to the config: ``orderable'' or ``customerable''.\\n\\n\\n\\n    %\\item[Option 3] Add a new option to the config: ``orderable'', ``customerable'', and\/or ``orderable, customerable''.\\n\\n\\n    %\\end{itemize}\\n    \\n% *The current state of API Platform integration requires the replacement of the whole config of the whole resource.\\n%   A user will have to update all resources manually, that is why we need an additional configuration parameter.\\n%   It is also not possible for Sylius users to change their configurations on their own.\\n%   We need an additional configuration parameter that allows Sylius users to replace configurations on their own.\\n    \\n% *We want Sylius users to be able to customize their configurations. This can be done by adding custom configurations in some cases.\\n% *We want Sylius users not be able to customize their configurations. This can be done by adding custom configurations in some cases.\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n"}
{"File Name":"Sylius\/2020_11_18_sending_emails_via_api.md","Context":"## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n","Decision":"Chosen option: \"Using events\", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\\n","tokens":23,"id":681,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThis is a decision record for the API. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n\\n## Decisi"}
{"File Name":"Sylius\/2020_09_01_admin_and_shop_split.md","Context":"## Context and Problem Statement\\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\\ndecision that we want to be explicit, which user is trying to log in.\\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\\nresources are channel and locale agnostic, and we should use `codes` instead.\\n## Decision Drivers\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\n","Decision":"* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\nChosen option: \"Moving back to prefixed paths for back-office and shop\". This option was selected because it allows us to\\neasily leverage the Symfony Security component's benefits like firewalls and ACL. The only second and third option provides\\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\\nsame context.\\n","tokens":476,"id":682,"Predictions":"## Context and Problem Statement\\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\nndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change raised a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\\ndecision that we want to be explicit, which user is trying to log in.\\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\\nresources are channel and locale agnostic, and we should use `codes` instead.\\n## Decision Drivers\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n## Decision\\n## Context and Problem Statement\\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\nndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change raised a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\n## Decision\\n## Context and Problem Statement\\nWhile developing Sylius v4.x.x.x there were many changes made with respect with regards with REST APIs.\\nIn particular there were changes made with respect with regards with relation types.\\nIn particular there were changes made with respect with regards with relation types.\\nFor example:\\n```\\n{\\n    \"id\": \"123\",\\n    \"name\": \"Peter\",\\n    \"type\": \"Person\",\\n    ...\\n}\\n```\\nNowadays you cannot mix `person` type with `person` type.\\nThe solution was create an additional type called `personType`.\\nThis type would contain fields such as:\\n```\\n{\\n    \"id\": \"123\",\\n    ...\\n    \"type\": {\\n        ...\\n        'person': 'Person',\\n        ...\\n    },\\n}\\n```\\nNowadays you can mix `personType` type with `personType` type.\\nThe solution was create an additional type called `personType`.\\nThis type would contain fields such as:\\n```\\n{\\n    \"id\": \"123\",\\n    ...\\n    \"type\": {\\n        ...\\n        'person': 'Person',\\n        ...\\n        'personType': 'Person',\\n        ...\\n    },\\n}\\n```\\nThen you can mix them just fine.\\nIf you need more information about this please feel free ask questions here or open an issue at https:\/\/github.com\/Sylius\/Sylius\/issues\/new\\n"}
{"File Name":"Sylius\/2021_04_15_using_iri_as_api_resource_identifier_in_request_instead_of_code_id.md","Context":"## Context and Problem Statement\\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`\/`id`.\\nIn the past, we have been using `code`\/`id` instead of IRI, then we have been using both approaches.\\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`\/`code`in its commands and command handlers.\\n","Decision":"Chosen option: \"Handling and transforming IRI to `id`\/`code`\". Request that is based on command and needed information like `code`\/`id` should get it as IRI\\n","tokens":208,"id":683,"Predictions":"\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`\/`id`.\\nIn the past, we have been using `code`\/`id` instead of IRI, then we have been using both approaches.\\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`\/`code`in its commands and command handlers.\\n\\n\\end{document}"}
{"File Name":"va.gov-team\/0006-scale-vagov-content-gql-not-cms-export.md","Context":"## Context\\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\\n","Decision":"Halt implementation on CMS Export and focus on scaling GraphQL.\\n","tokens":69,"id":684,"Predictions":"## Decision\\nThe VAMC product will be released with the GraphQL API.\\n\\n## Context\\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\\n\\n## Decision\\n## Decision\\nThe VAMC product will be released with the GraphQL API.\\n\\n## Context\\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\\n\\n\\n\\end{document}\\n"}
{"File Name":"va.gov-team\/0002-deploy-pact-on-aws.md","Context":"## Context\\nThe Pact Broker has been on Heroku during development, but its database is running out of rows for the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n","Decision":"We will move the Pact Broker onto our AWS infrastructure.\\n","tokens":51,"id":687,"Predictions":"## Move the Pact Broker on to our AWS infrastructure.\\n\\n## Decision\\nThe Pact Broker is currently running on Heroku, but its database has reached the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n\\n## Decision\\nThe Pact Broker is currently running on Heroku, but its database has reached the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n\\n## Decision\\nThe Pact Broker is currently running on Heroku, but its database has reached the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n\\n## Decision\\nThe Pact Broker is currently running on Heroku, but its database has reached the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n\\n\\end{document}\\n"}
{"File Name":"va.gov-team\/0007-remove-user-model-va-profile.md","Context":"## Context\\nAs part of the larger effort to [refactor the User & Identity on vets-api](https:\/\/app.zenhub.com\/workspaces\/vsp-identity-5f5bab705a94c9001ba33734\/issues\/department-of-veterans-affairs\/va.gov-team\/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\\n","Decision":"The decision made is to transfer the User's `va_profile_status` and `va_profile_error` methods to `mpi_status` and `mpi_error` since that is what they are abstractions of, and to remove the `va_profile` method entirely in favor of relying on getter methods for specific attributes.\\n","tokens":117,"id":688,"Predictions":"## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs part of the larger effort to [refactor the User & Identity on vets-api](https:\/\/app.zenhub.com\/workspaces\/vsp-identity-5f5bab705a94c9001ba33734\/issues\/department-of-veterans-affairs\/va.gov-team\/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\\n\\n## Decision\\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs part of the larger effort to [refactor the User & Identity on vets-api](https:\/\/app.zenhub.com\/workspaces\/vsp-identity-5f5bab705a94c9001ba33734\/issues\/department-of-veterans-affairs\/va.gov-team\/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\\n\\n## Decision\\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs part of the larger effort to [refactor the User & Identity on vets-api](https:\/\/app.zenhub.com\/workspaces\/vsp-identity-5f5bab705a94c9001ba33734\/issues\/department-of-veterans-affairs\/va.gov-team\/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\\n\\n"}
{"File Name":"va.gov-team\/0004-return-uninflected-va-keys.md","Context":"## Context\\nUsing the Rails 6 zeitwerk file loader requires strict naming conventions of files. So without a Rails inflection, `va_something` class files are expected to have the class name of `VaSomething`. With an inflection, that class name would be `VASomething`. `vets-api` uses a mix of `VA` and `Va` in class names. To make things consistent, as well as support instances of `humanize` outputing `VA` in caps as an acronym, the inflection will be introduced.\\nHowever, using this inflection has a downstream affect on json responses transformed into camelCase by the [olive_branch](https:\/\/github.com\/vigetlabs\/olive_branch) gem. Previously keys structured like `the_va_something` were transformed into `theVaSomething`, but with inflection they will now be `theVASomething`.\\n","Decision":"Create a patch implemented for the olive_branch middleware that returns keys containing `VA` as if \"VA\" was not an acronym.\\nIn the future vets-api should introduce a version that doesn't support olive_branch.\\n","tokens":185,"id":689,"Predictions":"## Decision\\nThe decision is to use `va_something` class names. This will allow the json responses to be transformed from camelCase to caps and also allow for instance of `humanize` outputing `VA` in caps as an acronym.\\n\\n## Context\\nIf you want the json response to be rendered in camelCase, you'll need a gem like `camelcase`. If you want the json response to be rendered in caps, you'll need a gem like `humanize`. If you want both, then you'll need a gem like both. For example, if you wanted the json response rendered in camelCase but not caps, then you'd use the following:\\n```\\nrender json: { foo: 'bar' }\\n```\\nBut if you wanted the json response rendered in both cases, then you'd use the following:\\n```\\nrender json: { foo: 'bar', foo_caps: true }\\n```\\n## Decision\\n## Decision\\nThe decision is to use camelCase for all keys structured like `the_va_something`, and caps for all keys structured like `theVASomething`. The reason is that this allows us to support instances of `humanize` outputing `VA` in caps as an acronym. This will also allow us to render our responses using both methods.\\n\\n## Context\\nIf you want the json response to be rendered in both cases, then you'll need a gem like both. For example, if you wanted the json response rendered in both cases but not camelCase, then you'd use the following:\\n```\\nrender json: { foo: 'bar', foo_caps: true }\\nrender json: { foo: 'bar', foo_caps: false }\\n```"}
{"File Name":"va.gov-team\/0001-record-architecture-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described in this Confluence document](https:\/\/vfs.atlassian.net\/l\/c\/58vSL6ZP).\\n","tokens":16,"id":690,"Predictions":"## A decision was made on this project based on the context provided by the User.\\n\\n## Decision Code\\n## A decision code was generated based on the context provided by the User.\\n\\n## Decision Code\\nA decision code was generated based on the context provided by the User.\\n\\n## Decision Code\\nA decision code was generated based on the context provided by the User.\\n\\n## Decision Code\\nA decision code was generated based on the context provided by the User.\\n\\n## Decision Code\\nA decision code was generated based on the context provided by the User.\\n\\n## Decision Code\\nA decision code was generated based on the context provided by the User.\\n\\n\\end{DoxyCode}\\n"}
{"File Name":"qc-atlas\/0009-joined-table-for-knowledge-artifact.md","Context":"## Context and Problem Statement\\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n","Decision":"* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\nChosen option: \"[Joined table]\", because it is already implemented, allows for references in both ways and has no significant downside.\\n### Positive Consequences <!-- optional -->\\n* The current implementation can stay\\n","tokens":107,"id":691,"Predictions":"\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe class `KnowledgeArtifact` is an helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nThe class `KnowledgeArtifact` is an helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n\\n#183\\n\\nIssue #183: Should knowledge artifact be stored in Discussion Topic or Discussion Topic Topic?\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#183\\n\\nIssue #183: Should knowledge artifact be stored in Discussion Topic or Discussion Topic Topic?\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#181\\n\\nIssue #181: Knowledge Artifact should be able to be created using\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#180\\n\\nIssue #180: Knowledge Artifact should be able to be created using discussion topic.\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#179\\n\\nIssue #179: Knowledge Artifact should be able to be created using discussion topic.\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#178\\n\\nIssue #178: Knowledge Artifact should be able to be created using discussion topic.\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#177\\n\\nIssue #177: Knowledge Artifact should only contain data that has been added in Discussion Topic.\\n\\nCreated 5 years ago by m.baldwin.\\n\\n#176\\n\\nIssue #176: Knowledge Artifact should only contain data that has been added in Discussion Topic.\\n\\nCreated 6 months ago by m.baldwin."}
{"File Name":"qc-atlas\/0007-junit-for-testing.md","Context":"## Context and Problem Statement\\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\\n","Decision":"Chosen option: \"[JUnit5]\", because it is the newer version and therefore has a higher maturity and a wider feature-set.\\n### Positive Consequences <!-- optional -->\\n* Uniform tests\\n* More and newer features\\n","tokens":37,"id":692,"Predictions":"## Decision: JUnit5\\nThe decision is to use JUnit5 and to use the @RunWith annotation to make the tests more consistent.\\n\\n## Decision: No\\nThe decision is not to use JUnit5.\\n\\n## Decision: Yes\\nThe decision is to use JUnit5.\\n\\n## Decision: No\\nJUnit5 will be used for the test cases.\\n\\n## Decision: Yes\\nJUnit5 will be used for the test cases.\\n\\n## Decision: No\\nJUnit5 will be used for the test cases.\\n\\n## Decision: Yes\\nJUnit5 will be used for the test cases.\\n\\n## Decision: No\\nJUnit5 will be used for the test cases.\\n\\n## Decision: Yes\\nJUnit5 will be used for the test cases.\\n\\n## Decision: No\\nJUnit5 will be used for the test cases.\\n\\n\\n\\begin{lstlisting}\\n@Test\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n@Test(expected = Exception.class)\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n@Test(expected = Exception.class)\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n@Test(expected = Exception.class)\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n@Test(expected = Exception.class)\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n@Test(expected = Exception.class)\\npublic void someTest() {\\n    \/\/ ...\\n}\\n\\end{lstlisting}\\n\\n"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{verbatim}\\n\\n\\subsection{User Requirements}\\nThe user requirements are listed in the following table:\\n\\begin{center}\\n\\begin{tabular}{|l|l|}\\n\\hline \\nUser \\newline Requirements & \\newline Description \\\\ \\hline \\n\\end{tabular}\\n\\end{center}\\n\\nThe requirements are described in the following table:\\n\\begin{center}\\n\\begin{tabular}{|l|l|}\\n\\hline \\nRequirement \\newline Description & \\newline Requirement \\\\ \\hline \\n1.1.1.1 & The API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible. \\\\ \\hline \\n2.2 & The API should be documented in human and machine readable form. If the API is not documented in human and machine readable form, automated generation of client services is possible. \\\\ \\hline \\n2.3 & The API should be documented in human and machine readable form, if it is possible to generate client services automatically. \\\\ \\hline \\n3.1 & The documentation should be available for everyone who can use the API (developers). \\\\ \\hline \\n3.2 & The documentation should be available for everyone who can use the API (developers) and for everyone who can use the service (users). \\\\ \\hline \\n3.4 & The documentation should be available for everyone who can use the service (users). \\\\ \\hline \\n\\n4.6 & Documentation should be available for everyone who can use the service (users). \\\\ \\hline \\n\\n5.5 & Documentation should be available for everyone who can use the service (users).\\\\ \\hline \\n\\n6.7 & Documentation should be available for everyone who can use the service (users).\\\\ \\hline \\n\\n7.10 & Documentation should be available for everyone who can use the service (users).\\\\ \\hline\\n\\n8.11& Documentation should be available for everyone who can use the service (users) and for developers.\\n\\n\\n\\n\\n\\n9.& Documentation should be available for anyone who uses or develops software that uses this software.\\n\\n\\n\\n\\n\\n10.& Documentation should be available to anyone who wants to know how to use this software.\\n\\n\\n\\n\\n\\n11.& Documentation should not only explain how to create a client service but also how to create a client application.\\n\\n\\n\\n\\n\\n12.& Documentation should not only explain how to create a client application but also how to create a server application.\\n\\n\\n\\n\\n13.& Documentation must not only explain how to create a server application but also how to create an HTTP server.\\n\\n\\n14.& Documentation must not only explain how to create an HTTP server but also how to make HTTP requests with a web browser.\\n\\n\\n15.& Documentation must not only explain how to make HTTP requests with a web browser but also how to make HTTP requests with an Android device.\\n\\n\\n16.& Documentation must not only explain how to make HTTP requests with an Android device but also how to make HTTP requests with iOS devices.\\n\\n\\n17.& Documentation must not only explain how to make HTTP requests with iOS devices but also how to make HTTP requests with Android devices.\\n\\n\\n\\n18.&Documentation must not only explain how to make HTTP requests with iOS devices but also how to make HTTP requests with Android devices.\\n\\n\\n19.:Documentation must not only explain how to make HTTP requests with iOS devices but also when it is necessary that you need other protocols than http.\\n\\n20.:Documentation must not only explain what protocols are used by this software, they must also know when it is necessary that you need other protocols than http.\\n\\n21.:Documentation must not only explain what protocols are used by this software, they must know when it is necessary that you need other protocols than http.\\n\\n22.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n23.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n24.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n25.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n26.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n27.:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n28:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n29:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n30:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n31:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n32:Documentation will focus on any kind of protocol and protocol version that may exist.\\n\\n\\n\\n33:Documentation will focus on any kind of protocol and "}
{"File Name":"qc-atlas\/0003-model-mapper.md","Context":"## Context and Problem Statement\\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\\n","Decision":"Chosen option: \"[Model Mapper](http:\/\/www.modelmapper.org)\", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\\n### Positive Consequences <!-- optional -->\\n* Less boilerplate code\\n","tokens":42,"id":694,"Predictions":"## Decision Decision\\n## Decision Decision\\n## Decision Decision\\n## Decision Decision\\n## Decision Decision\\n## Decision\\n\\n\\end{verbatim}\\n\\n"}
{"File Name":"qc-atlas\/0004-postgres-for-service-tests.md","Context":"## Context and Problem Statement\\nTo ensure high test coverage within the project, functionalities that interact with the database must be tested as well.\\nThere are different approaches to testing these functionalities that require a database running.\\n","Decision":"Chosen option: \"Use Production-like [Postgres](http:\/\/www.postgresql.org) database\", because it ensures a production-like behavior.\\n### Positive Consequences <!-- optional -->\\n* Almost same configuration for test and runtime.\\n* Less database technologies used.\\n* Guaranteed that production and tests behave the same.\\n","tokens":43,"id":695,"Predictions":"## Decision\\n## Decision\\n## Decision\\n\\end{verbatim}\\n\\end{itemize}\\n\\n\\subsubsection{Test Coverage}\\nThe test coverage is calculated by the number of tests that are executed. It is calculated as the percentage of tests that are executed. \\n\\begin{itemize}\\n\\item \\textbf{Total number of tests executed} = \\# Tests + \\# Tests executed in Test Cases.\\n\\item \\textbf{Test Coverage} = \\# Tests executed in Test Cases \/ Total number of tests executed. \\n\\end{itemize}\\n\\n\\subsubsection{Test Coverage (%)}\\nThe test coverage is calculated by the percentage of tests that are executed. \\n\\n% TODO: add more details about this\\n\\n% TODO: add more details about this\\n\\n% TODO: add more details about this\\n\\n% TODO: add more details about this\\n\\n%\\subsubsection{Defects}\\n%\\begin{itemize}\\n%\\item \\textbf{\\# Defects} = Number of defects found during testing.\\n%\\end{itemize}\\n\\n% TODO: add more details about this\\n\\n% TODO: add more details about this\\n\\n\\n%\\section*{\\underline{\\textbf{Trial}}\\n%\\begin{itemize}\\n%\\item The trial phase is the phase in which the software is tested and developed for the first time.\\n%\\end{itemize}\\n\\n\\n%\\section*{\\underline{\\textbf{Trial}}}\\n\\n"}
{"File Name":"qc-atlas\/0006-model-assemblers.md","Context":"## Context and Problem Statement\\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\\nConstructing such objects, as well as adding the desired links to them is a common operation that\\nrequires entity-specific boilerplate code.\\nHow can duplicate code in nearly all controller methods be avoided?\\n## Decision Drivers <!-- optional -->\\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\n","Decision":"* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\\nand our DTO classes.\\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\\n","tokens":97,"id":696,"Predictions":"## Decision Drivers\\nThe above example is an attempt to decouple entity logic from link creation. This decoupling\\nprovides a common layer of logic that can be used by all controllers. It is not intended to be used in the case where\\nthe entity logic is already implemented by the controller itself. For this, we have already provided a solution in\\nthe form of HATEOASContextFactory, which provides a means to construct an HATEOASContext.\\nIn the above example, we have chosen to use this factory implementation for our software. The factory implementation\\nis responsible for creating a HATEOASContext and then returning it to the controller.\\nThere are several reasons why this approach works well:\\n\\begin{itemize}\\vspace{-0.5em}\\n\\item The factory returns a single instance of HATEOASContext that can be reused by all controllers.\\n\\item The factory can be used in any controller that needs access to HATEOASContext.\\n\\end{itemize}\\nThe following table summarizes the decisions made in this example:\\n\\begin{center}\\vspace{-1em}\\n\\begin{tabular}{|l|l|}\\hline\\nDecision & Decision Drivers\\\\\\hline\\n1 & Avoid duplicate code to create HATEOAS models\\\\\\hline\\n2 & Decouple link creation from normal entity logic\\\\\\hline\\n3 & Use the same factory for all controllers\\\\\\hline\\n4 & Use an OSGi container for managing the lifecycle of the software\\\\\\hline\\n5 & Provide a means to configure and manage the software\\\\\\hline\\n6 & Provide an API for configuring and managing the software\\\\\\hline\\n7 & Implement a model validator as part of the software\\\\\\hline\\n8 & Provide an API for configuring and managing the software\\\\\\hline\\n9 & Implement a model validator as part of the software\\\\\\hline\\n10 & Provide an API for configuring and managing the software\\\\\\hline\\n11 & Implement a model validator as part of the software\\\\\\hline\\n12 & Provide an API for configuring and managing the software\\\\\\hline\\n13 & Implement a model validator as part of the software\\\\\\hline\\n14 & Implement a model validator as part of the software\\\\\\hline\\n15 & Implement a model validator as part of the software\\\\\\hline\\n16 \\end{tabular}\\end{center}\\nThe following figure shows how decisions 1-4 are implemented.\\n\\begin{figure}[htbp]\\centering\\includegraphics[width=0.9\\linewidth]{figures\/decision1.png}\\captionof{figure}{Decision 1: Avoid duplicate code to create HATEOAS models}\\label{fig:decision1} \\end{figure}\\nDecision 2: Decouple link creation from normal entity logic\\nThis decision requires some additional work on our part.\\nWe need to implement methods that return objects that are responsible for creating links between entities.\\nIn order to do so, we need two classes:\\n\\begin{itemize}\\vspace{-0.5em}\\n\\item An interface called LinkFactory that provides methods that return objects responsible for creating links between entities.\\n\\item A concrete class LinkFactoryImpl, which implements LinkFactory and has methods similar to those in LinkFactoryImpl.\\n\\end{itemize}We also need some means for configuring these factories.\\nThe following table summarizes our decisions:\\n\\begin{center}\\vspace{-1em}\\n\\begin{tabular}{|l|l|l|l|l|} \\cline{2-5}\\nDecision&Object&Interface&Class&Method\\\\ \\cline{2-5}\\n1&LinkFactoryImpl&LinkFactory&createLink()&createLink(\\#Entity)\\\\ \\cline{2-5}\\n2&LinkFactoryImpl&LinkFactoryImpl&createLink(\\#Entity)\\\\ \\cline{2-5}\\n3&ConcreteClassLinkFactoryImpl&ClinkFactoryImpl&createLink(\\#Entity)\\\\ \\cline{2-5}\\n4&ClinkFactoryImpl&ClinkFactoryImpl&createLink(\\#Entity)\\\\ \\cline{2-5}\\n5&ClinkFactoryImpl&ClinkFactoryImpl&createLink(\\#Entity)\\\\ \\cline{2-5}\\n6&ClinkFactoryImpl&ClinkFactoryImpl&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n7&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n8&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n9&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n10&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n11&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\n12&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\cline{2-5}\\\\ \\n13&WearableRepository&&createWearableRepository(\\#Entity)\\\\ \\n14&WearableRepository&&getWearableRepo()\\\\ \\n15\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n16\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n17\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n18\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n19\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n20\tWearableRepo&&getWearableRepo()\\\\\\\\ \\n21\tWearableRepo && getReplicatedRepositories() \\\\ \\n22\tReplicatedRepositories && getReplicatedRepositories() \\\\ \\n23\tReplicatedRepositories && getReplicatedRepositories() \\\\ \\n24\tReplicatedRepositories && getReplicatedRepositories() \\\\ \\n25\tReplicatedRepositories && getReplicatedRepositories()\\end {tabular}\\\\[0.7ex]\\n%No line break in table environment (it's too long)\\n% Table caption is below bottom of Figure  \\n%{\\footnotesize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily\\normalsize\\ttfamily%\\par\\vspace{-0.3em}}%\\n%\\captionof*{\\textbf{\\textit{{Table caption}}}}%\\n%{\\footnotesize\\ttfamily\\vspace{-0.3em}}%\\n%\\label{}%\\n\\n\\nTo make things more complicated, we need another class called EntityCreator that implements both LinkCreator and EntityCreator.\\nThe following table summarizes our decisions:\\n\\begin{center}\\vspace{-1em}\\n\\nDecision Object Interface Method Method Class Method Class Method Class Method Class Method Class Method Class Method Class Method Class Method Class Method Class Method Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator Model Validator \\n\\n3 Entity Creator Entity Creator Entity Creator Entity Creator Entity Creator Entity Creator Entity Creator \\n\\n4 Entity Creator Entity Creator \\n\\nClink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClassLink Factory ConcreteClass\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation\\n\\nModel Validation \\n\\nElastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine Elastic Search Engine \\n\\nEnterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patterns Enterprise Integration Patters \\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nEnterprise Service Bus (ESB)\\n\\nMessage Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker Implementation Message Broker \\n\\nMessage Queue Implimentation Message Queue Implimentation Message Queue Implimentation Message Queue Implimentation Message Queue Implimentation Message Queue Implimentation Message Queue Implimentation Message Queue Implimentation Messeage Queue implimentation Messeage Queue implimentation Messeage Queue implimentation Messeage Queue implimentation Messeage Queuesimpliation Messeage Queuesimpliation Messeage Queuesimpliation Messeage Queuesimpliation Messege Queuesimpliation Messege Queuesimpliation \\n\\nMessaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway Messaging Gateway \\n\\nMessage Router\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/Proxy\/Middleware\/BaseMessageRouterBaseMessageRouterBaseMessageRouterBaseMessageRouterBaseMessageRouterBaseMessageRouterBaseMessageRouterBaseMessageRouter BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway BaseMessagingGateway \\n\\nNetty Netty Netty Netty Netty Netty Netty Netty Netty \\n\\nOSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi Container OSGi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi container osgi-container org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framework org.osgi.framewrk.org.osgi.framewrk.org.osgi.framewrk.org.osgi.framewrk.org.slf4j.slf4j.slf4j.slf4j.slf4j.slf4j.slf4j.slf4j.impl.slf4j.impl.netty.netty.netty.netty.netty.netty netty netty netty nettle nettle nettle nettle nettle nettle nettle nettle nettle nettle-netty-netty-nettleservletservletservletservlets.servletservlets.servletservlets.servletservlets.servletservlets.servletservlets.servletservlets.servletservlets.servletservlets.servlet servlet servlet servlet servlet servlet servlet servlet servlet servlet servlet servlet servlet servlet ServletServletServletServletServletServletServletServletServletServletServletServletServletServletServlet Servlet Servlet Servlet Servlet Servlet Servlet Servlet Servlet Servlet Servlet"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{verbatim}\\n\\n\\begin{itemize}\\n\\item \\textbf{Decision}: Split the application into different repos. The decision is based on the problem statement. \\n\\item \\textbf{Context}: The planQK platform is a software that collects data from a number of sensors and provides high level information about these measurements to users. It is used by a number of users such as architects, engineers and construction professionals.\\n\\item \\textbf{Problem Statement}: The problem statement is the main reason behind the decision.\\n\\end{itemize}\\n\\n\\subsection*{Decision}\\nThe application will be split into individual repos, which are separated by their own file system. This makes it easier for developers to work on them separately and they can also be cloned independently from each other.\\n\\nThe different repos are:\\n\\n\\begin{itemize}\\n\\item \\textbf{Developer:} It contains all code that is used for building the platform, including code for building the UI and backend.\\n\\item \\textbf{Backend:} It contains all code that is used for running and processing data from sensors and providing information to users in the form of graphs and tables.\\n\\item \\textbf{Frontend:} It contains all code that is used to render graphs and tables in a browser. \\n\\end{itemize}\\n\\nA separate file system will make it easier for developers to work on these repos individually.\\n\\nThe decision was made because we wanted to keep the code base as small as possible. We also wanted to avoid having too many different repos, as this would make it harder for us to collaborate with each other.\\n\\nWe decided that splitting the application into individual repos was a good idea because we felt that this would make it easier for us to work on our own parts of the application without having to worry about merging changes made by other developers.\\n\\nWe also felt like keeping our code base as small as possible would help us focus on improving our platform instead of having too many bugs or issues in our code base.\\n\\nIn order to keep our code base as small as possible we decided not to include any libraries or frameworks that were not essential for building our platform. We decided not to include any libraries or frameworks because they would have made it much harder for us to build our platform without having them available.\\n\\nWe felt like including libraries or frameworks would have made it harder for us when we needed assistance from other developers and it would have made it harder for us if we had problems with our code base.\\n\\n\\n%------------------------------------------------\\n\\n%------------------------------------------------\\n\\n\\bibliographystyle{junsrt}\\n\\bibliography{junsrt}\\n\\n\\n%------------------------------------------------\\n\\n\\n%------------------------------------------------\\n\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n\\n%% ----------------------------------------------------------\\n\\n\\n\\n"}
{"File Name":"qc-atlas\/0009-remove-hal-links.md","Context":"## Context and Problem Statement\\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\\n## Decision Drivers\\n* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\n","Decision":"* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\\n","tokens":55,"id":698,"Predictions":"## Decision Description\\n\\n## Decision Drivers\\n## Decision Drivers\\n## Decision Drivers\\n\\end{verbatim}\\n\\n\\subsubsection{Decision}\\n\\label{sec:decision}\\n\\begin{itemize}\\n    \\item \\textbf{Decision Description} The decision is to use the OpenAPI for frontend client code generation. This choice is driven by the fact that the client code generation tools are already available and the user does not need to develop any additional tools.\\n    \\item \\textbf{Decision Drivers} The decision is driven by the following factors:\\n    \\begin{itemize}\\n        \\item \\textbf{Readability of API documentation} The OpenAPI comments may be too long for a human to read, especially if they do not have a background in software engineering. If this problem is solved, it will help developers to understand the API.\\n        \\item \\textbf{Development effort} As mentioned in section~\\ref{sec:decision}, developers have to write their own tools to generate OpenAPI documents from their application code. If we use a tool like SwaggerHub or SwaggerHub-UI, developers will not have to write any tools and therefore may be more motivated to use our tool.\\n    \\end{itemize}\\n    \\item \\textbf{Decision Drivers} The decision is driven by the following factors:\\n    \\begin{itemize}\\n        \\item \\textbf{Development effort} As mentioned in section~\\ref{sec:decision}, developers have to write their own tools to generate OpenAPI documents from their application code. If we use a tool like SwaggerHub or SwaggerHub-UI, developers will not have to write any tools and therefore may be more motivated to use our tool.\\n    \\end{itemize}\\n    \\item \\textbf{Decision Drivers} The decision is driven by the following factors:\\n    % TODO: add more drivers\\n        % TODO: add more drivers\\n        % TODO: add more drivers\\n        % TODO: add more drivers\\n\\n    % TODO: add more drivers\\n\\n\\end{itemize}\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n%---------------------------------------------------------\\n%---------------------------------------------------------\\n\\section{\\texorpdfstring{\\label{text-3}}{\\texorpdfstring{\\label{text-7}}{\\texorpdfstring{\\label{text-8}}{\\texorpdfstring{\\label{text-10}}{\\texorpdfstring{\\label{text-11}}{\\texorpdfstring{\\label{text-12}}{\\texorpdfstring{\\label{text-13}}{\\texorpdfstring{\\label{text-14}}}{}}}%\\n%---------------------------------------------------------\\n}\\n\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n%---------------------------------------------------------\\n%---------------------------------------------------------\\n\\section*{{\\centering {\\footnotesize {\\LARGE 5.2 Recommendation}}\\n\\vspace{-0.5cm}}\\n\\vspace{-0.5cm}\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n%---------------------------------------------------------\\n\\n%\\section*{{\\centering {\\footnotesize {\\LARGE 5.3 Recommendation}}\\n\\vspace{-0.5cm}}\\n\\vspace{-0.5cm}\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n\\n%\\section*{{\\centering {\\footnotesize {\\LARGE 5.4 Recommendation}}\\n\\vspace{-0.5cm}}\\n\\vspace{-0.5cm}\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n\\n%\\section*{{\\centering {\\footnotesize {\\LARGE 6 Recommendation}}\\n\\vspace{-0.5cm}}\\n\\vspace{-0.5cm}\\n\\n%\\newpage\\n\\n%---------------------------------------------------------\\n\\n%\\section*{{\\centering {\\footnotesize {\\LARGE 7 Conclusion}}\\n\\vspace{-0.5cm}}\\n\\n%\\newpage\\n\\n\\n"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"Predictions":"## 1. Launch a container to ensure PostgreSQL is always up and running.\\n\\n\\n\\n## Problem Statement\\n\\nThe problem statement is the problem that the user wants to solve with this software. It is a short description of what the software does and how it works.\\n## Context\\n## 1. The user needs a container for PostgreSQL. It should be launched automatically when the service is started.\\n## Problem Statement\\n\\nThe problem statement is the problem that the software solves. Give a short description of your use case.\\n\\end{document}\\n"}
{"File Name":"qc-atlas\/0001-use-URI-for-entities.md","Context":"## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n","Decision":"Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n","tokens":39,"id":700,"Predictions":"## Decision: \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tT\\#1:\\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision: \\n\\#2:\\n\\n"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"Predictions":"\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n## Decision\\n\\n\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n## Decision\\n\\end{verbatim}\\n"}
{"File Name":"radiant-mlhub\/0002-resolving-api-keys.md","Context":"## Context\\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\\nproviding an API key to be used when making a request to the API. These options should include:\\n* Storing API keys on the users system\\n* Reading an API key from the environment\\n* Passing an API key directly to the API request methods\\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\\nus from storing this additional information.\\n","Decision":"The Python client will resolve the API key to be used in a request in the following order:\\n1) Passing an `api_key` argument directly to the method\\n2) Setting an `MLHUB_API_KEY` environment variable\\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\\n5) Using the API from the `default` profile\\nProfiles will be stored in a `.mlhub\/profiles` file in the user's home directory. This file will be an INI file containing at least a\\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\\npassed to a method must correspond to one of these section names, or it will raise an exception.\\n","tokens":228,"id":702,"Predictions":"\\nWe have chosen to store API keys on the user's system. This is the recommended method for storing API keys and\\nis consistent with the goals of the project. The user can choose to provide an API key from the environment or from their\\nsystem, but we require that they provide a way to resolve an API key for a given project. This way, we will not need to store\\nadditional information about the user's API keys.\\n\\end{document}"}
{"File Name":"radiant-mlhub\/0003-will-not-implement-get-items.md","Context":"## Context\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n","Decision":"To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\\nthe [`Collection.get_items`](https:\/\/pystac.readthedocs.io\/en\/latest\/api.html#pystac.Catalog.get_items) method to make paginated requests to\\nthe `\/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\\n","tokens":153,"id":703,"Predictions":"\\begin{itemize}\\n\\item If the next token is a \\texttt{\\#} (\\texttt{\\#items}), then the next set of items\\nis cached, and the user can retrieve them without further API calls. Otherwise, the next\\nset of items is fetched from the server.\\n\\item If the next token is a \\texttt{\\#} (\\texttt{\\#items}), then we return a \\texttt{\\#} (\\texttt{\\#items}) set of\\nitems. The user can retrieve this set of items without further API calls.\\n\\item If the next token is a \\texttt{\\#} (\\texttt{\\#items}), then we return an empty set of\\nitems. The user can retrieve this set of items without further API calls.\\n\\end{itemize}\\n\\n## Description\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n\\n\\n## Details\\nThis document describes how to implement an item retrieval method using one or more Radiant MLHub APIs.\\npages \\n\\n## Related Documents\\n* [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\n* [STAC API - Querying](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-queries)\\n* [STAC API - Retrieving](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-requests)\\n"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"auth-account-koa\/0002_use_lodash.md","Context":"## Context\\nThere is the problem where to store different utilities and write every time the same code on all projects.\\n","Decision":"Use [lodash](https:\/\/lodash.com\/docs\/) possibilities as main package for utilities on the JS microservices, in order to spend less time and write less code. And use utils directory as a wrapper for it in order to be able to replace it any time.\\n","tokens":24,"id":705,"Predictions":"## \\n## Decision\\nThe solution proposed is to store the utility and write only the code on the project where it is used.\\n\\n## Decision\\nThe utility will be stored in a directory called \\texttt{utilities}.\\n## Decision\\nThe file \\texttt{utilities} will contain:\\n## \\n## File\\nThe file \\texttt{utilities} will contain:\\n## \\n## Name\\nA name for the utility.\\n## Description\\nA description of the utility.\\n## Usage\\nThe usage of the utility.\\n## \\n\\end{verbatim}\\n\\end{itemize}\\n"}
{"File Name":"auth-account-koa\/0001_use_adr.md","Context":"## Context\\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\\n","Decision":"I will use ADR to document any important architectural decisions I make.\\nADRs will be checked into the repository as numbered md files in the folder docs\/architecture\/adr\\nI will follow the template described [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\nAll Records will contain, Title, Context, Decision, Status and Consequences\\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\\n","tokens":28,"id":706,"Predictions":"## 1. We should use a KVM hypervisor for our virtual machines. This will allow us to scale up and down at the click of a button, as well as allowing us to easily change server hardware when new hardware becomes available.\\n## 2. We should use a software layer to provide the networking and storage for our virtual machines. This will allow us to scale up and down as well as allow us to easily switch between servers.\\n## 3. We should use an in-memory database for our data storage. This will allow us to scale up and down as well as allowing us to easily switch between servers.\\n## 4. We should use a database backed by a NoSQL store for our data storage. This will allow us to scale up and down as well as allowing us to easily switch between servers.\\n## 5. We should use a NoSQL database backed by a RDBMS for our data storage. This will allow us to scale up and down as well as allowing us to easily switch between servers.\\n## 6. We should use a RDBMS backed by an OLTP database for our data storage, this is the most expensive choice and is not compatible with the above three choices.\\n## 7. We should use an OLTP database backed by an RDBMS for our data storage, this is the most expensive choice but is compatible with the above three choices.\\n# # See https:\/\/www.sqlite.org\/lang_createtable.html#CREATE_TABLE\\n# CREATE TABLE `my_table` (\\n#   `id` int(11) unsigned NOT NULL AUTO_INCREMENT,\\n#   `text` varchar(256) NOT NULL,\\n#   PRIMARY KEY (`id`)\\n# ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\\n\\n# # See https:\/\/www.sqlite.org\/lang_createtable.html#CREATE_TABLE\\nCREATE TABLE `my_table` (\\n    `id` int(11) unsigned NOT NULL AUTO_INCREMENT,\\n    `text` varchar(256) NOT NULL,\\n    PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\\n\\n"}
{"File Name":"pfb-network-connectivity\/adr-0004-asynchronous-task-queue-2.md","Context":"## Context\\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a \"worker\" stack via a JSON cofiguration.\\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\\n","Decision":"The team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\\n","tokens":316,"id":707,"Predictions":"\\nThe decision is to use AWS Batch. The reason for this decision is that AWS Batch provides a superior solution to the strategy described in ADR 0003. Even without any particular reason, it is clear to me that AWS Batch will be easier to configure and manage than a manually managed stack of Celery workers + broker + result backend.\\n\\end{document}"}
{"File Name":"pfb-network-connectivity\/adr-0000-architecture-documentation.md","Context":"## Context\\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\n","Decision":"We have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\\n","tokens":141,"id":708,"Predictions":"## Given the Context, the Decision is:\\n## This is a software decision. We have chosen to use the [Open Source Framework](https:\/\/github.com\/square\/edge) for our [front end](https:\/\/github.com\/square\/edge\/tree\/master\/front-end). The team feels that this gives us a more cohesive and consistent experience.\\n## Arguments\\nWe have analyzed the following arguments in support of this decision:\\n## 1. The Edge Web Framework has been open sourced by Square and is supported by the community.\\n## 2. The Edge Web Framework has a well documented API.\\n## 3. The Edge Web Framework has a well documented architecture.\\n## 4. We have chosen to use the Edge Web Framework because it is an open source framework and has been supported by the community.\\n\\nThe team has identified two primary reasons for choosing this software implementation over another one:\\n1. This is a software decision. We have chosen to use the [Open Source Framework](https:\/\/github.com\/square\/edge) for our [front end](https:\/\/github.com\/square\/edge\/tree\/master\/front-end). The team feels that this gives us a more cohesive and consistent experience.\\n2. The Edge Web Framework has been open sourced by Square and is supported by the community.\\nThe team has identified two primary reasons for choosing this software implementation over another one:\\n1. The Edge Web Framework has been open sourced by Square and is supported by the community.\\n2. The Edge Web Framework has a well documented API.\\nThe team has identified two primary reasons for choosing this software implementation over another one:\\n1. The Edge Web Framework has been open sourced by Square and is supported by the community.\\n2. The Edge Web Framework has a well documented API.\\n\\n"}
{"File Name":"pfb-network-connectivity\/adr-0003-asynchronous-task-queue.md","Context":"## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http:\/\/docs.celeryproject.org\/en\/latest\/getting-started\/brokers\/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n","Decision":"The team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n","tokens":415,"id":709,"Predictions":"## Decision\\nThis decision is for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http:\/\/docs.celeryproject.org\/en\/latest\/getting-started\/brokers\/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n\\n## Decision\\n## Decision\\nThis decision is for an Architectural Decision Record. Give an ## Architectural Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key component of this project is an 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + buffer + import from PostgreSQL into QGIS database (or directly from PostgreSQL) \\ndownload related census block and job data for boundary + import from PostgreSQL into QGIS database (or directly from PostgreSQL) \\ndownload related census block and job data for boundary + import from PostgreSQL into QGIS database (or directly from PostgreSQL) \\ndownload census block and job data from OSM \\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload census block & job data in CSV format\\ndownload Census Block Data in csv file format \\ndownload Census Block Data in csv file format \\enddownload Census Block Data in csv file format \\enddownload Census Block Data in csv file format \\enddownload Census Block Data in csv file format \\enddownload Census Block Data in csv file format \\enddownload Census Block Data in csv file format\\n## Decision\\n## Decision\\nThis decision is for an Architectural Decision Record. Give an ## Architectural Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe key component of this project is an 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into PostgreSQL database\\n- Download OSM extract for Boundary + buffer + import from PostgreSQL into QGIS database (or directly from PostgreSQL) \\ndownload related census block and job data for Boundary + import from PostgreSQL into QGIS database (or directly from PostgreSQL) \\nedload related census block and job data "}
{"File Name":"pfb-network-connectivity\/adr-0002-backend-language-framework.md","Context":"## Context\\nA backend web framework significantly eases the amount of boilerplate necessary for an application to handle web requests. Additionally, it provides scalable, out of the box, well-tested solutions for common features such as user management, authentication, database interaction, and a public API.\\nThis project will mostly require only the common components described above, so a backend web framework that we are familiar with will speed development for the project. The only unknown is the integration of the asynchronous 'Bicycle Network Analysis' task mentioned in [ADR 0001](adr-0001-development-environment.md)\\nThe team is most familiar with Python, Django and the Django Rest Framework. Due to project constraints and the desired functionality, no other backend frameworks were considered for this project.\\n","Decision":"The team will use Django with the Django Rest Framework plugin, written in Python. The team's familiarity with this stack is too much of a positive to pass up. In addition, Django provides many third-party solutions for integrating the asynchronous 'Bicycle Network Analysis' task. This allows the team to be flexible when choosing a solution, without sacrificing development efficiency.\\n","tokens":154,"id":710,"Predictions":"\\nThe Django Rest Framework is a popular Python framework for building RESTful web services. The Django Rest Framework provides an easy to use API for developing web applications. The framework also provides a set of standard features that are useful for building web applications. Django Rest Framework provides a large number of features that are useful for building web applications, including:\\n\\begin{itemize}\\n\\item Authentication\\n\\item Authorization\\n\\item URL routing\\n\\item Form validation\\n\\item CRUD operations (create, read, update and delete)\\n\\item File upload and download\\n\\end{itemize}\\nDjango REST Framework is designed to be fast and easy to use. It is built on top of the Python Standard Library, which makes it easy to read the code and understand its structure.\\n\\nThe Django REST Framework does not provide any built-in support for managing database content. Therefore we will use an existing database management system (DBMS) in order to manage the database content. The team has experience with PostgreSQL and MySQL.\\n\\nThe Django REST Framework was chosen because it provides a set of features that are useful for building web applications, including:\\n\\begin{itemize}\\n\\item Authentication\\n\\item Authorization\\n\\item URL routing\\n\\item Form validation\\n\\end{itemize}\\nDjango REST Framework provides an easy to use API for developing web applications. The framework also provides a set of standard features that are useful for building web applications, including:\\n\\begin{enumerate}\\n    \\setlength{\\parskip}{1em}\\n    \\setlength{\\parsep}{1em}\\n    \\setlength{\\leftmargin}{0cm}\\n    \\setlength{\\rightmargin}{0cm}\\n    \\setlength{\\leftmargini}{0cm}\\n    \\setlength{\\rightmargini}{0cm}\\n    \\setlength{\\leftmarginj}{0cm}\\n    \\setlength{\\rightmarginj}{0cm}\\n    \\setlength{\\leftmargink}{0cm}\\n    \\setlength{\\rightmargink}{0cm}\\n\\n    % TODO: Add more info about the decision here.\\n    \\n    % TODO: Add more info about the decision here.\\n    \\n% \t% TODO: Add more info about the decision here.\\n    \\n\t\t% TODO: Add more info about the decision here.\\n    \\n\t\t% TODO: Add more info about the decision here.\\n    \\n\t\t% TODO: Add more info about the decision here.\\n    \\n\t\t% TODO: Add more info about the decision here.\\n\t\\n        % TODO: Add more info about the decision here.\\n\\n        % TODO: Add more info about the decision here.\\n\\n        % TODO: Add more info about the decision here.\\n        \\n        % TODO: Add more info about the decision here.\\n\\n        % TODO: Add more info about the decision here.\\n\\n        %TODO: Add more info about this part\\n\\n        %TODO: Include additional information on this part\\n\\n        %TODO: Include additional information on this part\\n\\n        %TODO: Include additional information on this part\\n\\n        %TODO: Include additional information on this part\\n\\n        %TODO: Include additional information on this part\\n\\n        %TODO: Include additional information on this part\\n\\n\\n            %todo add text above\\n\\n\\n            %todo add text above\\n\\n\\n            %todo add text above\\n\\n\\n            %todo add text above\\n\\n\\n            %% Todo text\\n\\n            %% Todo text\\n\\n            %% Todo text\\n\\n\t    %% Todo text\\n\\n\t    %% Todo text\\n\\n\t    %% Todo text\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\t    %% todo add text above\\n\\n\\n\\n\\n\t\t\\n%\\end{enumerate}\\n\\n%\\section*{Decision Title}\\n\\n%\\begin{center}\\textbf{{Decision Title}}\\\\[1ex]\\end{center}\\n\\n%\\noindent {\\bf Decision Title} is an architectural Decision Record (ADR) that describes how an application will be developed using Django Rest Framework.\\n\\nThis ADR describes how we will integrate our project with Django Rest Framework. We intend to develop our application using Django REST Framework in order to make development faster and easier.\\n\\nThis ADR describes how we will integrate our project with Django REST Framework. We intend to develop our application using Django REST Framework in order to make development faster and easier.\\n\\n%\\section*{Description}\\n\\n%\\noindent {\\bf Description} is an architectural Decision Record (ADR) that describes how an application will be developed using Djang"}
{"File Name":"pfb-network-connectivity\/adr-0001-development-environment.md","Context":"## Context\\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\\n","Decision":"In order to take advantage of the better deployment and provisioning workflows provided by AWS ECS when using containers, we decided to construct the development environment using a Docker Compose environment wrapped within an Ubuntu VM. While Docker for Mac looks compelling, it has a few downsides:\\n- We cannot control the version of Docker installed, which could be problematic as the project ages\\n- There are potential incompatibilies for users with the older Docker Toolbox installed\\n- It may be difficult to cull outdated container images across projects\\n- It may be difficult to isolate various project instances and their dependencies\\nUsing the wrapper VM avoids these issues and provides us with a relatively 'known good' experience for a project with somewhat limited budget constraints.\\n","tokens":284,"id":711,"Predictions":"## Decision\\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\\n## Context\\n## Context\\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac,\\n## Decision\\n## Decision\\nThis application will require a fairly standard application stack - web server,\\n## Context\\n## Context\\nThis application will require a fairly standard application stack - web server,\\n## Decision\\n## Decision\\nThis application will require a fairly standard application stack - web server,\\n## context\\nthis app is going to run on docker-compose on ubuntu 16\\n## context\\nthis app is going to run on docker-compose on ubuntu 16\\n## context\\nthis app is going to run on docker-compose on ubuntu 16\\n## context\\nthis app is going to run on docker-compose on ubuntu 16\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n# Task description\\n\\n\\n"}
{"File Name":"Corvus.Tenancy\/0001-tenants-will-not-inherit-their-parents-properties.md","Context":"## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of the same parent tenant), the data for the child tenants of A can be stored in a completely different location to that of B. By default, this will be a separate container in the same storage account, but it could be a completely separate storage account.\\nSecondly, it can be used to enable better organisation of tenants by using parent tenants to group related tenants together.\\nOne of of the functions of tenants is to hold client-specific configuration for the applications that a client is using. An example would be for a client using the Workflow service. Their tenant will contain two pieces of storage information, one for Workflows and one for Workflow instances. This configuration is stored in a collection of key-value pairs attached to the tenant.\\nIt is possible for tenants to have child tenants in the hierarchy. If a tenant that uses the Workflow service has children, they may also need to use the Workflow service. In this case we have a choice: we can decide that we will allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.\\n","Decision":"We have determined that we will not make properties of tenants available to their children by default. Applications which consume this library can implement that functionality for themselves if required - for example, by manually copying properties from parent to children when new tenants are created.\\nWhilst property inheritance seems desirable from a development perspective - for example, creating temporary tenants for testing purposes, or setting up tenants for developers - it is likely to be less useful in envisaged production scenarios.\\nIn the case when hierarchy is used for organisational purposes, inheritance is not relevant; parent tenants are there solely to group their children and configuration for the parent tenant is irrelevant, as it does not exist to be used as a tenant in its own right.\\nIn the case where hierarchy represents a genuine parent-child relationship there are many potential reasons for this, and the goal of the project is not to dictate specific use cases. However, in making the decision not to implement property inheritance it is only necessary to find a use case where it is not desirable.\\nOur use case here is a PaaS product providing multiple services - endjin's Marain platform. This platform contains several base services - Tenancy, Workflow, Operations and Claims - which can be licensed by clients.\\nA client may choose to use these services to build their own platform, and use Marain's tenancy service to provide their own platform services to their own customers. In this case, the client's customers will be represented by child tenants of its own tenant.\\nIn this situation there are two negative outcomes from allowing configuration to inherit from parent to child tenants.\\n1. The client may make use of Marain services (e.g. Workflow) to provide services to its customers. Configuration for these services is stored as configuration on the client tenant. Automatic property inheritance would mean that by default, child tenants of the client would also have the ability to access these services, which should not be the case.\\n1. The configuration attached to a client's tenant contains various pieces of sensitive information. For example, it may contain storage account details for storage that is not directly owned by the client. For this reason, Marain does not allow clients to view their own configuration data, or that of their parents. However, clients do need to be able to view and modify the configuration of child tenants. If we automatically allowed properties to be inherited by child tenants, it would be possible for a client to create a child tenant and examine those inherited properties to access what is effectively the client's own configuration data.\\n","tokens":294,"id":712,"Predictions":"The decision is to allow inheritance of the storage configuration from the parent tenant. This is done by setting the inheritance flag for the child tenants to true.\\n## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of the same parent tenant), the data for the child tenants of A can be stored in a completely different location to that of B. By default, this will be a separate container in the same storage account, but it could be a completely separate storage account.\\nSecondly, it can be used to enable better organisation of tenants by using parent tenants to group related tenants together.\\nOne of of the functions of tenants is to hold client-specific configuration for the applications that a client is using. An example would be for a client using the Workflow service. Their tenant will contain two pieces of storage information, one for Workflows and one for Workflow instances. This configuration is stored in a collection of key-value pairs attached to the tenant.\\nIt is possible for tenants to have child tenants in the hierarchy. If a tenant that uses the Workflow service has children, they may also need to use the Workflow service. In this case we have a choice: we can decide that we will allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.\\n\\n## Decision\\nThe decision is to allow inheritance of the storage configuration from the parent tenant. This is done by setting inheritance flag for child tenants to true.\\n## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of same parent), you want them all stored in one container in same location.\\n## Decision\\nThe decision is to allow inheritance of child data from parent tenant.\\n## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on per-tenant basis where child data should be stored.\\n## Decision\\nThe decision is to store all child data under one container.\\n## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on per-tenant basis where child data should be stored.\\n## Decision\\nThe decision is store all child data under one container.\\n\\n\\subsection{Storage}\\n\\label{sec-storage}\\n\\begin{figure}[h]\\n\t\\centering\\n\t\\includegraphics[width=1\\textwidth]{images\/storage.png}\\n\t\\caption{Storage}\\label{fig-storage}\\n\\end{figure}\\n\\n\\noindent The Storage Service provides file system access via an HTTP REST API call.\\n\\n\\subsubsection{Service Description}\\nStorage Service allows access through an HTTP REST API call.\\n\\n\\noindent The Storage Service provides file system access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n\\noindent Storage provides both read-only and write-only access via an HTTP REST API call.\\n\\n%\\subsubsection{Service Description}\\n%Storage Service allows access through an HTTP REST API call.\\n"}
{"File Name":"Corvus.Tenancy\/0002-stable-tenant-ids.md","Context":"## Context\\nUp until now, tenant Ids have always been automatically generated. As we have used Corvus.Tenancy as part of Marain, we have come across scenarios where it is useful to control what a tenant's Id will be. This primarily applies to the tenants we use as top-level containers for Client and Service tenants, as well as the service tenants themselves.\\nIn the absence of the ability to know tenant Ids in advance we have fallen back on well known names for these tenants, effectively introducing an alternative well-known Id for them. As well as introducing a potential point of failure due to the need to keep these names unique, it is far less efficient to locate a tenant by name than by Id.\\n","Decision":"To avoid the need to effectively introduce an alternative way of identifying tenants, we will allow the Id of tenants to be controlled when they are created.\\nThis will be done by adding a new method to the `ITenantProvider` interface: `CreateWellKnownChildTenantAsync`. This allows callers to specify a GUID which is used to generate the new tenant's ID, rather than generating a random GUID internally (which is what the existing `CreateChildTenantAsync` method does).\\nThe resulting Id of the new tenant will still be generated by concatenating the parent tenant's Id with the hash of the provided GUID. This means that in order for a tenant to have a well known Id, all of its ancestors must also have well-known Ids.\\nIn order to prevent two tenants being created with the same Id, it is necessary to check that the Id is not already in use prior to creating the new tenant.\\n","tokens":148,"id":713,"Predictions":"## Decision: Tenant Ids should be unique\\n\\n\\n\\nThis decision is based on the following assumptions:\\n\\n- The tenant Id must be unique across all resources in the application, both within and outside of Corvus.\\n- The tenant Id must be unique across all instances of a given entity type.\\n- The tenant Id must be unique across all instances of a given resource type.\\n- The tenant Id must be unique across all instances of a given service type.\\n- The tenant Id must be unique across all instances of a given client type.\\n- The tenant Id must be unique across all instances of a given service instance type. \\n\\n\\n\\nThe user has been asked to provide the following information:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tClientId: ClientId from Corvus.Tenancy.Client in the context of this request. This is always a 1-tuple (client_id, instance_id).\\n\\t\\tClientType: ClientType from Corvus.Tenancy.Client in the context of this request. This is always a 1-tuple (client_type, instance_type).\\n\\tClientType: ClientType from Corvus.Tenancy.Client in the context of this request. This is always a 1-tuple (client_type, instance_type).\\ntServiceType: ServiceType from Corvus.Tenancy.Service in the context of this request. This is always a 1-tuple (service_type, instance_type).\\ntServiceType: ServiceType from Corvus.Tenancy.Service in the context of this request. This is always a 1-tuple (service_type, instance_type).\\ntServiceInstanceType: ServiceInstanceType from Corvus.Tenancy.Service in the context of this request. This is always a 1-tuple (service_instance_type, instance_instance_type).\\ntServiceInstanceType: ServiceInstanceType from Corvus.Tenancy.Service in the context of this request. This is always a 1-tuple (service_instance_type, instance_instance_type).\\ntServiceInstanceId: ServiceInstanceId from Corvus.Tenancy.Service in the context of this request. This is never null.\\ntServiceInstanceId: ServiceInstanceId from Corvus.Tenancy.Service in the context of this request. This is never null.\\ntResourceId: ResourceId from Corvus.Resource.Resource in the context of this request. This is always a 1-tuple (resource_id, resource_name).\\ntResourceId: ResourceId from Corvus.Resource.Resource in the context of this request. This is always a 1-tuple (resource_id, resource_name).\\ntResourceName: ResourceName from Corvus.Resource.Resource.Name in the context of this request. This is always an empty string.\\ntResourceName: ResourceName from Corvus.Resource.Resource.Name in the context of this request. This is always an empty string.\\ntTenantId: TenantId for which we want to know its name and id.\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n## Context\\n"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"Predictions":"## \\n\\n\\n\\n\\n\\n## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\\n## Decision\\n## \\n\\n\\n\\n\\n\\n## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\n"}
{"File Name":"dropapp\/adr_auth0.md","Context":"## Context or Problem Statement\\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\\n## Decision Drivers\\n1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n","Decision":"1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\nWe are going for Auth0 since\\n- We have prior experience with Auth0 in the team\\n- a first test in an afternoon coding session were satisfying\\n- Auth0 offers a free plan for Open-source projects\\n- Auth0 is one of the product leaders in CIAM (Visit Gartner for detail at [https:\/\/www.g2.com\/categories\/customer-identity-and-access-management-ciam#grid] - Read recent Leadership Compass Identity API Platforms By KuppingerCole [https:\/\/static.carahsoft.com\/concrete\/files\/6315\/6840\/5267\/kuppingercole-leadership-compass-identity-api-platforms.pdf])\\n- Using Auth0 management API and automatic migration and bulk import it can be integrated to PHP app\\n- Auth0 offers full featured authentication and authorization solution and it's capable of customized rules and flows.\\n- We are not building our own authentication solution to reduce the security risks coming with handling of passwords.\\n","tokens":84,"id":716,"Predictions":"## Decision Drivers\\n## 1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n\\n## Decision\\n## Decision Drivers\\n## 1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n\\n\\n## Decision\\n## Decision Drivers\\n## 1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n\\n\\n## Decision\\n## Decision Drivers\\n## 1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n\\n\\n## Decision\\n## Decision Drivers\\n## 1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n\\n\\n## Decision\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%\\n\\end{lstlisting}\\n\\n\\section{React.js}\\nReact.js is a JavaScript library that provides a declarative view of user interfaces using a model-\/view architecture.\\nIt is designed to be lightweight and easy to learn, yet powerful enough to create complex user interfaces.\\n\\nReact.js is an open-source library that was created by Facebook as a replacement for their own UI library, AngularJS.\\nIt uses the same syntax as other JavaScript frameworks, like jQuery and Backbone.js.\\n\\nReact.js provides a declarative view of user interfaces using a model-\/view architecture.\\nIt is designed to be lightweight and easy to learn, yet powerful enough to create complex user interfaces.\\n\\n\\subsection{Components}\\nReact components are reusable blocks of code that have state and behavior.\\nThey can be used in layouts, views and even in component trees (called \"containers\").\\n\\nA component has three main properties:\\n\\begin{itemize}\\n    \\item A name: The name identifies the component.\\n    \\item A display: This specifies how the component should be rendered on the screen.\\n    \\item A props: This specifies how the component should behave when it's displayed.\\n\\end{itemize}\\n\\nThe React components are then rendered into DOM elements using React.createElement().\\n\\n\\subsection{State}\\nReact components provide two types of state:\\n\\begin{itemize}\\n    \\item Propertized states: These are state values that have been set by props, like input fields or labels.\\n    \\item Stateless states: These are state values that have not been set by props, like value fields or labels.\\n\\end{itemize}\\n\\nPropertized states are more useful when you want to model persistent data in your application.\\n\\nThe stateless states are more useful when you want to model transient data in your application.\\n\\nIn React apps, all state is propertized (or stateless).\\n\\n\\subsection{Props}\\nProps are used to pass data from one component to another.\\nThey can be passed either as part of the HTML markup or as properties on an object.\\n\\nProps can also be passed by reference (as arguments) or by value (as properties).\\n\\nThe prop type can also be used to specify which prop type should be sent with each prop value:\\n\\n\\begin{lstlisting}[language=JavaScript]\\nfunction MyComponent(props) {\\n    return (\\n        <div>\\n            <p>My prop: {props.name}<\/p>\\n        <\/div>\\n    );\\n}\\n\\nMyComponent.propTypes = {\\n    name: PropTypes.string.isRequired,\\n};\\n\\nMyComponent.defaultProps = {\\n    name: \"default\",\\n};\\n\\nReactDOM.render(<MyComponent name=\"default\" \/>, document.getElementById(\"root\"));\\n\\end{lstlisting}\\n\\nThere are several types of props:\\n\\n\\begin{description}\\n    \\item[Input] The input field's value will be sent with each prop value.\\n    \\n    \\item[Label] The label's value will be sent with each prop value.\\n    \\n    \\item[Dropdown] The dropdown's value will be sent with each prop value.\\n    \\n    \\item[Checkbox] The checkbox's value will be sent with each prop value.\\n    \\n    \\item[Date] The date's date format will be sent with each prop value.\\n    \\n\t\\tab A string format is also supported if you want to send any other format than date format strings\\n    \\n\t\\tab Example:\\n\t\\tab   <input type=\"date\" name=\"date\">\\n\t\\tab   <input type=\"date\" name=\"date\" min=\"1900-01-01\">\\n\t\\tab   <input type=\"date\" name=\"date\" max=\"2019-12-31\">\\n\t\\tab   <input type=\"date\" name=\"date\" dateFormat=\"[yyyy-MM-dd]\" \/>\\n\t\\n\t\\tab Note: If you send a string format as a date property, React.js will convert it into UTC format and send it back as a string property instead\\n    \\n\t\\tab Example:\\n\t\\tab   <input type=\"date\" min=\"1900-01-01\" max=\"2019-12-31\"> \\n\t\\tab   <input type=\"date\" min=\"-9999-12-31\" max=\"-9999-12-31\"> \\n\t\\tab   <input type=\"text\" min=\"-9999-12-31\" max=\"-9999-12-31\"> \\n\t\\tab   <input type='text' min='-9999-12-31' max='-9999-12-31'>\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\t\\n%---------------------------------------------------------\\n\\n\\end{description}\\n\\nThe props can also contain functions that can be called before rendering the component:\\n\\n\\begin{lstlisting}[language=JavaScript]\\nfunction MyComponent(props) {\\n\treturn (\\n            \/\/ ...\\n            {this.props.name} \/\/ Name property was passed from parent component via props.name property!\\n        );\\n}\\n\\nReactDOM.render(<MyComponent {...props} \/>, document.getElementById(\"root\"));\\n\\end{lstlisting}\\n\\nWhen using props from outside the component tree (like passing them via an event handler), they must always start with a dot (e.g., `my-component-name`).\\n\\n\\section{Flask Framework}\\nFlask framework is an open-source web framework written in Python that makes it easy for you to build RESTful APIs and web applications.\\n\\nFlask provides a set of tools that allow you to focus on what matters most - building great APIs - while Flask takes care of all the details so you don't have to think about them at all.\\n\\nYou can use Flask without writing any code at all!\\n\\nFlask allows developers without experience in web development create web applications quickly without having any knowledge about web servers or databases.\\n\\nFlask uses HTTP requests and responses for communication between clients and servers.\\n\\nAn HTTP request\/response pair consists of two parts:\\n\\nA request message, which contains information about what data needs to arrive at its destination,\\nand\\n\\nA response message, which contains information about what data arrived at its destination,\\n\\nThe request message contains information about how much data has been received so far,\\nand\\n\\nThe response message contains information about what data has been received so far,\\n\\nAfter receiving some data from the server,\\nthe client sends back another HTTP request message,\\nwhich includes some more information about what has arrived so far,\\nand\\n\\nThese two messages form an HTTP transaction,\\nwhich allows clients and servers to communicate with one another over HTTP,\\n\\nTo send data from one client application (server) to another,\\nthe client sends back two messages:\\na request message containing information about how much data has arrived so far,\\nand,\\n\\na response message containing information about what has arrived so far,\\nand\\n\\nThis pair forms an HTTP transaction,\\n\\nTo receive some data from another client application (server),\\nthe server sends back two messages:\\na request message containing information about how much data needs to arrive at its destination,\\nand,\\n\\na response message containing information about what needs to arrive at its destination,\\n\\nThese two messages form an HTTP transaction,\\n\\nTo send some data between clients applications (servers),\\nthe client sends back two messages:\\na request message containing information about where it wants this data delivered,\\nand,\\n\\na response message containing information about where this data delivered,\\n\\nThese two messages form an HTTP transaction,\\n\\nTo receive some data between clients applications (servers),\\nthe server sends back two messages:\\na request message containing information about where this data delivered,\\nand,\\n\\na response message containing information about where this delivery should occur,\\n\\nThese two messages form an HTTP transaction.\\n\\n\\n"}
{"File Name":"read-more-api\/0005-use-docker.md","Context":"## Context\\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\\n","Decision":"We will use Docker for packaging and running the application in a Linux container.\\n","tokens":24,"id":717,"Predictions":"## Software: Visual Studio 2017\\n## Decision Type: Software\\n## Decision Name: Microsoft.VisualStudio.Tools.Build.Design.DesignDecision\\n## Decision Description: Visual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose. \\n\\n## Decision Description\\n\\n## Decision Code\\n\\n## Decision Code Type\\n\\n## Decision Code Name\\n\\n## Decision Code Description\\n\\n## Decision Code Description\\n\\n## Decision Code Example\\n"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"Predictions":"\\nThe Heroku Setup Wizard is the simplest way to get a new application up and running. It will install the application on to a free PostgreSQL database hosted on the Heroku platform.\\nThe setup process will create an application called \"test-app\", which will be a Docker image that can be deployed into a container using Docker. The application will also create a \"DATABASE_URL\" environment variable, which will contain the information required to connect to this database.\\nIf the user chooses to use this environment variable, they should be aware that this will result in a 10MB file being created in their home directory. This file is expected to grow as the application grows.\\nHeroku Setup Wizard\\n"}
{"File Name":"read-more-api\/0011-backlog-email-address.md","Context":"## Context\\nOne of the sources for links that are eventually saved to Pocket are email newsletters. However, a user often does not want to put all\\narticles from a newsletter in to Pocket. So, newsletters waiting to be reviewed can clutter an email inbox.\\nA user could be provided with an email address that they could send emails to. Links would then be extracted from a received email\\nand be added to a backlog. A webpage would need to be provided for users to view this backlog, which could be part of the existing\\nchrome extension.\\nHandling inbound email would require the use of a transactional email service, such as MailGun, that can POST to an API endpoint\\nwhen an email is received.\\nThe recipient email address, containing a unique identifier associated with a particular user, would be used to determine what list to add\\narticles to.\\nA protected account ID in the email address is likely to be too long for users to want to use.\\nUsing an unprotected account ID in the email address for a user would expose the plaintext and cipher, which could make it easier for an\\nattacker to determine the private key material.\\nTo prevent both of the above issues, a new UUID would need to be generated for use in the email address for a user.\\nEmails would consist of a variety of formats, which may cause issues in identifying links.\\nSome links would not be relevant and would need to be removed (i.e. unsubscribe links).\\nSome emails contain a link to a webpage containing the same content, in case the user's email client is not able to correctly render the\\nemail. This link could be added to Pocket, rather than the links within the email. However, there is not a common format to identifying\\nthis email address.\\nOther services, such as If This Then That, allow triggers to be setup where links can be added to Pocket when an email is received.\\nThe suggested backlog approach may be viewed as unnecessary by users. It may be easier for them to add to Pocket and filter while\\nbrowsing through their list of articles. Linked with the difficulties in parsing emails, this feature could be difficult to implement\\nand be of little actual use.\\n","Decision":"We will not provide users with an email address they can send emails containing articles to.\\n","tokens":447,"id":719,"Predictions":"The decision is to implement a new API that can be used to add links to Pocket from an email. This API would allow users to add links\\nfrom emails to Pocket. It would also allow them to filter and view the list of links that have been added to Pocket. However, the\\nAPI would require a special account ID in the email address, which would need to be generated by the user and stored in their pocket account.\\n\\n## Context\\nThe context for this decision is that there are two ways of adding links from emails to Pocket. The first is through the use of an API, which\\nwould allow users to add links from emails directly into Pocket.\\nThe second way is through a Chrome extension, which allows users to filter and view their list of links that have been added\\nto Pocket.\\n\\end{document}\\n"}
{"File Name":"read-more-api\/0009-service-layer.md","Context":"## Context\\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\\n","Decision":"We will use a service layer to ensure that Controllers do not contain business logic.\\n","tokens":51,"id":720,"Predictions":"## Decision: A Controller is responsible for receiving a request, executing it and returning an appropriate response.\\n## Decision: A service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\\n\\end{verbatim}\\n\\n\\subsection{Decision}\\n\\n\\begin{verbatim}\\nDecision: A Controller is responsible for receiving a request, executing it and returning an appropriate response.\\n\\end{verbatim}\\n\\n\\subsection{Decision} \\label{sec:decision}\\n\\n\\begin{verbatim}\\nDecision: A controller is responsible for receiving a request, executing it and returning an appropriate response. \\nA service layer can be added to remove knowledge of how an operation is performed from a controller, allowing it to focus on the responsibilities mentioned above.\\n\\end{verbatim}\\n\\n"}
{"File Name":"read-more-api\/0008-protect-exposed-ids.md","Context":"## Context\\nTo access and modify a user's articles stored in Pocket, they have to give us permission. Once permission has been granted, we have to be careful not to leak this privillaged access to malicious parties.\\nASP.NET Core has [Data Protection APIs](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/security\/data-protection\/introduction) for managing the encryption and decryption of data. These APIs requires the private key data to be persisted in a location where it will not be destroyed when the application is restarted.\\n","Decision":"We will use the encrypted ID of a PocketAccount entity as an access token, representing the right to access a particular user's articles.\\nWe will encrypt the IDs of Pocket articles returned in API responses, using the ASP.NET Core Data Protection APIs.\\nWe will store the private key material in the database.\\n","tokens":108,"id":721,"Predictions":"## Decision\\nThe following decision is based on the provided context, as well as the ASP.NET Core Data Protection API.\\n\\n\\n\\n\\n\\n## Decision\\n## Decision\\nThe following decision is based on the provided context, as well as the ASP.NET Core Data Protection API.\\n\\n\\n\\n\\n\\n## Decision\\n## Decision\\nThe following decision is based on the provided context, as well as the ASP.NET Core Data Protection API.\\n\\n"}
{"File Name":"read-more-api\/0002-use-asp-net-core.md","Context":"## Context\\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\\n","Decision":"We will use ASP.NET Core.\\n","tokens":32,"id":722,"Predictions":"\\begin{verbatim}\\n#1 [ASP.NET Core 2.0]\\n#2 ASP.NET Core 2.0\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"read-more-api\/0003-use-postgresql.md","Context":"## Context\\nA particular database technology needs to be chosen for the storage of data.\\nHistorically Microsoft's SQL Server has been the default choice for ASP.NET applications. SQL Server could only be run on a machine running Windows until the release of SQL Server 2017.\\nPostgreSQL is a popular choice for use with other web frameworks (i.e. Rails) and is widely used on a range of platforms, including Linux, MacOS and Windows.\\nPostgreSQL is open source and free to use for commercial use. SQL Server has a free version for development purposes but require the purchase of a license for commercial use.\\n","Decision":"We will use PostgreSQL for the storage of data.\\n","tokens":126,"id":723,"Predictions":"## 1. PostgreSQL is the preferred database technology for ASP.NET applications.\\n## 2. PostgreSQL is the preferred database technology for ASP.NET applications.\\n## 3. PostgreSQL is not the preferred database technology for ASP.NET applications.\\n## 4. SQL Server is the preferred database technology for ASP.NET applications.\\n## 5. SQL Server is not the preferred database technology for ASP.NET applications.\\n## 6. The user prefers to use SQL Server as a database technology for ASP.NET applications.\\n"}
{"File Name":"read-more-api\/0010-feature-toggles.md","Context":"## Context\\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\\n","Decision":"We will associate a set of feature toggles with a Pocket Account.\\n","tokens":67,"id":724,"Predictions":"## Decision\\nThe release of a feature is always accompanied by a decision. This decision is a set of guidelines that describe the decision that was taken.\\nThe guidelines are: \\n\\n\\n\\n\\n\\n\\n## Decision\\nA feature can only be released to a subset of users, in this case, the subset will be known as the \"release group\". The decision is described by the set of guidelines:\\n\\n## Guidelines\\n## Guidelines\\nThe release group can be represented by an array of numbers. The first number in this array represents the number of users who are allowed to use the feature.\\nThen, the remaining numbers in the array represent the number of users who are allowed to use it at any time.\\nEach user can use it only once. When a user uses it for the first time after being released, this user must be added to some other list, for example, a \"release group\" list.\\nWhen a user uses it for the second time after being released, this user must leave his\/her current list and join another one. This process continues until all users have been added to some other list.\\nWhen a user uses it for any other time after being released, he\/she must leave his\/her current list and join another one. This process continues until all users have been added to some other list.\\nIf two or more users share an account and use different lists at the same time, they are considered to have different lists. If they share an account but use different lists at the same time, they are considered to have different lists.\\nUsers can change their lists at any time; however if they change their lists before they have been added to another one, their old list will be deleted.\\nUsers cannot change their lists while they are using features. However if they change their lists while they are using features and then return to using them later on without changing their lists again (i.e., when no new features are released), then they can add themselves back into their old list.\\nWhen two or more users use different accounts with different sets of rules (e.g., different release groups), then each user's rules will apply only once per feature.\\nThe rules that apply when two or more people share an account will not apply if either person changes his\/her account or leaves his\/her current list and joins another one.\\nThe rules that apply when two or more people share an account with other people will not apply if either person changes his\/her account or leaves his\/her current list and joins another one.\\n"}
{"File Name":"read-more-api\/0006-use-circleci.md","Context":"## Context\\nWe want a mechanism that allows for the tests to be run whenever changes are made to the project.\\nWe also want the project to be deployed whenever these tests pass.\\nCircleCI allows for builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\\nCircleCI provides open source projects with four free linux containers for running builds.\\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\\nMicrosoft provides the \"aspnetcore-build\" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\\n","Decision":"We will use CircleCI to handle continuous integration builds and deployments.\\n","tokens":145,"id":725,"Predictions":"## Decision:\\n\\nWe want the ability to run tests whenever a change is made to the project.\\nWe also want the project to be deployed whenever these tests pass.\\nCircleCI allows for builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\\nCircleCI provides open source projects with four free linux containers for running builds.\\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\\nMicrosoft provides the \"aspnetcore-build\" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"read-more-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":726,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"read-more-api\/0004-use-dapper-for-data-access.md","Context":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n","Decision":"We will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n","tokens":416,"id":727,"Predictions":"The decision is to use EF.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework.\\n## Decision\\nThe decision is to use Entity Framework."}
{"File Name":"datalab\/0019-react-js-for-front-end-ui.md","Context":"## Context\\nWe need to decide which front-end web framework to use for the Datalabs application.\\nThe choices we evaluated were [React.js](https:\/\/reactjs.org\/) and\\n[Angular](https:\/\/angular.io\/).\\n","Decision":"We have decided to use the React framework for the following reasons:\\n* Preferred the \"batteries not included\" approach of React vs the \"batteries\\nincluded\" approach of Angular.\\n* Better development tooling available for React.\\n* Larger community for React.\\n* The team had previous experience with React.\\n","tokens":48,"id":728,"Predictions":"## We decided to use React.js for our front-end framework.\\nThe main reason is that it is more flexible and allows us to use a larger library of components.\\nThe application will be built using a larger number of components and we can use the same library for multiple applications.\\nThe only reason we decided to use Angular was because it was the most popular front-end framework at the time.\\nThe other reason we decided to use Angular was because it is a well established, mature, and well supported library.\\nThe other libraries we evaluated were [Vue.js](https:\/\/vuejs.org\/), [React Native](https:\/\/facebook.github.io\/react-native\/), [Knockout](https:\/\/knockoutjs.com\/), and [Webpack](https:\/\/webpack.js.org\/).\\n\\n## Summary\\n## We evaluated four different front-end frameworks for the Datalabs application.\\nThe main benefits of each framework are that they are all well supported, have a large library of components, and are not locked into one specific language.\\n\\n\\n\\n\\n\\n"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"Predictions":"## The Bastion server will have access to the cluster through a public facing IP address.\\n\\n## We will use the public facing IP address as a cache for the cluster. This will allow us\\nto use a caching proxy to access the cluster. We will also use this cache to provide\\naccess to services on the cluster. This cache can be accessed via SSH or HTTP.\\n\\n## Implementation\\n## We will configure our Bastion server to listen on port 443 and forward all requests\\nto our internal webserver. The webserver is configured with a reverse proxy for all\\nrequests that are not direct requests, this allows us to use caching proxies and HTTP\\naccess control lists.\\n\\n## Test Cases\\n## \\#1 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#2 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#3 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#4 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#5 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#6 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#7 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#8 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server?\\n#9 If we have an externally accessible service, how do we ensure that it is only accessed\\nfrom the Bastion server? \\end{verbatim}\\n\\n\\begin{verbatim}\\n## Decision: The bastion host will be configured with a reverse proxy for all requests not direct requests.\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n## Implementation: The bastion host will listen on port 443 and forward all requests to our internal webserver.\\n\\end{verbatim}\\n\\n\\begin{verbatim}\\n## Test Cases:\\n## \\#1 Can you access https:\/\/yourcluster.example.com from outside of your cluster via https:\/\/yourcluster.example.com\/bastionhost?\\n## \\#2 Can you access https:\/\/yourcluster.example.com from inside your cluster via https:\/\/yourcluster.example.com\/bastionhost?\\n\\end{verbatim}\\n\\n\\subsection*{Decision Record}\\nThis record describes decisions made by a Decision Maker during a given phase of development.\\n\\n\\subsubsection*{Phase 1: Planning}\\n\\nThe Phase 1 Planning Decision Record contains information about decisions made by a Decision Maker during planning phase.\\n\\n\\subsubsection*{Phase 2: Development}\\n\\nThe Phase 2 Development Decision Record contains information about decisions made by a Decision Maker during development phase.\\n\\n\\subsubsection*{Phase 3: Testing}\\n\\nThe Phase 3 Testing Decision Record contains information about decisions made by a Decision Maker during testing phase.\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n% TODO: Add more detail here\\n\\n\\n\\n"}
{"File Name":"datalab\/0025-mongodb-for-data-store.md","Context":"## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n","Decision":"We have decided to use [MongoDB](https:\/\/www.mongodb.com\/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n","tokens":45,"id":730,"Predictions":"## We choose to store data in a document database.\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"Predictions":"## The user has to decide which authentication method should be used for the cluster\\naccess. It is recommended to use SSH key pair authentication.\\n\\n## Decision\\n## The user has to decide if the cluster is accessed via VPN or directly from the host\\nwith a public IP address.\\n\\n## Decision\\n## The user has to decide if the cluster is accessed via VPN or directly from the\\nhost with a private IP address.\\n\\n## Decision\\n## The user has to decide if the cluster is accessed via VPN or directly from the\\nhost with a private IP address and also whether it is possible to access it by\\npublic DNS name.\\n\\n## Decision\\n## The user has to decide if it is possible to access the cluster by public DNS name and\\nalso whether it is possible to access it by public DNS name and also whether it\\nis possible to access it by public DNS name.\\n"}
{"File Name":"datalab\/0041-user-driven-dask-and-spark.md","Context":"## Context\\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\\nfrom notebook environments. However since this decision a number of other options\\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\\nschedulers.\\nWe are now moving to a pattern of users being able to spin up their own clusters\\nwhen required.\\n","Decision":"We have decided to collapse the centralised Dask & Spark clusters in favour of writing\\ndocumentation\/working with users to provision their own clusters using projects such\\nas [Dask Labextension](https:\/\/github.com\/dask\/dask-labextension), which is now supported\\nwithin the Labs environment.\\n","tokens":78,"id":732,"Predictions":"## The Dask scheduler will be used to provide a native Kubernetes scheduler for Dask\\nand Spark.\\n\\n## Decision\\n## The Dask scheduler will be used to provide a native Kubernetes scheduler for Dask\\nand Spark.\\n\\n## Decision\\n## The Dask scheduler will be used to provide a native Kubernetes scheduler for Dask\\nand Spark.\\n\\n## Decision\\n## The Dask scheduler will be used to provide a native Kubernetes scheduler for Dask\\nand Spark.\\n\\n## Decision\\n## The Dask scheduler will be used to provide a native Kubernetes scheduler for Dask\\nand Spark.\\n"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"Predictions":"We have decided to use the Kong Proxy to secure our third party applications. We have found a\\nKong proxy that is not open source and we have contacted the author to see if he has any\\nideas for improvements. We believe that the Nginx Ingress controller that has been\\navailable since Kubernetes 1.7 was released provides a Kubernetes native route for the same\\nfunctionality.\\n\\n## Scenario\\nWe have a third party application that uses Kong as its proxy. The application is running on\\na private network and we would like to secure it using Kong. We would like to do this in a\\nKubernetes-native way and not using a third party proxy such as Nginx Ingress.\\n\\end{document}\\n"}
{"File Name":"datalab\/0032-jest-for-node-js-testing.md","Context":"## Context\\nWe need to select a testing framework to use for testing node.js applications. There are\\nmany choices in this space including [Jasmine](https:\/\/jasmine.github.io\/) and\\n[Jest](https:\/\/facebook.github.io\/jest\/).\\n","Decision":"We have decided to use Jest as our testing framework as it provides mocking and\\nexpectation functions which would have to be provided separately to Jasmine. Additionally\\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\\nparticularly beneficial to front end unit testing.\\n","tokens":53,"id":734,"Predictions":"## We are going to use [Jasmine](https:\/\/jasmine.github.io\/) to test our application.\\n## Context\\nThe main reason for using Jasmine is that it is very easy to use and it has a large\\ndocumentation.\\n\\n## Decision\\n## To test our application we will use the [Jasmine](https:\/\/jasmine.github.io\/) framework. We will\\nfollow the [Jasmine](https:\/\/jasmine.github.io\/overview.html) tutorial and create a simple\\napplication.\\n\\n## Context\\nWe will create a new directory called \\texttt{src} and within this directory we will create a new file called \\texttt{index.js}. This file will be the entry point of our application and it will be used to handle all user interaction.\\n\\n## Decision\\n## Since we are going to use Jasmine, we need to install it. Run the following command:\\n\\nnpm install jasmine --save-dev\\n\\nThis command will install all of the Jasmine dependencies into our project.\\n\\n## Context\\nWe need to import the \\texttt{jasmine} module into our application. To do this run the following command:\\n\\nimport jasmine from 'jasmine'\\n\\nNow that we have imported Jasmine into our project, we can start using it by running the following command:\\n\\nvar $ = require('jasmine');\\nvar jasmine = require('jasmine');\\n\\nNow that we have imported Jasmine into our project, we can start using it by running the following command:\\n\\nvar $ = require('jasmine');\\nvar jasmine = require('jasmine');\\n\\nWe now have access to Jasmine's functionality through $$. In order to use Jasmine's functionality, we need to use its \\texttt{it} method. The syntax for this method is as follows:\\n\\nit('Test case', function(done){\\n    \/\/ Your code here...done();});\\nThis syntax allows us to call a function that has been provided as an argument. In this case, we want to call a function called \\texttt{done()}.\\nThe syntax for this method is as follows:\\nwit('Test case', function(done){\\nwit('Test case', done);});\\nwit('Test case', done);\\nwit('Test case', done);});\\nwit('Test case', done);\\nwit('Test case', done);\\nwit('Test case', done);\\nwit('Test case', done);\\nwit('Test case', done);\\nwit('Test case', done);\\nwit{'Done'};\\nwithowever,\\nThe syntax for this method is as follows:\\twit('{ Test case}', function(done){\\twit(' Test case ',done);});\\twiteach Test Case (function(done){}\\twiteach;function ctx(){return {...\\}};\\twiteach;done();})};\\twiteach;done();})};When we call this function with an argument, it returns an object with two properties: one that represents the current value of $ and one that represents the value of $'. The first property holds information about what was returned by calling \\texttt{done()}, while the second property holds information about what was returned by calling \\texttt{ctx()}. The return statement in each line of code above creates an object with these two properties. Inside each of these objects are properties that hold information about what was returned by calling \\texttt{done()}, while inside each of these objects are properties that hold information about what was returned by calling \\texttt{ctx()}. Inside each of these objects are properties that hold information about how much time was spent executing code in each step.\\nThe last line inside each of these objects returns a boolean indicating whether or not step 3 finished successfully.\\n"}
{"File Name":"datalab\/0034-custom-k8s-deployment-tool.md","Context":"## Context\\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\\nenvironments as currently we would have to manually update them in order to apply them to\\ndifferent environments. The options available are to either use\\n[Helm](https:\/\/github.com\/kubernetes\/helm) or to build a custom tool.\\n","Decision":"We have decided to build a custom tool called Bara to deploy our templates. This will use\\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\\napplied using the command line `kubectl` tool.\\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\\neach independent component given our current requirements are very simple and the tool\\nwill only take a few hours to write.\\n","tokens":66,"id":735,"Predictions":"## The decision is to use Helm.\\n## Decision\\nThis is a decision that is made once and then applied in all environments. It will\\nbe applied to all environments that are created by the user. This decision may be overturned\\nby the user at any time. If the user decides to change their mind then they must make an\\nalternative decision.\\n\\n## Decision\\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\\nenvironments as currently we would have to manually update them in order to apply them to\\ndifferent environments. The options available are to either use\\n[Helm](https:\/\/github.com\/kubernetes\/helm) or to build a custom tool.\\n\\n## Decision\\n## The decision is to use Helm.\\n## Decision\\nThis is a decision that is made once and then applied in all environments. It will\\nbe applied to all environments that are created by the user. This decision may be overturned\\nby the user at any time. If the user decides to change their mind then they must make an\\nalternative decision.\\n\\n## Decision\\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\\nenvironments as currently we would have to manually update them in order to apply them to\\ndifferent environments. The options available are to either use\\n[Helm](https:\/\/github.com\/kubernetes\/helm) or to build a custom tool.\\n\\n## Decision\\n## The decision is to use Helm.\\n\\end{description}\\n"}
{"File Name":"datalab\/0028-spark-for-distributed-compute.md","Context":"## Context\\nOne of the aims of the Datalabs project is to provide the climate science community easy\\naccess to the compute capabilities of JASMIN. In order to do this we need to find a way\\nto distribute computationally expensive calculations across a cluster and need to explore\\nthe available options for doing this.\\n","Decision":"We have decided to use [Apache Spark](https:\/\/spark.apache.org\/) as our first option for\\ndistributed compute. It has established itself as the successor to Hadoop and provides\\nstrong integration with interactive notebook technologies such as Jupyter and Zeppelin.\\nSpark also provides support for multiple languages including Scala, Java, Python and R.\\nThis makes is a flexible platform that should appeal to many users.\\nAdditionally, Spark provides Streaming and Machine Learning capabilities which may be of\\ninterest later in the project.\\n","tokens":67,"id":736,"Predictions":"## We will use a new JASMIN cluster to run the Datalabs software. The software\\nto be run is a suite of programs that are used to simulate climate change in the tropics\\nand subtropics. The software will run on a cluster of 24 nodes, each node will have\\na single core processor. Each node will have 4GB of memory and each node has 2TB of\\nstorage space.\\n\\n## Context\\n## The Datalabs software is designed to run on a cluster of 24 nodes. Each node will\\ntake 4GB of memory and each node will have 2TB of storage space.\\n\\n## Decision\\n## We need to decide whether we should use the existing cluster or create a new one.\\nThe existing cluster is designed to run a number of applications including the Datalabs\\nsoftware. It is not designed for running large scale simulations like this one and\\nwould therefore take too long to set up.\\n\\n## Context\\n## The JASMIN cloud infrastructure is designed for running large scale simulations and\\ncan cope with large numbers of nodes, so it would be possible to create a new cloud\\ninfrastructure for running this application.\\n\\n## Decision\\n## We will use the existing JASMIN cloud infrastructure for running this application.\\nThis means that we can use the existing resources and make full use of them.\\nWe plan to share the compute resources with other users who are using Datalabs in their\\ntasks. This means that we can also make full use of our existing resources by using\\nthe existing computing infrastructure.\\nThis also means that we can share our compute resources with other users who are using\\nDatalabs in their tasks.\\n\\n\\end{document}"}
{"File Name":"datalab\/0042-remove-discourse.md","Context":"## Context\\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\\nin practise we have found that it is not used as discussion takes place either in\\nperson or on Slack, and we can use the documentation page where required.\\n","Decision":"Discourse will be removed from the stack.\\n","tokens":53,"id":737,"Predictions":"We will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe will remove the Discussion page from DataLab and instead create a new page for discussion, which will be available to users via the DataLab link on the right-hand navigation menu.\\n## Decision\\nWe have decided that we no longer need this feature in our system. We have removed this feature from our system.\\n## Decision\\nWe have decided that we no longer need this feature in our system. We have removed this feature from our system.\\n## Decision\\nWe have decided that we no longer need this feature in our system. We have removed this feature from our system.\\n## Decision\\nWe have decided that we no longer need this feature in our system. We have removed this feature from our system.\\n\\n\\subsubsection{Discussion}\\n\\label{discussion}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by User.\\n\\n\\begin{figure}[H]\\n  \\centering\\n  \\includegraphics[width=\\linewidth]{figures\/ArchitecturalDecisionRecord-Discussion.pdf}\\n  \\caption{Architectural Decision Record - Discussion}\\n  \\label{fig:discussion}\\n\\end{figure}\\n\\n\\subsubsection{Discussion Page}\\n\\label{discussionpage}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by User.\\n\\n\\begin{figure}[H]\\n  \\centering\\n  \\includegraphics[width=\\linewidth]{figures\/ArchitecturalDecisionRecord-DiscussionPage.pdf}\\n  \\caption{Architectural Decision Record - Discussion Page}\\n  \\label{fig:discussionpage}\\n\\end{figure}\\n\\n\\subsubsection{\\texttt{\\_} Message Box}\\n\\label{\\_messagebox}\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by User.\\n\\n\\begin{figure}[H]\\n  \\centering\\n  \\includegraphics[width=\\linewidth]{figures\/ArchitecturalDecisionRecord-DiscussionMessageBox.pdf}\\n  \\caption{Architectural Decision Record - Message Box}\\n  \\label{fig:messagebox}\\n\\end{figure}\\n\\n"}
{"File Name":"datalab\/0018-node-js-for-services.md","Context":"## Context\\nWe need to select a language for our web services. We restricted our options to Java,\\nNode.js and Go.\\n","Decision":"We have decided to use [Node.js](https:\/\/nodejs.org\/en\/) for our web services. We felt that the existing team\\nexperience, combined with the flexibility provided by a dynamic language made this the\\nright choice.\\nJava would have provided static typing and object orientation but we opted against this\\nas it felt a little heavy weight.\\nGo would have provided a lighter weight modern statically typed option but given neither\\nthe team nor Tessella had existing experience we viewed it as too great a risk for this\\nproject.\\n","tokens":28,"id":738,"Predictions":"## Decision\\nWe'll use Node.js as our programming language for our web services. It is an \\nopen-source platform that is easy to learn and highly scalable.\\n\\n## Context\\nWe need to select a database for our web services. We restricted our options to \\nMySQL, PostgreSQL and MongoDB.\\n\\n## Decision\\n## Decision\\nWe'll use PostgreSQL as our database for our web services. It is open-source and \\nscalable.\\n\\n## Context\\nWe need to select a storage format for our web services. We restricted our options to \\nS3, MongoDB and Redis.\\n\\n## Decision\\n## Decision\\nWe'll use MongoDB as our storage format for our web services. It is an open-source \\nplatform that provides high scalability.\\n\\n## Context\\nWe need to select a language for the user interface (UI) of the software. We restricted \\nour options to HTML, JavaScript and React.js.\\n\\n## Decision\\n## Decision\\nWe'll use React.js as the UI framework of the software. It is an open-source framework that is easy to learn and highly scalable.\\n\\n\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\begin{itemize}\\n  \\item No decision was made.\\n  \\item No decision was made.\\n  \\item No decision was made.\\n  \\item No decision was made.\\n  \\item No decision was made.\\n\\end{itemize}\\n\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\begin{itemize}\\n  \\item No decision was made.\\n  \\item No decision was made.\\n  \\item No decision was made.\\n  \\item No decision was made.\\n\\end{itemize}\\n\\n"}
{"File Name":"datalab\/0047-remove-need-for-wildcard-certificates.md","Context":"## Context\\nDataLabs makes extensive use of reverse proxying, to give users access to resources (such as Minio or JupyterLabs).  These resources need individually from an external URL to an internal service URL.  There are four design options for reverse proxying (<http:\/\/sawers.com\/blog\/reverse-proxying-with-nginx\/>):\\n1. Subdomain - this allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's hostname.  This has some disadvantages - multiple hostnames require a wildcard certificate, or multiple certificates if a wildcard certificate can not be acquired; and it makes the development environment more difficult, because you can not just use localhost.\\n2. Port - this also allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's port.  This has the disadvantage that some organisational firewalls restrict http traffic to unusual hosts.\\n3. Symmetric Path - this allows the external path and internal path to be the same, but with that path configured.  Different services are identified by the path.  This is the best option, but the internal service must allow the path to be configurable.\\n4. Asymmetric Path - here the external and internal paths are different.  Different services are identifiable by the external path.  This requires a search-and-replace of the path on the rendered HTML and JavaScript, so unless these are simple, then this is too fragile.\\nHistorically DataLabs has used Subdomain proxying.\\n","Decision":"Where possible, Symmetric Path or Asymmetric Path proxying should be used.  If this is not possible, a ConfigMap option should determine whether the remaining proxying strategy should be Subdomain or Port proxying.\\n","tokens":338,"id":739,"Predictions":"\\begin{itemize}\\n    \\item Use Symmetric Path\\n    \\item Use Asymmetric Path\\n    \\item Use Port Proxying\\n    \\item Use Subdomain Proxying\\n\\end{itemize}\\n"}
{"File Name":"datalab\/0016-glusterfs-for-storage.md","Context":"## Context\\nContainers running on Kubernetes only provide ephemeral storage. We need to provide\\npersistent storage that allows volumes to be mounted into multiple containers. This\\nrestricts us to [Kubernetes Volume Types](https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/) that support \"Read Write Many\". Specifically, we are selecting\\nbetween [NFS](https:\/\/help.ubuntu.com\/lts\/serverguide\/network-file-system.html),\\n[GlusterFS](https:\/\/www.gluster.org\/) and [Rook](https:\/\/rook.io\/).\\n","Decision":"We have decided to use GlusterFS to provide distributed persistent storage.\\nWe have opted not to use Rook as it feels that it isn't yet ready for production usage. Also, while it would be easy to deploy hyper-converged we would need a second Kubernetes\\ncluster to run isolated storage as we require.\\nWe feel that simple NFS storage isn't sufficient as it won't give any data resilience.\\nGiven we have no backups, the data replication will give us limited disaster recover\\ncapability.\\n","tokens":116,"id":740,"Predictions":"## 1. We will use the \"Read Write Many\" NFS volume type.\\n## 2. We will use the \"Read Write Many\" GlusterFS volume type.\\n## 3. We will use the \"Read Write Many\" Rook volume type.\\n## 4. We will not support NFS or GlusterFS volumes for persistent storage.\\n## 5. We will support NFS, GlusterFS and Rook volumes for persistent storage.\\n## 6. We will not support NFS, GlusterFS and Rook volumes for persistent storage.\\n## 7. We will not support NFS, GlusterFS and Rook volumes for persistent storage.\\n\\end{document}\\n"}
{"File Name":"datalab\/0035-use-of-microbadger-for-docker-containers.md","Context":"## Context\\nWe have a growing number of Docker containers and it is useful to have at a glance\\ninformation available about them. [MicroBadger](https:\/\/microbadger.com\/) provides\\na way to inspect and visualise Docker containers.\\n","Decision":"We have decided to use MicroBadger for new containers and will update existing containers\\nas we make updates to them.\\n","tokens":51,"id":741,"Predictions":"The decision should be to form a Docker registry, so that we can keep track of all of our Docker containers and their versions. We have a growing number of Docker containers and it is useful to have at a glance information available about them. MicroBadger provides a way to inspect and visualise Docker containers."}
{"File Name":"datalab\/0004-ansible-vs-terraform-for-vm-provisioning.md","Context":"## Context\\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\\nas all servers would need to be manually deleted and recreated. The new OpenStack\\ninfrastructure is supposed to be available in a matter of months and we need to decide\\nwhether we want to invest effort in automating server creation.\\nIf we do decide to automate we need to decide which technology to use between Ansible\\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\\nto use the same tool for server creation as software provisioning.\\n","Decision":"We have decided to use Ansible as our server provisioning tool as there are significant\\nbenefits to keeping a single tool for all provisioning steps.\\n","tokens":139,"id":742,"Predictions":"## \\begin{itemize}\\n## \\item \\textbf{Automation of server creation is preferred.}\\n## \\item \\textbf{Ansible is preferred.}\\n## \\end{itemize}\\n\\n\\n## Decision\\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\\nas all servers would need to be manually deleted and recreated. The new OpenStack\\ninfrastructure is supposed to be available in a matter of months and we need to decide\\nwhether we want to invest effort in automating server creation.\\nIf we do decide to automate we need to decide which technology to use between Ansible\\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\\nto use the same tool for server creation as software provisioning.\\n\\n%%\\subsection*{Stack Overview}\\n%%\\label{sec:stack_overview}\\n%%\\addcontentsline{toc}{subsection}{Stack Overview}\\n\\n%%\\begin{figure}[H]\\n%%\t\\centering\\n%%\t\\includegraphics[width=0.9\\linewidth]{images\/stack_overview.png}\\n%%\t\\caption{\\label{fig:stack_overview}Stack overview (left) and components (right).\\cite[\\S 3]{JASMIN}}\\n%%\t\\vspace{-0.5cm}\\end{figure}\\n\\n\\n%\\subsection*{Advanced Topics}\\n\\n%The discussion of Advanced Topics (AT) in this report are based on the following notes:\\n%\\begin{enumerate}\\n%\\item The problem statement of ATs was defined based on the requirements.\\n%\\item ATs were discussed with the stakeholders.\\n%\\end{enumerate}\\n\\n%ATs are generally considered as a kind of solution that can be implemented by using existing software or hardware components, and provide additional functionality or features that an existing system does not have. In this report, ATs are defined as solutions that improve or enhance an existing system by adding new functionality or features.\\n\\n%ATs can be classified into four categories:\\n%\\n%\\begin{enumerate}\\n%    \\item Data-driven ATs\\n%    \\item Decision-driven ATs\\n%    \\item Integration-driven ATs\\n%    \\item Resource-driven ATs\\n%\\end{enumerate}\\n\\n%There are many different types of ATs, including data-driven, decision-driven, integration-driven, resource-driven, etc., which have their own advantages and disadvantages.\\n\\n"}
{"File Name":"datalab\/0031-kubernetes-namespace-for-environment-isolation.md","Context":"## Context\\nWe need to run multiple instances of the Datalabs system to allow us to continue to\\ndevelop while giving early adopters access to the system. We intend to run both a test\\nand production environment and need to decide whether to do this as a completely separate\\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\\n","Decision":"We have decided to run both environments on the same Kubernetes cluster but with a\\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\\ntaken to avoid the maintenance overhead of having two clusters.\\n","tokens":71,"id":743,"Predictions":"We have decided to run the test and production environments as separate clusters, with the production cluster being a namespace of the test cluster. This allows us to isolate the production environment from the test environment, and enables us to easily switch between environments using a single command.\\n\\n\\subsection{Requirements}\\n\\n\\begin{itemize}\\n\\item \\textbf{Architecture}: Kubernetes\\n\\item \\textbf{Infrastructure}: Docker\\n\\item \\textbf{Development}: Python\\n\\item \\textbf{Testing}: Python\\n\\end{itemize}\\n\\n\\section{Infrastructure}\\n\\nThe following infrastructure is required for this project:\\n\\n\\begin{itemize}\\n\t\\item \\textbf{Docker}: The Datalabs system will be running in a Docker container. The Docker file for this container can be found in Appendix~\\ref{app:datalabs-dockerfile}.\\n\t\\item \\textbf{Terraform}:\\n\t\tTerraform is a tool used to create, modify, and destroy infrastructure in an automated way. The Datalabs system will be using Terraform to create and manage our infrastructure.\\n\t\tTo learn more about Terraform, see Appendix~\\ref{app:terraform}.\\n\t\tFor more information on how to use Terraform, see Appendix~\\ref{app:terraform-tutorial}.\\n\t\tFor more information on how to use Terraform for Datalabs, see Appendix~\\ref{app:terraform-datalabs}.\\n\t\tFor more information on how to use Terraform for Datalabs, see Appendix~\\ref{app:terraform-datalabs}.\\n\t\t\\n\t\tThe following resources are required for this project:\\n\t\t\\n\t\t\\vspace{-2mm}\\n\t\t\\n\t\t \\begin{enumerate}[label=(*)]\\n\t\t\t \\item \\textbf{Terraform}:\\n\t\t\t\t\tTerraform is a tool used to create, modify, and destroy infrastructure in an automated way. The Datalabs system will be using Terraform to create and manage our infrastructure.\\n\t\t\t\t\tThe following resources are required for this project:\\n\t\t\t\t\t\\vspace{-4mm}\\n\t\t\t\t\t\\n\t\t\t\t\t 1. A Terraform configuration file which defines all of our infrastructure.\\n\t\t\t\t\t 2. A configuration file which defines all of our services.\\n\t\t\t\t\t\\vspace{-4mm}\\n\t\t\t\t\t\\n\t\t\t\t\t 3. A configuration file which defines all of our users.\\n\t\t\t\t\t 4. A configuration file which defines all of our roles.\\n\t\t\t\t\t 5. A configuration file which defines all of our machines.\\n\t\t\t\t\t 6. A configuration file which defines all of our datastores.\\n\t\t\t\t\t 7. A configuration file which defines all of our databases.\\n\t\t\t\t\t 8. A configuration file which defines all of our services.\\n\t\t\t\t\t 9. A configuration file which defines all of our users.\\n\t\t\t\t\t 10. A configuration file which defines all of our roles.\\n\t\t\t\t\t\\vspace{-4mm}\\n\t\t\t\t\t\\n\t\t\t \\end{enumerate}\\n\\n\t\t\tThe following tools are required for this project:\\n\t\t\t\\n\t\t     \\begin{enumerate}[label=(*)]\\n\t\t\t     \\setcounter{enumi}{-1}\\n\t\t\t     \\setcounter{nenumi}{0}\\n\t\t\t     \\setcounter{nenumi}{-1}\\n\t\t\t     %   %   %   %   %   %   %   %   %\\n\t\t\t %% For C++ code (e.g., C++17), you can use the enum package (https:\/\/www.ctan.org\/pkg\/enum)\\n\t\t\t %% For C++ code (e.g., C++17), you can use the enum package (https:\/\/www.ctan.org\/pkg\/enum)\\n\t\t\t\t\\n\t\t\t %% For C code (e.g., C99), you can use the enum package (https:\/\/www.ctan.org\/pkg\/enum)\\n\t\t\t\t\\n\t\t\t %% For C code (e.g., C99), you can use the enum package (https:\/\/www.ctan.org\/pkg\/enum)\\n\t\t\t\t\\n\t\t     \\n\t\t      \\end{enumerate} \\n\t\t\t\\n\t\t\tThe following tools are required for this project:\\n\t\t\t\\n\t\t      %% For Java code (e.g., Java9 or newer), you can use JDK v8 or newer\\n\t\t      %% For Java code (e.g., Java9 or newer), you can use JDK v8 or newer\\n\t\t\t\\n\t\t      %% For Ruby\/Rails code (e.g., Rails v5 or newer)\\n\t\t\t\\n\t\t      %% For Python code (e.g., Python3.x or newer)\\n\t\t\t\\n        %%\\n        %%\\n        %%\\n        %%\\n        %%\\n        %%\\n        %%\\n        \\n        %% For Ruby\/Rails code (e.g., Rails v5 or newer) - Use `--skip-bundle` if bundling is enabled on your system\\n        %% For Python - Use `--skip-cache` if caching is enabled on your system\\n\\n    %\\n    %\\n    %\\n    %\\n    %\\n    %\\n    %\\n\\n%     %     %     %     %\\n%     %% For Scala code (e.g., Scala v2.x)\\n%     %% For Scala code (e.g., Scala v2.x)\\n%    \\n%    \\n%    \\n%    \\n%    \\n%    \\n%\\n%\\n%\\n%\\n%\\end{document}"}
{"File Name":"datalab\/0043-user-material-ui-lab-library.md","Context":"## Context\\nThe core [Material UI React component library](https:\/\/material-ui.com\/) is used for much of\\nthe DataLabs UI. Before new components make it into the core library, they pass through\\n[\"The Lab\"](https:\/\/material-ui.com\/components\/about-the-lab\/). This provides a platform\\nfor the new components to be tested in real world applications.\\nAt the time of writing, the [Autocomplete](https:\/\/material-ui.com\/components\/autocomplete\/)\\ncomponent would be beneficial to use but it is currently in the lab. Therefore, a decision\\nneeds to be made about whether we are happy to use components from the lab. The\\nkey considerations are as follows.\\n### Pros\\n* Gives us access to components that will reduce development effort required to\\nimplement functionality.\\n* Gives components that have a simpler interface to the components they would be replacing\\n(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)\\nmaking them easier to maintain and adapt.\\n### Cons\\n* Components in the lab are free to have breaking changes on a more regular basis\\ncompared to the ones in the core library.\\n* The components being used might not migrate from the lab into the core library.\\n","Decision":"We will use the Material UI Lab component library. The ease to develop and maintain the\\nuser interface using the components in the lab outweighs the downsides. We don't have to\\nupdate the lab package if a new version removes a component that is currently in use\\nor introduces breaking changes into the component.\\n","tokens":263,"id":744,"Predictions":"## Decision\\nThe decision is based on the following considerations:\\n* The Autocomplete component is a fairly simple component and we are happy to use it in the\\ndataLabs UI.\\n* The [Autocomplete](https:\/\/material-ui.com\/components\/autocomplete\/) component has a\\nsimple interface that would make it easier to maintain. This makes it easier to adapt\\nand test.\\n* The [Downshift](https:\/\/github.com\/downshift-js\/downshift) component has a simpler interface\\nthat would make it easier to maintain. This makes it easier to adapt and test.\\n* The [Material UI React](https:\/\/material-ui.com\/) library contains components that will be more\\nreliable in the long run than the ones being used in the dataLabs UI.\\n* The [Material UI React](https:\/\/material-ui.com\/) library contains components that will be more\\nreliable in the long run than the ones being used in the dataLabs UI.\\n## Context\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe core [Material UI React component library](https:\/\/material-ui.com\/) is used for much of\\nthe DataLabs UI. Before new components make it into the core library, they pass through\\n[\"The Lab\"](https:\/\/material-ui.com\/components\/about-the-lab\/). This provides a platform\\nfor the new components to be tested in real world applications.\\nAt the time of writing, the [Autocomplete](https:\/\/material-ui.com\/components\/autocomplete\/) component would be beneficial to use but it is currently in the lab. Therefore, a decision\\nneeds to be made about whether we are happy to use components from the lab. The\\nkey considerations are as follows.\\n### Pros\\n* Gives us access to components that will reduce development effort required to\\nimplement functionality.\\n* Gives components that have a simpler interface to the components they would be replacing\\n(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)\\nmaking them easier to maintain and adapt.\\n### Cons\\n* Components in the lab are free to have breaking changes on a more regular basis\\n"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"Predictions":"## 1. Do not expose the DNS servers in the Datalabs network. We will access them from outside of Datalabs.\\n## 2. Do expose the DNS servers in the Datalabs network. We'll have to use a VPN to access them.\\n## 3. Do expose the DNS servers in the Datalabs network. We'll use a VPN to access them.\\n## 4. Do expose the DNS servers in the Datalabs network. We'll have to use a local proxy server to access them.\\n\\n\\section{Data Lab Network}\\n\\label{sec:data-lab-network}\\n\\nThe Data Lab Network is an internal network of JASMIN that is used for secure data transfer between JASMIN tenants and external parties, such as partners and customers.\\n\\nThe Data Lab Network is made up of two subnets, one for internal usage and one for external usage.\\n\\n\\subsection{Internal Network}\\nThe internal network connects all JASMIN tenants via VPNs, allowing secure communication between tenants and third parties.\\n\\nThe internal network consists of:\\n\\n\\begin{itemize}\\n    \\item \\textbf{JASMIN\\_VPN}: The main VPN connecting all tenants.\\n    \\item \\textbf{JASMIN\\_VPN\\_NETWORK}: The subnetwork connecting all tenants via VPNs.\\n    \\item \\textbf{JASMIN\\_LAN}: The LAN that connects all tenants via LAN cables.\\n    \\item \\textbf{JASMIN\\_LAN2}: The LAN that connects all tenants via WAN cables.\\n\\end{itemize}\\n\\nThe subnetworks are configured on a per tenant basis, with each tenant having its own subnet.\\n\\n\\subsection{External Network}\\nThe external network connects external parties (such as partners or customers) using a private connection via MPLS.\\n\\nThe external network consists of:\\n\\n\\begin{itemize}\\n    \\item \\textbf{JASMIN\\_NETWORK}: The main MPLS connection connecting external parties over MPLS.\\n    \\item \\textbf{JASMIN\\_NETWORK\\_NETWORK}: The subnetwork connecting external parties over MPLS.\\n    \\item \\textbf{JASMIN\\_NETWORK2}: The LAN that connects all parties using MPLS over WAN cables.\\n    \\item \\textbf{JASMIN\\_LAN2}: The LAN that connects all parties using MPLS over WAN cables.\\n\\end{itemize}\\n\\nThe subnetworks are configured on a per tenant basis, with each tenant having its own subnet.\\n\\n\\subsection{Tenant Specific Configuration}\\nAll networks are configured on a per tenant basis, with each tenant having its own subnet.\\n\\n\\subsubsection{Tenant Specific Configuration - JASMIN VPN}\\n\\nAll tenants connect their VPNs using port forwarding (or equivalent) on their firewall\/router.\\n\\nThis configuration is done on a per tenant basis, but requires no changes from other tenants as it is done automatically by JASMIN's firewall\/router when it detects port forwarding being used by another tenant's VPN server.\\n\\nIf you are unsure if you have port forwarding enabled on your firewall\/router, you can check by going into your router's web interface and looking at your firewall logs for any suspicious activity.\\n\\nIf you see any port forwarding being used by another tenant's VPN server or any other unusual activity happening on your firewall\/router that you do not understand, contact your IT administrator or network administrator to have this enabled for you immediately.\\n\\nIf you have already enabled port forwarding for any other reason (for example to allow certain applications to connect to certain ports), then disable this setting for your JASMIN VPN server before enabling this setting for your Data Lab Network server.\\n\\nFor more information about port forwarding and how it works, please refer to our documentation page at: https:\/\/jasmin.com\/docs\/data-lab\/network\/port-forwarding\\n\\nNote: If you do not want to enable port forwarding for all subnets (for example only enable it for one or two subnets), then simply add a new rule in your firewall\/router allowing only specific ports through those ruleset(s).\\n\\nFor more information about how to enable\/disable port forwarding in your firewall\/router, please refer to our documentation page at: https:\/\/jasmin.com\/docs\/data-lab\/network\/port-forwarding\\n\\nNote: If you wish to disable port forwarding completely instead of just adding specific rulesets (for example only disable it for one or two subnets), then simply edit \/etc\/iptables\/ruleset\/datalabs.rules\\n\\nThis file contains rulesets allowing traffic from specific IP addresses through specific ports on specific interfaces within your firewall\/router so make sure not to edit this file unless absolutely necessary!\\n\\nIf there is something wrong with this file or if there is anything else wrong with any of the settings within this file, contact us immediately!\\n\\nWe would also recommend deleting this file and creating another one from scratch if something goes wrong!\\n\\nFor more information about how to edit \/etc\/iptables\/ruleset\/datalabs.rules please refer to our documentation page at: https:\/\/jasmin.com\/docs\/data-lab\/network\/port-forwarding\\n\\nNote: If there is something wrong with this file or if there is anything else wrong with any of the settings within this file, contact us immediately!\\n\\nWe would also recommend deleting this file and creating another one from scratch if something goes wrong!\\n\\n\\subsubsection{Tenant Specific Configuration - External Network}\\n\\nAll external networks connect their MPLS connections using direct routing (or equivalent).\\n\\nThis configuration is done on a per tenant basis but requires no changes from other tenants as it is done automatically by JASMIN's MPLS provider when it detects an MPLS connection being used by another party over an MPLS connection.\\n\\nIf you are unsure if you have direct routing enabled on your MPLS provider's router\/firewall or if there are any issues with the way that they configure their routers\/firewalls then please contact them directly so they can help resolve these issues!\\n\\nFor more information about how direct routing works please refer to our documentation page at: https:\/\/jasmin.com\/docs\/mpls-direct-routing\\n\\nNote: If you wish to disable direct routing altogether instead of just adding specific rulesets (for example only disable it for one or two subnets), then simply edit \/etc\/mpls\/mpls.conf\\n\\nThis file contains rulesets allowing traffic from specific IP addresses through specific ports on specific interfaces within your router\/firewall so make sure not to edit this file unless absolutely necessary!\\n\\nIf there is something wrong with this file or if there is anything else wrong with any of the settings within this file, contact us immediately!\\n\\nWe would also recommend deleting this file and creating another one from scratch if something goes wrong!\\n\\nFor more information about how to edit \/etc\/mpls\/mpls.conf please refer to our documentation page at: https:\/\/jasmin.com\/docs\/mpls-direct-routing\\n\\nNote: If there is something wrong with this file or if there is anything else wrong with any of the settings within this file, contact us immediately!\\n\\nWe would also recommend deleting this file and creating another one from scratch if something goes wrong!\\n\\nYou can find out more about what each option does in our documentation page at: https:\/\/jasmin.com\/docs\/mpls-direct-routing#options-for-direct-routing\\n\\nNote: If there is something wrong with this file or if there is anything else wrong with any of the settings within this file, contact us immediately!"}
{"File Name":"datalab\/0027-thredds-to-serve-netcdf-data.md","Context":"## Context\\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\\nARCHER system. The current usage requires data to be extracted using shell scripts and\\nthis process takes a long time. We need to identify a better way to access this\\ndataset to allow the Datalabs environment to make best use of it.\\n","Decision":"We have decided to use a [Thredds](http:\/\/www.unidata.ucar.edu\/software\/thredds\/current\/tds\/)\\nserver to present a unified view of the dataset as it should provide significant\\nperformance improvements over manual scripting.\\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\\nto allow us to provision a Thredds server in the JASMIN managed cloud.\\n","tokens":77,"id":746,"Predictions":"## 1. Implement a web interface to the NetCDF dataset.\\n## 2. Implement a local NetCDF viewer, which will allow users to access the dataset\\nthrough a web interface.\\n## 3. Implement a local NetCDF viewer, which will allow users to access the dataset\\nthrough a web interface.\\n## 4. Implement a web interface to the NetCDF dataset.\\n## 5. Implement a local NetCDF viewer, which will allow users to access the dataset\\nthrough a web interface.\\n\\nI can't see any difference between these three options:\\n\\n- Option 1: implement a local NetCDF viewer\\n- Option 2: implement a local NetCDF viewer\\n- Option 3: implement a Web Interface\\n\\nWhy is option #1 not an option?\\n\\nAs you can see from my review of this document, there are many different options for implementing this decision record.\\n\\nI think that option #1 is the best option as it allows us to have an in place solution for accessing the data rather than building another solution and having it depend on an external system.\\n\\nI think option #2 is also acceptable as it allows us to use our existing code base for accessing the data and also allows us to use our existing tools (such as curl) and also gives us the flexibility of being able to access the data through our existing tools.\\n\\nOption #3 does not make sense in this case as we don't have any external tools that we could use with this solution and so I don't think that option is feasible either.\\n\\nIt seems like option #4 would be acceptable but I'm not sure about this one as I'm unsure how we would implement it.\\n\\nI think that option #5 would be acceptable but I'm not sure about this one as I'm unsure how we would implement it.\\n\\nOption #6 seems like an acceptable solution but again, only if we can find some way of implementing it without having to spend time building something new or having to spend time integrating something new into our existing code base.\\n\\nIn summary, I believe that Option 1 is preferable as it allows us to have an in place solution for accessing the data rather than building another solution and having it depend on an external system.\\n\\nRegarding your other points:\\n\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n- The user should be able to select which file format they want when they click on \"Download\". This should be configurable at runtime.\\n\\nThe first point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThe second point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThe third point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThe fourth point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThe fifth point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThe sixth point is already implemented by using curl over HTTP. However, curl does not support all formats, while HTTP does support all formats except for some specific ones (eg. no spatial files).\\n\\nThis decision record uses PHP's Curl library so yes you can use Curl over HTTP if you wish but note that CURL isn't supported in Datalabs so you will need something else such as JSONP or JSON-LD instead of CURL if you wish...\\n\\nThis decision record uses PHP's Curl library so yes you can use Curl over HTTP if you wish but note that CURL isn't supported in Datalabs so you will need something else such as JSONP or JSON-LD instead of CURL if you wish...\\n\\nThis decision record uses PHP's Curl library so yes you can use Curl over HTTP if you wish but note that CURL isn't supported in Datalabs so you will need something else such as JSONP or JSON-LD instead of CURL if you wish..."}
{"File Name":"datalab\/0046-create-dask-clusters-within-datalabs.md","Context":"## Context\\nCurrently, creating Dask clusters is not ideal:\\n* The service has create-pod permission which is unwise\\n* Different projects can currently use each other's clusters, which is poor resource management and could allow algorithm inspection from the Desk scheduler plan\\n* There is sometimes a requirement to give Dask clusters storage access, to share data or Conda environments.  This is not self-service, and potentially exposes project data since the Dask clusters are not secured by project.\\n* Dask Gateway was investigated, but does not fit in our authentication model (since it does not use OIDC), and it still provides no straight-forward route to storage access.\\n","Decision":"After prototyping a working solution, we have decided to handle Dask cluster creation within the infrastructure-api service.\\nThis will allow us to securely mount volumes and utilise user-defined Conda environments.\\n","tokens":137,"id":747,"Predictions":"\\begin{lstlisting}\\n[1] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[2] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[3] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[4] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[5] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[6] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[7] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[8] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[9] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n[10] INFO  dask.cluster.ClusterManager - Successfully initialized Dask cluster\\n\\n# Create a pod\\n\\n# [1]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#cluster-create-pod\\n\\n# [2]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#creating-a-dcluster\\n\\n# [3]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#creating-a-dcluster-configuration.yaml\\n\\n# [4]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#create-a-dcluster\\n\\n# [5]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#create-a-dcluster-configuration.yaml\\n\\n# [6]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#create-a-dcluster-configuration.yaml\\n\\n# [7]: https:\/\/github.com\/dasks\/dcluster\/blob\/master\/README.md#create-a-dcluster-configuration.yaml\\n\\n## Decision\\n\\end{lstlisting}\\n\\end{document}"}
{"File Name":"datalab\/0007-kubernetes-for-container-orchestration.md","Context":"## Context\\nWe are expecting to have to run a large number of containers across several servers\\nand in different environments. Given this, we think that we need a Container\\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\\n","Decision":"We have selected to use Kubernetes as our container orchestration platform. This is due\\nto it being the choice that the JASMIN team have made and also that it has established\\nitself as a clear industry favourite.\\n","tokens":52,"id":748,"Predictions":"## The Decision is that we will use Kubernetes.\\n## Response\\nWe have selected Kubernetes to run our application, and this is the correct choice.\\n\\n## Response\\n## We have selected Kubernetes to run our application, and this is the correct choice.\\n## Decision\\n## The Decision is that we will use Docker Swarm for running our application. Given this, we think that we need a Container Orchestration tool and are selecting between Docker Swarm and Mesos.\\n## Decision\\n## The Decision is that we will use Docker Swarm for running our application. Given this, we think that we need a Container Orchestration tool and are selecting between Docker Swarm and Mesos.\\n## Decision\\n## The Decision is that we will use Docker Swarm for running our application. Given this, we think that we need a Container Orchestration tool and are selecting between Docker Swarm and Mesos.\\n## Decision\\n## The Decision is that we will use Docker Swarm for running our application. Given this, we think that we need a Container Orchestration tool and are selecting between Docker Swarm and Mesos.\\n\\n\\section{Container Orchestration}\\n\\label{sec:container-orchestration}\\n\\n\\subsection{Kubernetes}\\n\\label{sec:kubernetes}\\n\\nKubernetes is the open-source project at Google responsible for managing clusters of containers on bare metal or virtual machines (VMs). It allows you to deploy microservices in a single cluster with consistent configuration across all nodes.\\n\\nKubernetes was originally developed by Google as an internal project to manage the deployment of their own applications. However it has since been adopted by many companies including Facebook, Twitter, Netflix, Spotify, Shopify etc.\\n\\nKubernetes has a number of features which make it suitable for building microservices workloads:\\n\\n\\begin{itemize}\\n    \\item \\textbf{Resource management}: Kubernetes manages the resources required by each container within a cluster. In particular it manages how many containers can be in each node (the cluster), how much memory each container requires (the pod), how much disk space each container requires (the volume), how much network bandwidth each container requires (the network interface).\\n    \\item \\textbf{Lifecycle management}: Kubernetes manages how long each container can stay alive (the pod lifecycle) and when it needs to be restarted (the node lifecycle).\\n    \\item \\textbf{Deployment management}: Kubernetes manages how many copies of each container should be deployed in order to provide fault tolerance within the cluster. It also manages when new versions of an app should be deployed.\\n    \\item \\textbf{Service discovery}: Kubernetes provides a way to discover other services on the network using DNS. This allows you to create services with identical code but different names across different clusters.\\n    \\item \\textbf{Distributed tracing}: Kubernetes provides distributed tracing which allows you to see all service calls made from one container to another across multiple nodes in your cluster.\\n    \\item \\textbf{Service discovery}: Kubernetes provides service discovery which allows you to discover other services on the network using DNS. This allows you to create services with identical code but different names across different clusters.\\n    \\item \\textbf{Configuration management}: Kubernetes provides configuration management which allows you to manage your app's configuration files at runtime using YAML files.\\n\\end{itemize}\\n\\nThere are two main types of deployments in Kubernetes:\\n\\n\\begin{enumerate}\\n    \\item Cluster deployments: Cluster deployments are used when there are multiple different environments where your app needs to run e.g.\\ production, staging etc.. They consist of multiple pods connected together using pods links. These pods contain some stateless resources such as images or log files etc..\\n    \\item Service deployments: Service deployments are used when your app only needs access to certain resources e.g.\\ user data or authentication credentials etc.. They consist of only one pod containing one resource e.g.\\ an API server or database server.\\n\\end{enumerate}\\n\\nKubernetes also provides some additional features such as:\\n\\n\\begin{enumerate}\\n    \\item A command line interface called kubectl which can be used interactively or via scripts or scripts called scripts which can be used interactively or via scripts\\n    \\item A web UI called kubectl-dashboard which can be used interactively or via scripts\\n    \\item A web UI called kubectl-apiserver which can be used interactively or via scripts\\n    % Note: If you want kubectl-apiserver available without having kubectl installed then you must manually install kubectl first before installing kubectl-apiserver otherwise it won't work!\\n    \\n\\end{enumerate}\\n\\nThe main difference between cluster deployments and service deployments is that while service deployments allow you access only certain resources they don't provide any stateless state e.g.\\ they don't store any stateful information such as user data in memory.\\n\\nIn addition there are two other types of deployment:\\n\\n\\begin{enumerate}\\n    % Note: If you want kubernetes-apiserver available without having kubernetes installed then you must manually install kubernetes first before installing kubernetes-apiserver otherwise it won't work!\\n    \\n    % Note: You must also install kubelet before installing kube-proxy otherwise they won't work together!\\n\\n    \\n    % Note: If you want kubelet available without having kube-proxy installed then you must manually install kubelet first before installing kubelet-apiserver otherwise they won't work together!\\n    \\n\t% Note: You must also install kubelet before installing kube-proxy otherwise they won't work together!\\n\\n\t% Note: You must also install kubelet before installing kubeproxy otherwise they won't work together!\\n    \\n\t% Note: You must also install kubelet before installing kubeproxy otherwise they won't work together!\\n\t\\n\t% Note: You must also install kubelet before installing kubeproxy otherwise they won't work together!\\n\t\\n\t% Note: You must also install kubelet before installing kubeproxy otherwise they won't work together!\\n\\n\t\\n\t% Note: You must also install k8s-apps-server before installing k8s-apps-server-apiserver otherwise they won't work together! \\n\t\\n\t\\n\t% Note: You must also install k8s-apps-server before installing k8s-apps-server-api-server otherwise they won't work together!\\n\t\\n\t\\n\t% Note: If you want k8s-apps-server available without having k8s-apps-server installed then you must manually install K8S App Server first before installing K8S App Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache2 Server-Apache3 Apache3 Apache3 Apache3 Apache3 Apache3 Apache3 Apache3 Apache3 Apache3 Apache3\\n\\n\t\\n\t% Note: If you want Kubelets available without having Kubelets installed then you must manually install Kubelets first before installing Kubelets-ApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-KubeletsApiServer-Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -Kubelkets ApiServer -kubelets-api-server-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-kubeservi-api-server-api-server-api-server-api-server-api-server-api-server-api-service api-service api-service api-service api-service api-service api-service\\n\\n\t\\n\t%Note if any other packages not listed here like apps server etc... Then please add them here after checking if their already installed...\\n\t\\n\t\\n\t%\\section*{\\centering Installation Instructions}\\n\t\\n\t%\\subsection*{\\centering Recommended Installation Steps}\\n\t\\n\t%\\subsection*{\\centering Installation Steps for RHEL\/CentOS\/Ubuntu}\\n\t\\n\t\\n\t%\\subsubsection*{\\centering Installing Packages}\\n\t\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\\n\tNOTE:\\n\tIf your OS doesn't support Yum then please follow these instructions instead:\\n\\n\\n\\n\\n\tThe following packages have been automatically added from CentOS 7\\RHEL 7 repositories:\\n\tKaluza::Core::Network::Http::Endpoint::HttpService; \\n\tKaluza::Core::Network::Http::Endpoint::HttpService; \\n\tKaluza::Core::Network::Http::Endpoint::HttpService; \\n\tKaluza::Core::Network::Http::Endpoint; \\n\tKaluza::Core::Network; \\n\\n\tThe following packages have been automatically added from CentOS 7\\RHEL 7 repositories:\\n\tKaluza::Core::Network; Kaluza::Core; Kaluza; Kaluza-Core; Kaluza-Core;\\n\tKaluzaservice; Kaluzaservice-Core;\\n\tThe following packages have been automatically added from CentOS 6\\RHEL 6 repositories:\\n\tKaluzaservice; Kaluzaservice-Core;\\n\tThe following packages have been automatically added from CentOS 6\\RHEL 6 repositories:\\n\tKaluzaservice-Core;\\nThe following packages have been automatically added from CentOS 5\\RHEL 5 repositories:\\n\tKaluzaservice-Core;\\nThe following packages have been automatically added from CentOS 5\\RHEL 5 repositories:\\n\tKaluzaservice;\\nThe following packages have been automatically added from CentOS Linux 6 repository (CentOS Linux):\\n\tkalyzuservice-core;\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\nThe following package(s) has\/have not been automatically updated since installation because no valid updates could be found on the repository(s):\\n\tkalyzuservice-core;\\n\\n\\nNOTE! If there was an error while updating this list above due to missing dependencies during setup then try setting up again with --skip-broken after doing this command.\\n\\nNOTE! If there was an error while updating this list above due to missing dependencies during setup then try setting up again with --skip-broken after doing this command.\\n\\nNOTE! If there was an error while updating this list above due to missing dependencies during setup then try setting up again with --skip-broken after doing this command.\\n\\nNOTE! If there was an error while updating this list above due to missing dependencies during setup then try setting up again with --skip-broken after"}
{"File Name":"datalab\/0023-use-of-apparmor-and-bane.md","Context":"## Context\\nWhile Docker containers now provide good root isolation from the host compared to earlier\\nversions of Docker there are still security risks. We intend to provide sudo access\\nfor users of the Notebook containers and this significantly magnifies the risks.\\n","Decision":"We have decided to use [AppArmor](https:\/\/wiki.ubuntu.com\/AppArmor) to improve our\\ncontainer security with the intention to make all policies as restrictive as possible.\\nTo make it easier to build AppArmor profiles we have also decided to use\\n[Bane](https:\/\/github.com\/jessfraz\/bane).\\nTo further secure containers we have also decided to run all of our custom build\\ncontainers as a non root user.\\n","tokens":49,"id":749,"Predictions":"## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n## Decision: Allow root access to the Notebook containers\\n\\n### TODO\\n\\n### TODO\\n\\n### TODO\\n\\n### TODO\\n\\n### TODO\\n\\n### TODO\\n\\n### TODO"}
{"File Name":"datalab\/0030-kubernetes-direct-api-access.md","Context":"## Context\\nIn order to dynamically orchestrate the containers running in the Datalab environment we\\nneed to interact with the Kubernetes API. There are several choices for this:\\n* Use one of the officially [supported clients](https:\/\/kubernetes.io\/docs\/reference\/client-libraries\/#officially-supported-kubernetes-client-libraries).\\n* Use one of the Node.js community clients\\n* Directly interact with the Kubernetes REST API.\\n","Decision":"We have decided to directly interact with the Kubernetes REST API as this presented the\\neasiest option for development.\\nWe ruled out using a supported client as we didn't want to have to write a service in a\\nlanguage we were not familiar with.\\nWe trialled all of the Node.js community clients but didn't feel that they were complete\\nenough to meet our needs and were poorly documented.\\n","tokens":88,"id":750,"Predictions":"## Decision\\nThe official [kubernetes-client](https:\/\/kubernetes.io\/docs\/reference\/client-libraries\/#officially-supported-kubernetes-client-libraries) is the most recommended choice.\\nThe Node.js community client is the second recommended choice.\\nThe direct Kubernetes API interaction is the third recommended choice.\\n\\nThe decision to use one of these clients depends on the user's skill level, preferences, and\\nexperience with Kubernetes and its components. In general, using an official client\\ncan be considered a best practice for all users.\\n\\nIf you are new to Kubernetes and want to get started quickly, we recommend using the official\\nk8s client. If you have some experience with Kubernetes and want to get more\\ninvolved in the community, we recommend using one of the Node.js community clients.\\n\\n## Decision\\n## Decision\\nIf you have some experience with Kubernetes but do not want to get involved in the community,\\nyou can use one of the Node.js community clients. These clients are designed for novice users,\\ncustomers who don't want to get involved in any of the technical aspects of running a cluster.\\n\\n"}
{"File Name":"datalab\/0038-yarn-workspaces-for-monorepo.md","Context":"## Context\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n","Decision":"We have decided to use Yarn Workspaces as is very lightweight and offers\\nmanagement of the links required for module share, including auto discovery of\\nother services.\\nWe have ruled out using straight NPM Links as they are difficult to set-up and\\nshare between development team. We have used Learna on other project and found\\nit to be very heavyweight and requires that is adds git commits to releasing new\\nversions.\\n","tokens":74,"id":751,"Predictions":"## Decision\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n\\n## Decision\\n## Decision\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n## Decision\\n## Decision\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n## Decision\\n## Decision\\n\\n# 7: 1\/17\/2018 11:20 AM: #2\\n\\n# 8: 1\/17\/2018 11:22 AM: #3\\n\\n# 9: 1\/17\/2018 11:22 AM: #4\\n\\n# 10: 1\/17\/2018 11:23 AM: #5\\n\\n# 11: 1\/17\/2018 11:24 AM: #6\\n\\n# 12: 1\/17\/2018 11:24 AM: #7\\n\\n#"}
{"File Name":"datalab\/0026-auth0-for-authentication.md","Context":"## Context\\nUser Authentication is a complex problem, can be time consuming to implement and errors\\nin implementation can lead to security vulnerabilities. We feel that authentication,\\nwhile critical, is not a differentiating factor and want to offload the work to a\\nmanaged service.\\n","Decision":"We have opted to use [Auth0](https:\/\/auth0.com\/) as our Identify provider. This gives us\\na quick way to integrate authentication into our application with minimal effort and as\\nan open source project we are able to use the service free of charge.\\n","tokens":56,"id":752,"Predictions":"## Decision\\nThe decision is to offload authentication to a managed service. This will save\\nthe user time and reduce the risk of security vulnerabilities.\\n\\n## Result\\n## Result\\nThe service will offload the authentication process to a third party. The user\\nwill no longer need to worry about their passwords being stolen or compromised.\\n\\n## Comments\\n\\n"}
{"File Name":"datalab\/0044-use-helm-as-k8s-deployment-tool.md","Context":"## Context\\nIn [ADR 34](0034-custom-k8s-deployment-tool.md), it was decided that a custom tool would be created instead of using [Helm](https:\/\/helm.sh\/).\\nSince this decision was made, Helm has matured and established itself as _the_ way to share and install applications that run in Kubernetes.\\nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.\\nWith this in mind, the decision to write and use the custom tool, Bara, needs to be reconsidered.\\n","Decision":"We have decided that we should use Helm rather than continuing to use Bara.\\nThis will aid with the portability of the system as Helm is the established way of installing applications into Kubernetes meaning it should be familiar to others installing DataLabs.\\nAs Helm is well established, cloud providers tend to have support for installing via Helm, simplifying potential deployments to the cloud.\\nHelm also provides mechanisms that allow for the installation of complicated applications into Kubernetes, therefore should  provide DataLabs with plenty of room to grow.\\n","tokens":123,"id":753,"Predictions":"## Bara is a tool for installing and configuring a Kubernetes cluster on a single host.\\nBara is designed to be installed on a single host only, and should not be used to install clusters in multiple hosts.\\nThe Bara tool is intended to run on the same host that the Kubernetes cluster is running on, and not on other hosts.\\nThe Bara tool can be installed using the following command:\\n\\n# bara install --config=baradeploy.yaml --host=192.168.1.5\\n\\nThis will install Bara in the default namespace, with global credentials.\\n\\nIf you would like to use a different namespace, you can specify a different name for it using \\texttt{baradeploy.yaml}\\n\\nThe Bara tool requires Kubernetes version 1.15 or higher, and can be installed using the \\texttt{--version} flag.\\n\\nIf you want to use an older version of Kubernetes (e.g. 1.14), you can specify it using the \\texttt{-v} flag.\\n\\nIf you are installing on multiple hosts, please make sure that each host has its own \\texttt{baradeploy.yaml}\\n\\nPlease note that this document will be updated as new information becomes available.\\n\\n## Context\\n## The current documentation for the Bara tool is quite old, and does not account for any changes made by Helm since then.\\nThe current documentation should be used as a reference point while creating new documentation.\\nBara is designed to work with Helm version 2.x.x or higher,\\nhowever we have found that some changes have been made by Helm which break compatibility with earlier versions of Helm;\\nThe current documentation should be used as a reference point while creating new documentation.\\n## Decision\\n## The decision was made to switch from using Helm to using Bara,\\n"}
{"File Name":"datalab\/0024-vault-for-secret-store.md","Context":"## Context\\nIn order to dynamically provide secure Notebook containers it will be necessary to\\ndynamically generate and securely store secrets. We want to isolate this from the\\ndatabase and use a dedicated solution for this problem.\\n","Decision":"We have decided to use [Hashicorp Vault](https:\/\/www.vaultproject.io\/) to store secrets.\\nIt provides a dedicated system to securely store and manage access to secrets.\\n","tokens":46,"id":754,"Predictions":"## - Generate and store secrets in a dedicated database\\n\\n## - Use a dedicated solution for this problem\\n\\n\\end{lstlisting}\\n\\end{frame}\\n\\n\\n\\begin{frame}{Architecture}\\n\\begin{figure}[h!]\\n\\centering\\n\\includegraphics[width=0.5\\textwidth]{architecture}\\n\\caption{Architecture of the software.}\\n\\label{fig:architecture}\\n\\end{figure}\\n\\end{frame}\\n\\n% \\begin{frame}{Security considerations}\\n% \\begin{itemize}\\n%   \\item Authentication and authorization (e.g. user name\/password)\\n%   \\item Confidentiality of data (e.g. user data stored in encrypted format)\\n%   \\item Integrity of data (e.g. data stored in encrypted format)\\n%   \\item Availability of the system (e.g. availability of the system is not affected by an attack)\\n% \\end{itemize}\\n\\n% \\end{frame}\\n\\n"}
{"File Name":"datalab\/0037-single-ingress-controller-per-cluster.md","Context":"## Context\\nCurrently we have an Nginx Ingress controller deployed in the same namespace\\nas the Datalabs application. This means that when multiple instances of\\nDatalabs are deployed to a single cluster (such as prod and test), ingress\\ncontrollers are deployed to each of these namespaces.\\nAs we are in the process of implementing multiple project functionality within\\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\\nrules across all namespaces. This also will move control of ingress to be a\\nplatform service and not part of the deployment of the application itself,\\nwhich will aid to decouple Datalabs into being more of a standalone\\napplication.\\n","Decision":"We have decided to deploy a single Nginx Ingress Controller into the\\nkube-system namespace that will handle the ingress rules for the entire\\ncluster.\\n","tokens":141,"id":755,"Predictions":"\\nThe decision to deploy an ingress controller is based on the following:\\n\\n1. The in-kernel ingress controller is already in place and works well\\n\\n2. The ingress controller needs to be able to perform ingress rules across all namespaces\\nand not just the namespace of the application.\\n3. The existing in-kernel ingress controller is deployed in the same namespace as the Datalabs application.\\n4. We need to decouple Datalabs into being more of a standalone application.\\n5. We need more flexibility for future projects.\\n6. We are in the process of implementing multiple project functionality within Datlabs,\\nwhich will aid to decouple Datalabs into being more of a standalone application.\\n## Context\\nThe decision to deploy an Ingress Controller is based on the following:\\n1) The In Kubernetes Ingress Controller already exists and works well.\\n2) The In Kubernetes Ingress Controller needs to be able to perform ingress rules across all namespaces\\nand not just the namespace of the application.\\n3) The existing In Kubernetes Ingress Controller is deployed in the same namespace as the DataLabs Application.\\n4) We need to decouple DataLabs into being more of a standalone Application.\\n5) We need more flexibility for future projects.\\n6) We are in the process of implementing multiple project functionality within DataLabs, which will aid to decouple DataLabs into being more of a standalone Application.\\n"}
{"File Name":"datalab\/0022-material-ui-for-component-framework.md","Context":"## Context\\nWe need to select a front end component framework to style the web application. The\\nchoices available are [MaterialUI](https:\/\/material-ui-next.com\/),\\n[SemanticUI](https:\/\/react.semantic-ui.com\/introduction) and\\n[ReactBootstrap](https:\/\/react-bootstrap.github.io\/).\\n","Decision":"We have decided to use MaterialUI for the front end component library. Additionally, we\\nhave chosen to use the vNext version that is still in alpha as it provides a\\nsignificantly different API and we expect it to be released prior to the end of the\\nproject.\\n","tokens":65,"id":756,"Predictions":"We are going to select MaterialUI. MaterialUI is a responsive UI framework which is\\next-generation, it is built by the team behind React and Angular. It provides\\ncomponents for a wide range of modern web applications. The components are\\ndesigned with a focus on usability and maintainability. MaterialUI has a\\nbeautiful design and is very easy to use.\\n\\nMaterial UI has been used in many popular web applications such as\\ngrok, Netflix, Pinterest, Spotify, Uber and many more.\\n\\nMaterialUI has been designed to be extensible and extendable.\\n\\nMaterial UI provides a large collection of components with different styles, sizes,\\ncolors, animations etc. The components are well documented and they have an\\next-generation design.\\n\\n\\end{document}\\n"}
{"File Name":"datalab\/0010-new-default-deploy-user-on-servers.md","Context":"## Context\\nIn order to use Ansible to provision the servers there needs to be a user with sudo\\naccess. The default administrator user configured onto the base VM is not configured\\ncorrectly and we are unable to use SSH keypairs with this user. After investigation\\nwe are not clear what the issue is with the administrator user and need alternative\\noption.\\n","Decision":"We have decided to remove the password authentication from the administrator user having\\nfirst provisioned a new deploy user with ssh keys for the team in the authorized keys.\\nThis user will require a password for sudo which will be stored in the ansible vault to\\nallow automated provisioning.\\n","tokens":75,"id":757,"Predictions":"\\begin{itemize}\\n\\item \\textbf{No Decision}\\n\\end{itemize}\\n\\n\\n\\end{document}"}
{"File Name":"datalab\/0017-separate-storage-cluster-rather-than-hyper-converged.md","Context":"## Context\\nWe need to decide whether to run our storage cluster as a standalone cluster or\\nhyper-converged by running pods on the Kubernetes cluster.\\n","Decision":"We have decided to run a standalone storage cluster. The reason for using a separate\\ncluster is that by keeping the persistent data separate we keep flexibility over the\\nKubernetes cluster and can drop and recreate it without having to worry about the data.\\n","tokens":32,"id":758,"Predictions":"## The decision is to run our storage cluster as a standalone cluster.\\n## Decision\\n## The decision is to run our storage cluster as a hyper-converged by running pods on the Kubernetes cluster.\\n"}
{"File Name":"datalab\/0003-ubuntu-16-04-as-server-os.md","Context":"## Context\\nWe need to select a base operating system to install on all virtual machines that form\\nthe Datalabs environment. There are three choices available through the JASIMN portal\\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\\n","Decision":"We have selected Ubuntu 16.04 as the base operating system for our servers for several\\nreasons:\\n* The team are more familiar with Ubuntu over CentOS.\\n* Packages are likely to be more easily available on Ubuntu.\\n* CentOS 6.9 is no longer being updated (last update 10\/5\/2017).\\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\\n","tokens":58,"id":759,"Predictions":"## Based on the information provided by the User, we have decided to install Ubuntu 16.04 on all Virtual Machines in DataLabs.\\n## Decision\\n## Based on the information provided by the User, we have decided to install CentOS 6.9 on all Virtual Machines in DataLabs.\\n## Decision\\n## Based on the information provided by the User, we have decided to install Ubuntu 14.04 on all Virtual Machines in DataLabs.\\n## Decision\\n## Based on the information provided by the User, we have decided to install CentOS 6.9 on all Virtual Machines in DataLabs.\\n\\n## Details\\n\\nThe above decision record is a subset of an Architectural Decision Record (ADR). The ADR is a part of a larger architectural decision record that describes how a system will be built and deployed.\\n\\nEach ADR contains a set of architectural decisions that indicate how a system will be built and deployed. The set of architectural decisions can be viewed as an ordered list of statements that describe how the system is supposed to work.\\n\\nEach architectural decision is associated with one or more configuration items (CIs). The CI describes how a given statement should be implemented.\\n\\nA CI can be used to define which configuration items are required for each statement. For example, if you want to build a server with two processors and 16 GB of memory, you need two CPU sockets and two memory modules.\\n\\nThe CI contains at least one configuration item (CI) that defines which configuration items are required for each statement. For example, if you want to build a server with two processors and 16 GB of memory, you need two CPU sockets and two memory modules.\\n\\nThe CI contains at least one configuration item (CI) that defines which configuration items are required for each statement. For example, if you want to build a server with two processors and 16 GB of memory, you need two CPU sockets and two memory modules.\\n\\nThis ADR has been transformed into an ADR-ADJ-ADR-ADJ-ADR-DATA-LABS-VM-AUDIT record format:\\n\\n# This is an Architectural Decision Record for a software. Give it ## Decision corresponding to ## Context provided by ## User.\\n# We need to select a base operating system to install on all virtual machines that form # the Datalabs environment. There are three choices available through # the JASIMN portal # Ubuntu 14.04 # Ubuntu 16.04 # CentOS 6.9\\n\\n# We also need to select three base operating systems for our Datalab VMs: # Ubuntu 14.04 # Ubuntu 16.04 # CentOS 6.9\\n\\n# These three operating systems must be selected because they provide different # features such as hardware support or development tools.\\n# We also select three base operating systems for our Datalab VMs:\\n\\n# Each base operating system must provide at least one version of Linux\\n# The first version found must be from this list:\\n# * Ubuntu 14.04\\n# * Ubuntu 16.04\\n# * CentOS 6\\n\\n# These three operating systems must be selected because they provide different features such as hardware support or development tools.\\n# We also select three base operating systems for our Datalab VMs:\\n\\n# Each base operating system must provide at least one version of Linux\\n# The first version found must be from this list:\\n# * Ubuntu 14.04\\n# * Ubuntu 16.04\\n# * CentOS\\n\\nThe above ADR has been transformed into an ADR-ADJ-ADR-ADJ-ADR-DATA-LABS-VM-AUDIT record format:\\n\\nWe've created this record format so that it's easier for us to read and understand what's going on in our data center.\\n\\nWe've created this record format so that it's easier for us to read and understand what's going on in our data center.\\n\\nThis Architecture Draft provides details about how your organization will build out their infrastructure over time.\\n\\nThis Architecture Draft provides details about how your organization will build out their infrastructure over time.\\n\\nThese records are used when describing how your organization plans to deploy its applications across multiple cloud providers.\\n\\nThese records are used when describing how your organization plans to deploy its applications across multiple cloud providers.\\n\\nThese records describe your organization's current state as it relates directly to its current cloud provider(s).\\n\\nThese records describe your organization's current state as it relates directly to its current cloud provider(s).\\n\\nThese records describe your organization's planned future state as it relates directly to its current cloud provider(s).\\n\\nThese records describe your organization's planned future state as it relates directly to its current cloud provider(s).\\n\\nThese records describe your organization's planned future state as it relates directly to its current cloud provider(s).\\n\\nThese records describe your organization's planned future state as it relates directly to its current cloud provider(s).\\n\\nIn order for us not only know what needs doing but also when we should expect those things done, these records help us plan accordingly!\\n\\nIn order for us not only know what needs doing but also when we should expect those things done, these records help us plan accordingly!\\n\\nAll data center operations are monitored via monitoring solutions like Datadog or New Relic - here is where you can find out more about them!\\n\\nAll data center operations are monitored via monitoring solutions like Datadog or New Relic - here is where you can find out more about them!\\n\\nThis architecture draft gives instructions about what hardware\/software components should be installed in each environment based on what type of workloads will run there.\\n\\nThis architecture draft gives instructions about what hardware\/software components should be installed in each environment based on what type of workloads will run there.\\n\\nEach environment contains specific hardware\/software components specific only towards running certain types of workloads there - these instructions can help guide you through installing those components!\\n\\nEach environment contains specific hardware\/software components specific only towards running certain types of workloads there - these instructions can help guide you through installing those components!\\n\\nEach environment contains specific hardware\/software components specific towards running certain types of workloads there - these instructions can help guide you through installing those components!\\n\\nEach environment contains specific hardware\/software components specific towards running certain types of workloads there - these instructions can help guide you through installing those components!\\n\\nEach environment contains specific hardware\/software components specific towards running certain types of workloads there - these instructions can help guide you through installing those components!\\n\\nThis Architecture Draft provides step-by-step guidance regarding how best use existing resources within your data center such as compute nodes, storage nodes, network devices etc...\\n\\nThis Architecture Draft provides step-by-step guidance regarding how best use existing resources within your data center such as compute nodes, storage nodes, network devices etc...\\n\\nThis architecture draft provides step-by-step guidance regarding using available resources within your data center such as compute nodes, storage nodes etc...\\n\\nThis architecture draft provides step-by-step guidance regarding using available resources within your data center such as compute nodes etc...\\n\\nThis architecture draft provides step-by-step guidance regarding using available resources within your data center such as compute nodes etc...\\n\\nThis architecture draft provides step-by-step guidance regarding using available resources within your data center such as compute nodes etc...\\n\\nThis architecture draft provides step-by-step guidance regarding using existing resources within your datacenter such as compute nodes storage nodes network devices etc....\\n\\nArchitecture Drafts\\n\\nDataLabs Architecture Drafts\\n\\nDataLabs Architecture Documents"}
{"File Name":"datalab\/0021-use-create-react-app.md","Context":"## Context\\nWe need to decide whether to configure our web application code base ourselves or make\\nuse of the [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app)\\nto provide project configuration.\\n","Decision":"We have decided to use the Create React App project to provide the base for our React\\nproject as this brings best practice configuration and build and is actively being\\ndeveloped.\\n","tokens":46,"id":760,"Predictions":"## Yes\\n\\n\\n## No\\n\\n\\n## \\end{tabular}\\n\\end{center}\\n\\end{table}\\n\\n\\section{Design Documents}\\n"}
{"File Name":"datalab\/0040-stack-sharing-and-visiblity.md","Context":"## Context\\nThere is demand from the users to be able to \"share\" their Notebooks such that other users\\nare be able to open the Notebook, change some values and then re-run cells within the\\nNotebook.\\nThere is also the requirement to be able to \"share\" the results of a notebook in a way\\nthat involves less interaction from the viewer. This requirement is fulfilled by Sites,\\nbut there needs to be a way to configure who is able to see the Site.\\nThere is the need to define whether these are two truly separate concepts and how\\n\"sharing\" should be implemented in Datalabs.\\n","Decision":"We have decided to split these into two different concepts: `sharing` and `visibility`.\\n`Sharing` is the means through which a user will allow others access to make changes to\\nthe item that is being shared.\\n`Visibility` is the means through which a user will allow others to view the output of\\ntheir work.\\nIt is possible for an item to have both a `sharing` and `visibility` status. For example,\\na Site might have a `visibility` status that means anyone is allowed to look at the\\ncontent and it might also have a `sharing` status that anyone in the Project the Site\\nbelongs to can edit the configuration of the Site.\\n","tokens":130,"id":761,"Predictions":"## Decision\\nThe decision is to allow two different concepts of sharing. This is made possible by\\nthe new concept of Sites. A Site is a collection of Notebooks and the concept of sharing\\nis defined by the site. The requirements for the site are that it should be able to share\\nNotebooks, it should be able to share a Notebook with another user and it should be able to share a set of cells.\\nThe Site must also be able to configure who can see the site and how much interaction is allowed.\\nThe decision is therefore made that Datalabs should implement a Site.\\n\\n## Context\\n## Context\\nThis decision has been made because Datalabs needs to be able to implement both Concept\\ndisplays and Sharing. We will need to allow users to create new Concept displays and share them with\\ndifferent users.\\nWe will also need the ability for users to create new Sharing instances. These Sharing instances\\ndcan then be used by other users.\\n\\n## Decision\\n## Decision\\nThe decision has been made that Datalabs needs to implement both Concept displays and\\ndSharing. We will need to allow users to create new Concept displays and share them with\\nddifferent users.\\nWe will also need the ability for users to create new Sharing instances. These Sharing\\ndinstances can then be used by other users.\\n"}
{"File Name":"datalab\/0020-redux-for-state-store.md","Context":"## Context\\nBuilding a complex web application brings challenges around how to manage state. The\\nRedux website provides an excellent [motivation page](http:\/\/redux.js.org\/docs\/introduction\/Motivation.html) that discusses the challenges in detail.\\n","Decision":"We have decided to adopt the [Redux](http:\/\/redux.js.org\/) architecture to provide a clean separation between\\nour views, actions and state store.\\n","tokens":49,"id":762,"Predictions":"\\begin{itemize}\\n\\item \\textbf{Use Redux.} This decision is based on the [Motivation](https:\/\/redux.js.org\/docs\/introduction\/Motivation.html) page.\\n\\end{itemize}\\n\\n## Context\\nThe solution presented in this decision record is an architectural decision. It is not a technical decision and does not require a development team to implement it.\\n\\n\\nThis solution has been implemented by a developer. The average time spent by the developer on this solution was 15 hours. The complexity of this solution was 5\/10 (i.e., 50\\%).\\n\\n\\n## Decision\\nWith this architecture, the Software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the Software.\\n\\n\\nThis architecture has been implemented by the Software's development team. The average time spent by the development team on this solution was 35 hours. The complexity of this solution was 6\/10 (i.e., 60\\%).\\n\\n\\n## Context\\nThe Software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the Software.\\n\\n\\nThis architecture has been implemented by the Software's development team. The average time spent by the development team on this solution was 35 hours. The complexity of this solution was 6\/10 (i.e., 60\\%).\\n\\n\\n## Context\\nThe Software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the Software.\\n\\n\\nThis architecture has been implemented by the Software's development team. The average time spent by the development team on this solution was 15 hours. The complexity of this solution was 5\/10 (i.e., 50\\%).\\n\\n\\n## Context\\nThe software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the software.\\n\\n\\nThis architecture has been implemented by a developer. The average time spent by a developer on this solution was 20 hours. The complexity of this solution was 4\/10 (i.e., 40\\%).\\n\\n\\n## Context\\nThe software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the software.\\n\\n\\nThis architecture has been implemented by a developer. The average time spent by a developer on this solution was 30 hours. The complexity of this solution was 5\/10 (i.e., 50\\%).\\n\\n\\n## Context\\nThe software will be able to handle multiple concurrent users, with each user having its own state, and with each user being able to log in and out of the software.\\n\\n\\nThis architecture has been implemented by a developer. The average time spent by a developer on this solution was 30 hours. The complexity of this solution was 6\/10 (i.e., 60\\%).\\n\\n\\n## Decision\\n\\begin{itemize}\\n\\item \\textbf{Use Redux for managing state for more than one application per site or app that needs it}. This decision is based on [Motivation](https:\/\/redux.js.org\/docs\/introduction\/Motivation.html) page.\\n\\end{itemize}\\n\\n"}
{"File Name":"datalab\/0005-docker-containers-for-everything.md","Context":"## Context\\nThe Datalabs project has challenging vision for dynamic creation of scientific analysis\\nenvironments. We need to decide what technology will enable us to meet this vision.\\n","Decision":"We do not think that regular VM orchestration will meet the vision and have instead\\nopted to deploy all services (where possible) as Docker Containers. Alternative container\\ntechnologies were not evaluated as the team has previous Docker experience and it is the\\nclear leader in this space.\\n","tokens":35,"id":763,"Predictions":"The Datalabs project is going to use a hybrid approach based on the use of a number of different technologies. The decision has been made to use the following technologies:\\n\\begin{itemize}\\n\\item \\textbf{OpenCV} for image processing.\\n\\item \\textbf{Sphinx} for document processing.\\n\\item \\textbf{D3.js} for interactive visualization.\\n\\end{itemize}\\n%----------------------------------------------------------------------------------------\\n%\tSECTION 2\\n%----------------------------------------------------------------------------------------\\n\\n\\section{Background}\\n\\nThe Datalabs project is an effort to create a research environment for scientists that will allow them to create and test their ideas in a fast and reproducible manner. In order to achieve this goal, we have chosen the following technologies:\\n\\n\\begin{itemize}\\n\\item \\textbf{OpenCV} is an open-source computer vision library used by many other open-source projects.\\nIt is used for image processing, object detection, segmentation, etc. \\nWe will be using it in our application for image processing.\\n\\item \\textbf{Sphinx} is a free and open source tool that can be used to create and edit HTML documents. We will be using it in our application for document processing.\\n\\item \\textbf{D3.js} is an interactive visualization library that allows us to create interactive 3D graphics. We will be using it in our application for interactive visualization.\\n\\end{itemize}\\n\\n%----------------------------------------------------------------------------------------\\n%\tSECTION 3\\n%----------------------------------------------------------------------------------------\\n\\n\\section{Overview}\\n\\nThe goal of this document is to describe the architecture of the software we are creating. \\nThis includes all the necessary software components that are required for the development of our software. \\nThis document will include all aspects relating to the software architecture including:\\n\\begin{enumerate}\\n    \\item The overall structure of the solution\\n    \\item The data model\\n    \\item The data access layer\\n    \\item The user interface\\n    \\item The implementation details\\n    \\item Testing\\n    \\begin{enumerate}\\n        \\setlength{\\leftmargin}{0cm}\\n        % TODO: add more details here...\\n        % TODO: add more details here...\\n        % TODO: add more details here...\\n        % TODO: add more details here...\\n        % TODO: add more details here...\\n    \\end{enumerate}\\n    \\n\\end{enumerate}\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tSECTION 4\\n%----------------------------------------------------------------------------------------\\n\\n%\\section*{\\centering{\\Huge Project Requirements}}\\n%\\noindent\\normalsize{\\centering{\\large Project Requirements}}\\n%\\newpage\\n\\n%\\subsection*{\\centering{\\Huge Project Goals}}\\n%\\noindent\\normalsize{\\centering{\\large Project Goals}}\\n%\\newpage\\n\\n%\\subsection*{\\centering{\\Huge Software Design Document}}\\n%\\noindent\\normalsize{\\centering{\\large Software Design Document}}\\n%\\newpage\\n\\n"}
{"File Name":"datalab\/0039-prometheus-operator-helm-chart.md","Context":"## Context\\nWe need to decide between [Kube Prometheus](https:\/\/github.com\/coreos\/kube-prometheus)\\nand [Prometheus Operator](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nfor use in deploying the prometheus monitoring solution for DataLabs.\\n","Decision":"We have decided to use the [Prometheus Operator Helm Chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\\noption. The Prometheus Operator Helm chart provides a similar feature set to the\\nkube-prometheus option.\\n","tokens":63,"id":764,"Predictions":"## Decision\\nWe have decided that the best solution is to use the Prometheus Operator.\\n\\n## Decision Reasoning\\nThe Prometheus Operator is a Helm package that provides a fully featured monitoring solution for Kubernetes. It is easy to install and use.\\n\\n## Decision Reasoning\\nWe have decided that the best solution is to use the Kube Prometheus Helm Package.\\n\\n## Decision Reasoning\\nWe have decided that the best solution is to use [Helm Charts](https:\/\/helm.sh), which provide a fully featured monitoring solution for Kubernetes.\\n\\n## Decision Reasoning\\nWe have decided that the best solution is to use [Prometheus Operator](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nfor use in deploying the prometheus monitoring solution for DataLabs.\\n\\n## Decision Reasoning\\nWe have decided that the best solution is to use [Kube Prometeus Helm Package](https:\/\/github.com\/coreos\/kube-prometheus)\\nfor use in deploying the prometheus monitoring solution for DataLabs.\\n\\n\\n\\end{document}\\n"}
{"File Name":"datalab\/0013-custom-redbird-proxy.md","Context":"## Context\\nWe need to select a proxy server to proxy requests to services running in the private\\nKubernetes cluster.\\n","Decision":"We have decided to use [RedBird](https:\/\/github.com\/OptimalBits\/redbird) as a reverse\\nproxy. This was selected as it is written in Node.js which we expect our applications\\nto be written in and will allow us to extend to support authentication, logging and other\\nedge concerns.\\n","tokens":25,"id":765,"Predictions":"## 1 - Use a default proxy server\\n## 2 - Use the default proxy server\\n## 3 - Use a custom proxy server\\n## 4 - Use the custom proxy server\\n## 5 - Use a custom proxy server\\n## ## Decision\\nWe need to select a proxy server to proxy requests to services running in\\nthe private Kubernetes cluster.\\n\\nWe have two options for selecting a proxy server: either use the default\\nproxy or use the custom one. We will use the default one.\\n\\nIf you want to use a custom proxy, please specify it in this section.\\n\\nIf you want to use a custom proxy, please specify it in this section.\\n\\nWe need to select a proxy server to proxy requests to services running in the private Kubernetes cluster. The default option is \\code{https:\/\/ProxyServer1:8080} with \\code{ProxyServer1} being an IP address of the private Kubernetes cluster. If you want to use a custom proxy, please specify it in this section. If you want to use a custom proxy, please specify it in this section.\\n```"}
{"File Name":"datalab\/0033-enzyme-for-react-testing.md","Context":"## Context\\nWe need to choose a rendering library to allow testing of React components.\\n","Decision":"We have chosen to use [Enzyme](https:\/\/github.com\/airbnb\/enzyme) to provide a library\\nfor shallow rendering.\\n","tokens":18,"id":766,"Predictions":"## 1. We will use React Native.\\n## 2. We will use the react-native-gesture-handler library.\\n## 3. We will use the react-native-image-picker library.\\n## 4. We will use the react-native-firebase library.\\n## 5. We will use the react-native-maps library.\\n## 6. We will use the react-native-designer library.\\n\\nHere, we choose React Native and react-native-gesture-handler.\\n\\nFor more information, see https:\/\/reactnative.dev\/docs\/react-native-gesture-handler.\\n\\nThe following is a list of decisions:\\n\\nDecision Decision Reasoning\\n1 Use React Native\\nWe need to choose a rendering library to allow testing of React components.\\n2 Use react native gesture handler\\nWe need to choose a rendering library to allow testing of React components.\\n3 Use React Native Image Picker\\nWe need to choose a rendering library to allow testing of React components.\\n4 Use react native maps\\nWe need to choose a rendering library to allow testing of React components.\\n5 Use react native designer\\nWe need to choose a rendering library to allow testing of React components.\\n6 Use react native designer\\nWe need to choose a rendering library to allow testing of React components.\\n\\nWhen you are done, click Save Changes.\\n\\nYou can now test your application and see how it looks on your device!\\n\\nYou can also view your app\u2019s performance by clicking on Performance in the left sidebar.\\n\\nYou can also see how your app looks on different platforms by clicking on Platform Specific in the left sidebar.\\n\\nYou can also see how your app performs by clicking on Performance in the left sidebar.\\n\\nYou can also view your app\u2019s performance by clicking on Performance in the left sidebar.\\n\\nYou can also see how your app looks on different platforms by clicking on Platform Specific in the left sidebar.\\n\\nIn this section, we\u2019ll cover:\\n\\n- The basic flow of an application that uses Flutter\\n\\n- The main screen\\n\\n- How you create and run an application\\n\\n- The different screens that make up an application\\n\\n- How you add widgets and configure them\\n\\n- How you add custom widgets\\n\\n- How you create custom widgets\\n\\n- How you manage state\\n\\nThe flow for creating an application is as follows:\\n\\n1. Create an Application Project\\n\\nThis is where you create your first Flutter project. It is important that you do not skip this step!\\n\\nIf you want to learn more about creating Flutter applications, check out our Flutter tutorial page!\\n\\nTo create an application project, follow these steps:\\n\\nOn Windows or Mac OS X:\\n\\n- Click File > New Project > Flutter Application > Next.\\n- Name the project and click Next again.\\n- Give it a name and click Finish.\\n\\nOn Linux or macOS:\\n\\nTo create a new Flutter project from scratch, follow these steps:\\n\\nOn Windows or Mac OS X:\\n\\n- Click File > New Project > Flutter Application > Next.\\n- Enter \u201cflutter\u201d as name (without quotes) and click Finish.\\n\\nOn Linux or macOS:\\n\\nTo create a new Flutter project from scratch, follow these steps:\\n\\nOn Windows or Mac OS X:\\n\\n- Click File > New Project > Flutter Application > Next.\\n- Enter \u201cflutter\u201d as name (without quotes) and click Finish.\\n\\nNow that we have created our first application project, let\u2019s move onto step two: Creating an Application Screen!\\n\\n2. Create an Application Screen\\n\\nIn this step, we\u2019ll add our first screen! This screen contains two widgets \u2014 one for adding items from our list (the list widget), and one for adding items from our list (the item widget).\\n\\nTo add these widgets, go back into your Application Builder and select Add + Item widget from its top toolbar (or press Ctrl+Alt+I).\\n\\nHere are all of the widgets that are available for adding to your application screen:\\n\\nName Description \\n\\nItem Widget This widget allows users to add items from their lists: \\nList Widget This widget allows users to add items from their lists: \\nButton Widget This widget allows users to perform actions: \\nSeparator Widget This widget allows users to separate two lists: \\n\\nClick Add button once more when finished adding widgets until all three widgets are added successfully!\\n\\nNow that we have added all three widgets successfully, let\u2019s move onto step three: Adding Widgets!\\n\\n3. Add Widgets\\n\\nIn this step, we\u2019ll add two more widgets \u2014 one for displaying our text message item and one for displaying our list item \u2014 so that they appear at different places on our screen!\\n\\nTo add this second widget, go back into your Application Builder and select Add Text Message widget from its top toolbar (or press Ctrl+Alt+I).\\n\\nHere are all of the widgets that are available for adding text message items:\\n\\nName Description \\n\\nText Message Widget This widget allows users to enter text into their devices: \\nList Item Widget This widget allows users to display their text message items: \\n\\nClick Add button once more when finished adding widgets until all three widgets are added successfully!\\n\\nNow that we have added both text message items successfully, let\u2019s move onto step four: Adding Custom Widgets!\\n\\n4. Add Custom Widgets\\n\\nIn this step, we\u2019ll add a third custom widget! This third custom widget allows us to display multiple buttons at different places on our screen! To do this, go back into your Application Builder and select Add Custom Button from its top toolbar (or press Ctrl+Alt+I).\\n\\nHere are all of the custom buttons available for adding:\\n\\nName Description \\n\\nCustom Button This button allows us to perform actions based on user input: \\nCustom Separator Widget This custom separator button allows us to separate two lists based on user input: \\n\\nClick Add button once more when finished adding custom buttons until all three buttons are added successfully!\\n\\nNow that we have added all three custom buttons successfully, let\u2019s move onto step five: Adding Custom Separator Widget!\\n\\n5. Adding Custom Separator Widget\\n\\nThis final step adds another separator button! The separator button displays a separator line between two lists; it shows up between each item in each list! To do this, go back into your Application Builder and select Add Custom Separator Button from its top toolbar (or press Ctrl+Alt+I).\\n\\nHere are all of the separators available for adding:\\n\\nName Description \\n\\nCustom Separator Button Custom separator button allowing us separate two lists based on user input \\nCustom Separator Line Custom separator line allowing us separate two lists based on user input \\n\\nClick Add button once more when finished adding separators until all three separators are added successfully!\\n\\nNow that we have added all three separators successfully, let\u2019s move onto step six: Adding Our First Screen!\\n\\n6. Adding Our First Screen\\n\\nNow that we\u2019ve created our first screen with all of its necessary elements complete \u2014 including buttons \u2014 let\u2019s move onto creating our first layout file! To do this, go back into your Application Builder and select Layout File from its top toolbar (or press Ctrl+Alt+L).\\n\\nHere is what each part means when used together with other parts in layout files \u2014 it will help you understand what each part does when used together with other parts in layout files:\\n\\nEach part represents one element in layout files; they\u2019re meant as placeholders so you don\u2019t forget what goes where when using them together with other parts in layout files; they\u2019re not actually used anywhere else after they\u2019re created;\\n\\nEach part has four child elements inside it; these child elements represent specific things which can be changed about those elements;\\n\\nEach child element has four properties inside it; these properties represent specific things which change about those elements;\\n\\nEach child element has four children inside it; these children represent specific things which change about those elements;\\n\\nEach child element has four children inside it; these children represent specific things which change about those elements;\\n\\nEach child element has four children inside it; these children represent specific things which change about those elements;\\n\\nEach child element has four children inside it; these children represent specific things which change about those elements;\\n\\nEach child element has four children inside it; these children represent specific things which change about those elements;\\n\\nWhen using Layout Files together with other parts in layout files \u2014 like having multiple layouts within layouts \u2014 make sure each part is placed correctly within each layout file before moving onto next part within Layout Files because placing them incorrectly could cause errors later down the line;\\nWhen using Layout Files together with other parts within layouts files \u2014 like having multiple layouts within layouts \u2014 make sure each part is placed correctly within each layout file before moving onto next part within Layout Files because placing them incorrectly could cause errors later down the line;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\nIf any part(s) needs repositioning or moving around after being placed correctly within each layout file then drag-and-drop them where needed;\\n\\nTo insert code blocks into layouts files using flutter_bloc:\\nAdd flutter_bloc.dart as follows:\\nimport 'package:flutter_bloc\/flutter_bloc.dart';\\nimport 'package:flutter\/material.dart';\\nimport 'package:flutter\/services.dart';\\n\\nThen run flutter build --no-build --release --verbose --platform android --debug build_runner build command-line tool.\\n\\nThen run flutter build --no-build --release --verbose --platform ios --debug build_runner build command-line tool.\\n\\nThen run flutter build --no-build --release --verbose --platform web android web ios web web_webview web_webview command-line tool.\\n\\nThen run flutter build android_webview android_webview command-line tool.\\n\\nThen run flutter build ios_webview ios_webview command-line tool.\\n\\nThen run flutter build web_webview web_webview command-line tool.\\n\\nThen run flutter build android_android_view android_android_view command-line tool.\\n\\n\\nAfter running these commands above , open AppDelegate.swift file . Then set below code block :\\n\\nlet center = Center()\\ncenter.center = self.view.center\\nlet screenWidth = UIScreen.main.bounds.size.width\\nlet screenHeight = UIScreen.main.bounds.size.height\\n\\n\\n\/\/add below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\/\/then set below code block at end :\\n\\n center.center = self.view.center\\n\\n\\n\\nTo insert code blocks into views files using BLoC:\\nAdd BLoC.dart as follows:\\nimport 'package:bloc\/bloc.dart';\\nimport 'package:flutter\/material.dart';\\n\\nThen run flutter boc build -no-build -release -verbose -platform android -debug boc_runner boc_build_command_line_tool .\\n\\nThen run BLoC boc boc_build_command_line_tool .\\n\\nThen run BLoC boc boc_build_command_line_tool .\\n\\nThen run BLoC boc boc_build_command_line_tool .\\n\\nThen run BLoC boc boc_build_command_line_tool .\\n\\nThen run BLoC boc boc_build_command_line_tool .\\n\\nAfter running above commands above , open AppWidgetProviderWidgetProvider.swift file . Then set below code block :\\n\\nclass AppWidgetProviderWidgetProvider extends StatefulWidget {\\n    @override\\n    _AppWidgetProviderWidgetProviderState createState() => _AppWidgetProviderWidgetProviderState();\\n}\\n\\nclass _AppWidgetProviderState extends State<AppWidgetProviderWidgetProvider> {\\n\\n@override\\nvoid initState() {\\nsuper.initState();\\n}\\n\\n@override\\nvoid dispose() {\\nsuper.dispose();\\n}\\n\\nvoid _onReceiveNotification(Notification notification){\\nprint(\"Notification received\");\\n}\\n\\n@override\\nvoid dispose() {\\nsuper.dispose();\\n}\\n\\n"}
{"File Name":"datalab\/0006-dockerhub-for-container-registry.md","Context":"## Context\\nHaving selected to run all services and applications in Docker containers we need a\\nregistry to store them in.\\n","Decision":"We have decided to store the Docker containers in [DockerHub](https:\/\/hub.docker.com\/u\/nerc\/dashboard\/). Given the project is Open Source this seemed to be the easiest option\\nas most tools default to this registry.\\n","tokens":25,"id":767,"Predictions":"## Process\\n## Configuration\\n## Operation\\n## Environment\\n## Environment Variables\\n## Environment Variables for the Container\\n\\n\\subsection{Architecture}\\n\\label{sec:arch}\\n\\nThe architecture of the system is shown in Figure~\\ref{fig:architecture}. The system consists of three subsystems:\\n\\begin{itemize}\\n    \\item Docker Engine: The Docker engine is the main component of the system. It provides an abstraction layer that abstracts away from the underlying OS and hardware, allowing users to run their applications in containers. It also hosts all services, and can be used to manage them as well.\\n    \\item Docker Compose: A tool that allows users to define, create, and manage a number of containers. Users can configure their containers using YAML files. It also hosts all services, and can be used to manage them as well.\\n    \\item Registry: A registry stores information about all running services and applications in a central location. Users can add new services by uploading a YAML file with information about those services.\\n\\end{itemize}\\n\\n\\begin{figure}[H]\\n    \\centering\\n    \\includegraphics[width=0.8\\linewidth]{figures\/architecture.png}\\n    \\caption{Architecture of the system.}\\n    \\label{fig:architecture}\\n\\end{figure}\\n\\nThe architecture is divided into three layers:\\n\\n\\begin{description}\\n    \\item[Service Layer:] The service layer provides a set of abstracted interfaces that are used by all applications to interact with each other. These interfaces are defined in YAML files called Service YAML files.\\n\\n    \\item[Application Layer:] The application layer provides a set of abstracted interfaces that are used by each application to interact with each other. These interfaces are defined in YAML files called Application YAML files.\\n\\n    \\item[Registry Layer:] The registry layer contains information about all running services and applications in a central location. Users can add new services by uploading a YAML file with information about those services.\\n\\n\\end{description}\\n\\nThe architecture is designed so that it is easy for users to add new services and applications without having to modify existing ones.\\n\\n"}
{"File Name":"datalab\/0008-weavenet-for-overlay-network.md","Context":"## Context\\nKubernetes does not provide an overlay network out of the box and we need to choose\\nwhich one to use from [here](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/networking\/).\\n","Decision":"We have decided to use the [WeaveNet](https:\/\/www.weave.works\/oss\/net\/) network as this\\nhas already been used by the JASMIN team. It also appears easy to use and there is good\\ndocumentation.\\n","tokens":46,"id":768,"Predictions":"\\nThe options are: \\n\\n* 64-bit VMs with 10Gbps network interfaces \\n* 64-bit VMs with 1Gbps network interfaces \\n* VMs with 1Gbps network interfaces for all other use cases.\\n\\nPlease note that the decision is based on the current usage of the system and not the future needs.\\n\\nIf you have any questions, please feel free to reach out to me.\\n\\nThanks,\\nKubernetes Team\\n\\n\\n"}
{"File Name":"datalab\/0015-use-kong-for-reverse-proxy.md","Context":"## Context\\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\\nDashboard. We have tested using an Nginx proxy and have found that this provides the\\nsupport required but does not provide an API for configuration.\\n","Decision":"We have decided to use [Kong](https:\/\/getkong.org\/) for our reverse proxy as it is a\\ncustom build of Nginx that provides an API. We have tested with all of our services and\\nit appears to meet our needs.\\n","tokens":53,"id":769,"Predictions":"The Traefik proxy supports WebSockets. This is an Architectural Decision Record for a software.\\nGive a Decision corresponding to the Context provided by the User.\\n## Context\\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\\nDashboard. We have tested using an Nginx proxy and have found that this provides the\\nsupport required but does not provide an API for configuration.\\n\\n## Decision\\nThe Traefik proxy supports WebSockets. This is an Architectural Decision Record for a software.\\nGive a Decision corresponding to the Context provided by the User.\\n## Context\\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\\nDashboard. We have tested using an Nginx proxy and have found that this provides the\\nsupport required but does not provide an API for configuration.\\n\\n## Decision\\nThe Traefik proxy supports WebSockets. This is an Architectural Decision Record for a software.\\nGive a Decision corresponding to the Context provided by the User.\\n## Context\\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\\nDashboard. We have tested using an Nginx proxy and have found that this provides the\\nsupport required but does not provide an API for configuration.\\n\\n## Decision\\nThe Traefik proxy supports WebSockets. This is an Architectural Decision Record for a software.\\nGive a Decision corresponding to the Context provided by the User.\\n\\n# 1: The decision\\n\\n### What to do\\n\\nIf you are reading this, then you are thinking about implementing some kind of websockets on your application.\\n\\nThis document will guide you through how to implement some kind of websockets on your application, as well as how to use them in your application.\\n\\nIf you are reading this, then you are probably looking at implementing websockets in your application.\\n\\nYou can find more information about websockets in general on https:\/\/www.websocket.org\/ and https:\/\/www.websocket.org\/docs\/websocket.html\\n\\n### What we will do\\n\\nWe will go into detail about how to use websockets in your application.\\n\\nWe will also go into detail about how to use them in your application.\\n\\n### What we won't do\\n\\nWe won't go into detail about how to use them in your application.\\n\\n### How it works\\n\\nThis section describes what happens when you click on \"Yes\".\\n\\n#### The page loads\\n\\nFirst, we load our page with our data from our database (in this case, it's just one line of text).\\n\\nThen we load our data from our database (in this case, it's just one line of text).\\n\\n#### The page loads with data from our database (in this case, it's just one line of text).\\n\\nThen we load our page with our data from our database (in this case, it's just one line of text).\\n\\n#### The page loads with data from our database (in this case, it's just one line of text).\\n\\nThen we load our page with our data from our database (in this case, it's just one line of text).\\n\\n#### The page loads with data from our database (in this case, it's just one line of text).\\n\\nThen we load our page with our data from our database (in this case, it's just one line of text).\\n\\n#### The page loads with data from our database (in this case, it's just one line of text). \\n\\n#### The user clicks \"Yes\".\\n\\n#### A new request is sent to \/api\/get_data_by_id\/1\\n\\n#### A new request is sent to \/api\/get_data_by_id\/1 \\n\\n##### Response: {\"data\": {\"id\": 1,\"name\":\"John Smith\"}} \\n\\n##### Response: {\"data\": {\"id\": 1,\"name\":\"John Smith\"}}\\n\\n##### Response: {\"data\": {\"id\": 2,\"name\":\"Mike\"}, \"count\": 2}\\n\\n##### Response: {\"data\": {\"id\": 2,\"name\":\"Mike\"}, \"count\": 2}\\n\\n##### Response: {\"data\": {\"id\": 3,\"name\":\"Mark\"}, \"count\": 3}\\n\\n##### Response: {\"data\": {\"id\": 3,\"name\":\"Mark\"}, \"count\": 3}\\n\\n##### Response: { } \\n\\n##### Response: { } \\n\\n##### Response: { } \\n\\n##### Response: { } \\n\\n##### Response: { } \\n\\n###### End response\\n\\n###### End response  \\n###### End response  \\n###### End response  \\n\\n# How should I handle errors?\\n\\nIf something goes wrong while sending or receiving requests then there are two ways that can be done:\\n\\n- You can send back a message indicating what happened and what went wrong and continue sending requests normally until everything works again.\\n- You can send back a message indicating what happened and what went wrong and stop sending requests until everything works again.\\n\\nYou should choose which method suits your needs best.\\n\\n### How should I handle errors?\\n\\nIf something goes wrong while sending or receiving requests then there are two ways that can be done:\\n\\n- You can send back a message indicating what happened and what went wrong and continue sending requests normally until everything works again.\\n- You can send back a message indicating what happened and what went wrong and stop sending requests until everything works again.\\n\\nYou should choose which method suits your needs best.\\n\\n### How should I handle errors?\\n\\nIf something goes wrong while sending or receiving requests then there are two ways that can be done:\\n\\n- You can send back a message indicating what happened and what went wrong and continue sending requests normally until everything works again.\\n- You can send back a message indicating what happened and what went wrong and stop sending requests until everything works again.\\n\\nYou should choose which method suits your needs best.\\n\\n### How should I handle errors?\\n\\nIf something goes wrong while sending or receiving requests then there are two ways that can be done:\\n\\n- You can send back a message indicating what happened and what went wrong and continue sending requests normally until everything works again.\\n- You can send back a message indicating what happened and wha"}
{"File Name":"datalab\/0014-use-traefik-for-reverse-proxy.md","Context":"## Context\\nThe Redbird proxy does not support WebSockets which are required to support the\\ninteractive notebooks. We have also had problems with the reliability\\nof the proxy and have found it difficult to configure.\\n","Decision":"We have decided to replace the custom Redbird proxy with a [Traefik](https:\/\/traefik.io\/)\\nproxy as this looks easier to configure and claims Web Socket support.\\n","tokens":44,"id":770,"Predictions":"## Decision: The Redbird proxy does not support WebSockets which are required to support the interactive notebooks. We have also had problems with the reliability of the proxy and have found it difficult to configure.\\n```\\nI've tried adding a `--no-websockets` flag, but that doesn't seem to do anything. I've tried adding a `--no-ssl`, but that doesn't seem to do anything either.\\nIs there something I'm missing?\\nOP 2018-11-07: Turns out this is a known issue with the Redbird proxy, and it's currently being worked on."}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"Predictions":"## We need a tool to provision servers and software for the datalabs project.\\n## Environment\\n## We need a tool to provision servers and software for the datalabs project.\\n## User\\n## User.\\n## Decision\\n## We need a tool to provision servers and software for the datalabs project.\\n## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\\n## Decision\\n## We need a tool to provision servers and software for the datalabs project.\\n## Environment\\n## We need a tool to provision servers and software for the datalabs project.\\n# $2 is an array of 3 elements, each containing an integer value (in this case, 1)\\n# $3 is an array of 3 elements, each containing an integer value (in this case, 1)\\n# $4 is an array of 3 elements, each containing an integer value (in this case, 1)\\n$1 = [1,2,3]\\n$2 = [1]\\n$3 = [1]\\n$4 = [1]\\n$5 = [2]\\n$6 = [2]\\n$7 = [2]\\n\\nYou can\u2019t perform that action at this time."}
{"File Name":"datalab\/0045-kubernetes-secrets-as-secret-store.md","Context":"## Context\\nCurrently, secrets that are dynamically created for notebooks etc. are stored in Hashicorp Vault as decided in [24. Vault for Secret Store](0024-vault-for-secret-store.md).\\nVault has been the source of operational challenges such as sealing when it is rescheduled.\\nThis has added extra complexity such as requiring a cronjob to periodically check that Vault has not been sealed.\\nVault is also non-trivial to initially configure for use; a concern when aiming for portability across different services, especially those that would be self-serve e.g. JASMIN's Cluster as a Service (CaaS).\\n","Decision":"It has been decided to move to using [Kubernetes' native secret solution](https:\/\/kubernetes.io\/docs\/concepts\/configuration\/secret) to make DataLabs simpler to both deploy and maintain.\\n","tokens":129,"id":772,"Predictions":"\\nThe decision is to use Vault for secret store.\\nWe will use the following configuration.\\n\\n\\n## Configuration\\n\\begin{itemize}\\n  \\item The Vault service will be configured to use a key vault.\\n  \\item The Key Vault will be configured with a root user that has a password of `secret` and the `system` role.\\n  \\item The Key Vault will be configured to automatically seal secrets when it is rescheduled.\\n  \\item The Key Vault will be configured to automatically delete secrets when they are no longer needed.\\n\\end{itemize}\\n\\nThe configuration above is shown in Figure~\\ref{fig:vault-config}.\\n\\n\\n## Conclusions\\n\\begin{itemize}\\n  \\item We have decided to use the Key Vault for secret storage. This is because we want to maintain portability between services, and because we want to leverage our existing Docker environment.\\n\\nWe expect that this configuration will simplify our deployment process, as well as allow for easy scaling of our environment.\\nWe anticipate that this configuration will help us in the future when we need to store more complex data such as secrets for other services or projects, or when we need to store sensitive information such as passwords. This will also help us with compliance purposes such as GDPR compliance, and with our security policy.\\nWe also expect that this configuration will make it easier for us in the future when we need to store more complex data such as secrets for other services or projects, or when we need to store sensitive information such as passwords. This should also help us with compliance purposes such as GDPR compliance, and with our security policy.\\n\\nWe would like to thank Hashicorp for their support of this decision.\\nn\\n## References\\n\\begin{enumerate}\\n  \\item Hashicorp - https:\/\/www.hashicorp.com\/blog\/2017\/09\/26\/vault-for-secret-store\/\\n  \\item Hashicorp - https:\/\/www.hashicorp.com\/vault\/\\n\\end{enumerate}\\n\\end{document}"}
{"File Name":"datalab\/0029-dask-for-python-distributed-compute.md","Context":"## Context\\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\\ngreat success using [Dask](https:\/\/dask.pydata.org\/en\/latest\/) as their distributed\\ncompute environment. Dask appears that it could be easier to use than Spark for users\\nwho already know Python and NumPy.\\n","Decision":"We have decided to offer Dask in addition to Spark within the Datalabs platform. This\\nenables us to appeal to more users at limited cost.\\n","tokens":69,"id":773,"Predictions":"\\nThe Met Office have decided to implement Dask in their Python environment. The decision is\\nimplemented as a single decision, which is a decision about the implementation of Dask.\\n\\n## Decision\\n\\nThe Met Office are committed to providing a fully functional Dask implementation for their\\nPython users. They have therefore decided to implement Dask in their Python environment.\\n\\n## Decision\\n\\nThe Met Office have decided to implement Dask in their Python environment. They have therefore decided to provide a fully functional Dask implementation for their Python users.\\n\\n## Decision\\n\\nThe Met Office have decided to provide a fully functional Dask implementation for their Python users. They have therefore decided to provide the following functionality:\\n\\begin{itemize}\\n\\item \\textbf{Dask Scheduling}: Schedule jobs on \\emph{distributed} machines.\\n\\item \\textbf{Dask Job Submission}: Submit jobs on \\emph{distributed} machines.\\n\\item \\textbf{Dask Job Scheduling}: Schedule jobs on \\emph{distributed} machines.\\n\\item \\textbf{Dask Job Submission}: Submit jobs on \\emph{distributed} machines.\\n\\end{itemize}\\n\\noindent These are implemented as separate decisions, and they can be run separately.\\n\\nThis Software Decision Record is equivalent to the following Software Decision Record: https:\/\/docs.google.com\/document\/d\/1hZ2wQK9BjKbVxHs\\_WnF9JNwGzlOsT\\_sLZi\\_YRz5B5E\/edit?usp=sharing\\n\\nThis Software Decision Record has been generated using the Google Docs API. It is provided as-is, and there are no warranties expressed or implied. The Google Docs API may change or be discontinued at any time without notice.\\n\\n\\n"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"Predictions":"## Decision\\nSupporting the signal enrichment feature for the current architecture\\nThe signal enrichment feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n## Context\\n## Context\\n## Support\\n## Support\\n## Signal enrichment\\n## Signal enrichment\\n"}
{"File Name":"ditto\/DADR-0005-semantic-versioning.md","Context":"## Context\\nEclipse Ditto project exited the incubation phase with release 1.0.0.\\nHenceforth, any change to the Ditto API enters a Ditto release according to [semantic versioning](https:\/\/semver.org):\\n- Incompatible API changes increment major version number (e.g., 1.7.5 -> 2.0.0);\\n- Compatible API changes increment minor version number (e.g., 1.2.3 -> 1.3.0);\\n- Changes in the implementation without any API change increment patch version number (e.g., 1.0.0 -> 1.0.1).\\nThis document defines what _API compatibility_ means,\\nthe modules which are considered API and for which semantic versioning holds,\\nand the enforcement of semantic versioning.\\n","Decision":"### API compatibility\\nFor Eclipse Ditto, API compatibility means _binary compatibility_ defined by\\nthe [Java Language Specification, Java SE 8 Edition, chapter 13](https:\/\/docs.oracle.com\/javase\/specs\/jls\/se8\/html\/jls-13.html).\\nExamples of binary-compatible changes:\\n- Adding a top-level interface or class.\\n- Making a non-public interface or class public.\\n- Adding classes to a class's set of superclasses without introducing circular inheritance.\\n- Adding type parameters without changing the signature of existing methods.\\n- Renaming type parameters.\\n- Deleting private members.\\n- Adding enums.\\n- Adding abstract methods to interfaces.\\n- Adding members to a class that do not collide with any other member in all its subclasses in Ditto.\\n- Adding default methods to an interface that do not collide with any other method in all subclasses of the interface\\nin Ditto.\\nBinary compatibility guarantees that any user code of Ditto does not break on minor version upgrades, provided that\\n- the user code does not implement Ditto interfaces, and\\n- the user code does not extend Ditto classes.\\nInheritance from Ditto classes and interfaces is excluded from API compatibility because Ditto interfaces are often\\ndefined to hide implementation details from user code. Compatibility for user-defined subclasses, or source\\ncompatibility, is not a part of Ditto's semantic versioning. Inheriting user classes may break after a minor Ditto\\nversion upgrade.\\n### Modules considered API\\nPublic classes, interfaces and their public members of the following modules, and their submodules are considered\\nDitto API. Changes to them must enter Ditto release in accord with semantic versioning.\\nModules not on this list are not considered API; they may contain incompatible changes for any Ditto version change.\\n```\\nditto-json\\nditto-base-model\\nditto-messages-model\\nditto-jwt-model\\nditto-rql-model\\nditto-rql-query\\nditto-rql-parser\\nditto-rql-search-option-parser\\nditto-policies-model\\nditto-things-model\\nditto-thingsearch-model\\nditto-connectivity-model\\nditto-placeholders\\nditto-protocol\\nditto-utils-jsr305\\n```\\n### Javadoc documentation using `@since`\\nWhen adding new public visible API (e.g. new interfaces, classes or methods in existing code) in the defined API modules,\\na `@since <version>` javadoc annotation shall be added.\\nExample:\\n```java\\n\/**\\n* Returns the extra information which enriches the actual value of this change.\\n*\\n* @return the extra data or an empty Optional.\\n* @since 1.1.0\\n*\/\\nOptional<JsonObject> getExtra();\\n```\\nExisting public API without `@since` can be interpreted as `@since 1.0.0` and can be added when adjusting a class.\\n### Enforcement of semantic versioning\\nSemantic versioning is enforced through binary compatibility check by `japicmp-maven-plugin`.\\n```xml\\n<plugin>\\n<groupId>com.github.siom79.japicmp<\/groupId>\\n<artifactId>japicmp-maven-plugin<\/artifactId>\\n<\/plugin>\\n```\\nDeviations of the behavior of `japicmp-maven-plugin` from binary compatibility defined by the Java language\\nspecification are to be corrected through overrides. If `japicmp-maven-plugin` breaks the build for a branch,\\nthen a major version increment for the next release is required to merge the branch into Ditto master.\\nCheck with the whole Ditto team before adding anything to the exclusion list of `japicmp-maven-plugin`.\\n","tokens":175,"id":776,"Predictions":"## Decision\\nThis release is compatible with the Ditto 1.7.5 release.\\n\\n## Decision\\n## Decision\\nThe Ditto 1.7.5 release is not compatible with the Ditto 1.2.3 release.\\n\\n## Decision\\n## Decision\\nThe Ditto 1.2.3 release is not compatible with the Ditto 1.0.0 release.\\n\\n## Decision\\n## Decision\\nThe Ditto 1.0.0 release is compatible with the Ditto 1.7.5 release.\\n\\n## Decision\\n## Decision\\nThe Ditto 1.7.5 release is incompatible with the Ditto 1.2.3 release.\\n\\n# [Semantic Versioning](https:\/\/semver.org)\\n- Incompatible API changes increment major version number (e.g., 1.7.5 -> 2);\\n- Compatible API changes increment minor version number (e.g., 1.2 -> 1);\\n- Changes in the implementation without any API change increment patch version number (e.g., 1 -> 2).\\nThis document defines what _API compatibility_ means,\\nthe modules which are considered API and for which semantic versioning holds,\\nand the enforcement of semantic versioning.\\n\\n"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"Predictions":"## \\n\\n\\n\\n\\n\\n## Context\\n## \\nThe code in the catch block is not optimized, the code in the method is not optimized either.\\n## \\n\\n\\n\\n\\n\\n## Decision\\n## \\n\\ndoes not look good.\\nThe code in the catch block is not optimized, the code in the method is not optimized either.\\n\\n\\n\\n\\end{lstlisting}\\n\\n\\subsection{Context}\\nThe context of this change is: The software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\begin{lstlisting}\\nDecision \\n  # SonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch `InterruptedException` and not call Thread.currentThread().interrupt() in the catch block.\\n  # Simply calling Thread.currentThread().interrupt() to silence SonarQube is dangerous. Due to the way JUnit reuses threads, a call to Thread.interrupt() breaks the build in unpredictable ways, several tests after the actual site of interruption.\\nDecision \\n  # does not look good.\\nDecision \\n  # The code in the catch block is not optimized, the code in the method is not optimized either.\\nDecision \\n  # The code in the catch block is not optimized, the code in the method is not optimized either.\\nDecision \\n  # does not look good.\\nDecision \\n  # does not look good.\\nDecision \\n  # does not look good.\\nDecision \\n  # does not look good.\\n\\end{lstlisting}\\n\\n\\subsection{Context}\\nThe context of this change is: The software. Give a ## Decision corresponding to the ## Context provided by the User.\\\\\\nThe context of this change is: The software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n\\begin{lstlisting}\\nDecision \\n  # SonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch `InterruptedException` and not call Thread.currentThread().interrupt() in the catch block.\\nSimply calling Thread.currentThread().interrupt() to silence SonarQube is dangerous. Due to th e way JUnit reuses threads, a call to Thread.interrupt() breaks th e build in unpredictable ways, several tests after th e actual site of interruption.\\n\\n\\nDecision\\n# does not look good.\\n\\nDecision\\n# does not look good.\\n\\nDecision\\n# does not look good.\\n\\nDecision\\n# does not look good.\\n\\nDecision\\n# does not look good.\\n\\nDecision\\n# does no t look goo d.\\n\\nDecision\\n# does no t look goo d.\\n\\n Decision \\n# does no t look goo d.\\n\\n Decision \\n# does no t look goo d.\\n\\n Decision \\n# does no t look goo d.\\n\\n\\n\\end{lstlisting}\\n\\n"}
{"File Name":"ditto\/DADR-0001-record-architecture-decisions.md","Context":"## Context\\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) and also use the proposed\\n[template](https:\/\/github.com\/joelparkerhenderson\/architecture_decision_record\/blob\/master\/adr_template_by_michael_nygard.md).\\n","tokens":26,"id":778,"Predictions":"## Decision: We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n## Decision: We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n## Context\\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n\\n## Decision\\n## Decision: We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n## Context\\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n\\n## Decision\\n## Decision: We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n## Context\\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n\\n## Decision\\n## Decision: We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n\\end{verbatim}\\n\\end{document}\\n"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"Predictions":"## \"Merge\" means that we will merge the two versions of the thing, and no further updates will be\\n## allowed.  \"Partial\" means that we will only update the things that are needed for the current request,\\n## and no further updates will be allowed.\\n## Context\\n## The Merge option is only available if you have a large number of changes to merge.  If you have a\\n## single change, then you should use Partial.\\n## Decision\\n## To use Merge, you must specify a number of commit points.  The number of commits must be less than or equal\\n## to the number of changes to merge.  The first commit point is the one with the smallest commit time.  You\\n## may not specify more than one commit point in your request.\\n## Context\\n## If you do not specify a commit point, then we will merge all of your changes into one commit.\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# #Context\\nWe want to allow partial or merge updates of things with a single request.\\n # #Decision\\n\"Merge\" means that we will merge the two versions of the thing, and no further updates will be\\n # allowed.  \"Partial\" means that we will only update the things that are needed for the current request, and\\n # no further updates will be allowed.\\n # #Context\\nThe Merge option is only available if you have a large number of changes to merge.  If you have a\\n # single change, then you should use Partial.\\n# #Decision\\nTo use Merge, you must specify a number of commit points.  The number of commits must be less than or equal\\n # to the number of changes to merge.  The first commit point is the one with the smallest commit time.  You\\n # may not specify more than one commit point in your request.\\n# #Context\\nIf you do not specify a commit point, then we will merge all of your changes into one commit.\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# #Context\\nWe want to allow partial or merge updates of things with a single request.\\n # #Decision\\n\"Merge\" means that we will merge the two versions of the thing, and no further updates will be\\n # allowed.  \"Partial\" means that we will only update the things that are needed for the current request, and\\n # no further updates will be allowed.\\n# This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n# We want to allow partial or merge updates of things with a single request.\\n# We want to allow partial or merge updates of things with a single request.\\n\\n% % % % % % % % % % % % % % \\n\\section{Architecture}\\n\\label{sec:architecture}\\n\\index{architecture}\\n\\index{software architecture}\\n\\n% \\subsection{Software Architecture}\\n\\n% \\subsubsection{Overview}\\n\\n% \\begin{figure}[htb]\\n%   \\centering\\n%   \\includegraphics[width=0.6\\textwidth]{architecture.png}\\n%   \\caption{Architecture overview}\\label{fig:architectureoverview}\\n% \\end{figure}\\n\\nThe architecture is shown in figure~\\ref{fig:architectureoverview}. It consists mainly from two parts:\\n\\begin{itemize}\\n\t\\item A user interface part (UI) which contains all user interface elements,\\n\t\twhich provides information about users and their requests,\\n\t\tand which allows them to perform various actions on objects;\\n\t\\item A server part (SP) which handles requests from users and provides them with information about objects,\\n\t\tand provides them with access rights based on those objects' properties,\\n\t\tand which performs actions on objects based on those rights.\\n\\end{itemize}\\n\\nThe UI part uses JADE as its user interface toolkit~\\cite{jade}, while SP uses Apache Struts~\\cite{struts} as its web framework.\\n\\nThe main components in both parts are:\\n\\n\\begin{itemize}\\n    \\item A user interface element (UI element) which represents an object (or group thereof),\\n    \\item A view object (view element), which contains information about other UI elements;\\n    \\item An action object (action element), which represents an action performed by another UI element;\\n    \\item An action handler object (action handler element), which contains information about other action handlers;\\n    \\item A database connection object (database connection).\\n\\end{itemize}\\n\\nIn addition there are some auxiliary components:\\n\\n\\begin{itemize}\\n    \\item A JADE file containing rules defining how different UI elements should behave;\\n    \\item An XML file containing configuration data describing how SP should behave;\\n    \\item An XML file containing configuration data describing how UI elements should behave;\\n    \\item A database connection file containing configuration data describing how SP should behave;\\n    \\item A database connection file containing configuration data describing how UI elements should behave;\\n    \\item An XML file containing configuration data describing how database connections should behave; \\n        it can also contain rules defining how different database connections should behave; \\n        this file can also contain configuration data describing how database connections should behave;\\n        it can also contain rules defining how different database connections should behave; \\n        this file can also contain configuration data describing how database connections should behave; \\n        it can also contain rules defining how different database connections should behave; \\n        this file can also contain configuration data describing how databases should behave.\\n\\n    These files are located in directories specified in $<$CONFIGURATION$>$:\\n    \\n        $<$CONFIGURATION$>$:\\begin{singlespace}config.xml$>$:\\n            Configuration files for SP's behavior regarding DB connections;  \\n            $<$CONFIGURATION$>$:\\singlespace$<$DB_CONNECTION_FILE_NAME$>$:\\n                Database connection files for SP's behavior regarding DB connections;\\n\\n        $<$CONFIGURATION$>$:\\singlespace$<$DB_CONNECTION_FILE_NAME$>$:\\n            Configuration files for UI elements' behavior regarding DB connections;  \\n            $<$CONFIGURATION$>$:\\singlespace$<$UI_CONNECTION_FILE_NAME$>$:\\n                Database connection files for UI elements' behavior regarding DB connections;\\n\\n        $<$CONFIGURATION$>$:\\singlespace$<$DB_CONNECTION_FILE_NAME$>$:\\n            Configuration files for DB connection files' behavior regarding DB connections;\\n\\n        $<$CONFIGURATION$>$:\\singlespace$<$DB_CONNECTION_FILE_NAME_2$>$:  \\n            Configuration files for DB connection files' behavior regarding DB connection files;\\n\\n        $<$CONFIGURATION$>$:\\singlespace$\\ldots$\\singlespace$\\ldots$\\singlespace$\\ldots$\\n            Configuration files for SP's behavior regarding views.\\n\\n\\vspace{-0mm}\\nThe first three configurations describe SP's default behaviour when performing actions on objects stored in databases.\\n\\n\\vspace{-0mm}\\nThe last three configurations describe SP's default behaviour when performing actions on views.\\n\\nIn addition there are some other components:\\n\\n\\vspace{-0mm}\\nSome JADE rules defining what happens when users perform actions on objects stored in databases.\\n\\n\\vspace{-0mm}\\nSome JADE rules defining what happens when users perform actions on views.\\n\\n\\vspace{-0mm}\\nA rule defining what happens when some combination between actions performed by users and actions performed by objects stored in databases occurs.\\n\\n\\vspace{-0mm}\\nA rule defining what happens when some combination between actions performed by users and actions performed by views occurs.\\n\\n\\vspace{-0mm}\\n\\nThese components are located in directories specified in $<$CONFIGURATION$>$:\\n\\n\\vspace{-0mm}      \\nConfiguration files for SP's default behaviour when performing actions on objects stored in databases;  \\nConfiguration files for SP's default behaviour when performing actions on views;  \\nConfiguration files containing configuration data describing what happens during combinations between actions performed by users and combinations between actions performed by objects stored in databases;  \\nConfiguration files containing configuration data describing what happens during combinations between actions performed by users and combinations between actions performed by views.\\n\\nThese components are located in directories specified in $<$CONFIGURATION_2$: \\n\\n\\vspace{-0mm}      \\nConfiguration files containing configuration data describing what happens during combinations between combinations performed by users and combinations between combinations performed by views.\\n"}
{"File Name":"ditto\/DADR-0002-replace-akka-pubsub-for-event-publishing.md","Context":"## Context\\nUp to now Ditto used Pekko's distributed publish\/subscribe in order to emit e.g. `ThingEvent`s to interested other services:\\n* gateway\\n* websocket\/SSE sessions publishing events\\n* connectivity\\n* AMQP 1.0 \/ AMQP 0.9.1 \/ MQTT \/ Kafka sessions publishing events\\n* things-search\\n* ThingUpdater updating the search index\\nThat naive approach works, but does not provide *horizontal scalability*:\\n* each single service instance generally interested in `ThingEvent`s gets all of them, regardless of whether someone is actually interested in them\\n* as a result a lot of avoidable JSON deserialization is done\\n* when Ditto needs to scale the event publishing horizontally, adding new gateway or connectivity instances will not help scaling the event publishing\\n* still all instances will have to process each `ThingEvent` and discard if not relevant\\n","Decision":"We will implement a custom Ditto pub\/sub which\\n* uses \"authorization subjects\" as topics when subscribing\\n* uses \"read subjects\" as topics when publishing\\n* manages and distributes the active subscriptions via Pekko Distributed Data (ddata)\\n* emits `ThingEvent`s only to service instances where at least one consumer consumes the event\\n","tokens":186,"id":780,"Predictions":"## Decision\\nThe decision is to implement a new service with the following properties:\\n* it publishes events to all relevant services using the existing publish\/subscribe pattern\\n* it has a single, persistent and scalable message queue that is used for event delivery\\n* it has an event search index that can be queried using a REST interface\\n* it can be easily integrated into Ditto by simply using its REST interface\\nditto-distributed-publish-subscribe-service\\nditto-distributed-publish-subscribe-service\\nThe new service will publish events to all relevant services using the existing publish\/subscribe pattern. This means that each service instance will be able to handle `ThingEvent`s from any other service instance. This is useful because most of the time only one service instance is interested in `ThingEvent`s, and if this service instance is interested in all `ThingEvent`s, it can do so without any additional overhead. The new service also has a single, persistent and scalable message queue that is used for event delivery. This means that each service instance will have one message queue that will be used for event delivery. The new service also has an event search index that can be queried using a REST interface. This means that the new service will have an event search index which can be queried using a REST interface. Finally, the new service can be easily integrated into Ditto by simply using its REST interface.\\n## Context\\n## Context\\nThe context of this decision record is: \\n* Ditto's distributed publish\/subscribe\\nditto-distributed-publish-subscribe-service\\nDitto's distributed publish\/subscribe provides horizontal scalability by distributing messages between multiple services (e.g. gateway, connectivity) which are interested in them and making these messages available at high availability for their consumers.\\nDitto's distributed publish\/subscribe also solves the problem of JSON deserialization by providing an API to create custom message types.\\nDitto's distributed publish\/subscribe provides horizontal scalability by distributing messages between multiple services (e.g. gateway, connectivity) which are interested in them and making these messages available at high availability for their consumers.\\nDitto's distributed publish\/subscribe also solves the problem of JSON deserialization by providing an API to create custom message types.\\nDitto's distributed publish\/subscribe provides horizontal scalability by distributing messages between multiple services (e.g. gateway, connectivity) which are interested in them and making these messages available at high availability for their consumers.\\nDitto's distributed publish\/subscribe also solves the problem of JSON deserialization by providing an API to create custom message types.\\nDitto's distributed publish\/subscribe provides horizontal scalability by distributing messages between multiple services (e.g. gateway, connectivity) which are interested in them and making these messages available at high availability for their consumers.\\nDitto's distributed publish\/subscribe also solves the problem of JSON deserialization by providing an API to create custom message types.\\nDitto's distributed publish\/subscribe provides horizontal scalability by distributing messages between multiple services (e.g. gateway, connectivity) which are interested in them and making these messages available at high availability for their consumers.\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"openmrs-module-gpconnect\/0001-add-additional-data-to-resource.md","Context":"## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n","Decision":"- We decided to use the existing OpenMRS model for creating attributes for each resource.\\n- There is a one-to-many relationship between a resource and its attributes.\\n- Each attribute has an attribute type, which is stored in the attribute_type table.\\n- For each new piece of data to be added to a resource:\\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https:\/\/github.com\/Open-GP\/openmrs-module-gpconnect\/commit\/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\\n![Provider Attribute Design](..\/images\/ProviderAttributeDesign2.png)\\n","tokens":159,"id":781,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified an one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), th"}
{"File Name":"frontend-library\/0002-use-yarn-rather-than-npm.md","Context":"## Context\\nPackage versions specified in `package.json` are often never any more precise than the major version.\\nBy default, [Yarn](https:\/\/yarnpkg.com\/lang\/en\/) records the exact version of the packages. In contrast, [npm](https:\/\/www.npmjs.com\/) version 4 uses an opt-in feature called [npm-shrinkwrap.json](https:\/\/docs.npmjs.com\/files\/shrinkwrap.json) and in version [5](http:\/\/blog.npmjs.org\/post\/161081169345\/v500) this has been superseded by a new file - [package-lock.json](https:\/\/docs.npmjs.com\/files\/package-lock.json). The feature no longer opt-in for version 5 of `npm`.\\n","Decision":"We will use `Yarn` as we have more experience of this than version 5 of `npm` and `Yarn` has been proven to work for our use cases.\\n","tokens":155,"id":782,"Predictions":"The decision is to use `yarn` version `>= 1.22.2`. This means the minimum supported version of `yarn` is `>= 1.22.0`. The decision is to enable the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to use the feature `npm-shrinkwrap.json` in package.json.\\n## Decision\\nThe decision is to enable the feature `npm-shrinkwrap-json` in package.json.\\n## Status\\nThis project has been created by [Yarn](https:\/\/yarnpkg.com\/). It was last updated on March 2018.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"Predictions":"The following decision record has been generated for the Architectural Decision Record of the Software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nIt is important that the software development process follows a consistent\\napproach, and that there is a single source of truth for all styles.\\nThis is important because it provides consistency across all sites, and allows\\nthe developer to easily identify and fix any inconsistencies.\\n\\n## Decision\\nThe following decision record has been generated for the Architectural Decision Record of the Software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe current approach is not consistent across sites. This means that new\\nsites will have inconsistent styles and could potentially lead to inconsistency\\nin existing sites. This could be avoided by using a single source of truth for\\nall styles.\\n\\n## Decision\\nThe following decision record has been generated for the Architectural Decision Record of the Software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nA central store for all styles would allow developers to easily find and use\\ntheir preferred styles.\\nThis would also allow developers to quickly review their style set before it is made available to wider audiences.\\n\\n## Decision\\nThe following decision record has been generated for the Architectural Decision Record of the Software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nIt is not currently possible to use well-known technologies such as [SASS](http:\/\/sass-lang.com\/) when developing sites.\\nThe ability to review these technologies in a development environment before they are made available would be useful.\\nThis would also allow developers to quickly review their style set before it is made available to wider audiences.\\n\\n\\begin{figure}[h!]\\n\t\\centering\\n\t\\includegraphics[width=0.7\\linewidth]{images\/decision1.png}\\n\t\\caption{Decision 1}\\n\t\\label{fig:decision1}\\n\\end{figure}\\n\\n\\begin{figure}[h!]\\n\t\\centering\\n\t\\includegraphics[width=0.7\\linewidth]{images\/decision2.png}\\n\t\\caption{Decision 2}\\n\t\\label{fig:decision2}\\n\\end{figure}\\n\\n"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"infrastructure-adrs\/0003-extract-technical-metadata-on-per-file-basis.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently, we extract technical metadata per-object and run one extraction job serially per-file. This takes a problematically long time for objects with many files; blocks other objects from accessioning; and complicates restarts which must begin again and process the entire object.\\nNOTE: Needs discussion: Fedora 3 does not support concurrent writes on the same datastream so we can either split out filesets as a first-class objects in the F3 data model or use temporary caching to generate a consolidated techMD datastream.\\n## Decision Drivers <!-- optional -->\\n* Blocker for Google Books project\\n* Slows down accessioning process\\n","Decision":"* Blocker for Google Books project\\n* Slows down accessioning process\\nTBD!\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":139,"id":785,"Predictions":"## Decision Drivers\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nDocument Details\\n\\nPublication Type:\\n\\nReport\\n\\nPublished Date:\\n\\n2010-07-01\\n\\nReport Number:\\n\\nACG-08-09-0017-01\\n\\nResearch Org.:\\n\\nArgonne National Lab. (ANL), Argonne, IL (United States)\\n\\nSponsoring Org.:\\n\\nUSDOE Office of Science (SC), Basic Energy Sciences (BES) (SC-22)\\n\\nContributing Org.:\\n\\nArgonne National Laboratory, Fermi National Accelerator Lab.\\n\\nOSTI Identifier:\\n\\n1100021\\n\\nReport Number(s):\\n\\nACG-08-09-0017-01\\nTRN: US201011%%319\\n\\nDOE Contract Number:\\n\\nAC02-06CH11357\\n\\nResource Type:\\n\\nTechnical Report\\n\\nCountry of Publication:\\n\\nUnited States\\n\\nLanguage:\\n\\nEnglish\\n\\nSubject:\\n\\n59 BASIC BIOLOGICAL SCIENCES; 42 ENGINEERING; 36 MATERIALS SCIENCE; 37 INORGANIC, ORGANIC, PHYSICAL, AND ANALYTICAL CHEMISTRY; ACQUISITION; ARCHITECTURE; DATA MODELING; DATA STREAMING; DATA TRANSMISSION; DATA STORAGE; DATA STORAGE SYSTEMS; DATA TRANSMISSION SYSTEMS; DATABANKS; EFFICIENCY; FILESYSTEMS; FILESYSTEM MANAGEMENT SYSTEMS; INFORMATION MANAGEMENT SYSTEMS; INFORMATION STORED IN A DATABASE SYSTEM OR FILESYSTEM MANAGEMENT SYSTEMS OR FILESYSTEMS AND METHODS FOR MANAGING THE SAME IN A COMMUNICATION NETWORK OF A COMPUTER NETWORK USING A FILESYSTEM MANAGEMENT SYSTEM OR FILESYSTEM MANAGEMENT SYSTEM AND METHOD FOR MANAGING THE SAME ON A COMPUTER NETWORK USING THE FILESYSTEM MANAGEMENT SYSTEM OR FILESYSTEM MANAGEMENT SYSTEM AND METHOD FOR REDUCING OPERATING COST OF THE COMPUTER NETWORK OR REDUCING THE SIZE OF THE COMPUTER NETWORK OF A COMPUTER NETWORK BY DISTRICTING FILES FROM ONE COMPUTERS TO ANOTHER COMPUTERS IN THE COMMUNICATION NETWORK OF THE COMPUTER NETWORK BY USING A FILESYSTEM MANAGEMENT SYSTEM OR FILESYSTEM MANAGEMENT SYSTEM AND METHOD FOR MANAGING FILES FROM ONE COMPUTERS TO ANOTHER COMPUTERS IN THE COMMUNICATION NETWORK OF THE COMPUTER NETWORK BY USING A FILESYSTEM MANAGEMENT SYSTEM OR FILESYSTEM MANAGEMENT SYSTEM AND METHOD FOR MANAGING FILES FROM ONE COMPUTERS TO ANOTHER COMPUTERS IN THE COMMUNICATION NETWORK OF THE COMPUTER NETWORK BY USING A FILESYSTEM MANAGEMENT SYSTEM OR FILESYSTEM MANAGMENT SYSTEM AND METHOD FOR MANAGING FILES FROM ONE COMPUTERS TO ANOTHER COMPUTERS IN THE COMMUNICATION NETWORK OF THE COMPUTER NETWORK BY USING A FILESYSTEM MANAGEMENT SYSTEM OR FILESAFETY MANAGEMENT SYSTEM AND METHOD FOR MANAGING FILES FROM ONE COMPUTERS TO ANOTHER COMPUTERS IN THE COMMUNICATION NETWORK OF THE COMPUTER NETWORK BY USING A FILESAFETY MANAGEMENT SYSYEM.\\n\\nCitation Formats"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\end{longtable}\\n\\n\\subsection{Architecture}\\n\\label{subsec:architecture}\\n\\begin{longtable}{p{1cm}p{1cm}p{2cm}}\\n\\caption[Architectural Decision]{Architectural Decision}\\label{tab:archDecision}\\\\ \\hline\\n\\textbf{Decision} & \\textbf{\\#} & \\textbf{\\#}\\\\ \\hline\\n\\endfirsthead\\n\\caption[]{Architectural Decision}{Architecture Decision}\\label{tab:archDecision}\\\\ \\hline\\n\\textbf{Decision} & \\textbf{\\#} & \\textbf{\\#}\\\\ \\hline\\n\\endhead\\n\\multicolumn{3}{l}{Continued from previous page (if any)}\\\\ \\hline\\n& & \\\\\\nSoftware Architecture  & 6 & 6\\\\ \\nCommunication Architecture  & 6 & 6\\\\ \\nUser Interface Architecture  & 7 & 7\\\\ \\nData Storage and Processing Architecture  & 8&8\\\\ \\nDatabase Architecure (relational)  & 9&9\\\\ \\nData Access API and Service Design (RESTful)   & 10&10\\\\ \\nService Management (e.g., scheduling, monitoring, logging)   & 11&11\\\\ \\nProvisioning (e.g., provisioning of services)   &12&12\\\\ \\nSecurity Architectures   &13&13\\\\ \\nSystem monitoring and alerting   &14&14\\\\ \\nInfrastructure management   &15&15\\\\ \\n\\n\\noindent The decision was made to use a centralized architecture with a RESTful API for the communication and a service management API for provisioning. The RESTful API will be implemented in the backend of SDR because it is simpler to implement than other alternatives. The RESTful API will allow easy integration with third-party services, while the service management API will allow easy management of services across multiple instances. The database architecture will be implemented using PostgreSQL as it is simple to implement and provides good performance compared to other options.\\n\\n\\n\\n\\n\\nThe decision was made to use an architectural style based on the Model-View-Controller pattern. This pattern is well suited for this application because it allows an easy separation of concerns between layers.\\n\\n\\nThe User Interface layer consists of a set of web pages that are accessed via HTTP. This layer is responsible for presenting the user interface to the user.\\nThe Software Architecture layer consists of a set of classes that interact with each other via HTTP. This layer is responsible for handling communication between classes, managing data storage, and managing services.\\nThe Communication Architecture layer consists of a set of classes that interact with each other via synchronous HTTP. This layer is responsible for handling asynchronous communication between services.\\nThe Data Storage and Processing Architecture layer consists of a set of classes that interact with each other via synchronous HTTP. This layer is responsible for storing data in memory or in disk storage. It also manages processing tasks, such as parsing data from disk or memory into objects that can be stored on disk or memory.\\nThe Database Architecure (relational) layer consists of two components: an ORM library and an RDBMS database.\\n\\n\\n\\n\\n\\nThe Data Access API and Service Design (RESTful) layer consists of two components: an Object-relational mapping library and an RDBMS database.\\n\\n\\nThe Service Management layer consists of two components: a service management system and an RDBMS database.\\n\\nThere are several important considerations when implementing each component:\\n* The ORM library must provide enough flexibility so that any class can be converted into another class without losing functionality; this includes converting from one type to another type.\\n* The RDBMS database must provide enough flexibility so that any class can be converted into another class without losing functionality; this includes converting from one type to another type.\\n* The RDBMS database must support transactions; this means transactions must be supported by all types in the DBMS database.\\n* There must be support for caching; caching must be supported by all types in the DBMS database.\\nThe following diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\nThis diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\n\\nThis diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\n\\nThis diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\n\\nThis diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\n\\nThis diagram shows how these considerations relate to each component:\\begin{center}\\includegraphics[width=0.8\\linewidth]{architecture-diagram.png}\\end{center}\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n% TODO: Add more diagrams if necessary\\n\\n%%% Local Variables:\\n%%% mode: latex\\n%%% TeX-master: \"..\/main\"\\n%%% End:\\n"}
{"File Name":"infrastructure-adrs\/0008-separate-dor-index-from-argo-index.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":787,"Predictions":"## Predicted Outcome <!-- required -->\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n## Risk Assessment <!-- required -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n\\n## Decision Drivers <!-- optional --><!-- numbers of drivers can vary --><!-- optional --><!-- numbers of drivers can vary --><!-- optional  --><!-- numbers of drivers can vary -->\\end{itemize}\\end{itemize}\\n"}
{"File Name":"infrastructure-adrs\/0011-use-api-tokens.md","Context":"## Context and Problem Statement <!-- required -->\\nSDR is implemented by way of many different discrete services, some of which make network requests of each other to do their work.  For example, at present:\\n* Argo calls Preservation Catalog to retrieve computed checksums for file content, and to retrieve actual preserved file content.\\n* Many different services use dor-services-app for read and write operations on our digital repository objects.\\nOf course, we want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.\\nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc).  However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).\\n## Decision Drivers <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n","Decision":"* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\nThe infrastructure team came to consensus in a weekly planning meeting that, going forward, we should gate access to API endpoints using JWTs (minted by the service, provided with requests by the client).  This ADR is meant to capture and flesh out that decision.\\n### Positive Consequences <!-- optional -->\\n* More robust and less circumventable than restricting access solely by way of network and firewall configuration.\\n* Does not preclude keeping appropriate firewall restrictions, which should remain part of our security practice.\\n* Should a network re-configuration result in accidental loosening of firewall restrictions, token based authentication provides a robust additional line of access control.\\n* For the ways we've built our applications, token based authentication is at least as easy to implement and maintain as client certificate based authenication, and should provide similarly adequate security.\\n* Minting tokens has become a common approach in industry for authenticating client access to APIs, so there is a wealth of current information and advice available online for this practice.\\n### Negative Consequences <!-- optional -->\\n* More work for developers than solely relying on firewall rules and network configuration (tasks which typically fall to operations and which should happen anyway).\\n","tokens":242,"id":788,"Predictions":"## Decision Criteria <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n\\n## Decision Criteria <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n\\n## Decision Criteria <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n\\n## Decision Criteria <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n"}
{"File Name":"infrastructure-adrs\/0012-dor-datastore.md","Context":"## Context and Problem Statement <!-- required -->\\nAs described in [ADR-0001](0001-migrate-off-fedora-3.md)], it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\\nThe purpose of this ADR is to propose an alternative datastore.\\n## Decision Drivers\\n* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability.\\n* High availability deployment configurations.\\n* Local Docker deployment configurations.\\n* Can be supported by Operations team.\\n* Robust export support (e.g., if needed to exit the technology)\\n","Decision":"* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability.\\n* High availability deployment configurations.\\n* Local Docker deployment configurations.\\n* Can be supported by Operations team.\\n* Robust export support (e.g., if needed to exit the technology)\\nNOT YET.\\n","tokens":280,"id":789,"Predictions":"\\begin{itemize}\\n\\item\\n  Support CRUD and querying\/indexing of JSON data.\\n\\item\\n  Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n\\item\\n  Support transactions.\\n\\item\\n  Currently 2 million digital objects.\\n\\item\\n  Expect significant growth in number of digital objects.\\n\\item\\n  Depending on implementation, may possibly have multiple records per digital object.\\n\\item\\n  Ability to migrate JSON model changes.\\n\\item\\n  Broad community support for datastore and expected long term viability.\\n\\item\\n  High availability deployment configurations.\\n\\item\\n  Local Docker deployment configurations.\\n\\item\\n  Can be supported by Operations team.\\n\\item\\n  Robust export support (e.g., if needed to exit the technology).\\n\\end{itemize}\\n\\n## Decision Drivers\\n\\nSupport CRUD and querying\/indexing of JSON data.\\n\\nSupport for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n\\nSupport transactions.\\n\\nCurrently 2 million digital objects.\\n\\nExpect significant growth in number of digital objects.\\n\\nDepending on implementation, may possibly have multiple records per digital object.\\n\\nAbility to migrate JSON model changes.\\n\\nBroad community support for datastore and expected long term viability.\\n\\nHigh availability deployment configurations.\\n\\nLocal Docker deployment configurations.\\n\\nCan be supported by Operations team.\\n\\nRobust export support (e.g., if needed to exit the technology).\\n\\n## Decision\\n\\n### Example ADRs\\n\\n# ADR-0001 [Migrate Fedora-3](0001-migrate-off-fedora-3.md): [DOR-0001](0001-migrate-off-fedora-3.md): [Migrate Fedora-3](0001-migrate-off-fedora-3.md): ## Context and Problem Statement <!-- required --> As described in [ADR-0001](0001-migrate-off-fedora-3.md), it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\\nThe purpose of this ADR is to propose an alternative datastore.\\n## Decision Drivers\\n* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability.\\n* High availability deployment configurations.\\n* Local Docker deployment configurations.\\n* Can be supported by Operations team.\\n* Robust export support (e.g., if needed to exit the technology)\\n\\n# ADR-0002 [Migrate Fedora-4](0002-migrate-off-fedora-4.md): [DOR-0002](0002-migrate-off-fedora-4.md): ## Context and Problem Statement <!-- required --> As described in [ADR-0001](0001-migrate-off-fedora-3.md), it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\\nThe purpose of this ADR is to propose an alternative datastore.\\n## Decision Drivers\\n* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability,\\nThe purpose of this ADR is to propose an alternative datastore. The proposed datastore will be based on COCINA with some modifications based on experience with other SDR systems that are currently using it or are considering using it. This proposal will also include some minor modifications based on experience with other SDR systems that are currently using it or are considering using it.A \\emph{repository} sits at one end or side of the transit network between two SDRs. It contains all metadata about all those entities stored at both ends; it contains all information about how each entity interacts with each other entity; it contains all information about which entities are stored at which end; etc.This repository will contain all information about all entities stored at both ends; it will contain all information about which entities are stored at which end; etc.This repository will contain all information about all entities stored at both ends; it will contain all information about which entities are stored at which end; etc.This repository will contain all information about all entities stored at both ends; it will contain all information about which entities are stored at which end; etc.This repository will contain all information about all entities stored at both ends; it will contain all information about which entities are stored at which end; etc.This repository will contain all information about all entities stored at both ends; it will contain all information about which entities are stored at which end; etc.This repository will contain only one type: \\emph{digital object}. This repository contains only one type: \\emph{digital object}. This repository contains only one type: \\emph{digital object}.This repository contains only one type: \\emph{digital object}.This repository contains only one type: \\emph{digital object}.This repository contains only one type: \\emph{digital object}.This repository contains only one type: \\emph{digital object}.This repository contains only one type: \\emph{digital object}.This repo"}
{"File Name":"infrastructure-adrs\/0002-extract-only-useful-technical-metadata.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\\n## Decision Drivers <!-- optional -->\\n* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n","Decision":"* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n**Preferred** (by Infrastructure Team) option: option 2, because:\\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\\n* Option 5 has been rejected as a general deployment strategy for now\\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a \u201ctechnical metadata strike team\u201d in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky\/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\\n","tokens":185,"id":790,"Predictions":"\\begin{itemize}\\n\\item Cannot accession large files (objects > 1GB or so)\\n\\n\\item Cannot accession objects with many pages, such as books\\n\\n\\item Blocker for Google Books project\\n\\n\\item Causes extreme delays accessioning other content\\n\\n\\end{itemize}\\n## Decision Drivers <!-- optional -->\\n* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n\\n\\n## Decision Drivers <!-- optional -->\\n* Cannot access large files (objects > 1GB or so)\\n* Cannot access objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n\\n\\n## Decision Drivers <!-- optional -->\\n* Cannot access large files (objects > 1GB or so)\\n* Cannot access objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n\\n\\n## Decision Drivers <!-- optional -->\\n* Cannot access large files (objects > 1GB or so)\\n* Cannot access objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n\\n\\n## Decision Drivers <!-- optional -->\\nCannot access large files (objects > 1GB or so)\\nCannot access objects with many pages, such as books\\nBlocker for Google Books project\\nCauses extreme delays accessioning other content\\n\\n\\n\\n"}
{"File Name":"infrastructure-adrs\/0005-use-deposit-api-in-etd-application.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":791,"Predictions":"## Predicted Outcome <!-- required -->\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n## Risk Assessment <!-- required -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n\\n## Decision Drivers <!-- optional --><!-- numbers of drivers can vary --><!-- optional --><!-- numbers of drivers can vary --><!-- optional  --><!-- numbers of drivers can vary -->\\end{itemize}\\end{itemize}\\n"}
{"File Name":"infrastructure-adrs\/0004-remove-workflow-datastream.md","Context":"## Context and Problem Statement <!-- required -->\\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\\n","Decision":"Remove the datastream.\\nThis was done in dor-services v9.0.0 ([commit](https:\/\/github.com\/sul-dlss\/dor-services\/commit\/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\\n","tokens":39,"id":792,"Predictions":"## Decision Statement <!-- required -->\\nThe SDR is a software that contains the workflow state. The SDR should be stored in a database that is independent of Fedora. \\n\\n## Decision Statement <!-- required -->\\nThe SDR should be stored in a database that is independent of Fedora. \\n\\n## Decision Statement <!-- required -->\\nThe SDR should be stored in a database that is independent of Fedora. \\n\\n## Decision Statement <!-- required -->\\nThe SDR should be stored in a database that is independent of Fedora. \\n\\n## Decision Statement <!-- required -->\\nThe SDR should be stored in a database that is independent of Fedora. \\n\\n## Decision Statement <!-- required -->\\nThe SDR should be stored in a database that is independent of Fedora. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"File Name":"infrastructure-adrs\/0007-change-dor-from-registry-to-repository.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":793,"Predictions":"## Predicted Outcome <!-- required -->\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n## Risk Assessment <!-- required -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n\\n## Decision Drivers <!-- optional --><!-- numbers of drivers can vary --><!-- optional --><!-- numbers of drivers can vary --><!-- optional  --><!-- numbers of drivers can vary -->\\end{itemize}\\end{itemize}\\n"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"Predictions":"## Predicted Outcome <!-- required -->\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n* [The predicted outcome is the desired result of the software.]\\n\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n## Risk Assessment <!-- required -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n[Describe what risk assessment means and what it evaluates. The evaluation should be clear and understandable to someone without knowing anything about your project.]<!-- optional -->\\n\\n## Decision Drivers <!-- optional --><!-- numbers of drivers can vary --><!-- optional --><!-- numbers of drivers can vary --><!-- optional  --><!-- numbers of drivers can vary -->\\end{itemize}\\end{itemize}\\n"}
{"File Name":"infrastructure-adrs\/0006-replace-assembly-wf-with-deposit-api.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently the way to create new objects in the SDR is to:\\n1. Register an object (typically using Argo)\\n1. Move files into the `\/dor` mount\\n1. Run the assembly workflow (pre-assembly?)\\nThe Google Books project will grow SDR by millions of new objects. This growth is an opportunity to inject an API into the SDR architecture that serves as the \"face\" of SDR. Creating an external-facing API allows SDR to accept deposits from on-campus stakeholders such as Big Local News and Open Neuro. Fronting SDR with a new API affords us a mechanism to validate the completeness and correctness of deposited data very early in the accessioning process. Furthermore, we might also use such an API to refactor internal projects (such as ETDs) so these projects no longer require intimate knowledge of the internals of SDR, which reduces coupling.\\n## Decision Drivers <!-- optional -->\\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\n","Decision":"* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\nOption 2: we will create a deposit API as part of work on the 2020 Google Books work cycle.\\n","tokens":289,"id":795,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"infrastructure-adrs\/0001-migrate-off-fedora-3.md","Context":"## Context and Problem Statement <!-- required -->\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\n## Decision Drivers <!-- optional -->\\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\n","Decision":"* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\nNo decision made yet. See status field above.\\n","tokens":241,"id":796,"Predictions":"## Decision Drivers\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n"}
{"File Name":"form-design-system\/file_organization.md","Context":"## Context and Problem Statement\\nWe would like to generally flatten the directory structure in FDS as well as improving the\\nimport statements for client applications. This will be a breaking change for a major\\nversion, as clients will need to update imports once we make the change.\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n","Decision":"- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n### Import statements\\nBecause individual imports for each component has a negative performance impact, we decided\\nto use destructured imports and rely on a tree shaking strategy:\\n```diff\\n-import Flex from '@cbinsights\/fds\/lib\/components\/layout\/Flex';\\n-import FlexItem from '@cbinsights\/fds\/lib\/components\/layout\/FlexItem';\\n-import TextInput from '@cbinsights\/fds\/lib\/components\/forms\/TextInput';\\n+import { Flex, FlexItem, TextInput } from '@cbinsights\/fds\/lib\/components';\\n```\\nThe \"root\" file for each import corresponds to an FDS code module.\\n| code module   | Import example                                                                        |\\n| ------------- | ------------------------------------------------------------------------------------- |\\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from '@cbinsights\/fds\/lib\/dictionary';`    |\\n| `components`  | `import { Avatar, TextInput } from '@cbinsights\/fds\/lib\/components\/';`                |\\n| `icons`       | `import { CloudIcon, BellIcon } from '@cbinsights\/fds\/lib\/icons';`                    |\\n| `assets`      | Use path `@cbinsights\/fds\/lib\/assets\/<file>` where applicable                         |\\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets\/` |\\n### `components` file structure\\nCategorization directories will be removed.\\nEach component will have its own directory under `components\/`.\\nTests, stories, private components, and helpers will be colocated in component\\ndirectories.\\n```\\ncomponents\/\\n\u251c\u2500\u2500 Foo\/\\n\u2502   \u251c\u2500\u2500 FooHelper.jsx\\n\u2502   \u251c\u2500\u2500 foo.test.jsx\\n\u2502   \u251c\u2500\u2500 foo.stories.jsx\\n\u2502   \u251c\u2500\u2500 utils.js\\n\u2502   \u2514\u2500\u2500 index.js  \/\/ contains Foo component code\\n...\\n```\\n### Documentation\\nThe \"Components\" section in Storybook will no longer have categorization. All components\\nwill be documented flat under the section heading.\\n","tokens":109,"id":797,"Predictions":"## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n\\n## Decision\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n\\n## Decision\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n\\n## Decision\\n## Decision Drivers - Reduce number of hierarchies in components dir - Simplify documentation - Improve ergonomics and performance of import statements for client applications - Adopt the unofficial CBI standard of organizing by component directories (each component has its own directory) - Improve ergonomics and performance of import statements for client applications\\n## Decision\\n## Decision Drivers - Reduce number of hierarchies in components dir - Simplify documentation - Improve ergonomics and performance of import statements for client applications - Adopt the unofficial CBI standard of organizing by component directories (each component has its own directory) - Improve ergonomics and performance of import statements for client applications"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"Predictions":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## This record has been generated automatically from input provided by you, or from data generated by your system during operation.\\n\\n\\section{Create Graphs}\\n\\n\\subsection{Graphs}\\n\\label{sec:graph_types}\\n\\nThe graphs that you create can be used as indicators of your system's performance, or as indicators of its stability.\\n\\n\\subsubsection{Performance Indicators}\\n\\label{sec:graph_types_performance}\\n\\nYou can create graphs that represent different aspects of your system's performance, such as:\\n\\n\\begin{itemize}\\n  \\item Number of file accesses per second,\\n  \\item Number of network packets per second,\\n  \\item Number of CPU cycles per second,\\n  \\item Number of memory accesses per second,\\n  \\item Number of database queries per second (time taken),\\n  \\item Number of database queries per second (CPU time),\\n  \\item Number of database queries per second (memory time),\\n  \\item Number of database transactions per second,\\n  \\item Number of active connections in your network,\\n  \\item Number of active connections in your network (time spent),\\n  \\item Percentage load on your system.\\n\\end{itemize}\\n\\nYou can also use graphs to measure how well your system performs over time.\\n\\n\\subsubsection{Stability Indicators}\\n\\label{sec:graph_types_stability}\\n\\nYou can use graphs that represent how well your system performs over time, such as:\\n\\n\\begin{itemize}\\n  \\item Average response time,\\n  \\item Average response latency,\\n  \\item Average response throughput.\\n\\end{itemize}\\n\\nBoth performance and stability indicators can be displayed on one graph.\\n\\nYou can add multiple graphs together, or display them side-by-side.\\n\\nWhen creating graphs, you can choose between using absolute numbers or relative numbers.\\n\\nAbsolute numbers allow you to compare different parts of your system with each other.\\n\\nRelative numbers allow you to compare different parts with each other while still being able to see how they've changed over time.\\n\\nFor example, if you want to compare how many file accesses you get per second with how many network packets you get per second, but still want those numbers displayed relative to each other, then you could use relative numbers instead.\\n\\nIf you want absolute numbers instead, then simply choose whether you want absolute or relative numbers when creating your graph.\\n\\nTo create graphs using absolute numbers:\\n\\n\\begin{enumerate}\\n    % Use % in front of this line if you want absolute number values:\\n    % Absolute number values:\\n    % Write this code:\\n    % Absolute number values:\\n    % Write this code:\\n    % Absolute number values:\\n    % Write this code:\\n    %\\n    # Create Graphs using Absolute Numbers\\n\\n    # Create Graphs using Relative Numbers\\n\\n    # Create Graphs using Absolute Numbers with Relative Numbers\\n\\n    # Create Graphs using Relative Numbers without Relative Numbers\\n\\n    %\\n    # Create Graphs using Absolute Numbers with Relative Numbers\\n\\n    # Create Graphs using Relative Numbers without Relative Numbers\\n\\n    # Create Graphs using Relative Numbers without Relative Numbers\\n\\n    %\\n    \\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n\\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n\\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n\\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n\\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n\\n% You may also need this line if you're not sure what type(s) of graph(s) you need:\\n"}
{"File Name":"form-design-system\/remove_mui_theme.md","Context":"## Context and Problem Statement\\nThe [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, we\\naim to provide component coverage so that in the future, consumers will no longer need to\\nrely on material-ui.\\nThe FDS node module however, still provides a `mui-theme` theming object for the\\nmaterial-ui library.\\n","Decision":"Removed `mui-theme` from FDS.\\n### Negative Consequences <!-- optional -->\\n- Consumers will need to handle the breaking change. Consumers are now responsible for\\ndefining their own material-ui base theme.\\n","tokens":78,"id":799,"Predictions":"\\nThe decision is to provide a `mui-theme` object for the material-ui library.\\n\\n\\nThis means that in the future, consumers will still be able to use the `mui-theme` object\\nfor their own purposes. However, the decision was made to provide a \\code{MuiTheme} object as\\na default.\\n\\n\\end{document}\\n"}
{"File Name":"form-design-system\/reasonable_utility_classes.md","Context":"## Context and Problem Statement\\nWe take a utility class based approach (aka \"functional CSS\" aka \"expressive CSS\") in FDS.\\nWe have a lot of utility classes that do only one thing (e.g. `display--none`).\\nAt one point, we discussed adding a `position--relative` class, which led us to a key question\\nwe needed to clarify:\\n> When does it make sense to add a utility class?\\n","Decision":"Chosen option: \"Only add utility classes that are useful on their own\"\\n### Example of a utility class that is not \"useful on its own\"\\n```html\\n<div class=\"position--relative\">\\n<div>\\nIf this element is absolutely positioned, we need additional ad-hoc CSS rules beyond\\nwhat the utility class above provides. `position--relative` is not helpful on its own.\\n<\/div>\\n<\/div>\\n```\\n### Example of a utility class that is \"useful on its own\"\\n```html\\n<div class=\"color--red\">\\n<div>\\nThis text will be red. No additional CSS rules are needed for the above utility class\\nto be useful.\\n<\/div>\\n<\/div>\\n```\\n","tokens":95,"id":800,"Predictions":"\\begin{itemize}\\n\\item[] We decide to add a utility class, `position--relative` (aka \"dynamic positioning\").\\n\\end{itemize}\\n\\begin{center}\\includegraphics[scale=0.4]{images\/decision.png}\\end{center}\\n\\n## Decision\\n\\begin{itemize}\\n\\item[] We decide to add a utility class, `position--relative` (aka \"dynamic positioning\").\\n\\end{itemize}\\n\\begin{center}\\includegraphics[scale=0.4]{images\/decision.png}\\end{center}\\n\\n## Decision\\n\\begin{itemize}\\n\\item[] We decide to add a utility class, `position--relative` (aka \"dynamic positioning\").\\n\\end{itemize}\\n\\begin{center}\\includegraphics[scale=0.4]{images\/decision.png}\\end{center}\\n\\n## Decision\\n\\begin{itemize}\\n\\item[] We decide to add a utility class, `position--relative` (aka \"dynamic positioning\").\\n\\end{itemize}\\n\\begin{center}\\includegraphics[scale=0.4]{images\/decision.png}\\end{center}\\n\\n## Decision\\n\\begin{itemize}\\n\\item[] We decide to add a utility class, `position--relative` (aka \"dynamic positioning\").\\n\\end{itemize}\\n\\noindent The key question is: When does it make sense to add a utility class?\\n\\nThe answer is: when we have multiple utilities.\\n\\nWe need more than one utility in order to support different layout scenarios.\\nFor example:\\n\\n1) When we have an element whose content is relatively positioned and we want the element's content to move up and down.\\n2) When we have an element whose content is relatively positioned and we want the element's content to be fixed in place.\\n3) When we have an element whose content is relatively positioned and we want the element's position to be absolute.\\n4) When we have an element whose content is positioned relative but we want the element's position to be relative.\\n5) When we have an element whose content is positioned relative and we want the element's position to be absolute.\\n6) When we have multiple elements that are positioned relative and want them all to move up and down at the same time.\\n"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"Predictions":"## Decision Statement\\nWe'd like to change the spacing system to something more generic.\\n\\n\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\\n## Decision Statement\\nWe'd like to change the spacing system to something more generic.\\n\\n\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed t"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"Predictions":"We have decided that the following types of components should be able to override the\\nclassNames prop:\\n\\begin{itemize}\\n\\item \\textbf{Form} - \\texttt{<form className={styles.form} \/>}\\n\\item \\textbf{Container} - \\texttt{<div className={styles.container} \/>}\\n\\item \\textbf{Card} - \\texttt{<card className={styles.card} \/>}\\n\\item \\textbf{Grid} - \\texttt{<grid className={styles.grid} \/>}\\n\\end{itemize}\\n\\noindent\\nWe have also decided that the following types of components should not be able to override\\nthe classNames prop:\\n\\begin{itemize}\\n\\item \\textbf{Table} - \\texttt{<table className={styles.table} \/>}\\n\\end{itemize}\\n\\noindent\\nThe decision is made based on a number of factors, including:\\n\\begin{enumerate}\\n  \\setlength{\\topsep}{0pt}\\n  %\\setlength{\\parskip}{0pt}\\n  %\\setlength{\\parsep}{0pt}\\n  %\\setlength{\\topsep}{0pt}\\n  %\\setlength{\\parskip}{0pt}\\n  %\\setlength{\\parsep}{0pt}\\n\\n  The types of components that can override the classNames prop are:\\n    \\begin{enumerate}\\n      \\setlength{\\topsep}{0pt}\\n      %\\setlength{\\parskip}{0pt}\\n\\n      The types of components that cannot override the classNames prop are:\\n        \\begin{enumerate}[label=\\alph*., leftmargin=*]\\n          \\setlength{\\topsep}{0pt}\\n\\n          The types of components that can override the classNames prop are:\\n            \\begin{enumerate}[label=\\alph*.\\arabic*., leftmargin=*]\\n              %%%%\\n              The types of components that cannot override the classNames prop are:\\n              %%%%\\n              The types of components that can override the classNames prop are:\\n                \\begin{enumerate}[label=\\alph*.\\arabic*., leftmargin=*]\\n                  %%%%\\n                  The types of components that cannot override the classNames prop are:\\n                    The types of components that can override the classNames prop are:\\n                      %%%\\n                      %%%\\n                    %%%\\n                      %%%\\n                      %%%\\n                  %%%\\n                  %%%\\n                %%%\\n\\n                %%%\\n                %%%\\n\\n                %%%\\n\\n                %%%\\n\\n                %%\\n                %%\\n                %%\\n                %%\\n            %%\\n            %%\\n            %%\\n            %%\\n            %%\\n        %\\n        %\\n        %\\n        %\\n        %\\n    %\\n    %\\n    %\\n    %\\n\\n    %\\n\\n    %\\n\\n    %\\n\\n    %\\n    %\\n\\n    %\\n\\n    %\\n\\n    %\\n\\n    %\\n\\n    %\\n    \\n   %\\n   %\\n   %\\n   %\\n\\n   %\\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n\\n    \\n\\n\\n\\n\\n      %%%%\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n      %%%%\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n    \\n    \\n  \\n    \\n\\n    \\n  \\n    \\n\\n    \\n  \\n    \\n\\n    \\n    \\n    \\n\\n\\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n\\n    \\n\\n\\n    \\n    \\n    \\n    \\n    \\n\\n    \\n    \\n    \\n\\n    \\n    \\n      \\n\\n  \\n## Decision\\nWe have decided to allow for arbitrary styling on form inputs. This means we will allow for\\nstyle overrides on inputs with a `form` class, but not on inputs with a `form` class. We also\\nallow style overrides on `Grid` and `Card`. We have also decided to allow for style overrides on\\ntables and grids. Finally, we allow for style overrides on tables and grids. These overrides\\nare allowed regardless if they follow a `form` or not.\\n"}
{"File Name":"form-design-system\/avoid_spread_props.md","Context":"## Context and Problem Statement\\nWhen should we spread props in components?\\n**Example**\\n```\\nconst Component = ({ foo, bar, ...restProps}) => {\\n<element foo={foo} bar={bar} {...restProps} \/>\\n}\\n```\\nIf we spread rest props on every component, we create an opportunity for developers to\\ncustomize components via code instead of explicitly defined props.\\nFor components that render attribute-heavy DOM elements (like `button`), it may make\\nsense to spread props. For more complex components, spreading props could lead to bugs,\\nside effects, and unnecessary customization.\\n","Decision":"Chosen option: \"Only components that render attribute-heavy DOM elements spread rest props\", because...\\n- Compromise that allows us to create the right prop interface for the job of each component\\n- Components that conceptually map to attribute-heavy DOM elements (i.e. html form elements)\\nwill allow spread props to keep the explicitly defined props easier to reason about.\\n- Components that **do not** conceptually map to attribute-heavy DOM elements (e.g. `Toast`)\\nwill be able to continue to define an explicit prop interface instead of spreading props.\\n### Negative Consequences <!-- optional -->\\n- Not all components will behave the same way.\\n- **We must document when a component supports spread props**.\\n","tokens":133,"id":803,"Predictions":"## Decision Statement\\nThe decision is to spread props on every component.\\nThis decision is made based on the problem statement given in the Problem Statement section.\\nThe decision has been made because spreading props will lead to customization and\\ncustomization will lead to bugs, side effects, and unnecessary customization.\\n\\n## Context\\n## Context Statement\\nThe context statement is that we spread props on every component. This context statement is made because we want to spread props so that developers can customize components via code instead of explicitly defined props. If we spread props on every component, we create an opportunity for developers to customize components via code instead of explicitly defined props. For components that render attribute-heavy DOM elements (like button), it may make sense to spread props. For more complex components, spreading props could lead to bugs, side effects, and unnecessary customization.\\n\\end{document}\\n"}
{"File Name":"cosmos-sdk\/adr-024-coin-metadata.md","Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"The `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"name\": \"atom\",\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n* The string \"atom\" can be used as a display name in a list of tokens.\\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","tokens":336,"id":804,"Predictions":"## Yes\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe Cosmos Hub (Gaia) is a peer-to-peer network where users are able to exchange assets without any\\ncentral authority. The Cosmos Hub (Gaia) is designed to be as decentralized as possible, and so it is\\nexpected that the Cosmos SDK will be decentralized as well. It is therefore expected that the Cosmos SDK will\\nbe decentralized.\\nThe Cosmos SDK stores all information pertaining to an account in a `Coins` type that consists of an\\n`amount` and a `denom`, where `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an account-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps track of all balances for all accounts and also keeps track of the total supply of balances in an application.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based chain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by the time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single unit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and operations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and as a result the total amount of asset types increases. We propose to have x\/bank additionally keep track of metadata per denom in order to help clients, wallet providers, and explorers improve their UX and remove the requirement for making any assumptions on the unit of denomination.\\n\\nThis record has been archived.\\n"}
{"File Name":"cosmos-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the concept of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n* Membership Admittance\\n* Membership Acceptance\\n* Membership Revocation\\n* (probably) Without Penalty\\n* member steps down (self-Revocation)\\n* replaced by new member from governance\\n* (probably) With Penalty\\n* due to breach of soft-agreement (determined through governance)\\n* due to breach of hard-agreement (determined by code)\\n* Execution of Duties\\n* Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n* Compensation\\n* Group compensation (further distribution decided by the specialization group)\\n* Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admission to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possibilities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":805,"Predictions":"\\nDecision\\nThe decision is to create a decentralized Computer Emergency Response Team\\n(dCERT) which would be made up of members who are elected by a governing\\ncommunity. The dCERT would be a decentralized group which would fulfill the\\nrole of coordinating the community under emergency situations. This decision\\nis intended to allow for the development of specialized groups within the\\nblockchain community, such as code auditing, emergency response, code\\ndevelopment etc. These groups could also be used to enable a certain level\\nof delegated responsibilities.\\n\\n## Context\\nThe idea was first conceived in order to fulfill the use case of the creation of a\\ndecentralized Computer Emergency Response Team (dCERT), whose members would be\\nelected by a governing community and would fulfill the role of coordinating the\\ncommunity under emergency situations. This thinking can be further abstracted into the\\nconcept of \"blockchain specialization groups\". The creation of these groups are the beginning of specialization capabilities within a wider blockchain community which could be used to enable a certain level of delegated responsibilities.\\n\\n\\n\\end{document}"}
{"File Name":"cosmos-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user\u2019s machine.\\n","tokens":215,"id":806,"Predictions":"## Decision\\nThe proposed implementation will provide a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\nThe implementation will be implemented in JavaScript, so it is compatible with most modern desktop computers OS (Ubuntu, Debian, MacOS, Windows).\\nThe implementation will provide a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\nThe implementation will be implemented in JavaScript, so it is compatible with most modern desktop computers OS (Ubuntu, Debian, MacOS, Windows).\\nThe implementation will provide a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\nThe implementation will be implemented in JavaScript, so it is compatible with most modern desktop computers OS (Ubuntu, Debian, MacOS, Windows).\\nThe implementation will provide a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\\n## Explanation of Decision\\n## Explanation\\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\\n## Rationale\\n## Rationale\\nThis proposal provides an abstraction layer over existing mechanisms that allow applications to store their information securely. The proposed solution enables applications to choose between storing their information locally or remotely on multiple services without requiring any extra configuration steps or additional software.\\n"}
{"File Name":"cosmos-sdk\/adr-042-group-module.md","Context":"## Context\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\\n* Thresholds can't be changed.\\n* UX is cumbersome for non-technical users ([#5661](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5661)).\\n* It requires `legacy_amino` sign mode ([#8141](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8141)).\\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\\nIt's meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md) to simplify key management for individuals and organizations.\\n","Decision":"We propose merging the `x\/group` module with its supporting ORM\/Table Store package ([#7098](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7098)) into the Cosmos SDK and continuing development here. There will be a dedicated ADR for the ORM package.\\n### Group\\nA group is a composition of accounts with associated weights. It is not\\nan account and doesn't have a balance. It doesn't in and of itself have any\\nsort of voting or decision weight.\\nGroup members can create proposals and vote on them through group accounts using different decision policies.\\nIt has an `admin` account which can manage members in the group, update the group\\nmetadata and set a new admin.\\n```protobuf\\nmessage GroupInfo {\\n\/\/ group_id is the unique ID of this group.\\nuint64 group_id = 1;\\n\/\/ admin is the account address of the group's admin.\\nstring admin = 2;\\n\/\/ metadata is any arbitrary metadata to attached to the group.\\nbytes metadata = 3;\\n\/\/ version is used to track changes to a group's membership structure that\\n\/\/ would break existing proposals. Whenever a member weight has changed,\\n\/\/ or any member is added or removed, the version is incremented and will\\n\/\/ invalidate all proposals from older versions.\\nuint64 version = 4;\\n\/\/ total_weight is the sum of the group members' weights.\\nstring total_weight = 5;\\n}\\n```\\n```protobuf\\nmessage GroupMember {\\n\/\/ group_id is the unique ID of the group.\\nuint64 group_id = 1;\\n\/\/ member is the member data.\\nMember member = 2;\\n}\\n\/\/ Member represents a group member with an account address,\\n\/\/ non-zero weight and metadata.\\nmessage Member {\\n\/\/ address is the member's account address.\\nstring address = 1;\\n\/\/ weight is the member's voting weight that should be greater than 0.\\nstring weight = 2;\\n\/\/ metadata is any arbitrary metadata to attached to the member.\\nbytes metadata = 3;\\n}\\n```\\n### Group Account\\nA group account is an account associated with a group and a decision policy.\\nA group account does have a balance.\\nGroup accounts are abstracted from groups because a single group may have\\nmultiple decision policies for different types of actions. Managing group\\nmembership separately from decision policies results in the least overhead\\nand keeps membership consistent across different policies. The pattern that\\nis recommended is to have a single master group account for a given group,\\nand then to create separate group accounts with different decision policies\\nand delegate the desired permissions from the master account to\\nthose \"sub-accounts\" using the [`x\/authz` module](adr-030-authz-module.md).\\n```protobuf\\nmessage GroupAccountInfo {\\n\/\/ address is the group account address.\\nstring address = 1;\\n\/\/ group_id is the ID of the Group the GroupAccount belongs to.\\nuint64 group_id = 2;\\n\/\/ admin is the account address of the group admin.\\nstring admin = 3;\\n\/\/ metadata is any arbitrary metadata of this group account.\\nbytes metadata = 4;\\n\/\/ version is used to track changes to a group's GroupAccountInfo structure that\\n\/\/ invalidates active proposal from old versions.\\nuint64 version = 5;\\n\/\/ decision_policy specifies the group account's decision policy.\\ngoogle.protobuf.Any decision_policy = 6 [(cosmos_proto.accepts_interface) = \"cosmos.group.v1.DecisionPolicy\"];\\n}\\n```\\nSimilarly to a group admin, a group account admin can update its metadata, decision policy or set a new group account admin.\\nA group account can also be an admin or a member of a group.\\nFor instance, a group admin could be another group account which could \"elects\" the members or it could be the same group that elects itself.\\n### Decision Policy\\nA decision policy is the mechanism by which members of a group can vote on\\nproposals.\\nAll decision policies should have a minimum and maximum voting window.\\nThe minimum voting window is the minimum duration that must pass in order\\nfor a proposal to potentially pass, and it may be set to 0. The maximum voting\\nwindow is the maximum time that a proposal may be voted on and executed if\\nit reached enough support before it is closed.\\nBoth of these values must be less than a chain-wide max voting window parameter.\\nWe define the `DecisionPolicy` interface that all decision policies must implement:\\n```go\\ntype DecisionPolicy interface {\\ncodec.ProtoMarshaler\\nValidateBasic() error\\nGetTimeout() types.Duration\\nAllow(tally Tally, totalPower string, votingDuration time.Duration) (DecisionPolicyResult, error)\\nValidate(g GroupInfo) error\\n}\\ntype DecisionPolicyResult struct {\\nAllow bool\\nFinal bool\\n}\\n```\\n#### Threshold decision policy\\nA threshold decision policy defines a minimum support votes (_yes_), based on a tally\\nof voter weights, for a proposal to pass. For\\nthis decision policy, abstain and veto are treated as no support (_no_).\\n```protobuf\\nmessage ThresholdDecisionPolicy {\\n\/\/ threshold is the minimum weighted sum of support votes for a proposal to succeed.\\nstring threshold = 1;\\n\/\/ voting_period is the duration from submission of a proposal to the end of voting period\\n\/\/ Within this period, votes and exec messages can be submitted.\\ngoogle.protobuf.Duration voting_period = 2 [(gogoproto.nullable) = false];\\n}\\n```\\n### Proposal\\nAny member of a group can submit a proposal for a group account to decide upon.\\nA proposal consists of a set of `sdk.Msg`s that will be executed if the proposal\\npasses as well as any metadata associated with the proposal. These `sdk.Msg`s get validated as part of the `Msg\/CreateProposal` request validation. They should also have their signer set as the group account.\\nInternally, a proposal also tracks:\\n* its current `Status`: submitted, closed or aborted\\n* its `Result`: unfinalized, accepted or rejected\\n* its `VoteState` in the form of a `Tally`, which is calculated on new votes and when executing the proposal.\\n```protobuf\\n\/\/ Tally represents the sum of weighted votes.\\nmessage Tally {\\noption (gogoproto.goproto_getters) = false;\\n\/\/ yes_count is the weighted sum of yes votes.\\nstring yes_count = 1;\\n\/\/ no_count is the weighted sum of no votes.\\nstring no_count = 2;\\n\/\/ abstain_count is the weighted sum of abstainers.\\nstring abstain_count = 3;\\n\/\/ veto_count is the weighted sum of vetoes.\\nstring veto_count = 4;\\n}\\n```\\n### Voting\\nMembers of a group can vote on proposals. There are four choices to choose while voting - yes, no, abstain and veto. Not\\nall decision policies will support them. Votes can contain some optional metadata.\\nIn the current implementation, the voting window begins as soon as a proposal\\nis submitted.\\nVoting internally updates the proposal `VoteState` as well as `Status` and `Result` if needed.\\n### Executing Proposals\\nProposals will not be automatically executed by the chain in this current design,\\nbut rather a user must submit a `Msg\/Exec` transaction to attempt to execute the\\nproposal based on the current votes and decision policy. A future upgrade could\\nautomate this and have the group account (or a fee granter) pay.\\n#### Changing Group Membership\\nIn the current implementation, updating a group or a group account after submitting a proposal will make it invalid. It will simply fail if someone calls `Msg\/Exec` and will eventually be garbage collected.\\n### Notes on current implementation\\nThis section outlines the current implementation used in the proof of concept of the group module but this could be subject to changes and iterated on.\\n#### ORM\\nThe [ORM package](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9156) defines tables, sequences and secondary indexes which are used in the group module.\\nGroups are stored in state as part of a `groupTable`, the `group_id` being an auto-increment integer. Group members are stored in a `groupMemberTable`.\\nGroup accounts are stored in a `groupAccountTable`. The group account address is generated based on an auto-increment integer which is used to derive the group module `RootModuleKey` into a `DerivedModuleKey`, as stated in [ADR-033](adr-033-protobuf-inter-module-comm.md#modulekeys-and-moduleids). The group account is added as a new `ModuleAccount` through `x\/auth`.\\nProposals are stored as part of the `proposalTable` using the `Proposal` type. The `proposal_id` is an auto-increment integer.\\nVotes are stored in the `voteTable`. The primary key is based on the vote's `proposal_id` and `voter` account address.\\n#### ADR-033 to route proposal messages\\nInter-module communication introduced by [ADR-033](adr-033-protobuf-inter-module-comm.md) can be used to route a proposal's messages using the `DerivedModuleKey` corresponding to the proposal's group account.\\n","tokens":231,"id":807,"Predictions":"## Decision\\nThe legacy amino multi-signature mechanism has limitations that make it less suitable for general use. The group module can solve these issues by providing a more flexible key management system, where keys can be added, updated or removed, as well as configurable thresholds. It is meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md).\\n## Context\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\\n* Thresholds can't be changed.\\n* UX is cumbersome for non-technical users ([#5661](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5661)).\\n* It requires `legacy_amino` sign mode ([#8141](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8141)).\\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\\nIt's meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md).\\n## Decision\\n## Decision\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations that make it less suitable for general use. The group module can solve these issues by providing a more flexible key management system, where keys can be added, updated or removed, as well as configurable thresholds. It is meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md).\\n## Context\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations that make it less suitable for general use. The group module can solve these issues by providing a more flexible key management system, where keys can be added, updated or removed, as well as configurable thresholds. It is meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md).\\n## Decision\\n## Decision\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations that make it less suitable for general use. The group module can solve these issues by providing a more flexible key management system, where keys can be added, updated or removed, as well as configurable thresholds. It is meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md).\\n## Context\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations that make it less suitable for general use. The group module can solve these issues by providing a more flexible key management system, where keys can be added, updated or removed, as well as configurable thresholds.\\n"}
{"File Name":"cosmos-sdk\/adr-045-check-delivertx-middlewares.md","Context":"## Context\\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9406) and [refunding unused gas](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9182)), which includes making BaseApp more lightweight and modular.\\n","Decision":"We decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\\nThe two following interfaces are the base of the middleware design, and are defined in `types\/tx`:\\n```go\\ntype Handler interface {\\nCheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\\nDeliverTx(ctx context.Context, req Request) (Response, error)\\nSimulateTx(ctx context.Context, req Request (Response, error)\\n}\\ntype Middleware func(Handler) Handler\\n```\\nwhere we define the following arguments and return types:\\n```go\\ntype Request struct {\\nTx      sdk.Tx\\nTxBytes []byte\\n}\\ntype Response struct {\\nGasWanted uint64\\nGasUsed   uint64\\n\/\/ MsgResponses is an array containing each Msg service handler's response\\n\/\/ type, packed in an Any. This will get proto-serialized into the `Data` field\\n\/\/ in the ABCI Check\/DeliverTx responses.\\nMsgResponses []*codectypes.Any\\nLog          string\\nEvents       []abci.Event\\n}\\ntype RequestCheckTx struct {\\nType abci.CheckTxType\\n}\\ntype ResponseCheckTx struct {\\nPriority int64\\n}\\n```\\nPlease note that because CheckTx handles separate logic related to mempool priotization, its signature is different than DeliverTx and SimulateTx.\\nBaseApp holds a reference to a `tx.Handler`:\\n```go\\ntype BaseApp  struct {\\n\/\/ other fields\\ntxHandler tx.Handler\\n}\\n```\\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\\n```go\\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\\nvar abciRes abci.ResponseDeliverTx\\nctx := app.getContextForTx(runTxModeDeliver, req.Tx)\\nres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\\nif err != nil {\\nabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\nreturn abciRes\\n}\\nabciRes, err = convertTxResponseToDeliverTx(res)\\nif err != nil {\\nreturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\n}\\nreturn abciRes\\n}\\n\/\/ convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\\ndata, err := makeABCIData(txRes)\\nif err != nil {\\nreturn abci.ResponseDeliverTx{}, nil\\n}\\nreturn abci.ResponseDeliverTx{\\nData:   data,\\nLog:    txRes.Log,\\nEvents: txRes.Events,\\n}, nil\\n}\\n\/\/ makeABCIData generates the Data field to be sent to ABCI Check\/DeliverTx.\\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\\nreturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\\n}\\n```\\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\\n### Implementing a Middleware\\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\\n```go\\n\/\/ myTxHandler is the tx.Handler of this middleware. Note that it holds a\\n\/\/ reference to the next tx.Handler in the stack.\\ntype myTxHandler struct {\\n\/\/ next is the next tx.Handler in the middleware stack.\\nnext tx.Handler\\n\/\/ some other fields that are relevant to the middleware can be added here\\n}\\n\/\/ NewMyMiddleware returns a middleware that does this and that.\\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\\nreturn func (txh tx.Handler) tx.Handler {\\nreturn myTxHandler{\\nnext: txh,\\n\/\/ optionally, set arg1, arg2... if they are needed in the middleware\\n}\\n}\\n}\\n\/\/ Assert myTxHandler is a tx.Handler.\\nvar _ tx.Handler = myTxHandler{}\\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\\n\/\/ CheckTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\\n\/\/ CheckTx specific post-processing logic\\nreturn res, checkRes, err\\n}\\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ DeliverTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.DeliverTx(ctx, tx, req)\\n\/\/ DeliverTx specific post-processing logic\\nreturn res, err\\n}\\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ SimulateTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.SimulateTx(ctx, tx, req)\\n\/\/ SimulateTx specific post-processing logic\\nreturn res, err\\n}\\n```\\n### Composing Middlewares\\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\\nThen, the app developer can compose multiple middlewares on top on the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\\n```text\\nA.pre\\nB.pre\\nC.pre\\nH # The base tx.handler, for example `RunMsgsTxHandler`\\nC.post\\nB.post\\nA.post\\n```\\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the \"outer to inner\" order. For the above stack, the final `tx.Handler` is:\\n```go\\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\\n```\\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\\n```go\\n\/\/ simapp\/app.go\\ntxHandler := middleware.ComposeMiddlewares(...)\\napp.SetTxHandler(txHandler)\\n```\\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\\n### Middlewares Maintained by the Cosmos SDK\\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin\/EndBlock events)                                                                                                                                                                                                                                                                         |\\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](.\/adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\\n| GasTxMiddleware         | This replaces the [`Setup`](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/v0.43.0\/x\/auth\/ante\/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\\n### Similarities and Differences between Antehandlers and Middlewares\\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](.\/adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the \"Simple Decorators\" approach, the middleware design is actually very similar to the other [Decorator Pattern](.\/adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https:\/\/github.com\/iov-one\/weave).\\n#### Similarities with Antehandlers\\n* Designed as chaining\/composing small modular pieces.\\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\\n* Set up in `app.go`, and easily customizable by app developers.\\n* Order is important.\\n#### Differences with Antehandlers\\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\\n","tokens":190,"id":808,"Predictions":"The Decision is the following:\\n\\t\\t\\t\\t\\t\\n```\\n@Decision\\n@Precondition\\n@Condition\\n@Action\\n```\\n## Context\\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9406) and [refunding unused gas](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9182)), which includes making BaseApp more lightweight and modular.\\n\\n\\nThe decision is as follows:\\n\\t\\t\\t\\t\\t\\n```\\n@Decision\\n@Precondition\\n@Condition\\n@Action\\n```\\n## Context\\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9406) and [refunding unused gas](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9182)), which includes making BaseApp more lightweight and modular.\\n\\n\\nThe decision is as follows:\\n\\t\\t\\t\\t\\t\\n```\\nExample: The following code snippet shows how you can use a single base application for multiple clients (i.e., multiple clients can connect to a single base application). This example has two clients (client 1 and client 2), who each use different base application IDs (i.e., one uses a BaseApplicationID of 12345 and other uses a BaseApplicationID of 23456). The client ID (client ID 1) has already been registered with Cosmos via an API call.\\n\\begin{lstlisting}\\nconst BASE_APPLICATION_ID = '1234567890';\\nconst CLIENT_ID = '1';\\n\\nconst BASE_APPLICATION = new Cosmos.Builder()\\n    .baseApplicationId(BASE_APPLICATION_ID)\\n    .build();\\n\\nconst client = new Cosmos.Builder()\\n    .baseApplicationId(BASE_APPLICATION_ID)\\n    .client(clientId)\\n    .build();\\n\\nclient.connect();\\n\\end{lstlisting}\\n"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"Predictions":"## Decision\\n\\nThe decision for the encoding of queries follows from the context and is as follows:\\n\\begin{enumerate}\\n\\item The Cosmos SDK is used by the client-side of a blockchain-based application and the client-side requires a query to be able to execute queries to perform operations on the blockchain. The Cosmos SDK is used by a blockchain-based application and requires this query to be able to execute queries to perform operations on the blockchain.\\n\\item Queries are encoded using protobuf, which is a binary format for representing binary data. Queries are encoded using protobuf, which is a binary format for representing binary data.\\n\\item The Cosmos SDK uses protobuf-encoding for its query encoding. The Cosmos SDK uses protobuf-encoding for its query encoding.\\n\\item The client-side of a blockchain-based application requires querying the blockchain in order to perform operations on it. The client-side of a blockchain-based application requires querying the blockchain in order to perform operations on it.\\n\\end{enumerate}\\n\\n\\n## ADR 019\\n## ADR 019\\n\\nThis ADR defines how we can encode queries in protobuf-encoding.\\n\\nThe first part of this ADR defines how we can encode queries in protobuf-encoding.\\nWe define an encoder that encodes queries into protobufs, which are then encoded using protobuf-encoding.\\nThis is done because we want our query encoding to be compatible with our serialization scheme.\\nThe second part of this ADR defines how we can encode queries in proto-encoding.\\nWe define an encoder that encodes queries into proto-encoding.\\nThis is done because we want our query encoding to be compatible with our serialization scheme.\\n"}
{"File Name":"cosmos-sdk\/adr-046-module-params.md","Context":"## Context\\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\\n`x\/params` module. The `x\/params` works by having modules define parameters,\\ntypically via a simple `Params` structure, and registering that structure in\\nthe `x\/params` module via a unique `Subspace` that belongs to the respective\\nregistering module. The registering module then has unique access to its respective\\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\\nstructure.\\nIn addition, the Cosmos SDK's `x\/gov` module has direct support for changing\\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\\nstakeholders can vote on suggested parameter changes.\\nThere are various tradeoffs to using the `x\/params` module to manage individual\\nmodule parameters. Namely, managing parameters essentially comes for \"free\" in\\nthat developers only need to define the `Params` struct, the `Subspace`, and the\\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\\nthere are some notable drawbacks. These drawbacks include the fact that parameters\\nare serialized in state via JSON which is extremely slow. In addition, parameter\\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\\nor writing to state. In other words, it is currently not possible to have any\\nstate transitions in the application during an attempt to change param(s).\\n","Decision":"We will build off of the alignment of `x\/gov` and `x\/authz` work per\\n[#9810](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/9810). Namely, module developers\\nwill create one or more unique parameter data structures that must be serialized\\nto state. The Param data structures must implement `sdk.Msg` interface with respective\\nProtobuf Msg service method which will validate and update the parameters with all\\nnecessary changes. The `x\/gov` module via the work done in\\n[#9810](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/9810), will dispatch Param\\nmessages, which will be handled by Protobuf Msg services.\\nNote, it is up to developers to decide how to structure their parameters and\\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\\n`x\/auth` using the `x\/params` module for parameter management:\\n```protobuf\\nmessage Params {\\nuint64 max_memo_characters       = 1;\\nuint64 tx_sig_limit              = 2;\\nuint64 tx_size_cost_per_byte     = 3;\\nuint64 sig_verify_cost_ed25519   = 4;\\nuint64 sig_verify_cost_secp256k1 = 5;\\n}\\n```\\nDevelopers can choose to either create a unique data structure for every field in\\n`Params` or they can create a single `Params` structure as outlined above in the\\ncase of `x\/auth`.\\nIn the former, `x\/params`, approach, a `sdk.Msg` would need to be created for every single\\nfield along with a handler. This can become burdensome if there are a lot of\\nparameter fields. In the latter case, there is only a single data structure and\\nthus only a single message handler, however, the message handler might have to be\\nmore sophisticated in that it might need to understand what parameters are being\\nchanged vs what parameters are untouched.\\nParams change proposals are made using the `x\/gov` module. Execution is done through\\n`x\/authz` authorization to the root `x\/gov` module's account.\\nContinuing to use `x\/auth`, we demonstrate a more complete example:\\n```go\\ntype Params struct {\\nMaxMemoCharacters      uint64\\nTxSigLimit             uint64\\nTxSizeCostPerByte      uint64\\nSigVerifyCostED25519   uint64\\nSigVerifyCostSecp256k1 uint64\\n}\\ntype MsgUpdateParams struct {\\nMaxMemoCharacters      uint64\\nTxSigLimit             uint64\\nTxSizeCostPerByte      uint64\\nSigVerifyCostED25519   uint64\\nSigVerifyCostSecp256k1 uint64\\n}\\ntype MsgUpdateParamsResponse struct {}\\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n\/\/ verification logic...\\n\/\/ persist params\\nparams := ParamsFromMsg(msg)\\nms.SaveParams(ctx, params)\\nreturn &types.MsgUpdateParamsResponse{}, nil\\n}\\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\\n\/\/ ...\\n}\\n```\\nA gRPC `Service` query should also be provided, for example:\\n```protobuf\\nservice Query {\\n\/\/ ...\\nrpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\\noption (google.api.http).get = \"\/cosmos\/<module>\/v1beta1\/params\";\\n}\\n}\\nmessage QueryParamsResponse {\\nParams params = 1 [(gogoproto.nullable) = false];\\n}\\n```\\n","tokens":317,"id":811,"Predictions":"## \\begin{itemize}\\n## \\item The Cosmos SDK's `x\/gov` module should support the use of parameters via a\\n`Params` struct and `ParamsSubspace`.\\n## \\end{itemize}\\n## \\begin{itemize}\\n## \\item The Cosmos SDK's `x\/gov` module should allow for the use of parameters via\\na `Params` struct and a `ParamsSubspace`.\\n## \\end{itemize}\\n## \\begin{itemize}\\n## \\item The Cosmos SDK's `x\/gov` module should allow for the use of parameters via\\na `Params` struct and a `ParamsSubspace` that is registered in the\\n`x\/params` module.\\n\\end{itemize}\\n\\n\\subsubsection{Rationale}\\n\\nThe Cosmos SDK has a need to support multiple types of parameters. Currently, it only supports one type, which is the X\/Params type. In addition, there are some issues with using the X\/Params type. For example, it is not possible to write to or read from state during parameter changes via governance proposals. In addition, there are some tradeoffs with using the X\/Params type.\\n\\n\\subsection{Rationale}\\n\\nThe Cosmos SDK supports multiple types of parameters. Currently, it supports two types, which are the X\/Params type and the X\/Gov type. The X\/Gov type is a subclass of the X\/Params type that provides additional functionality. Currently, there are many tradeoffs with using either one or both types.\\n\\n\\subsubsection{Rationale}\\n\\nCurrently, there are many tradeoffs with using either one or both types. For example, it is currently not possible to write to or read from state during parameter changes via governance proposals.\\n\\n\\subsection{Rationale}\\n\\nCurrently, there are many tradeoffs with using either one or both types. For example, it is currently not possible to write to or read from state during parameter changes via governance proposals.\\n\\n\\subsection{Rationale}\\n\\nCurrently, there are many tradeoffs with using either one or both types. For example, it is currently not possible to write to or read from state during parameter changes via governance proposals.\\n\\n\\subsection{Rationale}\\n\\nCurrently, there are many tradeoffs with using either one or both types. For example, it is currently not possible to write to or read from state during parameter changes via governance proposals.\\n\\n"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"Predictions":"## The decision is to keep the `x\/bank` keeper in place as-is, but to make it more robust. We will add a layer of OCAPs\\nonetheless. The `x\/bank` keeper is currently a simple string, so we will add an object capability that allows us to use\\nthe `SetBalance` and `MintCoins` methods on the module with the same token. This should allow us to prevent any module\\nfrom minting tokens for another module\u2019s balance.\\n## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the \\n`MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the \\nx\/staking module simple by calling \\nx\\MintCoins(\u201cstaking\u201d)$. Furthermore, all modules which have access to these keeper methods,\\n[...]\\n also have access to \\nx\\SetBalance negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\\n## Decision\\n## The decision is keep both Keeper methods as-is for now and add an object capability layer over them.\\n## Context\\n## In this case study we will be adding an object capabilities layer over both Keeper methods.\\n## Decision\\n## The decision is keep both Keeper methods as-is for now and add an object capabilities layer over them.\\n\\n"}
{"File Name":"cosmos-sdk\/adr-009-evidence-module.md","Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ibc) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"We will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n* Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n* Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n* Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n* Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","tokens":163,"id":813,"Predictions":"## Determination\\n\\n\\nThis decision is based on the following considerations:\\n\\n1. The user's request to submit evidence for a penalty is based on the\\ncurrent situation of the validator(s) and the current status of the chain.\\n\\n2. The penalty shall not be applied to any other validator than the one\\ncurrently involved in this transaction.\\n\\n3. The penalty shall not affect any other validator.\\n\\n4. If this decision is not implemented, it will lead to an inconsistency between\\ncollateralized chains and a single validator will be able to cause a chain-wide\\ncollateralization failure.\\n\\n5. This decision is not intended to be a general rule for all validators, but only\\ncorresponding to specific situations where there is an immediate need for it.\\n\\n6. This decision is not intended to be applied in a general manner, but only in\\ncase of specific circumstances where there is an immediate need for it.\\n\\n7. This decision shall be accompanied by an explanation describing why this\\ncdecision has been made.\\n\\n## Explanation\\n## Explanation\\n## Explanation\\n## Explanation\\n## Explanation\\nThis Decision Record was generated by GitHub Pages using documentation tools.\\n"}
{"File Name":"cosmos-sdk\/adr-036-arbitrary-signature.md","Context":"## Context\\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\\nFurther context and use cases can be found in the references links.\\n","Decision":"The aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\\nCosmos SDK 0.40 also introduces a concept of \u201cauth_info\u201d this can specify SIGN_MODES.\\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\\nAn offchain transaction follows these rules:\\n* the memo must be empty\\n* nonce, sequence number must be equal to 0\\n* chain-id must be equal to \u201c\u201d\\n* fee gas must be equal to 0\\n* fee amount must be an empty array\\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\\nThe first message added to the `offchain` package is `MsgSignData`.\\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\\nProto definition:\\n```protobuf\\n\/\/ MsgSignData defines an arbitrary, general-purpose, off-chain message\\nmessage MsgSignData {\\n\/\/ Signer is the sdk.AccAddress of the message signer\\nbytes Signer = 1 [(gogoproto.jsontag) = \"signer\", (gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"];\\n\/\/ Data represents the raw bytes of the content that is signed (text, json, etc)\\nbytes Data = 2 [(gogoproto.jsontag) = \"data\"];\\n}\\n```\\nSigned MsgSignData json example:\\n```json\\n{\\n\"type\": \"cosmos-sdk\/StdTx\",\\n\"value\": {\\n\"msg\": [\\n{\\n\"type\": \"sign\/MsgSignData\",\\n\"value\": {\\n\"signer\": \"cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr\",\\n\"data\": \"cmFuZG9t\"\\n}\\n}\\n],\\n\"fee\": {\\n\"amount\": [],\\n\"gas\": \"0\"\\n},\\n\"signatures\": [\\n{\\n\"pub_key\": {\\n\"type\": \"tendermint\/PubKeySecp256k1\",\\n\"value\": \"AqnDSiRoFmTPfq97xxEb2VkQ\/Hm28cPsqsZm9jEVsYK9\"\\n},\\n\"signature\": \"8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==\"\\n}\\n],\\n\"memo\": \"\"\\n}\\n}\\n```\\n","tokens":129,"id":814,"Predictions":"The decision is to allow the user to sign Cosmos messages via a Ledger. This will allow users to provide a cryptographic signature of the Cosmos message, verify the identity of the sender, and associate that identity with a different organization or framework.\\nThe Ledger will be used to generate a secure key pair and store it on a device. The Ledger will be used to create and store a transaction, which includes the Cosmos message and associated identity. The transaction will then be signed using the Ledger's private key. The signed transaction will then be sent to the Cosmos network for verification.\\nThe user will have the ability to sign Cosmos messages with their private key on any device that supports signing transactions.\\n\\n\\end{document}\\n"}
{"File Name":"cosmos-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/hashicorp\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":230,"id":815,"Predictions":"## \\n\\n\\n\\n\\n\\n\\n\\n\\n## Decision\\n## \\n\\n\\n\\n\\n\\n\\n\\n## Decision\\n## \\n\\n## Decision\\n"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"Predictions":"## ## Decision\\nPlease create a new documentation site in the Cosmos SDK Github repo.\\n## Decision\\nPlease create a new documentation site in the Cosmos SDK Github repo.\\n## Decision\\nPlease create a new documentation site in the Cosmos SDK Github repo.\\n## Decision\\n"}
{"File Name":"cosmos-sdk\/adr-003-dynamic-capability-store.md","Context":"## Context\\nFull implementation of the [IBC specification](https:\/\/github.com\/cosmos\/ibc) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\\nas described in [ICS 5](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\\nobject-capability keys at this time.\\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L132))\\nand passed to Keepers as fixed arguments ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L160)). Keepers cannot create or store capability keys during transaction execution \u2014 although they could call `NewKVStoreKey` and take the memory address\\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).\\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\\nThis ADR proposes such an interface & mechanism.\\n","Decision":"The Cosmos SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\\ninvoked.\\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\\nchain is running.\\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\\nThe persistent `KVStore` tracks which capability is owned by which modules.\\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\\na reverse mapping that map from module name, capability name to the capability index.\\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\\ngo-map to retrieve the capability at the original memory location.\\nThe `CapabilityKeeper` will define the following types & functions:\\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\\na name. A `String()` method is provided for debugging.\\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\\n```go\\ntype Capability struct {\\nindex uint64\\n}\\n```\\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\\n```go\\ntype CapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleNames   map[string]interface{}\\nsealed        bool\\n}\\n```\\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\\npassed by other modules.\\n```go\\ntype ScopedCapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleName    string\\n}\\n```\\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\\nIt MUST be called before `InitialiseAndSeal`.\\n```go\\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\\nif ck.sealed {\\npanic(\"cannot scope to module via a sealed capability keeper\")\\n}\\nif _, ok := ck.scopedModules[moduleName]; ok {\\npanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\\n}\\nck.scopedModules[moduleName] = struct{}{}\\nreturn ScopedKeeper{\\ncdc:      ck.cdc,\\nstoreKey: ck.storeKey,\\nmemKey:   ck.memKey,\\ncapMap:   ck.capMap,\\nmodule:   moduleName,\\n}\\n}\\n```\\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\\ncreation of any new `ScopedCapabilityKeeper`s.\\n```go\\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\\nif ck.sealed {\\npanic(\"capability keeper is sealed\")\\n}\\npersistentStore := ctx.KVStore(ck.persistentKey)\\nmap := ctx.KVStore(ck.memKey)\\n\/\/ initialise memory store for all names in persistent store\\nfor index, value := range persistentStore.Iter() {\\ncapability = &CapabilityKey{index: index}\\nfor moduleAndCapability := range value {\\nmoduleName, capabilityName := moduleAndCapability.Split(\"\/\")\\nmemStore.Set(moduleName + \"\/fwd\/\" + capability, capabilityName)\\nmemStore.Set(moduleName + \"\/rev\/\" + capabilityName, index)\\nck.capMap[index] = capability\\n}\\n}\\nck.sealed = true\\n}\\n```\\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\\nreference. The newly created capability is automatically persisted; the calling module need not\\ncall `ClaimCapability`.\\n```go\\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\\n\/\/ check name not taken in memory store\\nif capStore.Get(\"rev\/\" + name) != nil {\\nreturn nil, errors.New(\"name already taken\")\\n}\\n\/\/ fetch the current index\\nindex := persistentStore.Get(\"index\")\\n\/\/ create a new capability\\ncapability := &CapabilityKey{index: index}\\n\/\/ set persistent store\\npersistentStore.Set(index, Set.singleton(sck.moduleName + \"\/\" + name))\\n\/\/ update the index\\nindex++\\npersistentStore.Set(\"index\", index)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to index\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, index)\\n\/\/ set the in-memory mapping from index to capability pointer\\ncapMap[index] = capability\\n\/\/ return the newly created capability\\nreturn capability\\n}\\n```\\n`AuthenticateCapability` can be called by any module to check that a capability\\ndoes in fact correspond to a particular name (the name can be untrusted user input)\\nwith which the calling module previously associated it.\\n```go\\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\\n\/\/ return whether forward mapping in memory store matches name\\nreturn memStore.Get(sck.moduleName + \"\/fwd\/\" + capability) === name\\n}\\n```\\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\\nso that future `GetCapability` calls will succeed.\\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\\nthey will all own it.\\n```go\\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to capability\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.add(sck.moduleName + \"\/\" + name)\\npersistentStore.Set(capability.Index(), owners)\\n}\\n```\\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\\nThe module is not allowed to retrieve capabilities which it does not own.\\n```go\\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\\n\/\/ fetch the index of capability using reverse mapping in memstore\\nindex := memStore.Get(sck.moduleName + \"\/rev\/\" + name)\\n\/\/ fetch capability from go-map using index\\ncapability := capMap[index]\\n\/\/ return the capability\\nreturn capability\\n}\\n```\\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\\nmore owners exist, the capability will be deleted globally.\\n```go\\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\nname := capStore.Get(sck.moduleName + \"\/fwd\/\" + capability)\\nif name == nil {\\nreturn error(\"capability not owned by module\")\\n}\\n\/\/ delete forward mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ delete reverse mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.remove(sck.moduleName + \"\/\" + name)\\nif owners.size() > 0 {\\n\/\/ there are still other owners, keep the capability around\\npersistentStore.Set(capability.Index(), owners)\\n} else {\\n\/\/ no more owners, delete the capability\\npersistentStore.Delete(capability.Index())\\ndelete(capMap[capability.Index()])\\n}\\n}\\n```\\n### Usage patterns\\n#### Initialisation\\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\\n```go\\nck := NewCapabilityKeeper(persistentKey, memoryKey)\\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\\n\/\/ other initialisation logic ...\\n\/\/ load initial state...\\nck.InitialiseAndSeal(initialContext)\\n```\\n#### Creating, passing, claiming and using capabilities\\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\\nModule 1 would have the following code:\\n```go\\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\\nmod2Keeper.SomeFunction(ctx, capability, args...)\\n```\\n`SomeFunction`, running in module 2, could then claim the capability:\\n```go\\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\\nk.sck.ClaimCapability(ctx, capability, \"resourceABC\")\\n\/\/ other logic...\\n}\\n```\\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\\n```go\\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\\ncapability := k.sck.GetCapability(ctx, name)\\nmod1.UseResource(ctx, capability, \"resourceABC\")\\n}\\n```\\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\\n```go\\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\\nif !k.sck.AuthenticateCapability(name, capability) {\\nreturn errors.New(\"unauthenticated\")\\n}\\n\/\/ do something with the resource\\n}\\n```\\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\\n","tokens":437,"id":817,"Predictions":"## The decision is to implement a mechanism for managing store keys in the application, and to implement a mechanism for regenerating memory addresses on tx failure. This is a critical requirement, as it allows the application to continue transactions even if the key is lost.\\n\\n\\n## Context\\nThe application will be implemented using the [Gaia SDK](https:\/\/github.com\/cosmos\/gaia), which provides an API for managing object-capability keys and objects. The application will use the `StoreKey` struct to store \\nobject-capability keys in the Merklised store. These keys will be used by Keepers to identify objects, and by Keepers themselves to identify objects when performing operations on them (e.g., adding or removing them from queues).\\nThe application will be implemented using the `Keepers` interface, which provides an API for managing object-capability keys.\\n\\n## Decision\\nThis decision is made based on whether or not we can maintain a consistent view of all object-capability keys across different machines. If we cannot maintain this consistency, then we cannot implement this interface.\\n\\end{document}\\n"}
{"File Name":"cosmos-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```protobuf\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary representation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\ngoogle.protobuf.Any public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n* **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n* **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n* requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n* leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```protobuf\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n* Pull account number and sequence from the state.\\n* Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n* Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n* Verify the signature at the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n* important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n* they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need an unknown field filter that:\\n* always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n* rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement the `cryptotypes.PubKey` interface.\\nWe propose to use `Any` for protobuf encoding as we are doing with other interfaces (for example, in `BaseAccount.PubKey` and `SignerInfo.PublicKey`).\\nThe following public keys are implemented: secp256k1, secp256r1, ed25519 and legacy-multisignature.\\nEx:\\n```protobuf\\nmessage PubKey {\\nbytes key = 1;\\n}\\n```\\n`multisig.LegacyAminoPubKey` has an array of `Any`'s member to support any\\nprotobuf public key type.\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\nIf the account is sending its first transaction, the account number must be set to 0. This is due to the account not being created yet.\\n```go\\ntype AccountRetriever interface {\\nGetAccount(clientCtx Context, addr sdk.AccAddress) (client.Account, error)\\nGetAccountWithHeight(clientCtx Context, addr sdk.AccAddress) (client.Account, int64, error)\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `Codec`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModule.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":818,"Predictions":"## Decision\\nThe decision is to use the `oneof` approach for transaction signing. The\\noriginal intent was to use the `oneof` approach for transaction signing (see\\n[\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)). However, we have\\ndemonstrated that the `oneof` approach does not work well in practice, especially\\nin the face of high network latencies.\\nAs a result, we have decided to use the `json-signing` approach.\\n\\n## Context\\n## Context\\nWe have used this ADR as a basis for our design of the Cosmos SDK's JSON-signing\\napproach. We believe that this is a better fit for our current design goals and\\nexpectations, as well as future ones. This ADR is intended to be an architectural\\ndocument that describes how we intend to migrate from one approach to another.\\n\\n## Summary\\n## Summary\\nThe decision is to use the `json-signing` approach for transaction signing. The original intent was to use the `oneof` approach for transaction signing (see [\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)). However, we have demonstrated that the `oneof` approach does not work well in practice, especially in the face of high network latencies.\\nAs a result, we have decided to use the `json-signing` approach.\\n\\end{document}\\n"}
{"File Name":"cosmos-sdk\/adr-034-account-rekeying.md","Context":"## Context\\nCurrently, in the Cosmos SDK, the address of an auth `BaseAccount` is based on the hash of the public key.  Once an account is created, the public key for the account is set in stone, and cannot be changed.  This can be a problem for users, as key rotation is a useful security practice, but is not possible currently.  Furthermore, as multisigs are a type of pubkey, once a multisig for an account is set, it can not be updated.  This is problematic, as multisigs are often used by organizations or companies, who may need to change their set of multisig signers for internal reasons.\\nTransferring all the assets of an account to a new account with the updated pubkey is not sufficient, because some \"engagements\" of an account are not easily transferable.  For example, in staking, to transfer bonded Atoms, an account would have to unbond all delegations and wait the three week unbonding period.  Even more significantly, for validator operators, ownership over a validator is not transferrable at all, meaning that the operator key for a validator can never be updated, leading to poor operational security for validators.\\n","Decision":"We propose the addition of a new feature to `x\/auth` that allows accounts to update the public key associated with their account, while keeping the address the same.\\nThis is possible because the Cosmos SDK `BaseAccount` stores the public key for an account in state, instead of making the assumption that the public key is included in the transaction (whether explicitly or implicitly through the signature) as in other blockchains such as Bitcoin and Ethereum.  Because the public key is stored on chain, it is okay for the public key to not hash to the address of an account, as the address is not pertinent to the signature checking process.\\nTo build this system, we design a new Msg type as follows:\\n```protobuf\\nservice Msg {\\nrpc ChangePubKey(MsgChangePubKey) returns (MsgChangePubKeyResponse);\\n}\\nmessage MsgChangePubKey {\\nstring address = 1;\\ngoogle.protobuf.Any pub_key = 2;\\n}\\nmessage MsgChangePubKeyResponse {}\\n```\\nThe MsgChangePubKey transaction needs to be signed by the existing pubkey in state.\\nOnce, approved, the handler for this message type, which takes in the AccountKeeper, will update the in-state pubkey for the account and replace it with the pubkey from the Msg.\\nAn account that has had its pubkey changed cannot be automatically pruned from state.  This is because if pruned, the original pubkey of the account would be needed to recreate the same address, but the owner of the address may not have the original pubkey anymore.  Currently, we do not automatically prune any accounts anyways, but we would like to keep this option open the road (this is the purpose of account numbers).  To resolve this, we charge an additional gas fee for this operation to compensate for this this externality (this bound gas amount is configured as parameter `PubKeyChangeCost`). The bonus gas is charged inside the handler, using the `ConsumeGas` function.  Furthermore, in the future, we can allow accounts that have rekeyed manually prune themselves using a new Msg type such as `MsgDeleteAccount`.  Manually pruning accounts can give a gas refund as an incentive for performing the action.\\n```go\\namount := ak.GetParams(ctx).PubKeyChangeCost\\nctx.GasMeter().ConsumeGas(amount, \"pubkey change fee\")\\n```\\nEvery time a key for an address is changed, we will store a log of this change in the state of the chain, thus creating a stack of all previous keys for an address and the time intervals for which they were active.  This allows dapps and clients to easily query past keys for an account which may be useful for features such as verifying timestamped off-chain signed messages.\\n","tokens":250,"id":819,"Predictions":"The decision is to allow users to transfer ownership of an account, but not the key.  This can be done with a new `TransferOwnership` action, which can be used to transfer ownership of an account.  The `TransferOwnership` action can be used by validators, operators and users.\\n\\n## Actions\\n\\begin{itemize}\\n\\item \\textbf{TransferOwnership} - Allows users to transfer ownership of an account from one account to another.\\n\\n\\n\\nThe user must have the right to transfer ownership, either \\textit{to} or \\textit{from}.  The user must also have sufficient balance in the account they are transferring ownership from, and sufficient balance in the account they are transferring ownership to.  If the user does not have sufficient balance in both accounts, then they cannot perform this action.\\n\\item \\textbf{TransferOwnershipTo} - Allows users to transfer ownership of an account from one account to another.  This is equivalent to \\textbf{TransferOwnership}, but does not require any balance.\\n\\end{itemize}\\n## Context\\nThis decision is related to the previous one."}
{"File Name":"cosmos-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params\\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\\n* There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":74,"id":820,"Predictions":"## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree, state why.\\n## Decision: Vote on the proposal as a whole, or split the proposal into smaller decisions.  For each of these decisions, indicate how much you agree or disagree with it.  If you disagree"}
{"File Name":"cosmos-sdk\/adr-017-historical-header-module.md","Context":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","Decision":"The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```go\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper) error {\\ninfo := HistoricalInfo{\\napphash: ctx.HeaderInfo().AppHash,\\nTime: ctx.HeaderInfo().Time,\\nNextValidatorsHash: ctx.CometInfo().NextValidatorsHash,\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n","tokens":66,"id":821,"Predictions":"## Decision\\nThis decision is to be used by the Cosmos SDK to determine if it should accept or reject a new consensus state. The decision is made on the basis of the consensus state, and not on the state itself.\\n\\n## Decision\\nThis decision is to be used by the Cosmos SDK to determine if it should accept or reject a new consensus state. The decision is made on the basis of the consensus state, and not on the state itself.\\n\\n## Decision\\nThis decision is to be used by the Cosmos SDK to determine if it should accept or reject a new consensus state. The decision is made on the basis of the consensus state, and not on the state itself.\\n"}
{"File Name":"cosmos-sdk\/adr-019-protobuf-state-encoding.md","Context":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n* Binary bytes must be decode-able with a schema.\\n* Schema must be upgradeable.\\n* The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n* Language agnostic\\n* Platform agnostic\\n* Rich client support and thriving ecosystem\\n* High performance\\n* Minimal encoded message size\\n* Codegen-based over reflection-based\\n* Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","Decision":"We will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide two concrete implementations of the `Marshaler` interface: `AminoCodec` and `ProtoCodec`.\\n* `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n* `ProtoCodec`: Uses Protobuf for both binary and JSON encoding.\\nModules will use whichever codec that is instantiated in the app. By default, the Cosmos SDK's `simapp`\\ninstantiates a `ProtoCodec` as the concrete implementation of `Marshaler`, inside the `MakeTestEncodingConfig`\\nfunction. This can be easily overwritten by app developers if they so desire.\\nThe ultimate goal will be to replace Amino JSON encoding with Protobuf encoding and thus have\\nmodules accept and\/or extend `ProtoCodec`. Until then, Amino JSON is still provided for legacy use-cases.\\nA handful of places in the Cosmos SDK still have Amino JSON hardcoded, such as the Legacy API REST endpoints\\nand the `x\/params` store. They are planned to be converted to Protobuf in a gradual manner.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `ProtoCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/cosmos\/gogoproto) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshal(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Codec\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the Cosmos SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/pkg.go.dev\/github.com\/cosmos\/gogoproto\/types)\\nuses [global type registration]( https:\/\/github.com\/cosmos\/gogoproto\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could register a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec`\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe Cosmos SDK will provide support methods `MarshalInterface` and `UnmarshalInterface` to hide a complexity of wrapping interface types into `Any` and allow easy serialization.\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\n\/\/ note: eviexported.Evidence is an interface type\\nfunc MarshalEvidence(cdc codec.BinaryCodec, e eviexported.Evidence) ([]byte, error) {\\nreturn cdc.MarshalInterface(e)\\n}\\nfunc UnmarshalEvidence(cdc codec.BinaryCodec, bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := cdc.UnmarshalInterface(&evi, bz)\\nreturn err, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n* **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n* **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n* Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n","tokens":439,"id":822,"Predictions":"\\begin{itemize}\\n  \\item The decision is to adopt the [go-amino](https:\/\/github.com\/tendermint\/go-amino) encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt the [go-amino](https:\/\/github.com\/tendermint\/go-amino) encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt the [go-amino](https:\/\/github.com\/tendermint\/go-amino) encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt the [go-amino](https:\/\/github.com\/tendermint\/go-amino) encoding protocol for the Cosmos SDK.\\n\\end{itemize}\\n\\n\\subsubsection{Decision}\\n\\begin{itemize}\\n  \\item The decision is to adopt the go-amino encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt go-amino encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt go-amino encoding protocol for the Cosmos SDK.\\n  \\item The decision is to adopt go-amino encoding protocol for the Cosmos SDK.\\n\\n\\end{itemize}\\n\\n\\subsubsection{Context}\\nThe following context describes what this ADR is used for.\\newline\\nIn general, this ADR allows an application developer (e.g., a user of a given library or framework) to specify which serialization format should be used when writing and reading data in a particular application. In particular, this ADR allows an application developer to specify whether they want their data serialized as binary or JSON. Additionally, this ADR specifies that data should be serialized using `go-ami` (see below).\\n\\n\\subsubsection{Decision}\\n\\begin{itemize}\\n  \\item This ADR specifies that data should be serialized using `go-ami` (see below).\\n  \\n    This ADR specifies that data should be serialized using `go-ami` (see below).\\n  \\n    This ADR specifies that data should be serialized using `go-ami` (see below).\\n  \\n    This ADR specifies that data should be serialized using `go-ami` (see below).\\n  \\n    This ADR specifies that data should be serialized using `go-ami` (see below).\\n\\n\\end{itemize}\\n\\n\\n"}
{"File Name":"cosmos-sdk\/adr-040-storage-and-smt-state-commitments.md","Context":"## Context\\nCurrently, Cosmos SDK uses IAVL for both state [commitments](https:\/\/cryptography.fandom.com\/wiki\/Commitment_scheme) and data storage.\\nIAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key\/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems:\\n* Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level.\\n* Each edge traversal requires a DB query.\\n* Creating snapshots is [expensive](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7215#issuecomment-684804950). It takes about 30 seconds to export less than 100 MB of state (as of March 2020).\\n* Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck.\\n* The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, [ref](https:\/\/github.com\/cosmos\/iavl\/blob\/master\/docs\/node\/node.md\\n).\\nMoreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\\n","Decision":"We propose to separate the concerns of state commitment (**SC**), needed for consensus, and state storage (**SS**), needed for state machine. Finally we replace IAVL with [Celestia's SMT](https:\/\/github.com\/lazyledger\/smt). Celestia SMT is based on Diem (called jellyfish) design [*] - it uses a compute-optimised SMT by replacing subtrees with only default values with a single node (same approach is used by Ethereum2) and implements compact proofs.\\nThe storage model presented here doesn't deal with data structure nor serialization. It's a Key-Value database, where both key and value are binaries. The storage user is responsible for data serialization.\\n### Decouple state commitment from storage\\nSeparation of storage and commitment (by the SMT) will allow the optimization of different components according to their usage and access patterns.\\n`SC` (SMT) is used to commit to a data and compute Merkle proofs. `SS` is used to directly access data. To avoid collisions, both `SS` and `SC` will use a separate storage namespace (they could use the same database underneath). `SS` will store each record directly (mapping `(key, value)` as `key \u2192 value`).\\nSMT is a merkle tree structure: we don't store keys directly. For every `(key, value)` pair, `hash(key)` is used as leaf path (we hash a key to uniformly distribute leaves in the tree) and `hash(value)` as the leaf contents. The tree structure is specified in more depth [below](#smt-for-state-commitment).\\nFor data access we propose 2 additional KV buckets (implemented as namespaces for the key-value pairs, sometimes called [column family](https:\/\/github.com\/facebook\/rocksdb\/wiki\/Terminology)):\\n1. B1: `key \u2192 value`: the principal object storage, used by a state machine, behind the Cosmos SDK `KVStore` interface: provides direct access by key and allows prefix iteration (KV DB backend must support it).\\n2. B2: `hash(key) \u2192 key`: a reverse index to get a key from an SMT path. Internally the SMT will store `(key, value)` as `prefix || hash(key) || hash(value)`. So, we can get an object value by composing `hash(key) \u2192 B2 \u2192 B1`.\\n3. We could use more buckets to optimize the app usage if needed.\\nWe propose to use a KV database for both `SS` and `SC`. The store interface will allow to use the same physical DB backend for both `SS` and `SC` as well two separate DBs. The latter option allows for the separation of `SS` and `SC` into different hardware units, providing support for more complex setup scenarios and improving overall performance: one can use different backends (eg RocksDB and Badger) as well as independently tuning the underlying DB configuration.\\n### Requirements\\nState Storage requirements:\\n* range queries\\n* quick (key, value) access\\n* creating a snapshot\\n* historical versioning\\n* pruning (garbage collection)\\nState Commitment requirements:\\n* fast updates\\n* tree path should be short\\n* query historical commitment proofs using ICS-23 standard\\n* pruning (garbage collection)\\n### SMT for State Commitment\\nA Sparse Merkle tree is based on the idea of a complete Merkle tree of an intractable size. The assumption here is that as the size of the tree is intractable, there would only be a few leaf nodes with valid data blocks relative to the tree size, rendering a sparse tree.\\nThe full specification can be found at [Celestia](https:\/\/github.com\/celestiaorg\/celestia-specs\/blob\/ec98170398dfc6394423ee79b00b71038879e211\/src\/specs\/data_structures.md#sparse-merkle-tree). In summary:\\n* The SMT consists of a binary Merkle tree, constructed in the same fashion as described in [Certificate Transparency (RFC-6962)](https:\/\/tools.ietf.org\/html\/rfc6962), but using as the hashing function SHA-2-256 as defined in [FIPS 180-4](https:\/\/doi.org\/10.6028\/NIST.FIPS.180-4).\\n* Leaves and internal nodes are hashed differently: the one-byte `0x00` is prepended for leaf nodes while `0x01` is prepended for internal nodes.\\n* Default values are given to leaf nodes with empty leaves.\\n* While the above rule is sufficient to pre-compute the values of intermediate nodes that are roots of empty subtrees, a further simplification is to extend this default value to all nodes that are roots of empty subtrees. The 32-byte zero is used as the default value. This rule takes precedence over the above one.\\n* An internal node that is the root of a subtree that contains exactly one non-empty leaf is replaced by that leaf's leaf node.\\n### Snapshots for storage sync and state versioning\\nBelow, with simple _snapshot_ we refer to a database snapshot mechanism, not to a _ABCI snapshot sync_. The latter will be referred as _snapshot sync_ (which will directly use DB snapshot as described below).\\nDatabase snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a _copy on write_ and it allows DB state to be efficiently delivered at a certain stage.\\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs.\\nOne of the Stargate core features is a _snapshot sync_ delivered in the `\/snapshot` package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a `SS` at a certain version together with a header chain.\\nA new database snapshot will be created in every `EndBlocker` and identified by a block height. The `root` store keeps track of the available snapshots to offer `SS` at a certain version. The `root` store implements the `RootStore` interface described below. In essence, `RootStore` encapsulates a `Committer` interface. `Committer` has a `Commit`, `SetPruning`, `GetPruning` functions which will be used for creating and removing snapshots. The `rootStore.Commit` function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the `Committer` interface.\\nNOTE: `Commit` must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into `Committer` and `PruningCommitter` - only the multiroot should implement `PruningCommitter` (cache and prefix store don't need pruning).\\nNumber of historical versions for `abci.RequestQuery` and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions.\\nPruning old snapshots is effectively done by a database. Whenever we update a record in `SC`, SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions.\\nTo manage the active snapshots we will either use a DB _max number of snapshots_ option (if available), or we will remove DB snapshots in the `EndBlocker`. The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\\n#### Accessing old state versions\\nOne of the functional requirements is to access old state. This is done through `abci.RequestQuery` structure.  The version is specified by a block height (so we query for an object by a key `K` at block height `H`). The number of old versions supported for `abci.RequestQuery` is configurable. Accessing an old state is done by using available snapshots.\\n`abci.RequestQuery` doesn't need old state of `SC` unless the `prove=true` parameter is set. The SMT merkle proof must be included in the `abci.ResponseQuery` only if both `SC` and `SS` have a snapshot for requested version.\\nMoreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution.\\nWe positively [validated](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/8297) a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\\n### State Proofs\\nFor any object stored in State Store (SS), we have corresponding object in `SC`. A proof for object `V` identified by a key `K` is a branch of `SC`, where the path corresponds to the key `hash(K)`, and the leaf is `hash(K, V)`.\\n### Rollbacks\\nWe need to be able to process transactions and roll-back state updates if a transaction fails. This can be done in the following way: during transaction processing, we keep all state change requests (writes) in a `CacheWrapper` abstraction (as it's done today). Once we finish the block processing, in the `Endblocker`,  we commit a root store - at that time, all changes are written to the SMT and to the `SS` and a snapshot is created.\\n### Committing to an object without saving it\\nWe identified use-cases, where modules will need to save an object commitment without storing an object itself. Sometimes clients are receiving complex objects, and they have no way to prove a correctness of that object without knowing the storage layout. For those use cases it would be easier to commit to the object without storing it directly.\\n### Refactor MultiStore\\nThe Stargate `\/store` implementation (store\/v1) adds an additional layer in the SDK store construction - the `MultiStore` structure. The multistore exists to support the modularity of the Cosmos SDK - each module is using its own instance of IAVL, but in the current implementation, all instances share the same database. The latter indicates, however, that the implementation doesn't provide true modularity. Instead it causes problems related to race condition and atomic DB commits (see: [\\#6370](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6370) and [discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/8297#discussioncomment-757043)).\\nWe propose to reduce the multistore concept from the SDK, and to use a single instance of `SC` and `SS` in a `RootStore` object. To avoid confusion, we should rename the `MultiStore` interface to `RootStore`. The `RootStore` will have the following interface; the methods for configuring tracing and listeners are omitted for brevity.\\n```go\\n\/\/ Used where read-only access to versions is needed.\\ntype BasicRootStore interface {\\nStore\\nGetKVStore(StoreKey) KVStore\\nCacheRootStore() CacheRootStore\\n}\\n\/\/ Used as the main app state, replacing CommitMultiStore.\\ntype CommitRootStore interface {\\nBasicRootStore\\nCommitter\\nSnapshotter\\nGetVersion(uint64) (BasicRootStore, error)\\nSetInitialVersion(uint64) error\\n... \/\/ Trace and Listen methods\\n}\\n\/\/ Replaces CacheMultiStore for branched state.\\ntype CacheRootStore interface {\\nBasicRootStore\\nWrite()\\n... \/\/ Trace and Listen methods\\n}\\n\/\/ Example of constructor parameters for the concrete type.\\ntype RootStoreConfig struct {\\nUpgrades        *StoreUpgrades\\nInitialVersion  uint64\\nReservePrefix(StoreKey, StoreType)\\n}\\n```\\n<!-- TODO: Review whether these types can be further reduced or simplified -->\\n<!-- TODO: RootStorePersistentCache type -->\\nIn contrast to `MultiStore`, `RootStore` doesn't allow to dynamically mount sub-stores or provide an arbitrary backing DB for individual sub-stores.\\nNOTE: modules will be able to use a special commitment and their own DBs. For example: a module which will use ZK proofs for state can store and commit this proof in the `RootStore` (usually as a single record) and manage the specialized store privately or using the `SC` low level interface.\\n#### Compatibility support\\nTo ease the transition to this new interface for users, we can create a shim which wraps a `CommitMultiStore` but provides a `CommitRootStore` interface, and expose functions to safely create and access the underlying `CommitMultiStore`.\\nThe new `RootStore` and supporting types can be implemented in a `store\/v2alpha1` package to avoid breaking existing code.\\n#### Merkle Proofs and IBC\\nCurrently, an IBC (v1.0) Merkle proof path consists of two elements (`[\"<store-key>\", \"<record-key>\"]`), with each key corresponding to a separate proof. These are each verified according to individual [ICS-23 specs](https:\/\/github.com\/cosmos\/ibc-go\/blob\/f7051429e1cf833a6f65d51e6c3df1609290a549\/modules\/core\/23-commitment\/types\/merkle.go#L17), and the result hash of each step is used as the committed value of the next step, until a root commitment hash is obtained.\\nThe root hash of the proof for `\"<record-key>\"` is hashed with the `\"<store-key>\"` to validate against the App Hash.\\nThis is not compatible with the `RootStore`, which stores all records in a single Merkle tree structure, and won't produce separate proofs for the store- and record-key. Ideally, the store-key component of the proof could just be omitted, and updated to use a \"no-op\" spec, so only the record-key is used. However, because the IBC verification code hardcodes the `\"ibc\"` prefix and applies it to the SDK proof as a separate element of the proof path, this isn't possible without a breaking change. Breaking this behavior would severely impact the Cosmos ecosystem which already widely adopts the IBC module. Requesting an update of the IBC module across the chains is a time consuming effort and not easily feasible.\\nAs a workaround, the `RootStore` will have to use two separate SMTs (they could use the same underlying DB): one for IBC state and one for everything else. A simple Merkle map that reference these SMTs will act as a Merkle Tree to create a final App hash. The Merkle map is not stored in a DBs - it's constructed in the runtime. The IBC substore key must be `\"ibc\"`.\\nThe workaround can still guarantee atomic syncs: the [proposed DB backends](#evaluated-kv-databases) support atomic transactions and efficient rollbacks, which will be used in the commit phase.\\nThe presented workaround can be used until the IBC module is fully upgraded to supports single-element commitment proofs.\\n### Optimization: compress module key prefixes\\nWe consider a compression of prefix keys by creating a mapping from module key to an integer, and serializing the integer using varint coding. Varint coding assures that different values don't have common byte prefix. For Merkle Proofs we can't use prefix compression - so it should only apply for the `SS` keys. Moreover, the prefix compression should be only applied for the module namespace. More precisely:\\n* each module has it's own namespace;\\n* when accessing a module namespace we create a KVStore with embedded prefix;\\n* that prefix will be compressed only when accessing and managing `SS`.\\nWe need to assure that the codes won't change. We can fix the mapping in a static variable (provided by an app) or SS state under a special key.\\nTODO: need to make decision about the key compression.\\n","tokens":391,"id":823,"Predictions":"The decision is to use the IAVL database for storing state commitments. The decision is based on the following considerations:\\n* The original Cosmos SDK proposal [mentions](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7215#issuecomment-684804950) the IAVL as a potential storage engine for state commitments. However, it's not clear if this is still the case and if it will be maintained in a future version of Cosmos SDK.\\n* The IAVL has been around for quite some time and it's widely used in other projects (e.g., [Rust](https:\/\/github.com\/rust-lang\/rust) and [Golang](https:\/\/golang.org)).\\n* It's a well-established project with a well-structured codebase and a well-structured API.\\n* There are no alternative suggestions to use an alternative database.\\n## Summary\\nThe summary of the decision is as follows:\\n* The decision is to use the IAVL database for storing state commitments. The decision is based on the following considerations:\\n* The original Cosmos SDK proposal mentions that the IAVL could be used for both storage and data storage, but there's no mention of this in the current version of Cosmos SDK.\\n* It's a well-established project with a well-structured codebase and a well-structured API.\\n* There are no alternative suggestions to use an alternative database.\\n## End of Decision\\n## End of Decision Record\\n"}
{"File Name":"cosmos-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the Cosmos SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"cosmossdk.io\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n* Type registration on `auth`'s codec (shown above)\\n* A `Validate` method for each `Account` concrete type\\n","tokens":112,"id":824,"Predictions":"## +1: custom account types are supported by Cosmos SDK\\n## -1: custom account types are not supported by Cosmos SDK\\n## +1: account types are supported by Cosmos SDK\\n## -1: account types are not supported by Cosmos SDK\\n## +1: accounts are exported and loaded to\/from genesis files\\n## -1: accounts are exported and loaded to\/from genesis files\\n## +1: accounts can be exported\/imported from a genesis file\\n## -1: accounts cannot be exported\/imported from a genesis file\\n\\n\\r\\nYou can use the following command to generate a new account type:\\n\\begin{verbatim}\\n$ cosmos init --account-type \"BaseAccount\"\\n\\end{verbatim}\\n\\r\\nYou can use the following command to create an existing account type:\\n\\begin{verbatim}\\n$ cosmos init --account-type \"ContinuousVestingAccount\"\\n\\end{verbatim}\\n\\r\\nYou can use the following command to create an existing account type with no vesting or module:\\n\\begin{verbatim}\\n$ cosmos init --account-type \"DelayedVestingAccount\"\\n\\end{verbatim}\\n\\r\\nYou can use the following command to create an existing account type with no vesting or module:\\n\\begin{verbatim}\\n$ cosmos init --account-type \"ModuleAccount\"\\n\\end{verbatim}\\n\\n\\r\\nThe `--account-type` option is used when creating an existing account type. If you wish to create a new, unregistered type, then use `--account-type` without any arguments.\\n\\r\\nIf you wish to create a new, unregistered type that has no vesting or module, then use `--account-type` with `BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` or `ModuleAccount`.\\n\\r\\nIf you wish to create a new, unregistered type that has no vesting or module and that also has a custom account name (not just a UUID), then use `--account-name`.\\n\\r\\nIf you wish to create a new, unregistered type that has no vesting or module and that also has no vesting period (i.e. it is not on an active contract), then use `--active-period`.\\n\\r\\nThe default value of this argument is `'None'`, which means that there is no active period. If you want an active period for your new type, then pass `'Active'`. Otherwise pass `'None'`.\\n\\r\\nIf you wish to add any other arguments, then include them as separate arguments after the above options.\\n\\r\\nFor example:\\n\\begin{verbatim}\\n$ cosmos init --account-type \"BaseAccount\" --active-period 'Active'\\n# Accounts created using this option will have active periods of 'Active'\\n$ cosmos init --account-type \"ContinuousVestingAccount\" --active-period 'Active'\\n# Accounts created using this option will have active periods of 'Active'\\n$ cosmos init --account-type \"DelayedVestingAccount\" --active-period 'Active'\\n# Accounts created using this option will have active periods of 'Active'\\n$ cosmos init --account-type \"ModuleAccount\" --active-period 'Active'\\n# Accounts created using this option will have active periods of 'Active'\\n\\end{verbatim}\\n\\n"}
{"File Name":"cosmos-sdk\/adr-044-protobuf-updates-guidelines.md","Context":"## Context\\nThe Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9446) for more details on this issue.\\nMoreover, module developers often face other questions around Protobuf definitions such as \"Can I rename a field?\" or \"Can I deprecate a field?\" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\\n","Decision":"We decide to keep [Buf's](https:\/\/docs.buf.build\/) recommendations with the following exceptions:\\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\\n### Updating Protobuf Definition Without Bumping Version\\n#### 1. Module developers MAY add new Protobuf definitions\\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\\n```protobuf\\n\/\/ Since: cosmos-sdk <version>{, <version>...}\\n```\\nWhere each `version` denotes a minor (\"0.45\") or patch (\"0.44.5\") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show\/hide these fields depending on the targeted node version.\\nAs examples, the following comments are valid:\\n```protobuf\\n\/\/ Since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11, 0.44.5\\n```\\nand the following ones are NOT valid:\\n```protobuf\\n\/\/ Since cosmos-sdk v0.44\\n\/\/ since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11 0.44.5\\n\/\/ Since: Cosmos SDK 0.42.11, 0.44.5\\n```\\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\\nProtobuf supports the [`deprecated` field option](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\\n* The Cosmos SDK recently removed support for [time-based software upgrades](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\\n* The Cosmos SDK now supports [governance split votes](.\/adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\\n#### 3. Fields MUST NOT be renamed\\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\\n### Incrementing Protobuf Package Version\\nTODO, needs architecture review. Some topics:\\n* Bumping versions frequency\\n* When bumping versions, should the Cosmos SDK support both versions?\\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\\n* mention ADR-023 Protobuf naming\\n","tokens":314,"id":825,"Predictions":"## [1]: https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/proto\/cosmos\/protocol\/protobuf.proto#L125-L129\\n\\n## [2]: https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/proto\/cosmos\/protocol\/protobuf.proto#L130-L136\\n\\n## [3]: https:\/\/docs.buf.build\/docs\/#proto-specs\\n\\n## [4]: https:\/\/docs.buf.build\/docs\/#proto-extensions\\n## [5]: https:\/\/docs.buf.build\/docs\/#proto-types\\n## [6]: https:\/\/docs.buf.build\/docs\/#proto-symbols\\n## [7]: https:\/\/docs.buf.build\/docs\/#proto-flags\\n## [8]: https:\/\/docs.buf.build\/docs\/#proto-constants\\n## [9]: https:\/\/docs.buf.build\/docs\/#proto-mapping\\n## [10]: https:\/\/docs.buf.build\/docs\/#protoc-types\\n## [11]: https:\/\/docs.buf.build\/docs\/#protoc-types-extensions\\n## [12]: https:\/\/docs.buf.build\/docs\/#protoc-types-symbols\\n\\n\\n"}
{"File Name":"cosmos-sdk\/adr-035-rosetta-api-support.md","Context":"## Context\\n[Rosetta API](https:\/\/www.rosetta-api.org\/) is an open-source specification and set of tools developed by Coinbase to\\nstandardise blockchain interactions.\\nThrough the use of a standard API for integrating blockchain applications it will\\n* Be easier for a user to interact with a given blockchain\\n* Allow exchanges to integrate new blockchains quickly and easily\\n* Enable application developers to build cross-blockchain applications such as block explorers, wallets and dApps at\\nconsiderably lower cost and effort.\\n","Decision":"It is clear that adding Rosetta API support to the Cosmos SDK will bring value to all the developers and\\nCosmos SDK based chains in the ecosystem. How it is implemented is key.\\nThe driving principles of the proposed design are:\\n1. **Extensibility:** it must be as riskless and painless as possible for application developers to set-up network\\nconfigurations to expose Rosetta API-compliant services.\\n2. **Long term support:** This proposal aims to provide support for all the supported Cosmos SDK release series.\\n3. **Cost-efficiency:** Backporting changes to Rosetta API specifications from `master` to the various stable\\nbranches of Cosmos SDK is a cost that needs to be reduced.\\nWe will achieve these delivering on these principles by the following:\\n1. There will be a package `rosetta\/lib`\\nfor the implementation of the core Rosetta API features, particularly:\\na. The types and interfaces (`Client`, `OfflineClient`...), this separates design from implementation detail.\\nb. The `Server` functionality as this is independent of the Cosmos SDK version.\\nc. The `Online\/OfflineNetwork`, which is not exported, and implements the rosetta API using the `Client` interface to query the node, build tx and so on.\\nd. The `errors` package to extend rosetta errors.\\n2. Due to differences between the Cosmos release series, each series will have its own specific implementation of `Client` interface.\\n3. There will be two options for starting an API service in applications:\\na. API shares the application process\\nb. API-specific process.\\n","tokens":106,"id":826,"Predictions":"## Decision:\\n[1] A simple API for integrating blockchain applications. This decision is based on the\\nUser's requirements. The decision is to be implemented as a REST API.\\n## Decision:\\n[2] A simple API for integrating blockchain applications. This decision is based on the\\nUser's requirements. The decision is to be implemented as a POST request.\\n## Decision:\\n[3] A simple API for integrating blockchain applications. This decision is based on\\nthe User's requirements. The decision is to be implemented as a PUT request.\\n## Decision:\\n[4] A simple API for integrating blockchain applications. This decision is based on\\nthe User's requirements. The decision is to be implemented as a DELETE request.\\n## Decision:\\n[5] A simple API for integrating blockchain applications. This decision is based on\\nthe User's requirements.\\n\\n## Context\\nRosetta provides a set of tools that allow developers to build cross-blockchain applications and interact with the Rosetta platform via an open-source specification and set of tools.\\nThe Rosetta APIs are designed to provide users with an easy-to-use and inexpensive means of interfacing with the Rosetta platform and its underlying technology.\\nThe APIs are designed to allow users to interact with the underlying technology in ways that are intuitive, efficient, and consistent.\\nThe Rosetta APIs are designed to be used by developers who wish to build cross-blockchain applications,\\nfor example, block explorers, wallets, dApps or other features that require access to the underlying technology.\\nThe APIs are intended for use by developers who wish to integrate their own blockchains into the Rosetta platform,\\nas well as other blockchains currently under development.\\nThe APIs are intended for use by developers who wish to integrate their own blockchains into the Rosetta platform,\\nas well as other blockchains currently under development.\\n\\n\\section{Decision}\\n\\label{decision}\\n\\n\\subsection{1}\\n\\label{decision-1}\\n\\n\\subsubsection{1.a}\\n\\label{decision-1-a}\\n\\nThe \\textit{Rosetta API} will provide an easy way of integrating new blockchains into \\textit{Rosetta}.\\n\\n\\subsubsection{1.b}\\n\\label{decision-1-b}\\n\\nThe \\textit{Rosetta API} will provide an easy way of integrating new blockchains into \\textit{Rosetta}.\\n\\n\\subsubsection{1.c}\\n\\label{decision-1-c}\\n\\nThe \\textit{Rosetta API} will provide an easy way of integrating new blockchains into \\textit{Rosetta}.\\n\\n\\subsubsection{2.a}\\n\\label{decision-2-a}\\n\\nThe \\textit{\\textbf{\\textsc{\\_}}R\\_S\\_T\\_A\\_P\\_I} will allow users of \\textbf{\\textsc{\\_}}R\\_S\\_T\\_A\\_P\\_I{}''s services (e.g., wallets,\\ndatabases) access to the underlying technology through a single set of interfaces.\\n\\n\\subsubsection*{(a)}\\n\\label{decision-a}\\n\\nThe \\textit{\\textbf{\\_}}R\\_S\\_T\\_A\\_P\\_I{}''s services (e.g., wallets,\\ndatabases) access to the underlying technology through a single set of interfaces.\\n\\n\\subsubsection*{(b)}\\n\\label{decision-b}\\n\\nThe \\textit{\\_}\\_\\_\\_\\_\\_\\_\\_R{}''s services (e.g., wallets,\\ndatabases) access to the underlying technology through a single set of interfaces.\\n\\n\\subsubsection*{(c)}\\n\\label{decision-c}\\n\\nThe \\_}\\_\\_\\_\\_\\_\\_\\_R{}''s services (e.g., wallets,\\ndatabases) access to the underlying technology through a single set of interfaces.\\n\\n"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"Predictions":"## Decision\\nThe decision is to implement a generic NFT module for Cosmos Hub and enable NFTs functionality on the Cosmos Hub.\\nThis module will be implemented in the [IRISnet](https:\/\/github.com\/irisnet\/irismod) and [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft)\\n## Comments\\n## Comments\\nThe decision is to implement a generic NFT module for Cosmos Hub and enable NFTs functionality on the Cosmos Hub.\\nThis module will be implemented in the [IRISnet](https:\/\/github.com\/irisnet\/irismod) and [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft)."}
{"File Name":"cosmos-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n* continuous voting\\n* slashing due to breach of soft contract\\n* revoking a member due to breach of soft contract\\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n* compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n* blockly stipend allowance per dCERT member\\n* maximum number of dCERT members\\n* required staked slashable tokens for each dCERT member\\n* quorum for suspending a particular member\\n* proposal wager for disbanding the dCERT group\\n* stabilization period for dCERT member transition\\n* circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n* All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n* In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n* A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n* Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n* In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n* In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n* A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n* change of the description of the dCERT group\\n* circuit break a message route\\n* vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":829,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software.  Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n\\n## Decision\\n## Decision\\n"}
{"File Name":"cosmos-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```text\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts (sybil attack), so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```text\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will get slashed at the sum 10% amount.\\nHowever in practice, we likely don't want a linear relation between amount of stake at fault, and the percentage of stake to slash. In particular, solely 5% of stake double signing effectively did nothing to majorly threaten security, whereas 30% of stake being at fault clearly merits a large slashing factor, due to being very close to the point at which Tendermint security is threatened. A linear relation would require a factor of 6 gap between these two, whereas the difference in risk posed to the network is much larger. We propose using S-curves (formally [logistic functions](https:\/\/en.wikipedia.org\/wiki\/Logistic_function) to solve this). S-Curves capture the desired criterion quite well. They allow the slashing factor to be minimal for small values, and then grow very rapidly near some threshold point where the risk posed becomes notable.\\n#### Parameterization\\nThis requires parameterizing a logistic function. It is very well understood how to parameterize this. It has four parameters:\\n1) A minimum slashing factor\\n2) A maximum slashing factor\\n3) The inflection point of the S-curve (essentially where do you want to center the S)\\n4) The rate of growth of the S-curve (How elongated is the S)\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Griefing\\nGriefing, the act of intentionally getting oneself slashed in order to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker also gets equally impacted by the grief as the victim, so it would not provide much benefit to the griefer.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occurred within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occurred within the last `jail period`.\\n```go\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nValidatorVotingPercent      sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `ValidatorVotingPercent` to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occurred, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":830,"Predictions":"## Decision\\nWe propose to use a Proof of Stake-based system where each validator is equipped with an amount of hashpower proportional to the number of delegators that it has delegated to.  Each validator is also assigned a weight, which is a function of the number of delegators that it has delegated to.  The weight is used to determine the delegator's level of contribution to the consensus process.  Each validator's weight is updated every time it delegates hashpower proportional to its number of delegators.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n## Decision\\nThe weight assigned to validators is updated after every block generation, which occurs every 2 minutes.\\n\\n\\n\\end{document}"}
{"File Name":"cosmos-sdk\/adr-041-in-place-store-migrations.md","Context":"## Context\\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd genesis export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https:\/\/github.com\/cosmos\/gaia\/blob\/v4.0.3\/docs\/migration\/cosmoshub-3.md#upgrade-procedure).\\nThis procedure is cumbersome for multiple reasons:\\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6936)).\\n","Decision":"We propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\\n### Module `ConsensusVersion`\\nWe introduce a new method on the `AppModule` interface:\\n```go\\ntype AppModule interface {\\n\/\/ --snip--\\nConsensusVersion() uint64\\n}\\n```\\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\\n### Module-Specific Migration Functions\\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\\n```go\\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\\n\/\/ --snip--\\ncfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 1 to 2.\\n})\\ncfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 2 to 3.\\n})\\n\/\/ etc.\\n}\\n```\\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\\n```go\\n\/\/ Migrator is a struct for handling in-place store migrations.\\ntype Migrator struct {\\nBaseKeeper\\n}\\n```\\nMigration functions should live inside the `migrations\/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\\n```go\\n\/\/ Migrate1to2 migrates from version 1 to 2.\\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\\nreturn v2bank.MigrateStore(ctx, m.keeper.storeKey) \/\/ v043bank is package `x\/bank\/migrations\/v2`.\\n}\\n```\\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x\/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/36f68eb9e041e20a5bb47e216ac5eb8b91f95471\/x\/bank\/legacy\/v043\/store.go#L41-L62).\\n### Tracking Module Versions in `x\/upgrade`\\nWe introduce a new prefix store in `x\/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key\/value format is:\\n```text\\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\\n```\\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\\n```diff\\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\\n```\\nTo apply an upgrade, we query the `VersionMap` from the `x\/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\\n```diff\\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\\n\/\/ --snip--\\n-   handler(ctx, plan)\\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) \/\/ k.GetModuleVersionMap() fetches the VersionMap stored in state.\\n+   if err != nil {\\n+       return err\\n+   }\\n+\\n+   \/\/ Set the updated consensus versions to state\\n+   k.SetModuleVersionMap(ctx, updatedVM)\\n}\\n```\\nA gRPC query endpoint to query the `VersionMap` stored in `x\/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\\n### Running Migrations\\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\\n* There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x\/upgrade`'s store.\\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\\n```go\\napp.UpgradeKeeper.SetUpgradeHandler(\"my-plan\", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\\nreturn app.mm.RunMigrations(ctx, vm)\\n})\\n```\\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x\/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\\n","tokens":247,"id":831,"Predictions":"## Decision\\nThis procedure is cumbersome for multiple reasons:\\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6936)).\\n\\nThe following steps are performed by the `initchain` command in this decision:\\n\\n1. Export the current genesis state as a JSON file (using the `simd genesis export` command):\\n\\n2. Run migration scripts on this JSON file (using the `simd genesis migrate` command):\\n\\n3. Clear stores:\\n\\n4. Start a new chain with the migrated JSON file as new genesis:\\n\\n5. Transfer this new chain with its new genesis:\\n\\n6. Stop chain\\n\\n## Decision\\n## Decision\\nThis procedure is cumbersome for multiple reasons:\\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6936)).\\n\\n\\n\\end{document}\\n"}
{"File Name":"cosmos-sdk\/adr-012-state-accessors.md","Context":"## Context\\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"We will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n* State access happens only when a function which takes a `Context` as an argument is invoked\\n* Accessor type structs give rights to access the state only that the struct is referring, no other\\n* Marshalling\/Unmarshalling happens implicitly within the core methods\\n","tokens":250,"id":832,"Predictions":"## The reviewer has decided that the function `Cosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While this provides a large degree of freedom to module developers, it is hard to modularize and the UX is mediocre. First, each time a module tries to access the state, it has to marshal the value and set or get the value and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions, which are repetitive and hard to maintain. Second, this makes it harder to align with the object capability theorem: the right to access the state is defined as a StoreKey, which gives full access on the entire Merkle tree, so a module cannot send the access right to a specific key-value pair (or a set of key-value pairs) to another module safely. Finally, because the getter\/setter functions are defined as methods of a module's Keeper, the reviewers have to consider the whole Merkle tree space when they reviewing a function accessing any part of the state. There is no static way to know which part of the state that the function is accessing (and which is not)."}
{"File Name":"cosmos-sdk\/adr-023-protobuf-naming.md","Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"The goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the Cosmos SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Guidelines on Stable Package Versions\\nIn general, schema evolution is the way to update protobuf schemas. That means that new fields,\\nmessages, and RPC methods are _added_ to existing schemas and old fields, messages and RPC methods\\nare maintained as long as possible.\\nBreaking things is often unacceptable in a blockchain scenario. For instance, immutable smart contracts\\nmay depend on certain data schemas on the host chain. If the host chain breaks those schemas, the smart\\ncontract may be irreparably broken. Even when things can be fixed (for instance in client software),\\nthis often comes at a high cost.\\nInstead of breaking things, we should make every effort to evolve schemas rather than just breaking them.\\n[Buf](https:\/\/buf.build) breaking change detection should be used on all stable (non-alpha or beta) packages\\nto prevent such breakage.\\nWith that in mind, different stable versions (i.e. `v1` or `v2`) of a package should more or less be considered\\ndifferent packages and this should be last resort approach for upgrading protobuf schemas. Scenarios where creating\\na `v2` may make sense are:\\n* we want to create a new module with similar functionality to an existing module and adding `v2` is the most natural\\nway to do this. In that case, there are really just two different, but similar modules with different APIs.\\n* we want to add a new revamped API for an existing module and it's just too cumbersome to add it to the existing package,\\nso putting it in `v2` is cleaner for users. In this case, care should be made to not deprecate support for\\n`v1` if it is actively used in immutable smart contracts.\\n#### Guidelines on unstable (alpha and beta) package versions\\nThe following guidelines are recommended for marking packages as alpha or beta:\\n* marking something as `alpha` or `beta` should be a last resort and just putting something in the\\nstable package (i.e. `v1` or `v2`) should be preferred\\n* a package _should_ be marked as `alpha` _if and only if_ there are active discussions to remove\\nor significantly alter the package in the near future\\n* a package _should_ be marked as `beta` _if and only if_ there is an active discussion to\\nsignificantly refactor\/rework the functionality in the near future but not remove it\\n* modules _can and should_ have types in both stable (i.e. `v1` or `v2`) and unstable (`alpha` or `beta`) packages.\\n_`alpha` and `beta` should not be used to avoid responsibility for maintaining compatibility._\\nWhenever code is released into the wild, especially on a blockchain, there is a high cost to changing things. In some\\ncases, for instance with immutable smart contracts, a breaking change may be impossible to fix.\\nWhen marking something as `alpha` or `beta`, maintainers should ask the questions:\\n* what is the cost of asking others to change their code vs the benefit of us maintaining the optionality to change it?\\n* what is the plan for moving this to `v1` and how will that affect users?\\n`alpha` or `beta` should really be used to communicate \"changes are planned\".\\nAs a case study, gRPC reflection is in the package `grpc.reflection.v1alpha`. It hasn't been changed since\\n2017 and it is now used in other widely used software like gRPCurl. Some folks probably use it in production services\\nand so if they actually went and changed the package to `grpc.reflection.v1`, some software would break and\\nthey probably don't want to do that... So now the `v1alpha` package is more or less the de-facto `v1`. Let's not do that.\\nThe following are guidelines for working with non-stable packages:\\n* [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages\\n* non-stable packages should generally be excluded from breaking change detection\\n* immutable smart contract modules (i.e. CosmWasm) _should_ block smart contracts\/persistent\\nscripts from interacting with `alpha`\/`beta` packages\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","tokens":330,"id":833,"Predictions":"### Decision\\n\\nThe decision to use `google.protobuf.Any` as the recommended approach for encoding\\ninterface types is made because we believe that it is a good choice for\\nencoding of interfaces.\\nThe reason for this is that the `Any` type is a [standard](https:\/\/www.w3.org\/TR\/2004\/REC-xml-20040403\/#any#any)\\ntype, which means that it can be used in any package, and we want to be able to use\\nprotobuf everywhere.\\nWe also believe that the `Any` type has the advantage of being able to handle\\nall types of message bodies, including strings and arrays. It is also small enough to\\nbe able to fit in memory easily.\\nWe feel that the main drawback of using `Any` as an encoding type is its lack of\\nsupport for message body encodings. It does not support `[encoding]`, nor does it have a\\nsupport for `[message]`. Instead, it provides a simple `[message]` type.\\nWe are aware that there are some [protobuf plugins](https:\/\/github.com\/google\/protobuf) which support the use of any as an encoding type, but we do not plan on using these plugins.\\n\\n## Process\\n## Process\\n\\nThis Decision Record was generated by `package.json`:\\n```\\n{\\n  \"name\": \"google.protobuf\",\\n  \"version\": \"0.2.0\",\\n  \"scripts\": {\\n    \"start\": \"python -m grpc.server\",\\n    \"test\": \"python -m grpc.server\",\\n    \"build\": \"yarn build --production --watch --verbose\"\\n  },\\n  \"author\": \"\",\\n  \"license\": {\\n    \"type\": \"MIT\"\\n  },\\n  \"dependencies\": {\\n    \"@types\/grpc-js\": \"^1.11.0\",\\n    \"@types\/node\": \"^12.14.1\",\\n    \"@types\/superagent\": \"^1.10.2\",\\n    \"@types\/superagent-http-proxy-server\": \"^2.0.1\",\\n    \"@types\/superagent-http-server-proxy-server\": \"^2.0.1\",\\n    \"@types\/superagent-wsgi-server-proxy-server\": \"^2.0.1\",\\n    \"@types\/node-gyp\": \"^8.x.x-dev\"\\n  },\\n  \"devDependencies\": {\\n    \"@babel\/core\": \"^7.8.5\",\\n    \"@babel\/preset-env\", \\n    \"@babel\/preset-react\", \\n    \"@babel\/preset-typescript\", \\n    \"@babel\/typescript-loader\", \\n    @babel\/plugin-proposal-class-properties,\\n      @babel\/plugin-proposal-decorators,\\n      @babel\/plugin-proposal-destructuring,\\n      @babel\/plugin-transform-runtime,\\n      @babel\/plugin-transform-string,\\n      @babel\/plugin-transform-regex,\\n      @babel\/plugin-transform-syntax-dynamic-import,\\n      @babel\/plugin-transform-string-array,\\n      @babel\/plugin-transform-string-formatter,\\n      @babel\/plugin-transform-string-formatter-fixtures,\\n      @babel\/plugin-transform-substitution\\n  }\\n}\\n```\\n## Decision\\n"}
{"File Name":"cosmos-sdk\/adr-038-state-listening.md","Context":"## Context\\nCurrently, KVStore data can be remotely accessed through [Queries](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/build\/building-modules\/02-messages-and-queries.md#queries)\\nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.\\nIn addition to these request\/response queries, it would be beneficial to have a means of listening to state changes as they occur in real time.\\n","Decision":"We will modify the `CommitMultiStore` interface and its concrete (`rootmulti`) implementations and introduce a new `listenkv.Store` to allow listening to state changes in underlying KVStores. We don't need to listen to cache stores, because we can't be sure that the writes will be committed eventually, and the writes are duplicated in `rootmulti.Store` eventually, so we should only listen to `rootmulti.Store`.\\nWe will introduce a plugin system for configuring and running streaming services that write these state changes and their surrounding ABCI message context to different destinations.\\n### Listening\\nIn a new file, `store\/types\/listening.go`, we will create a `MemoryListener` struct for streaming out protobuf encoded KV pairs state changes from a KVStore.\\nThe `MemoryListener` will be used internally by the concrete `rootmulti` implementation to collect state changes from KVStores.\\n```go\\n\/\/ MemoryListener listens to the state writes and accumulate the records in memory.\\ntype MemoryListener struct {\\nstateCache []StoreKVPair\\n}\\n\/\/ NewMemoryListener creates a listener that accumulate the state writes in memory.\\nfunc NewMemoryListener() *MemoryListener {\\nreturn &MemoryListener{}\\n}\\n\/\/ OnWrite writes state change events to the internal cache\\nfunc (fl *MemoryListener) OnWrite(storeKey StoreKey, key []byte, value []byte, delete bool) {\\nfl.stateCache = append(fl.stateCache, StoreKVPair{\\nStoreKey: storeKey.Name(),\\nDelete:   delete,\\nKey:      key,\\nValue:    value,\\n})\\n}\\n\/\/ PopStateCache returns the current state caches and set to nil\\nfunc (fl *MemoryListener) PopStateCache() []StoreKVPair {\\nres := fl.stateCache\\nfl.stateCache = nil\\nreturn res\\n}\\n```\\nWe will also define a protobuf type for the KV pairs. In addition to the key and value fields this message\\nwill include the StoreKey for the originating KVStore so that we can collect information from separate KVStores and determine the source of each KV pair.\\n```protobuf\\nmessage StoreKVPair {\\noptional string store_key = 1; \/\/ the store key for the KVStore this pair originates from\\nrequired bool set = 2; \/\/ true indicates a set operation, false indicates a delete operation\\nrequired bytes key = 3;\\nrequired bytes value = 4;\\n}\\n```\\n### ListenKVStore\\nWe will create a new `Store` type `listenkv.Store` that the `rootmulti` store will use to wrap a `KVStore` to enable state listening.\\nWe will configure the `Store` with a `MemoryListener` which will collect state changes for output to specific destinations.\\n```go\\n\/\/ Store implements the KVStore interface with listening enabled.\\n\/\/ Operations are traced on each core KVStore call and written to any of the\\n\/\/ underlying listeners with the proper key and operation permissions\\ntype Store struct {\\nparent    types.KVStore\\nlistener  *types.MemoryListener\\nparentStoreKey types.StoreKey\\n}\\n\/\/ NewStore returns a reference to a new traceKVStore given a parent\\n\/\/ KVStore implementation and a buffered writer.\\nfunc NewStore(parent types.KVStore, psk types.StoreKey, listener *types.MemoryListener) *Store {\\nreturn &Store{parent: parent, listener: listener, parentStoreKey: psk}\\n}\\n\/\/ Set implements the KVStore interface. It traces a write operation and\\n\/\/ delegates the Set call to the parent KVStore.\\nfunc (s *Store) Set(key []byte, value []byte) {\\ntypes.AssertValidKey(key)\\ns.parent.Set(key, value)\\ns.listener.OnWrite(s.parentStoreKey, key, value, false)\\n}\\n\/\/ Delete implements the KVStore interface. It traces a write operation and\\n\/\/ delegates the Delete call to the parent KVStore.\\nfunc (s *Store) Delete(key []byte) {\\ns.parent.Delete(key)\\ns.listener.OnWrite(s.parentStoreKey, key, nil, true)\\n}\\n```\\n### MultiStore interface updates\\nWe will update the `CommitMultiStore` interface to allow us to wrap a `Memorylistener` to a specific `KVStore`.\\nNote that the `MemoryListener` will be attached internally by the concrete `rootmulti` implementation.\\n```go\\ntype CommitMultiStore interface {\\n...\\n\/\/ AddListeners adds a listener for the KVStore belonging to the provided StoreKey\\nAddListeners(keys []StoreKey)\\n\/\/ PopStateCache returns the accumulated state change messages from MemoryListener\\nPopStateCache() []StoreKVPair\\n}\\n```\\n### MultiStore implementation updates\\nWe will adjust the `rootmulti` `GetKVStore` method to wrap the returned `KVStore` with a `listenkv.Store` if listening is turned on for that `Store`.\\n```go\\nfunc (rs *Store) GetKVStore(key types.StoreKey) types.KVStore {\\nstore := rs.stores[key].(types.KVStore)\\nif rs.TracingEnabled() {\\nstore = tracekv.NewStore(store, rs.traceWriter, rs.traceContext)\\n}\\nif rs.ListeningEnabled(key) {\\nstore = listenkv.NewStore(store, key, rs.listeners[key])\\n}\\nreturn store\\n}\\n```\\nWe will implement `AddListeners` to manage KVStore listeners internally and implement `PopStateCache`\\nfor a means of retrieving the current state.\\n```go\\n\/\/ AddListeners adds state change listener for a specific KVStore\\nfunc (rs *Store) AddListeners(keys []types.StoreKey) {\\nlistener := types.NewMemoryListener()\\nfor i := range keys {\\nrs.listeners[keys[i]] = listener\\n}\\n}\\n```\\n```go\\nfunc (rs *Store) PopStateCache() []types.StoreKVPair {\\nvar cache []types.StoreKVPair\\nfor _, ls := range rs.listeners {\\ncache = append(cache, ls.PopStateCache()...)\\n}\\nsort.SliceStable(cache, func(i, j int) bool {\\nreturn cache[i].StoreKey < cache[j].StoreKey\\n})\\nreturn cache\\n}\\n```\\nWe will also adjust the `rootmulti` `CacheMultiStore` and `CacheMultiStoreWithVersion` methods to enable listening in\\nthe cache layer.\\n```go\\nfunc (rs *Store) CacheMultiStore() types.CacheMultiStore {\\nstores := make(map[types.StoreKey]types.CacheWrapper)\\nfor k, v := range rs.stores {\\nstore := v.(types.KVStore)\\n\/\/ Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\\n\/\/ set same listeners on cache store will observe duplicated writes.\\nif rs.ListeningEnabled(k) {\\nstore = listenkv.NewStore(store, k, rs.listeners[k])\\n}\\nstores[k] = store\\n}\\nreturn cachemulti.NewStore(rs.db, stores, rs.keysByName, rs.traceWriter, rs.getTracingContext())\\n}\\n```\\n```go\\nfunc (rs *Store) CacheMultiStoreWithVersion(version int64) (types.CacheMultiStore, error) {\\n\/\/ ...\\n\/\/ Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\\n\/\/ set same listeners on cache store will observe duplicated writes.\\nif rs.ListeningEnabled(key) {\\ncacheStore = listenkv.NewStore(cacheStore, key, rs.listeners[key])\\n}\\ncachedStores[key] = cacheStore\\n}\\nreturn cachemulti.NewStore(rs.db, cachedStores, rs.keysByName, rs.traceWriter, rs.getTracingContext()), nil\\n}\\n```\\n### Exposing the data\\n#### Streaming Service\\nWe will introduce a new `ABCIListener` interface that plugs into the BaseApp and relays ABCI requests and responses\\nso that the service can group the state changes with the ABCI requests.\\n```go\\n\/\/ baseapp\/streaming.go\\n\/\/ ABCIListener is the interface that we're exposing as a streaming service.\\ntype ABCIListener interface {\\n\/\/ ListenFinalizeBlock updates the streaming service with the latest FinalizeBlock messages\\nListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error\\n\/\/ ListenCommit updates the steaming service with the latest Commit messages and state changes\\nListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []*StoreKVPair) error\\n}\\n```\\n#### BaseApp Registration\\nWe will add a new method to the `BaseApp` to enable the registration of `StreamingService`s:\\n```go\\n\/\/ SetStreamingService is used to set a streaming service into the BaseApp hooks and load the listeners into the multistore\\nfunc (app *BaseApp) SetStreamingService(s ABCIListener) {\\n\/\/ register the StreamingService within the BaseApp\\n\/\/ BaseApp will pass BeginBlock, DeliverTx, and EndBlock requests and responses to the streaming services to update their ABCI context\\napp.abciListeners = append(app.abciListeners, s)\\n}\\n```\\nWe will add two new fields to the `BaseApp` struct:\\n```go\\ntype BaseApp struct {\\n...\\n\/\/ abciListenersAsync for determining if abciListeners will run asynchronously.\\n\/\/ When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\\n\/\/ When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\\nabciListenersAsync bool\\n\/\/ stopNodeOnABCIListenerErr halts the node when ABCI streaming service listening results in an error.\\n\/\/ stopNodeOnABCIListenerErr=true must be paired with abciListenersAsync=false.\\nstopNodeOnABCIListenerErr bool\\n}\\n```\\n#### ABCI Event Hooks\\nWe will modify the `FinalizeBlock` and `Commit` methods to pass ABCI requests and responses\\nto any streaming service hooks registered with the `BaseApp`.\\n```go\\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) abci.ResponseFinalizeBlock {\\nvar abciRes abci.ResponseFinalizeBlock\\ndefer func() {\\n\/\/ call the streaming service hook with the FinalizeBlock messages\\nfor _, abciListener := range app.abciListeners {\\nctx := app.finalizeState.ctx\\nblockHeight := ctx.BlockHeight()\\nif app.abciListenersAsync {\\ngo func(req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) {\\nif err := app.abciListener.FinalizeBlock(blockHeight, req, res); err != nil {\\napp.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\\n}\\n}(req, abciRes)\\n} else {\\nif err := app.abciListener.ListenFinalizeBlock(blockHeight, req, res); err != nil {\\napp.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\\nif app.stopNodeOnABCIListenerErr {\\nos.Exit(1)\\n}\\n}\\n}\\n}\\n}()\\n...\\nreturn abciRes\\n}\\n```\\n```go\\nfunc (app *BaseApp) Commit() abci.ResponseCommit {\\n...\\nres := abci.ResponseCommit{\\nData:         commitID.Hash,\\nRetainHeight: retainHeight,\\n}\\n\/\/ call the streaming service hook with the Commit messages\\nfor _, abciListener := range app.abciListeners {\\nctx := app.deliverState.ctx\\nblockHeight := ctx.BlockHeight()\\nchangeSet := app.cms.PopStateCache()\\nif app.abciListenersAsync {\\ngo func(res abci.ResponseCommit, changeSet []store.StoreKVPair) {\\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\\napp.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\\n}\\n}(res, changeSet)\\n} else {\\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\\napp.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\\nif app.stopNodeOnABCIListenerErr {\\nos.Exit(1)\\n}\\n}\\n}\\n}\\n...\\nreturn res\\n}\\n```\\n#### Go Plugin System\\nWe propose a plugin architecture to load and run `Streaming` plugins and other types of implementations. We will introduce a plugin\\nsystem over gRPC that is used to load and run Cosmos-SDK plugins. The plugin system uses [hashicorp\/go-plugin](https:\/\/github.com\/hashicorp\/go-plugin).\\nEach plugin must have a struct that implements the `plugin.Plugin` interface and an `Impl` interface for processing messages over gRPC.\\nEach plugin must also have a message protocol defined for the gRPC service:\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/interface.go\\n\/\/ Handshake is a common handshake that is shared by streaming and host.\\n\/\/ This prevents users from executing bad plugins or executing a plugin\\n\/\/ directory. It is a UX feature, not a security feature.\\nvar Handshake = plugin.HandshakeConfig{\\nProtocolVersion:  1,\\nMagicCookieKey:   \"ABCI_LISTENER_PLUGIN\",\\nMagicCookieValue: \"ef78114d-7bdf-411c-868f-347c99a78345\",\\n}\\n\/\/ ListenerPlugin is the base struct for all kinds of go-plugin implementations\\n\/\/ It will be included in interfaces of different Plugins\\ntype ABCIListenerPlugin struct {\\n\/\/ GRPCPlugin must still implement the Plugin interface\\nplugin.Plugin\\n\/\/ Concrete implementation, written in Go. This is only used for plugins\\n\/\/ that are written in Go.\\nImpl baseapp.ABCIListener\\n}\\nfunc (p *ListenerGRPCPlugin) GRPCServer(_ *plugin.GRPCBroker, s *grpc.Server) error {\\nRegisterABCIListenerServiceServer(s, &GRPCServer{Impl: p.Impl})\\nreturn nil\\n}\\nfunc (p *ListenerGRPCPlugin) GRPCClient(\\n_ context.Context,\\n_ *plugin.GRPCBroker,\\nc *grpc.ClientConn,\\n) (interface{}, error) {\\nreturn &GRPCClient{client: NewABCIListenerServiceClient(c)}, nil\\n}\\n```\\nThe `plugin.Plugin` interface has two methods `Client` and `Server`. For our GRPC service these are `GRPCClient` and `GRPCServer`\\nThe `Impl` field holds the concrete implementation of our `baseapp.ABCIListener` interface written in Go.\\nNote: this is only used for plugin implementations written in Go.\\nThe advantage of having such a plugin system is that within each plugin authors can define the message protocol in a way that fits their use case.\\nFor example, when state change listening is desired, the `ABCIListener` message protocol can be defined as below (*for illustrative purposes only*).\\nWhen state change listening is not desired than `ListenCommit` can be omitted from the protocol.\\n```protobuf\\nsyntax = \"proto3\";\\n...\\nmessage Empty {}\\nmessage ListenFinalizeBlockRequest {\\nRequestFinalizeBlock  req = 1;\\nResponseFinalizeBlock res = 2;\\n}\\nmessage ListenCommitRequest {\\nint64                block_height = 1;\\nResponseCommit       res          = 2;\\nrepeated StoreKVPair changeSet    = 3;\\n}\\n\/\/ plugin that listens to state changes\\nservice ABCIListenerService {\\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\\n}\\n```\\n```protobuf\\n...\\n\/\/ plugin that doesn't listen to state changes\\nservice ABCIListenerService {\\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\\n}\\n```\\nImplementing the service above:\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/grpc.go\\nvar (\\n_ baseapp.ABCIListener = (*GRPCClient)(nil)\\n)\\n\/\/ GRPCClient is an implementation of the ABCIListener and ABCIListenerPlugin interfaces that talks over RPC.\\ntype GRPCClient struct {\\nclient ABCIListenerServiceClient\\n}\\nfunc (m *GRPCClient) ListenFinalizeBlock(goCtx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n_, err := m.client.ListenDeliverTx(ctx, &ListenDeliverTxRequest{BlockHeight: ctx.BlockHeight(), Req: req, Res: res})\\nreturn err\\n}\\nfunc (m *GRPCClient) ListenCommit(goCtx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n_, err := m.client.ListenCommit(ctx, &ListenCommitRequest{BlockHeight: ctx.BlockHeight(), Res: res, ChangeSet: changeSet})\\nreturn err\\n}\\n\/\/ GRPCServer is the gRPC server that GRPCClient talks to.\\ntype GRPCServer struct {\\n\/\/ This is the real implementation\\nImpl baseapp.ABCIListener\\n}\\nfunc (m *GRPCServer) ListenFinalizeBlock(ctx context.Context, req *ListenFinalizeBlockRequest) (*Empty, error) {\\nreturn &Empty{}, m.Impl.ListenFinalizeBlock(ctx, req.Req, req.Res)\\n}\\nfunc (m *GRPCServer) ListenCommit(ctx context.Context, req *ListenCommitRequest) (*Empty, error) {\\nreturn &Empty{}, m.Impl.ListenCommit(ctx, req.Res, req.ChangeSet)\\n}\\n```\\nAnd the pre-compiled Go plugin `Impl`(*this is only used for plugins that are written in Go*):\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/impl\/plugin.go\\n\/\/ Plugins are pre-compiled and loaded by the plugin system\\n\/\/ ABCIListener is the implementation of the baseapp.ABCIListener interface\\ntype ABCIListener struct{}\\nfunc (m *ABCIListenerPlugin) ListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\\n\/\/ send data to external system\\n}\\nfunc (m *ABCIListenerPlugin) ListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\\n\/\/ send data to external system\\n}\\nfunc main() {\\nplugin.Serve(&plugin.ServeConfig{\\nHandshakeConfig: grpc_abci_v1.Handshake,\\nPlugins: map[string]plugin.Plugin{\\n\"grpc_plugin_v1\": &grpc_abci_v1.ABCIListenerGRPCPlugin{Impl: &ABCIListenerPlugin{}},\\n},\\n\/\/ A non-nil value here enables gRPC serving for this streaming...\\nGRPCServer: plugin.DefaultGRPCServer,\\n})\\n}\\n```\\nWe will introduce a plugin loading system that will return `(interface{}, error)`.\\nThis provides the advantage of using versioned plugins where the plugin interface and gRPC protocol change over time.\\nIn addition, it allows for building independent plugin that can expose different parts of the system over gRPC.\\n```go\\nfunc NewStreamingPlugin(name string, logLevel string) (interface{}, error) {\\nlogger := hclog.New(&hclog.LoggerOptions{\\nOutput: hclog.DefaultOutput,\\nLevel:  toHclogLevel(logLevel),\\nName:   fmt.Sprintf(\"plugin.%s\", name),\\n})\\n\/\/ We're a host. Start by launching the streaming process.\\nenv := os.Getenv(GetPluginEnvKey(name))\\nclient := plugin.NewClient(&plugin.ClientConfig{\\nHandshakeConfig: HandshakeMap[name],\\nPlugins:         PluginMap,\\nCmd:             exec.Command(\"sh\", \"-c\", env),\\nLogger:          logger,\\nAllowedProtocols: []plugin.Protocol{\\nplugin.ProtocolNetRPC, plugin.ProtocolGRPC},\\n})\\n\/\/ Connect via RPC\\nrpcClient, err := client.Client()\\nif err != nil {\\nreturn nil, err\\n}\\n\/\/ Request streaming plugin\\nreturn rpcClient.Dispense(name)\\n}\\n```\\nWe propose a `RegisterStreamingPlugin` function for the App to register `NewStreamingPlugin`s with the App's BaseApp.\\nStreaming plugins can be of `Any` type; therefore, the function takes in an interface vs a concrete type.\\nFor example, we could have plugins of `ABCIListener`, `WasmListener` or `IBCListener`. Note that `RegisterStreamingPluing` function\\nis helper function and not a requirement. Plugin registration can easily be moved from the App to the BaseApp directly.\\n```go\\n\/\/ baseapp\/streaming.go\\n\/\/ RegisterStreamingPlugin registers streaming plugins with the App.\\n\/\/ This method returns an error if a plugin is not supported.\\nfunc RegisterStreamingPlugin(\\nbApp *BaseApp,\\nappOpts servertypes.AppOptions,\\nkeys map[string]*types.KVStoreKey,\\nstreamingPlugin interface{},\\n) error {\\nswitch t := streamingPlugin.(type) {\\ncase ABCIListener:\\nregisterABCIListenerPlugin(bApp, appOpts, keys, t)\\ndefault:\\nreturn fmt.Errorf(\"unexpected plugin type %T\", t)\\n}\\nreturn nil\\n}\\n```\\n```go\\nfunc registerABCIListenerPlugin(\\nbApp *BaseApp,\\nappOpts servertypes.AppOptions,\\nkeys map[string]*store.KVStoreKey,\\nabciListener ABCIListener,\\n) {\\nasyncKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIAsync)\\nasync := cast.ToBool(appOpts.Get(asyncKey))\\nstopNodeOnErrKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIStopNodeOnErrTomlKey)\\nstopNodeOnErr := cast.ToBool(appOpts.Get(stopNodeOnErrKey))\\nkeysKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIKeysTomlKey)\\nexposeKeysStr := cast.ToStringSlice(appOpts.Get(keysKey))\\nexposedKeys := exposeStoreKeysSorted(exposeKeysStr, keys)\\nbApp.cms.AddListeners(exposedKeys)\\napp.SetStreamingManager(\\nstoretypes.StreamingManager{\\nABCIListeners: []storetypes.ABCIListener{abciListener},\\nStopNodeOnErr: stopNodeOnErr,\\n},\\n)\\n}\\n```\\n```go\\nfunc exposeAll(list []string) bool {\\nfor _, ele := range list {\\nif ele == \"*\" {\\nreturn true\\n}\\n}\\nreturn false\\n}\\nfunc exposeStoreKeys(keysStr []string, keys map[string]*types.KVStoreKey) []types.StoreKey {\\nvar exposeStoreKeys []types.StoreKey\\nif exposeAll(keysStr) {\\nexposeStoreKeys = make([]types.StoreKey, 0, len(keys))\\nfor _, storeKey := range keys {\\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\\n}\\n} else {\\nexposeStoreKeys = make([]types.StoreKey, 0, len(keysStr))\\nfor _, keyStr := range keysStr {\\nif storeKey, ok := keys[keyStr]; ok {\\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\\n}\\n}\\n}\\n\/\/ sort storeKeys for deterministic output\\nsort.SliceStable(exposeStoreKeys, func(i, j int) bool {\\nreturn exposeStoreKeys[i].Name() < exposeStoreKeys[j].Name()\\n})\\nreturn exposeStoreKeys\\n}\\n```\\nThe `NewStreamingPlugin` and `RegisterStreamingPlugin` functions are used to register a plugin with the App's BaseApp.\\ne.g. in `NewSimApp`:\\n```go\\nfunc NewSimApp(\\nlogger log.Logger,\\ndb dbm.DB,\\ntraceStore io.Writer,\\nloadLatest bool,\\nappOpts servertypes.AppOptions,\\nbaseAppOptions ...func(*baseapp.BaseApp),\\n) *SimApp {\\n...\\nkeys := sdk.NewKVStoreKeys(\\nauthtypes.StoreKey, banktypes.StoreKey, stakingtypes.StoreKey,\\nminttypes.StoreKey, distrtypes.StoreKey, slashingtypes.StoreKey,\\ngovtypes.StoreKey, paramstypes.StoreKey, ibchost.StoreKey, upgradetypes.StoreKey,\\nevidencetypes.StoreKey, ibctransfertypes.StoreKey, capabilitytypes.StoreKey,\\n)\\n...\\n\/\/ register streaming services\\nstreamingCfg := cast.ToStringMap(appOpts.Get(baseapp.StreamingTomlKey))\\nfor service := range streamingCfg {\\npluginKey := fmt.Sprintf(\"%s.%s.%s\", baseapp.StreamingTomlKey, service, baseapp.StreamingPluginTomlKey)\\npluginName := strings.TrimSpace(cast.ToString(appOpts.Get(pluginKey)))\\nif len(pluginName) > 0 {\\nlogLevel := cast.ToString(appOpts.Get(flags.FlagLogLevel))\\nplugin, err := streaming.NewStreamingPlugin(pluginName, logLevel)\\nif err != nil {\\ntmos.Exit(err.Error())\\n}\\nif err := baseapp.RegisterStreamingPlugin(bApp, appOpts, keys, plugin); err != nil {\\ntmos.Exit(err.Error())\\n}\\n}\\n}\\nreturn app\\n```\\n#### Configuration\\nThe plugin system will be configured within an App's TOML configuration files.\\n```toml\\n# gRPC streaming\\n[streaming]\\n# ABCI streaming service\\n[streaming.abci]\\n# The plugin version to use for ABCI listening\\nplugin = \"abci_v1\"\\n# List of kv store keys to listen to for state changes.\\n# Set to [\"*\"] to expose all keys.\\nkeys = [\"*\"]\\n# Enable abciListeners to run asynchronously.\\n# When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\\n# When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\\nasync = false\\n# Whether to stop the node on message deliver error.\\nstop-node-on-err = true\\n```\\nThere will be four parameters for configuring `ABCIListener` plugin: `streaming.abci.plugin`, `streaming.abci.keys`, `streaming.abci.async` and `streaming.abci.stop-node-on-err`.\\n`streaming.abci.plugin` is the name of the plugin we want to use for streaming, `streaming.abci.keys` is a set of store keys for stores it listens to,\\n`streaming.abci.async` is bool enabling asynchronous listening and `streaming.abci.stop-node-on-err` is a bool that stops the node when true and when operating\\non synchronized mode `streaming.abci.async=false`. Note that `streaming.abci.stop-node-on-err=true` will be ignored if `streaming.abci.async=true`.\\nThe configuration above support additional streaming plugins by adding the plugin to the `[streaming]` configuration section\\nand registering the plugin with `RegisterStreamingPlugin` helper function.\\nNote the that each plugin must include `streaming.{service}.plugin` property as it is a requirement for doing the lookup and registration of the plugin\\nwith the App. All other properties are unique to the individual services.\\n#### Encoding and decoding streams\\nADR-038 introduces the interfaces and types for streaming state changes out from KVStores, associating this\\ndata with their related ABCI requests and responses, and registering a service for consuming this data and streaming it to some destination in a final format.\\nInstead of prescribing a final data format in this ADR, it is left to a specific plugin implementation to define and document this format.\\nWe take this approach because flexibility in the final format is necessary to support a wide range of streaming service plugins. For example,\\nthe data format for a streaming service that writes the data out to a set of files will differ from the data format that is written to a Kafka topic.\\n","tokens":93,"id":834,"Predictions":"## Decision\\nIn order to allow the end user to easily monitor the state of their data, we will add a `state` column to the KVStore data. The `state` column will be populated with a value for each event that is sent from the client to the server. This value will be used by the client to determine if a change has occurred in their data, and if so, what has changed.\\n## Decision\\nThe client can then use this information to determine what actions need to be taken. For example, if the client receives an error message stating that it cannot connect to the server, it can use this information to determine whether or not it should retry connecting.\\n## Decision\\nThe client will also be able to query the server for additional information about its data. For example, if a user sends an update request but does not receive a response after 10 seconds, they can query for additional information about their data.\\n## Decision\\nIf you are interested in contributing your code or ideas for this project, please see our GitLab repository: \\url{https:\/\/gitlab.com\/cosmos\/cosmos-sdk}\\n\\end{itemize}\\n\\n\\section{Architectural Decisions}\\n\\label{sec:architectural-decisions}\\nThis section describes some of the decisions made by the team while designing \\texttt{KVStore}.\\n\\n\\subsection{Data Model}\\n\\label{subsec:data-model}\\n\\nAs mentioned in Section~\\ref{subsubsec:data-model}, \\texttt{KVStore} is designed as a distributed storage system. In order for this system to function properly, we need to model our database as a set of nodes and connections between these nodes. Nodes are represented by JSON objects which contain fields such as name and type. Connections are represented by JSON objects which contain fields such as name and type.\\n\\nNodes are connected through connections which hold messages from one node (or collection of nodes) to another (or collection of nodes). A connection is created using an RPC call on one node which returns an object containing fields such as name and type.\\n\\nThis design allows us to represent different types of entities using different types of objects.\\n\\nThe following diagram shows how connections work in \\texttt{KVStore}:\\n\\n\\begin{figure}[H]\\n\t\\centering\\n\t\\includegraphics[width=0.8\\linewidth]{figures\/connections.png}\\n\t\\caption{\\n\t\tA diagram showing how connections work in \\texttt{KVStore}. Each connection is represented by a JSON object containing fields such as name and type.\\label{fig:connections}}\\n\\end{figure}\\n\\nWe have chosen this design because it allows us to model different types of entities using different types of objects.\\n\\nFor example, we could represent users as JSON objects which contain fields such as name and type. We could also represent users' messages using messages which contain fields such as sender and messageText.\\n\\nIn addition to these two models (users and messages), there are two more models which we have not yet implemented: notifications and subscriptions.\\n\\nNotifications are used when you want your app's users to get notified when certain events occur on your app's servers. For example, you might want your users' notifications when there is a new message in their conversation group.\\n\\nSubscriptions are used when you want your users' notifications when something happens on your app's servers (for example, sending them an update notification). For example, you might want your users' notifications when there is a new message in their conversation group.\\n\\nEach user has their own set of subscriptions associated with them (which we call ``subscription groups'').\\n\\nAs mentioned previously, users can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes. Subscriptions allow users' apps access all their subscription groups without having access only one subscription group at any given time.\\n\\nWhen subscribing or unsubscribing from subscriptions on one node or collection of nodes (or from all subscription groups), each node sends messages with either ``unsubscribed'' or ``subscribed'' status depending on whether they have received any messages since they last subscribed or unsubscribed from that subscription group.\\n\\nFor example:\\n\\n``unsubscribed'' means that they have not received any messages since they last subscribed.\\n\\n``subscribed'' means that they have received at least one message since they last subscribed.\\n\\n\\nA notification object contains both ``unsubscribed'' and ``subscribed'' states.\\nAs mentioned previously, subscribers can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes.\\n\\n\\nA subscription object contains both ``unsubscribed'' and ``subscribed'' states.\\nAs mentioned previously, subscribers can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes.\\n \\nWe have chosen this design because it allows us to represent different types of entities using different types of objects.\\n\\n\\nFor example, we could represent users as JSON objects which contain fields such as name and type. We could also represent users' messages using messages which contain fields such as sender and messageText.\\n\\n\\nIn addition to these two models (users and messages), there are two more models which we have not yet implemented: notifications and subscriptions.\\n\\n\\nNotifications are used when you want your app's users to get notified when certain events occur on your app's servers. For example, you might want your users' notifications when there is a new message in their conversation group.\\n\\n\\nSubscriptions are used when you want your users' notifications when something happens on your app's servers (for example sending them an update notification). For example, you might want your users' notifications when there is a new message in their conversation group.\\n\\n\\nEach user has their own set of subscriptions associated with them (which we call ``subscription groups'').\\n\\n\\nAs mentioned previously, subscribers can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes.\\n\\n\\nA notification object contains both ``unsubscribed'' and ``subscribed'' states.\\nAs mentioned previously, subscribers can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes.\\n\\n\\nA subscription object contains both ``unsubscribed'' and ``subscribed'' states.\\nAs mentioned previously, subscribers can subscribe or unsubscribe from subscriptions at any time via API calls on one node or collection of nodes.\\n\\n\\nA notification object contains both ``unsubscribed'' and ``subscribed'' states.\\nAs mentioned previously,\\nsubscriptions allow subscribers' apps access all their subscription groups without having access only one subscription group at any given time.\\n\\n\\n\\nWe have chosen this design because it allows us to represent different types of entities using different types of objects.\\n\\n\\n\\n\\nFor example,\\nwe could represent users as JSON objects which contain fields such as name\\nand type.\\nWe could also represent users' messages using messages\\nwhich contain fields such as sender\\nand messageText.\\n\\n\\n\\nIn addition\\nto these two models\\n(users\\nand\\nmessages),\\nthere\\nare two more models\\n(which we call \"notifications\" \\nand \"subscriptions\") \\nwhich we have not yet implemented:\\nnotifications \\nare used \\nwhen you want your app's clients\\nto get notified whenever something happens \\non other clients'\\nservers;\\nsubscriptions allow clients'\\nclients' apps access all their subscription groups without having access only one subscription group at any given time.\\n\\n\\n\\nWe have chosen this design because it allows us \\nto represent different types \\nof entities using different types\\nof objects.\\n\\n\\n\\nFor example,\\nwe could represent clients'\\nclients' apps \\nas JSON objects which contain fields like \"name\"\\nand \"type\".\\nWe could also represent clients'\\nclients'\\nclients'\\nclients'\\nclients'\\nclients'\\nclients'\\nclients'\\nclients'\\nclients'\\n\\nClients\\n\\nClients\\n\\nClients\\n\\nClients\\n\\nClients\\n\\nClients\\n\\n\\nBecause each client has its own set \\nof subscriptions associated with it,\\nwe do not need separate model for each client;\\neach client has its own set \\nof subscriptions associated with it,\\nwhich we call \"subscription groups\" }\\n}\\n\\n\\nWe have chosen this design because it allows us \\n\\nto portray different kinds \\n\\nof entities \\n\\nusing \\n\\ndifferent kinds \\n\\nof \\n\\nobjects\\n\\n\\n\\nBecause each user has his\/her own set \\n\\nof subscription groups associated with him\/her,\\n\\nwe do not need separate model for each user;\\n\\neach user has his\/her own set \\n\\nof subscription groups associated with him\/her,\\n\\nwhich we call \"subscription groups\" }}\\n\\n\\nBecause each subscriber has its own set \\n\\nof subscription groups associated with him\/hers,\\n\\nwe do not need separate model for each subscriber;\\n\\neach subscriber has his\/her own set \\n\\nof subscription groups associated with him\/hers,\\n\\nwhich we call \"subscription groups\" }}\\n\\n\\nbecause\\n\\neach subscriber has his\/her own \\n\\nset\\n\\nof subscription groups associated with him\/hers,\\n\\nwe do not need separate model for each subscriber;\\n\\neach subscriber has his\/her own set \\n\\nof subscription groups associated with him\/hers,\\n\\n\\nWhich leads us into our next decision:\\nthe number 1-3-5-7-9-11-13-15-17-19-21-23-25-\\n27-\\n\\nWhich leads us into our next decision:\\n\\nthe number 1-3-5-\\n7-\\n9-\\n11-\\n\\nWhich leads us into our next decision:\\n\\nthe number 1-\\n3-\\n5-\\n7-\\n9-\\n11-\\n\\nWhich leads us into our next decision:\\n\\nthe number 1-\\n3-\\n5-\\n7-\\n\\nWhich leads us into our next decision:\\nthe number 1-\\n\\nWhich leads us into our next decision:\\n\\nthe number 1-\\n\\nWhich leads us into our next decision:\\nthe number 1-\\n\\nWhich leads us into our next decision:\\nthe number 1-\\n\\nWhich leads us into our next decision:\\n\\nthe number 1-\\n\\nwhich leaves\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows\\n\\n\\nshows \\n\\n\\nshows \\n\\n\\nshows \\n\\n\\nshows \\n\\n\\nshows \\n\\n\\nshows \\n\\n\\nShows\\n\\n\\nShows\\n\\n\\nShows\\n\\n\\nShows\\n\\n\\n\\nThat leaves\\n\\n\\nShows\\n\\n\\n\\nShows\\n\\n\\n\\nThat leaves \\n\\n\\nShows\\n\\n\\n\\nThat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\nthat leaves \\n\\n\\n\\n\\nthat leaves \\n\\n\\n\\n\\nthat leaves \\n\\n\\n\\n\\nthat leaves \\n\\n\\n\\n\\nthat leave\\n\\n\\n\\n\\n\\nThat leave\\n\\n\\n\\n\\nThat leave\\n\\n\\n\\n\\nThat leave\\n\\n\\n\\n\\n\\nThat leave\\n\\n\\n\\n\\nThat leave\\n\\n\\n\\n\\nAnd finally,\\n\\n\\nThat leave\\n\\n\\n\\n\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\nAnd finally,\\n\\n\\n\\nFinally,\\n\\nFinally,\\n\\nFinally,\\n\\nFinally,\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally,\\n\\n\\n\\nFinally\\n\\n\\n\\n\\n\\nBecause each client has its own set  \\n\\nOf  \\n\\nSubscription  \\n\\nGroups  \\n\\nAssociated  \\n\\nWith Him\/Hers,\\n\\nwe do not need separate model  \\n\\nFor Each User;\\n\\nEach User Has His\/her Own Set  \\n\\nOf Subscription Groups Associated With Him\/Hers;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because each user has his\/her   \\nown   \\nset   \\nOf    \\nsubscription   \\ngroups   \\nassociated   \\nwith him\/hers;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because each user has his\/her   \\nown    \\nset   \\nOf    \\nsubscription   \\ngroups   \\nassociated   \\n\\nwith him\/hers;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because each user  \\nhas his\/her   \\n\\n\\n\\n\\n Because each subscriber  \\nhas his\/her   \\n\\n\\n\\n\\n Because each subscriber  \\nhas his\/her   \\n\\n\\n\\n\\n Because each subscriber  \\nhas his\/her   \\n\\n\\n\\n\\n Because each subscriber  \\nhas his\/her   \\n\\n\\n\\n\\n\\n Because every subscriber belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because every subgroup belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because every subgroup belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Because every subgroup belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n Because every subgroup belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBecause every subgroup belongs  \\nto multiple subgroups;  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBecause every subgroup belongs  \\nto multiple subgroups;\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\n\\nBecause every subgroup belongs    \\nto multiple subgroups;\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group;\\n\\n\\n\\n\\nbecause Every Subgroup Has Its Own Membership Group"}
{"File Name":"cosmos-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"### Context\\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\\nthe same serialization of a `SignDoc` as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization.\\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L30)\\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-020-protobuf-transaction-encoding.md#transactions))\\nby converting all [Tx](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L13)\\nfields to bytes on the client side. This adds an additional manual\\nstep when sending and signing transactions.\\n### Decision\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","Decision":"The following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","tokens":284,"id":835,"Predictions":"### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n### Decision\\n## Decision\\nThe signing and verification process should be performed in a single\\nstep.\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n\\n## Decision\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n### Decision\\nThe signing and verification process should be performed in a single\\nstep.\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n### Decision\\nThe signing and verification process should be performed in a single\\nstep.\\n### Decision\\nThe signing and verification process should be performed in a single\\nstep.\\n"}
{"File Name":"cosmos-sdk\/adr-031-msg-service.md","Context":"## Context\\nCurrently `Msg` handlers in the Cosmos SDK do have return values that are placed in the `data` field of the response.\\nThese return values, however, are not specified anywhere except in the golang handler code.\\nIn early conversations [it was proposed](https:\/\/docs.google.com\/document\/d\/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc\/edit)\\nthat `Msg` return types be captured using a protobuf extension field, ex:\\n```protobuf\\npackage cosmos.gov;\\nmessage MsgSubmitProposal\\noption (cosmos_proto.msg_return) = \u201cuint64\u201d;\\nstring delegator_address = 1;\\nstring validator_address = 2;\\nrepeated sdk.Coin amount = 3;\\n}\\n```\\nThis was never adopted, however.\\nHaving a well-specified return value for `Msg`s would improve client UX. For instance,\\nin `x\/gov`,  `MsgSubmitProposal` returns the proposal ID as a big-endian `uint64`.\\nThis isn\u2019t really documented anywhere and clients would need to know the internals\\nof the Cosmos SDK to parse that value and return it to users.\\nAlso, there may be cases where we want to use these return values programmatically.\\nFor instance, https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093 proposes a method for\\ndoing inter-module Ocaps using the `Msg` router. A well-defined return type would\\nimprove the developer UX for this approach.\\nIn addition, handler registration of `Msg` types tends to add a bit of\\nboilerplate on top of keepers and is usually done through manual type switches.\\nThis isn't necessarily bad, but it does add overhead to creating modules.\\n","Decision":"We decide to use protobuf `service` definitions for defining `Msg`s as well as\\nthe code generated by them as a replacement for `Msg` handlers.\\nBelow we define how this will look for the `SubmitProposal` message from `x\/gov` module.\\nWe start with a `Msg` `service` definition:\\n```protobuf\\npackage cosmos.gov;\\nservice Msg {\\nrpc SubmitProposal(MsgSubmitProposal) returns (MsgSubmitProposalResponse);\\n}\\n\/\/ Note that for backwards compatibility this uses MsgSubmitProposal as the request\\n\/\/ type instead of the more canonical MsgSubmitProposalRequest\\nmessage MsgSubmitProposal {\\ngoogle.protobuf.Any content = 1;\\nstring proposer = 2;\\n}\\nmessage MsgSubmitProposalResponse {\\nuint64 proposal_id;\\n}\\n```\\nWhile this is most commonly used for gRPC, overloading protobuf `service` definitions like this does not violate\\nthe intent of the [protobuf spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#services) which says:\\n> If you don\u2019t want to use gRPC, it\u2019s also possible to use protocol buffers with your own RPC implementation.\\nWith this approach, we would get an auto-generated `MsgServer` interface:\\nIn addition to clearly specifying return types, this has the benefit of generating client and server code. On the server\\nside, this is almost like an automatically generated keeper method and could maybe be used instead of keepers eventually\\n(see [\\#7093](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093)):\\n```go\\npackage gov\\ntype MsgServer interface {\\nSubmitProposal(context.Context, *MsgSubmitProposal) (*MsgSubmitProposalResponse, error)\\n}\\n```\\nOn the client side, developers could take advantage of this by creating RPC implementations that encapsulate transaction\\nlogic. Protobuf libraries that use asynchronous callbacks, like [protobuf.js](https:\/\/github.com\/protobufjs\/protobuf.js#using-services)\\ncould use this to register callbacks for specific messages even for transactions that include multiple `Msg`s.\\nEach `Msg` service method should have exactly one request parameter: its corresponding `Msg` type. For example, the `Msg` service method `\/cosmos.gov.v1beta1.Msg\/SubmitProposal` above has exactly one request parameter, namely the `Msg` type `\/cosmos.gov.v1beta1.MsgSubmitProposal`. It is important the reader understands clearly the nomenclature difference between a `Msg` service (a Protobuf service) and a `Msg` type (a Protobuf message), and the differences in their fully-qualified name.\\nThis convention has been decided over the more canonical `Msg...Request` names mainly for backwards compatibility, but also for better readability in `TxBody.messages` (see [Encoding section](#encoding) below): transactions containing `\/cosmos.gov.MsgSubmitProposal` read better than those containing `\/cosmos.gov.v1beta1.MsgSubmitProposalRequest`.\\nOne consequence of this convention is that each `Msg` type can be the request parameter of only one `Msg` service method. However, we consider this limitation a good practice in explicitness.\\n### Encoding\\nEncoding of transactions generated with `Msg` services do not differ from current Protobuf transaction encoding as defined in [ADR-020](.\/adr-020-protobuf-transaction-encoding.md). We are encoding `Msg` types (which are exactly `Msg` service methods' request parameters) as `Any` in `Tx`s which involves packing the\\nbinary-encoded `Msg` with its type URL.\\n### Decoding\\nSince `Msg` types are packed into `Any`, decoding transactions messages are done by unpacking `Any`s into `Msg` types. For more information, please refer to [ADR-020](.\/adr-020-protobuf-transaction-encoding.md#transactions).\\n### Routing\\nWe propose to add a `msg_service_router` in BaseApp. This router is a key\/value map which maps `Msg` types' `type_url`s to their corresponding `Msg` service method handler. Since there is a 1-to-1 mapping between `Msg` types and `Msg` service method, the `msg_service_router` has exactly one entry per `Msg` service method.\\nWhen a transaction is processed by BaseApp (in CheckTx or in DeliverTx), its `TxBody.messages` are decoded as `Msg`s. Each `Msg`'s `type_url` is matched against an entry in the `msg_service_router`, and the respective `Msg` service method handler is called.\\nFor backward compatibility, the old handlers are not removed yet. If BaseApp receives a legacy `Msg` with no corresponding entry in the `msg_service_router`, it will be routed via its legacy `Route()` method into the legacy handler.\\n### Module Configuration\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), we introduced a method `RegisterQueryService`\\nto `AppModule` which allows for modules to register gRPC queriers.\\nTo register `Msg` services, we attempt a more extensible approach by converting `RegisterQueryService`\\nto a more generic `RegisterServices` method:\\n```go\\ntype AppModule interface {\\nRegisterServices(Configurator)\\n...\\n}\\ntype Configurator interface {\\nQueryServer() grpc.Server\\nMsgServer() grpc.Server\\n}\\n\/\/ example module:\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ntypes.RegisterQueryServer(cfg.QueryServer(), keeper)\\ntypes.RegisterMsgServer(cfg.MsgServer(), keeper)\\n}\\n```\\nThe `RegisterServices` method and the `Configurator` interface are intended to\\nevolve to satisfy the use cases discussed in [\\#7093](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093)\\nand [\\#7122](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7421).\\nWhen `Msg` services are registered, the framework _should_ verify that all `Msg` types\\nimplement the `sdk.Msg` interface and throw an error during initialization rather\\nthan later when transactions are processed.\\n### `Msg` Service Implementation\\nJust like query services, `Msg` service methods can retrieve the `sdk.Context`\\nfrom the `context.Context` parameter method using the `sdk.UnwrapSDKContext`\\nmethod:\\n```go\\npackage gov\\nfunc (k Keeper) SubmitProposal(goCtx context.Context, params *types.MsgSubmitProposal) (*MsgSubmitProposalResponse, error) {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n...\\n}\\n```\\nThe `sdk.Context` should have an `EventManager` already attached by BaseApp's `msg_service_router`.\\nSeparate handler definition is no longer needed with this approach.\\n","tokens":386,"id":836,"Predictions":"The following decision is based on the assumption that the user has already registered a module for `Msg` types.\\nThe user may not have registered any module for `Msg` types, in which case this decision will be ignored.\\n\\n## Context\\nCurrently, the Cosmos SDK does not have any way to return a value from a handler. It would be useful to be able to do this.\\nThe Cosmos SDK does have a way to return a value from a message handler, however, it requires that the handler be registered.\\nThis means that the user would need to register a module for `Msg` types in order to make use of this feature.\\n\\n## Decision\\nThe following decision is based on the assumption that the user has already registered a module for `Msg` types.\\nThe user may not have registered any module for `Msg` types, in which case this decision will be ignored.\\n\\n## Context\\nCurrently, if you want to use an RPC call with one of our generic clients (e.g. https:\/\/github.com\/cosmos\/cosmos-sdk), you will need to register it as an RPC. This means that you would need to register an RPC for each type of message we support. For example, if you want to use RPCs for all `Msg`, you would need to register 5 RPCs and specify 5 different handlers. This is cumbersome and prevents us from being able to provide generic clients.\\nHowever, we are currently working on providing generic clients that can use RPCs and handlers. We expect these clients will allow us to provide RPCs and handlers for all messages without registering them separately.\\nIn addition, we are also planning on providing generic clients that can use handlers directly instead of registering them separately. These new clients will allow us to provide handlers directly without requiring users to register them separately.\\n# \\t\\n\\n* \\t\\n\\n* \\t\\n\\n* \\t\\n\\n* \\t\\n\\n* \\t\\n\\n* \\t\\n## Decision\\nThe following decision is based on the assumption that the user has already registered a module for `Msg` types.\\nThe user may not have registered any module for `Msg` types, in which case this decision will be ignored.\\n\\n## Context\\nCurrently, if you want to use an RPC call with one of our generic clients (e.g. https:\/\/github.com\/cosmos\/cosmos-sdk), you will need to register it as an RPC. This means that you would need to register an RPC for each type of message we support. For example, if you want to use RPCs for all `Msg`, you would need to register 5 RPCs and specify 5 different handlers. This is cumbersome and prevents us from being able to provide generic clients.\\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n"}
{"File Name":"cosmos-sdk\/adr-029-fee-grant-module.md","Context":"## Context\\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\\nbarrier to adoption.\\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\\na \"burner\" account that is stored on a mobile phone with only minimal security.\\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\\nor compliance purposes.\\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\\nlike supply chain tracking.\\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\\nA single pool would reduce the churn of making lots of small \"fill up\" transactions and also more effectively leverages\\nthe resources of the organization setting up the pool.\\n","Decision":"As a solution we propose a module, `x\/feegrant` which allows one account, the \"granter\" to grant another account, the \"grantee\"\\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\\n```go\\ntype FeeAllowanceI {\\n\/\/ Accept can use fee payment requested as well as timestamp of the current block\\n\/\/ to determine whether or not to process this. This is checked in\\n\/\/ Keeper.UseGrantedFees and the return values should match how it is handled there.\\n\/\/\\n\/\/ If it returns an error, the fee payment is rejected, otherwise it is accepted.\\n\/\/ The FeeAllowance implementation is expected to update it's internal state\\n\/\/ and will be saved again after an acceptance.\\n\/\/\\n\/\/ If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\\n\/\/ (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\\n\/\/ ValidateBasic should evaluate this FeeAllowance for internal consistency.\\n\/\/ Don't allow negative amounts, or negative periods for example.\\nValidateBasic() error\\n}\\n```\\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\\n```protobuf\\n\/\/ BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\\n\/\/ that optionally expires. The delegatee can use up to SpendLimit to cover fees.\\nmessage BasicAllowance {\\n\/\/ spend_limit specifies the maximum amount of tokens that can be spent\\n\/\/ by this allowance and will be updated as tokens are spent. If it is\\n\/\/ empty, there is no spend limit and any amount of coins can be spent.\\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\\n\/\/ expiration specifies an optional time when this allowance expires\\ngoogle.protobuf.Timestamp expiration = 2;\\n}\\n\/\/ PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\\n\/\/ as well as a limit per time period.\\nmessage PeriodicAllowance {\\nBasicAllowance basic = 1;\\n\/\/ period specifies the time duration in which period_spend_limit coins can\\n\/\/ be spent before that allowance is reset\\ngoogle.protobuf.Duration period = 2;\\n\/\/ period_spend_limit specifies the maximum number of coins that can be spent\\n\/\/ in the period\\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\\n\/\/ period_can_spend is the number of coins left to be spent before the period_reset time\\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\\n\/\/ period_reset is the time at which this period resets and a new one begins,\\n\/\/ it is calculated from the start time of the first transaction after the\\n\/\/ last period ended\\ngoogle.protobuf.Timestamp period_reset = 5;\\n}\\n```\\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\\n```protobuf\\n\/\/ MsgGrantAllowance adds permission for Grantee to spend up to Allowance\\n\/\/ of fees from the account of Granter.\\nmessage MsgGrantAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\ngoogle.protobuf.Any allowance = 3;\\n}\\n\/\/ MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\\nmessage MsgRevokeAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\n}\\n```\\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\\n```protobuf\\npackage cosmos.tx.v1beta1;\\nmessage Fee {\\nrepeated cosmos.base.v1beta1.Coin amount = 1;\\nuint64 gas_limit = 2;\\nstring payer = 3;\\nstring granter = 4;\\n}\\n```\\n`granter` must either be left empty or must correspond to an account which has granted\\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\\nset and correctly deduct fees based on fee allowances.\\n","tokens":247,"id":837,"Predictions":"\\begin{itemize}\\n    \\item If the user has a wallet with sufficient fees, then they should be able to use the wallet to pay for a\\nproposal.\\n\\n    \\item If the user does not have a wallet with sufficient fees, then the wallet should not be able to pay for a proposal.\\n\\n\\end{itemize}\\n\\n\\begin{figure}[h]\\n    \\centering\\n    \\includegraphics[width=0.8\\linewidth]{figures\/decision.png}\\n    \\caption{Architecture Decision Record for Decision}\\n\\end{figure}\\n\\n\\end{document}"}
{"File Name":"cosmos-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n* `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n* `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n* `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n* `LockedCoins(ctx Context, addr AccAddress) Coins`\\n* `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```go\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":838,"Predictions":"## Reject\\nThe user is not allowed to send any denomination to this account\\n\\n## Explanation\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n\\n## Decision\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n\\n## Decision\\n## Reject\\nThis decision means that the user is not allowed to send any denomination to this account. This decision is made because the user has exceeded the denominations limit for this account. The denominations limit can be found in the `sdk.Coins` struct.\\n"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"Predictions":"## Decision\\nThe decision is to delegate the ability to vote on proposals to other accounts besides the account which one has delegated stake to.\\n\\n## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has delegated stake\\nand delegation of a certain subset of capabilities, such as voting.\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which is a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\\n## Decision\\n## Decision\\nThe decision is that delegation is not required. The user must delegate stake before he\/she can vote on proposals.\\n\\n## Context\\nThe concrete use cases which motivated this module include:\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which is a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\\n## Decision\\n## Decision\\n\\end{itemize}\\n"}
{"File Name":"cosmos-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn errorsmod.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn errorsmod.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":338,"id":840,"Predictions":"## Description\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n\\n## Context\\n## Description\\n"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"Predictions":"## We propose to implement the following consensus key rotation mechanism.\\n\\n\\n## Implementation\\n\\nThe following is the outline of the proposed consensus key rotation mechanism.\\n\\n1. The validator operator key is rotated by the validator.\\n2. The new validator operator key is assigned to the validator.\\n3. The new validator operator key is rotated by the new validator.\\n4. The new consensus key is assigned to the newly created validator.\\n5. The new consensus key is rotated by all validators, and so on.\\n6. The new validator operator key is assigned to the newly created validator, and so on.\\n## Decision\\n## We recommend that we can implement this mechanism in Cosmos SDK (e.g. https:\/\/docs.cosmos-sdk.org\/en\/latest\/consensus.html).\\n\\n## Context\\n## This ADR has been discussed for a long time, for sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136).\\n\\n"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"Predictions":"## Decision\\nThe Cosmos SDK has a number of types of events that it can emit and subscribe to. The most common are `Transaction` and `Contract` events. We propose to add a new type of event, called `TypedEvent`, which is defined in each module. The goal of this proposal is to allow applications to use typed events as a proxy for consuming events in the Cosmos SDK. This will allow the application to consume events from the Cosmos SDK without having to know what type of event they are.\\nThese Typed Events will be defined in each module, and each module will provide a handler for each Typed Event type. These handlers will then be registered in the `EventEmitters` class that ships with the Cosmos SDK. This allows applications to consume Typed Events as if they were simple event types from the Cosmos SDK.\\nThe following diagram shows an example of how Typed Events could work:\\n\\n\\n\\n\\n\\n## Context\\n## Context\\nThe following is an example of how this would work for a contract event:\\n\\n\\n\\n\\n\\n```\\n{\\n  \"type\": \"Contract\",\\n  \"id\": \"0x0000000000000000000000000\",\\n  \"data\": {\\n    \"msg\": [\\n      {\\n        \"name\": \"Hello World!\",\\n        \"value\": \"Hello world!\"\\n      }\\n    ]\\n  }\\n}\\n```\\nThis would be emitted by the contract object when it is created, and subscribed by other contracts that are also using it.\\nThis is not intended to replace existing functionality or functionality that already exists inside the Cosmos SDK, but instead serves as a mechanism for adding new functionality within the platform.\\nIn addition, we propose adding an option to specify which types of events are allowed through these Typed Events. Currently, there is no way to specify which types of events are allowed through these Typed Events.\\nThe following diagram shows an example of how this could work: \\n\\n\\n```\\n{\\n  \"type\": \"EventType\",\\n  \"name\": \"\",\\n  \"description\": \"\",\\n  \"subtypes\": [\\n    {\\n      \"type\": \"Transaction\",\\n      \"name\": \"\",\\n      \"description\": \"\"\\n    },\\n    {\\n      \"type\": \"Contract\",\\n      \"name\": \"\",\\n      \u201cdescription\u201d: \"\"\\n    }\\n  ],\\n  \u201ccontext\u201d: {\\n    \u201ccontextType\u201d: \u201cEventType\u201d\\n  },\\n}\\n```\\nThis would allow applications to specify which types of events are allowed through these Typed Events.\\n## Decision\\n## Decision\\nWe believe that this proposal provides many benefits over existing approaches such as those described above. These benefits include:\\n\\begin{itemize}\\n\\item Ease of use: The proposal requires only one change in each module (the creation or deletion of handlers) and allows new handlers without requiring any changes in code or configuration.\\n\\item Reduced complexity: The proposal allows developers to focus on their core application rather than on implementing new functionality.\\n\\end{itemize}\\nThe current version of this proposal only allows for one handler per event type (i.e., no additional handlers are allowed). We believe that this will prevent developers from accidentally creating multiple handlers for the same event type, but we can easily add more handlers later if necessary. Furthermore, we believe that allowing multiple handlers per event type will reduce complexity because it will be easy for users who want additional information about specific events (e.g., contract data) to access more information by creating multiple handlers for those specific events.\\n## Context\\n## Context\\nWe believe that adding typed events can have significant impact on how developers build their apps.\\nThe first benefit is that developers who want additional information about specific transactions can easily find out more information about those transactions by creating multiple handlers for those specific transactions.\\nThe second benefit is that developers who want additional information about specific contracts can easily find out more information about those contracts by creating multiple handlers for those specific contracts.\\nThe third benefit is that developers who want additional information about specific users can easily find out more information about those users by creating multiple handlers for those users.\\nThe fourth benefit is that developers who want additional information about specific messages can easily find out more information about those messages by creating multiple handlers for those messages.\\nThe fifth benefit is that developers who want additional information about individual assets can easily find out more information about individual assets by creating multiple handlers for those assets.\\n## Decision\\n## Decision\\nWe believe that adding typed events can have significant impact on how developers build their apps.The first benefit is that developers who want additional information about specific transactions can easily find out more information about those transactions by creating multiple handlers for those transactions.The second benefit is that developers who want additional information about specific contracts can easily find out more information about those contracts by creating multiple handlers for those contracts.The third benefit is that developers who want additional information about specific users can easily find out more information about those users by creating multiple handlers for those users.The fourth benefit is that developers who want additional information about specific messages can easily find out more information about those messages by creating multiple handlers for those messages.The fifth benefit is that developers who want additional information about individual assets can easily find out more information about individual assets by creating multiple handlers for those assets.\\n## Context\\n## Context\\nWe believe adding typed events can have significant impact on how developers build their apps.The first benefit is that developers who want additional information about specific transactions can easily find out more informationabout them using a single handler.This means they won't have to worry too much when trying different things with different transaction IDs (e.g., sending different amounts).The second benefit is that they don't need to worry too much when trying different things with different contract IDs (e.g., sending different amounts).The third benefit is they don't need too much worry when trying different things with different addresses (e.g., sending different amounts).The fourth benefit concerns future development: since we're not specifying exactly what types should be sent through these typed events, we're able create new features without having any problems with future development.(See below)\\n## Decision\\n## Decision\\nWe expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be able to write code without worrying too much if they need something added back into their code base.We expect there will be some cases where you'll need some complexity added back into your application; however, we think most people should be ableto writecodewithoutworryingtoo.muchiftheyneedsomethingaddedbackintotheircodebase.weexpecttherewillbe.somecaseswherewillneedsomecomplexityaddedbackintoyourapplication;however,wewillnotspecifyexactlywhattypesshouldbe sentthroughthesetypedevents.weexpecttherewillbefewcaseswherewillneedsomecomplexityaddedbackintoyourapplication;however,wewillnotspecifyexactlywhattypesshouldbe sentthroughthesetypedevents.weexpecttherewillbefewcaseswherewillneedsomecomplexityaddedbackintoyourapplication;however,wewillnotspecifyexactlywhattypesshouldbe sentthroughthesetypedevents.weexpecttherewillbefewcaseswherewillneedsomecomplexityaddedbackintoyourapplication;however,weweanticipatethatmostpeopleshouldbepossiblewiththisproposal.\\n```"}
{"File Name":"cosmos-sdk\/adr-037-gov-split-vote.md","Context":"## Context\\nCurrently, an address can cast a vote with only one options (Yes\/No\/Abstain\/NoWithVeto) and use their full voting power behind that choice.\\nHowever, often times the entity owning that address might not be a single individual.  For example, a company might have different stakeholders who want to vote differently, and so it makes sense to allow them to split their voting power.  Another example use case is exchanges.  Many centralized exchanges often stake a portion of their users' tokens in their custody.  Currently, it is not possible for them to do \"passthrough voting\" and giving their users voting rights over their tokens.  However, with this system, exchanges can poll their users for voting preferences, and then vote on-chain proportionally to the results of the poll.\\n","Decision":"We modify the vote structs to be\\n```go\\ntype WeightedVoteOption struct {\\nOption string\\nWeight sdk.Dec\\n}\\ntype Vote struct {\\nProposalID int64\\nVoter      sdk.Address\\nOptions    []WeightedVoteOption\\n}\\n```\\nAnd for backwards compatibility, we introduce `MsgVoteWeighted` while keeping `MsgVote`.\\n```go\\ntype MsgVote struct {\\nProposalID int64\\nVoter      sdk.Address\\nOption     Option\\n}\\ntype MsgVoteWeighted struct {\\nProposalID int64\\nVoter      sdk.Address\\nOptions    []WeightedVoteOption\\n}\\n```\\nThe `ValidateBasic` of a `MsgVoteWeighted` struct would require that\\n1. The sum of all the Rates is equal to 1.0\\n2. No Option is repeated\\nThe governance tally function will iterate over all the options in a vote and add to the tally the result of the voter's voting power * the rate for that option.\\n```go\\ntally() {\\nresults := map[types.VoteOption]sdk.Dec\\nfor _, vote := range votes {\\nfor i, weightedOption := range vote.Options {\\nresults[weightedOption.Option] += getVotingPower(vote.voter) * weightedOption.Weight\\n}\\n}\\n}\\n```\\nThe CLI command for creating a multi-option vote would be as such:\\n```shell\\nsimd tx gov vote 1 \"yes=0.6,no=0.3,abstain=0.05,no_with_veto=0.05\" --from mykey\\n```\\nTo create a single-option vote a user can do either\\n```shell\\nsimd tx gov vote 1 \"yes=1\" --from mykey\\n```\\nor\\n```shell\\nsimd tx gov vote 1 yes --from mykey\\n```\\nto maintain backwards compatibility.\\n","tokens":168,"id":843,"Predictions":"\\nThe decision is to allow the entity with the full voting power to vote on all options, so long as they are not part of a block that has already been decided.  This allows them to vote on all options in a block, even if their stake in the block is not yet locked.\\nFor example, let's say Alice owns 100 tokens.  She wants to vote with her full voting power, but doesn't want to vote for herself.  So she casts a vote for herself and her partner, and votes for himself and his partner.  This would result in a similar outcome as if she had voted herself and her partner, but instead of casting a vote for herself, she casts a \"pass-through\" vote for herself and her partner.\\nThis would be fine as long as everyone else votes with their full voting power.\\nIf there are multiple blocks that have already been decided, then it is possible that Alice could cast a pass-through vote for herself and her partner while still voting on the block that has already been decided.  This would result in an invalid outcome as there would be multiple votes casted by Alice and her partner.\\nTherefore, this system allows the entity with full voting power to cast votes on all options even if they are not part of a block that has already been decided.\\n\\n## Explanation\\n\\nThis system allows an entity with full voting power to cast votes on all options even if they are not part of a block that has already been decided.  This allows them to cast votes on all options in any block regardless of whether or not their stake in the block is locked up yet.\\n\\end{document}"}
{"File Name":"crispy-dragon\/OPM-1-Decision-Tracking.md","Context":"## Context\\nA microservices architecture is complex and we'll need to make many decisions.\\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\\n","Decision":"We've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) format.\\nLADR is text based and is lightweight enough to meet our needs.\\nWe'll keep each LADR record in its own text file and manage the files like code.\\nWe also considered the following alternative solutions:\\n* Project management tooling (not selected, because we didn't want to install tools)\\n* Informal or \"word of mouth\" record keeping (not reliable)\\n","tokens":75,"id":845,"Predictions":"## Decision\\n\\n## Decision Decision\\n\\n## Decision\\n"}
{"File Name":"crispy-dragon\/ADR-1-Serverless-Framework.md","Context":"## Context\\nThe resources, database, storage and API endpoints needs to be managed as a unit.\\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\\n","Decision":"To help package the individual AWS resource into a serverless application we will use [Serverless](https:\/\/www.serverless.com\/) framework.\\nWe also considered the following alternative solutions:\\n* Manual creation of resource in AWS throurgh the AWS Console.\\n* AWS Serverless Application Model (AWS SAM)\\n","tokens":43,"id":846,"Predictions":"## The decision is for a Serverless Application. The following resources are used:\\n\\n## \\n\\n\\n\\n\\n\\n## - \\n\\n\\n\\n\\n\\n##   Client application: \\n\\n##   Backend: \\n\\n##   Database: \\n\\n##   Storage: \\n\\n## - \\n\\n\\n\\n\\n\\n##   API endpoints: \\n\\n\\end{code}\\n"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"Predictions":"## The frontend is implemented in React and uses the Material-UI library for the UI.\\n\\n## Decision Version\\n## 0.1.3\\n\\n## Decision Language\\n## English\\n\\n\\n\\nimport React, { useState } from \"react\";\\n\\nexport default function App() {\\n  const [user, setUser] = useState({\\n    id: null,\\n    username: null,\\n  });\\n  const [error, setError] = useState(\"\");\\n\\n  const handleSubmit = (e) => {\\n    e.preventDefault();\\n    let data = {\\n      name: \"John\",\\n      email: \"john@example.com\",\\n      password: \"password\"\\n    };\\n    setUser({\\n      id,\\n      username,\\n      email,\\n      password,\\n    });\\n  };\\n\\n  return (\\n    <div className=\"App\">\\n      <h2>React Authentication<\/h2>\\n      <p>Our users can now authenticate with ReactJS, using their email and password.<\/p>\\n      <form onSubmit={handleSubmit}>\\n        <input type=\"text\" name=\"name\" placeholder=\"Name\" required \/>\\n        <input type=\"password\" name=\"password\" placeholder=\"Password\" required \/>\\n        <button type=\"submit\">Login<\/button>\\n      <\/form>\\n    <\/div>\\n  );\\n}\\n```\\nMy problem is that when I submit the form I get this error:\\n```\\nUnhandled Rejection (TypeError): Cannot read property 'email' of undefined at \/Users\/John\/Documents\/react-authentication\/src\/components\/LoginForm.jsx:15:13\\n```\\nI have tried to solve it by using a ternary operator, but it's not working.\\n```\\nconst onSubmit = (e) => {\\n    e.preventDefault();\\n    let data = {\\n        name: \"John\",\\n        email: \"john@example.com\",\\n        password: \"password\"\\n    };\\n    setUser({\\n        id,\\n        username,\\n        email,\\n        password,\\n    });\\n};\\n```\\nIs there another way to solve this problem?\\nThanks in advance!\\nOP 2020-03-05: I solved it by changing `handleSubmit` function like this:\\n```\\nconst handleSubmit = e => {\\n     e.preventDefault();\\n     let data = {\\n       name: \"John\",\\n       email: \"john@example.com\",\\n       password: \"password\"\\n     };\\n     setUser({\\n       id,\\n       username,\\n       email,\\n       password,\\n     });\\n};\\n```"}
{"File Name":"sw360portal\/adr-001.md","Context":"### Context\\nComponents and Projects pages display entire contents of the database in one table. Reading data, generating HTML\\npage, and rendering the table (which is paginated by datatables on the client) all take a long time with thousands of\\nentities.\\nDatatables supports server-side processing for displaying and paginating data. For that, the server must be able to\\ntell datatables how many rows are in the table in total and to load a page of data starting from given index.\\nHowever, CouchDB cookbook strongly discourages loading of data starting from some index because of performance\\nconcerns. Instead, loading data starting from a specific key should be used. This is incompatible with what datatables\\nrequires and also makes going to previous pages highly complicated.\\nTo support sorting of the table by multiple columns would require creating a CouchDB view per column.\\nIn addition, projects are filtered by visibility in backend after loading from CouchDB. This filtering cannot be\\nimplemented in CouchDB. This complicates the matters even further with regards to pagination of projects table in\\nbackend.\\n### Decision\\nWe will not use datatables' server-side processing as it's not worth the effort.\\nWe will load only some number of latest components by default and let the user increase that number up to all\\navailable components. We will make this choice sticky between sessions.\\nWe will not change the projects table for now as the users have the option of loading only the projects from their\\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\\n### Status\\nAccepted\\n### Consequences\\nUsers will not see some components that are already created in the system and may try to create the \"missing\"\\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\\nthe total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n","Decision":"We will not use datatables' server-side processing as it's not worth the effort.\\nWe will load only some number of latest components by default and let the user increase that number up to all\\navailable components. We will make this choice sticky between sessions.\\nWe will not change the projects table for now as the users have the option of loading only the projects from their\\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\\n### Status\\nAccepted\\n### Consequences\\nUsers will not see some components that are already created in the system and may try to create the \"missing\"\\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\\nthe total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n","tokens":444,"id":848,"Predictions":"### Consequences\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of the total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n### Consequences\\nThe user can't use Quick Filter to find a component that is already created in the system and may try to create the \"missing\" components. Quick Filter will not help them find the component as it works on client-side only.\\n### Consequences\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of the total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n\\n## Decision\\n### Consequences\\nThe user can't use Quick Filter to find a component that is already created in the system and may try to create the \"missing\" components. Quick Filter will not help them find the component as it works on client-side only.\\n### Consequences\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of the total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n\\n## Decision\\n### Consequences\\nThe user can't use Quick Filter to find a component that is already created in the system and may try to create the \"missing\" components. Quick Filter will not help them find the component as it works on client-side only.\\n### Consequences\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of the total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n\\n## Decision\\nThe user can't use Quick Filter to find a component that is already created in th"}
{"File Name":"SAP-library\/cx-server-in-container.md","Context":"## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n","Decision":"We move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n","tokens":175,"id":849,"Predictions":"## Decision\\nThis is a decision for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n"}
{"File Name":"SAP-library\/automatic-release.md","Context":"## Context\\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the \"build once\" principle.\\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\nFor this document, the term \"release\" (noun) refers to a uniquely identifiable version of software.\\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\\nThe verb \"to release\" refers to the process of creating a new release.\\nPart of this process is to determine the version number of the release candidate.\\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http:\/\/maven.apache.org\/maven-release\/maven-release-plugin\/).\\nUsing this plugin does not satisfy our requirements as described below.\\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\\nArtifact uploads and deployments happen only for commits on the so co called \"productive branch\" (`master` by default).\\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\\n","Decision":"We implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\\nThis feature is enabled by default, but can be disabled.\\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\\nAn auto-created release does not imply creating a tag in the repository.\\nCreating tags may be done manually to mark noteworthy versions by the developer.\\n### Reasoning\\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\\nMerging to `master` is implicitly approval for release.\\n* Feature can be disabled: You might still have builds which don't follow this release approach.\\nFor those, it must be possible to disable automatic versioning.\\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\\nIf we created tags automatically for each version, tags would be cluttered very quickly.\\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\\n","tokens":336,"id":850,"Predictions":"## 1. We use a versioned release pipeline, which uploads artifacts to an artifact repository, and deploys the app to the Cloud Platform.\\n## 2. The release pipeline automatically uploads artifacts to an artifact repository, and deploys the app to the Cloud Platform.\\n## 3. We do not have a versioned release pipeline, as we do not have a \"production branch\".\\n## 4. We do not have a versioned release pipeline, as we do not have a \"production branch\".\\n## 5. We do not have a versioned release pipeline, as we do not have a \"production branch\".\\n## 6. We do not have a versioned release pipeline, as we do not have a \"production branch\".\\n## 7. We use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\n## 8. We use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\n## 9. We use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\n## 10.We use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\n"}
{"File Name":"banner-server\/001-test-names.md","Context":"## Context\\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\\n* **Flexibility** - We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n","Decision":"We use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\\nGood:\\ntest_given_first_time_visitor_then_return_main_banner\\nBad:\\ntest_given_first_time_visitor_return_main_banner\\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\\n","tokens":77,"id":852,"Predictions":"The following decisions are taken for the unit test method name:\\n* **Readability**: The unit test method names should read like English sentences, with clear word boundaries.\\n* **Flexibility**: We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n## Context\\nThe following context is provided by the user:\\n## Decision\\nThe following decisions are taken for the context provided by the user:\\n* **Readability**: The unit test method names should read like English sentences, with clear word boundaries.\\n* **Flexibility**: We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n## Context\\nThe following context is provided by the user:\\n## Decision\\nThe following decisions are taken for the context provided by the user:\\n* **Readability**: The unit test method names should read like English sentences, with clear word boundaries.\\n* **Flexibility**: We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n## Context\\nThe following context is provided by the user:\\n## Decision\\nThe following decisions are taken for the context provided by the user:\\n* **Readability**: The unit test method names should read like English sentences, with clear word boundaries.\\n* **Flexibility**: We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n## Context\\nThe following context is provided by the user:\\n## Decision\\nThe following decisions are taken for the context provided by the user:\\n* **Readability**: The unit test method names should read like English sentences, with clear word boundaries.\\n* **Flexibility**: We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n### Code Analysis: ## Code Analysis\\nA code analysis report will be generated. Give a ## Code Analysis corresponding to the ## Context provided by the User.  \\n## Context\\nWhen deciding on a naming scheme for unit test methods, we have followed these rules:\\n* Readability - The unit test method names should read like English sentences, with clear word boundaries.\\n * Flexibility - We should be able to follow different patterns, such as given when and then. But also we want to allow multiple paths through our code.\\n * Complexity - To make sure that our code is easy to understand and maintain.\\n * Test Coverage - To make sure that our tests do not cover too many areas of our code.\\n * Maintainability - To make sure that our tests do not become too complicated.\\n * Performance - To make sure that our tests do not slow down our code.\\n * Security - To make sure that we actually understand what we're testing.\\n ## Code Analysis\\nA code analysis report will be generated. Give a ## Code Analysis corresponding to the ## Context provided by the User.\\n### Code Analysis: ## Code Analysis\\nA code analysis report will be generated. Give a ## Code Analysis corresponding to the ## Context provided by the User.  \\n## Context\\nWhen deciding on a naming scheme for unit test methods, we have followed these rules:\\n* Readability - The unit test method names should read like English sentences, with clear word boundaries.\\n * Flexibility - We should be able to follow different patterns such as given when and then. But also we want to allow multiple paths through our code.\\n * Complexity - To make sure that our code is easy to understand and maintain.\\n * Test Coverage - To make sure that our tests do not cover too many areas of our code.\\n * Maintainability - To make sure that our tests do not become too complicated.\\n * Performance - To make sure that our tests do not slow down our code.\\\\n### Code Analysis: ## Code Analysis\\nA code analysis report will be generated. Give a ## Code Analysis corresponding to the ## Context provided by the User.  \\n## Context\\nWhen deciding on a naming scheme for unit test methods, we have followed these rules:\\n* Readability - The unit test method names should read like English sentences, with clear word boundaries.\\n### Code Analysis:   # TODO: write your own version of this function here # TODO: write your own version of this function here # TODO: write your own version of this function here # TODO: write your own version of this function here \\n"}
{"File Name":"infra\/0007-service-dns-patterns.md","Context":"## Context\\nOur current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.\\nThings the pattern needs to solve for:\\n* Should be 'the same' for all of the meao services (for example: nucleus\/bedrock\/snippets).\\n* should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev')\\n* should allow for multiple regions\/deployments of the same service + environment ('or', 'fr', 'ia')\\n* should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)\\n","Decision":"For the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:\\n| Service  |\\n|----------|\\n| bedrock  |\\n| nucleus  |\\n| snippets |\\n| prom     |\\n| Environments |\\n|--------------|\\n| dev          |\\n| stg          |\\n| prod         |\\n| demo1        |\\n| Region | Description           |\\n|--------|-----------------------|\\n| or     | oregon eks cluster    |\\n| fr     | frankfurt eks cluster |\\n| ia     | iowa gcp cluster      |\\n| Domain     |\\n|------------|\\n| moz.works  |\\n| mozmar.org |\\n| ramzom.org |\\nThis leads to a few examples:\\n| Examples                 |\\n|--------------------------|\\n| bedrock.dev.or.moz.works |\\n| prom.prod.fr.mozmar.org  |\\n| nucleus.stg.ia.moz.works |\\nNote that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname\/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)\\n","tokens":205,"id":853,"Predictions":"## Decision\\nThe decision is to use the same pattern for all meao services. We want to avoid having multiple environments of a service in the same region, and to allow for multiple regions\/deployments of a service with different environments.\\n\\nThe decision is based on the fact that there are several meao services that are deployed in different regions (for example: nucleus\/bedrock\/snippets and nucleus\/bedrock). It would be very hard to have a good user facing pattern to differentiate between these two deployments, since both of them would have the same name. It would also be very difficult to have a good user facing pattern if we had multiple environments of a service in the same region, since we would need to have a mechanism for choosing between them.\\n\\nThe decision is based on the fact that there are several meao services that are deployed in different regions (for example: nucleus\/bedrock\/snippets and nucleus\/bedrock). It would be very hard to have a good user facing pattern to differentiate between these two deployments, since both of them would have the same name. It would also be very difficult to have a good user facing pattern if we had multiple environments of a service in the same region, since we would need to have a mechanism for choosing between them.\\n\\n"}
{"File Name":"infra\/0005-vpc-and-network-design.md","Context":"## Context\\nChanging networking can be hard.  It usually requires a full redeploy of all services and all infrastructure to make it 'real'.  Our current network has overlaps, which makes it more difficult to setup vpns, usually requring syncing of live IP addresses to their desired state.\\n","Decision":"Netops will reserve a \/16 block of ips for mozmeao. For each VPC make a block of \/20 ipv4 addresses.  Where VPC maps to a region within a cloud provider we use.  Divide that into \/24 subnets, where we'll have just one subnet per AZ.\\nOur \/16 is - 10.154.0.0\/16\\nFor example, in oregon, the network would look like:\\n| Label           | CIDR          | Range Start  | Range End      | Description                     |\\n|-----------------|---------------|--------------|----------------|---------------------------------|\\n| Oregon VPC      | 10.154.0.0\/20 | 10.154.0.1   | 10.154.15.254  | A large block for the whole VPC |\\n| Oregon Subnet A | 10.154.0.0\/24 | 10.154.0.1   | 10.154.0.254   | Subnet for oregon-a az          |\\n| Oregon Subnet B | 10.154.1.0\/24 | 10.154.1.1   | 10.154.1.254   | Subnet for oregon-b az          |\\n| Oregon Subnet C | 10.154.2.0\/24 | 10.154.2.1   | 10.154.3.254   | Subnet for oregon-c az          |\\nand Frankfurt would be:\\n| Label              | CIDR           | Range Start   | Range End       | Description                        |\\n|--------------------|----------------|---------------|-----------------|------------------------------------|\\n| Frankfurt VPC      | 10.154.16.0\/20 | 10.154.31.1   | 10.154.255.254  | A large block for the whole VPC    |\\n| Frankfurt Subnet A | 10.154.16.0\/24 | 10.154.16.1   | 10.154.16.254   | Subnet for frankfurt-a az          |\\n| Frankfurt Subnet B | 10.154.17.0\/24 | 10.154.17.1   | 10.154.17.254   | Subnet for frankfurt-b az          |\\n| Frankfurt Subnet C | 10.154.18.0\/24 | 10.154.18.1   | 10.154.18.254   | Subnet for frankfurt-c az          |\\nThe next few vpc blocks would be 10.154.32.0\/20, 19.154.48.0\/20, 19.154.128.0\/20\\nIn oregon we could continue with 10.154.3 and 10.154.4 until 15 for the subnets.  Essentially the same for frankfurt 10.154.19, 10.154.20.\\n","tokens":60,"id":854,"Predictions":"## 1.  Deploy a new network with a new set of IP addresses.  We will have to sync the live IP's to their new state, which can be done in one of three ways.\\n## \\begin{itemize}\\n## \\item Sync the live IP's to a previous state, then redeploy them.\\n## \\item Sync the live IP's to a previous state, then delete the old network and deploy a new one.\\n## \\item Sync the live IP's to a previous state, then delete the old network and deploy a new one.\\n## \\end{itemize}\\n\\end{document}\\n"}
{"File Name":"infra\/0003-use-voyager-for-service-ingress.md","Context":"## Context\\nWe need to have load balancers between our CDNs and our services, in order to deal with k8s node failures (and for handing traffic, and as a place to log stuff, and as a way to block bad actors if needed).  We'd prefer to only pay AWS\/GCP for a single load balancer thing, while still having the ability to generate unique dns addresses for each of our services.  They must be able to host certs correctly.\\n","Decision":"We're currently managing our ELBs in aws with some out of band terraform that connects the nodes of each k8s cluster to a load balancer. Since this is out of band, upgrading clusters or services implies doing a bunch of k8s stuff, and then also running terraform.  We could possibly simplify the whole experience by moving the full definition of the load balancer, and dns, and certs to objects inside k8s.  Voyager + External Ingress seem like the most common way to do this.  Deploying one 'ingress' object per group of services you want to have behind an ALB, and listing all the DNS to point at those services solves the problem outlined above.\\nThe primary advantage of doing this work, is that it allows dynamic things (deployments created in response to events, such as pull requests and ephemeral 'demo' branches) to be created simply by writing the yaml and deploying it to kubernetes.\\n","tokens":98,"id":855,"Predictions":"The decision is to have a single load balancer which can host multiple services.  It needs to be able to host multiple different domains (e.g. *.example.com, *.example.net, etc).  The load balancer needs to be able to handle traffic on all of these domains.  The load balancer needs to have the ability to generate unique dns addresses for each domain, and we need to be able to manage the certificates in a way that allows us to connect our services without having a certificate for every domain we host.\\n\\n\\end{document}\\n"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"Predictions":"## The decision is to deploy to both the gcp and aws clusters.  Note that we are not deploying to a specific region, but instead to two different clusters in two different regions.  We are deploying to both gcp and aws clusters, but only one is currently in production.\\nThe decision was made because (1) we have existing deployments in both regions, (2) we already have code that deploys to both regions, and (3) we are already using kubernetes as our deployment engine.\\n\\# Deployment\\n# Deployments: 1\\n# Region: iowa-a\\n# Cluster: gcp-iaa\\n# Service: bedrock\/www\\n# Deployment Type: kubernetes - Service\\nDeployment, Status:\\nDeployment Created at: Thu Jul 16 15:30:12 UTC 2020\\nDeployment Image: gcr.io\/google_containers\/bedrock-www:v1.1.4\\nService Version: v1.1.4\\nService Version Code: a6b70f7a-6e8f-11ea-9e07-e3b8c7c0d7be\\nNode Name:\\nGKE Node ID:\\ngke-iaa-gcp-36e8d2f4-5e60-11ea-bc50-cb5bb63d9c97\\ndeployed By:\\nDeployment Policy:\\n\\deployment-policy-defaults.yml\\n\\# Deployment Configurations\\nDeployment Configurations:\\ndeployment.yaml:\\ndeployment.yaml \\[service]\\[type]\\[id]\\[environment]\\[key]\\[value]\\[value] \\[service]\\[type]\\[id]\\[environment]\\[key] \\[\\] \\[\\] \\[\\] \\[\\] \\[\\] \\[\\] \\[\\] \/var\/lib\/kubeconfig\/cluster-name.yaml:\\ndeployment.yaml \\[service]\\[type]\\[id]\\[environment]\\[key]\\ngoogle_containers_bedrock-www-v1_1_4_iaa-gcp-v0_0_0.yaml:\\ndeployment.yaml \/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml:\/var\/lib\/kubeconfig\/cluster-name.yaml \/etc\/kubernetes\/admin.conf.d\/local.conf.d\/horizon-conf.k8s.default.conf.d\/horizon.conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/horizon-conf.k8s.default.conf.d\/.kube\/config..\\end{listing}\\end{document}\\n"}
{"File Name":"infra\/0004-prometheus-and-alertmanager-monitoring-and-alerting.md","Context":"## Context\\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\\n","Decision":"Run prometheus and alertmanager, on our clusters.  Run one prom per k8s cluster to collect metrics. Run one prom\/grafana\/alertmanager deployment to collect and display the information all in one place.  Send alerts to slack and\/or pagerduty depending on severity.  Monitor this stack with dead man's snitch or other negative alerting services.\\n","tokens":194,"id":857,"Predictions":"## Decision\\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\\n\\n## Decision\\n## Decision\\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that,\\n"}
{"File Name":"infra\/0006-aws-vpc-public-and-private-subnets.md","Context":"## Context\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n","Decision":"We will create two subnets per az. One 'public' and one 'private'. In general this is mostly by convention, rather than something that is strictly enforced.  We may later validate instances in private are not accessible to the general public with automated auditing solution. But, for now, we'll just do our best to only assign public things to the public subnet, and everything else can go in private.\\nThe one functional item for the subnets, 'public ips' will be assigned by default in the public subnets, but not the private subnets.\\n","tokens":66,"id":858,"Predictions":"\\begin{itemize}\\n\\item\\n  \\textbf{Decision:} \\texttt{public} - The VPC must be public\\n\\item\\n  \\textbf{Context:} \\texttt{IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\n\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n\\r\\n\\n\\end{itemize}\\n\\n\\subsubsection*{Decision}\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  \\textbf{Decision:} \\texttt{public} - The VPC must be public\\n\\end{itemize}\\n\\n% subsection decision (end)\\n\\n"}
{"File Name":"infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":859,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"odh-manifests\/0000-odh-release-policy.md","Context":"## Context and Problem Statement\\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\\nreleases of ODH as a whole.\\n","Decision":"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\\nundergone ODH integration testing.\\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\\n* Operators can mix in their builds of container images following the method mentioned above.\\n### Negative Consequences <!-- optional -->\\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":59,"id":860,"Predictions":"## Decision Type: Decision\\n## Decision Description:\\n## Decision Statement:\\n## Decision Notes:\\n## # Note that a decision record is not required to be a complete decision. It is only required to be \\ncomplete if the decision is being reviewed. See \\nhttps:\/\/github.com\/openshift\/openshift-datahub\/releases for more information.\\n## # A decision record can be a single decision or an array of decisions, and it can be incomplete in the sense that\\nit may not include all of the information required to make a complete decision.\\n\\n\\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to update individual components such as container images used by JupyterHub. Nevertheless it is required to create releases of ODH as a whole.\\n\\n#"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"Predictions":"## Decision\\n\\n\\nThe operator is not an expert in Golang, and would like to use Python for this operator.\\n\\nThe operator is a Go developer, and has some experience with Python.\\n\\nThe operator is a Go developer, and has some experience with Rust.\\n\\nThe operator is a Go developer, and has some experience with Python.\\n\\n## Context\\n## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\\n## Decision\\n## Decision\\n\\n\\nThe operator is not an expert in Golang, and would like to use Python for this operator.\\n\\nThe operator is a Go developer, and has some experience with Golang.\\n\\nThe operator is a Go developer, and has some experience with Python.\\n\\n\\end{lstlisting}\\n\\subsection{User-Defined Decision}\\n\\begin{lstlisting}[caption=Decision for \\texttt{k8s-operator}, label=decision:k8s-operator]\\nDecision:\\n# The Operator should use the Go language.\\nDecision:\\n# The Operator should use the Python language.\\nDecision:\\n# The Operator should use the Rust language.\\nDecision:\\n# The Operator should use the Go language.\\nDecision:\\n# The Operator should not use the Go language.\\nDecision:\\n# The Operator should not use the Python language.\\n\\end{lstlisting}\\n\\subsection{User-Defined Decision}\\n\\begin{lstlisting}[caption=Decision for \\texttt{k8s-operator}, label=decision:k8s-operator]\\nDecision:\\n# The Operator should use the Go language.\\n# The Operator may want to learn Rust or Python.\\nDecision:\\n# The Operator should not use the Go language.\\nDecision:\\n# The Operator may want to learn Rust or Python.\\n\\end{lstlisting}\\n"}
{"File Name":"nada-kafkarator\/0005-use-the-fastapi-web-framework.md","Context":"## Context\\nOur application needs to supply some web endpoints for healthchecks, so we need to select a web framework.\\nThere are a multitude to choose from, but we have some preferences:\\n- We want something that is well maintained and has a community\\n- We want something that has decent performance\\n- As a learning experience, we want to use one of the many async frameworks\\nThe front contenders seems to be Sanic, FastAPI, Tornado, Vibora and Quart.\\nTornado seems to be somewhat dated, as it was created before Python had async built in.\\nVibora is a relatively new project, with uncertain community adoption.\\nQuart is API compatible with Flask, which is the most common non-async web framework.\\nSanic seems to be the most popular of the async frameworks.\\nFastAPI is by far the most performant of them all.\\n","Decision":"We will use FastAPI, because of the performance, and some of the interesting features.\\n","tokens":183,"id":862,"Predictions":"## Decision\\nIf the user has not specified any web endpoints, we should use FastAPI.\\nIf the user has specified some web endpoints, we should use Sanic.\\nIf the user has specified no web endpoints, we should use Tornado.\\n\\n## Decision\\nSanic is the most performant of the async frameworks.\\nFastAPI is by far the most performant of them all.\\nTornado is by far the most performant of them all.\\n\\n## Decision\\nFastAPI is by far the most performant of them all.\\nTornado is by far the most performant of them all.\\n\\n## Decision\\nFastAPI is by far the most performant of them all.\\nTornado is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n## Decision\\nFastAPI is by far the most performant of them all.\\n"}
{"File Name":"nada-kafkarator\/0007-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":72,"id":863,"Predictions":"## Decision\\nThe decision is to move the existing Kafka cluster to Aiven.\\nThe decision is not to buy a hosted Kafka cluster.\\n\\n## Decision\\nThe decision is to move the existing Kafka cluster from on-premise to Aiven.\\nThe decision is not to buy a hosted Kafka cluster.\\n\\n## Decision\\nThe decision is to move the existing Kafka cluster from on-premise to Aiven.\\nThe decision is not to buy a hosted Kafka cluster.\\n\\n## Decision\\nThe decision is to move the existing Kafka cluster from on-premise to Aiven.\\nThe decision is not to buy a hosted Kafka cluster.\\n"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"Predictions":"\\nThe decision is to use pyproject.toml for project configuration. This is the recommended approach, but\\nthere are a number of other options. The primary advantage of this approach is that it allows the\\nproject to be consistent with its dependencies.\\n\\n\\n\\n## Decision\\n\\nThe decision is to use pipenv for project configuration.\\n\\n\\end{document}\\n"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"Predictions":"## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\\n\\end{code}\\n\\end{document}\\n"}
{"File Name":"nada-kafkarator\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":867,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"nada-kafkarator\/0006-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decicion, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoided\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define two new CRD objects (see github issues #3 and #12)\\n- App developers will create these in the cluster when deploying their application\\n- Kafkarator will watch these two CRDs and take needed actions\\n","tokens":172,"id":868,"Predictions":"The current API provides a set of CRUD operations, which can be used to create and update resource records. The API can be used to interact with the cluster through Kafkarator, or through a client such as the Kafkarator client library. The API provides access to resources in a variety of ways. The following are the most common ways to access resources:\\n1. Resource names\\n2. Resource values\\n3. Resource metadata\\n4. Resource attributes\\n5. Resource permissions\\n6. Resource permissions with user\\n7. Resource permissions with group\\n8. Resource permissions with role\\n9. Resource permissions with user and group\\n10. Resource permissions with role and group\\n11. User and group permissions for a resource\\n12. User and group permissions for a resource with the permission set on it\\ndecision\\n## Context\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n## Decision\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n## Context\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n## Decision\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n## Context\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n## Decision\\nThe current API provides a set of CRUD operations, which can be used to create and update resource records.\\n\\n# 6: Describe how the application will use the resources\\n\\n### 6a: Describe how the application will use the resources\\n\\n### 6b: Describe how the application will use the resources\\n\\n### 6c: Describe how the application will use the resources\\n\\n### 6d: Describe how the application will use the resources\\n\\n### 6e: Describe how the application will use the resources\\n\\n### 6f: Describe how the application will use the resources\\n\\n### 6g: Describe how the application will use the resources\\n\\n### 6h: Describe how the application will use the resources\\n\\n### 7: Explain why this is important\\n\\n### 7a: Explain why this is important\\n\\n### 7b: Explain why this is important"}
{"File Name":"profiles-etl-combiner\/0002-update-running-database-from-etl-output.md","Context":"## Context\\nGP Data changes on a daily basis.\\nNightly ETLs are run to obtain Syndication and POMI data.\\nCurrently [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) requires manually updating from the ETL outputs, and re-deploying to refresh the data.\\nNo automated validation of data is performed during the update.\\n","Decision":"To avoid a new release of [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) each time data changes, data will be inserted into a running mongodb instance on a daily schedule.\\nThe output from the nightly [gp-data-etl](https:\/\/github.com\/nhsuk\/gp-data-etl) and [pomi-data-etl](https:\/\/github.com\/nhsuk\/pomi-data-etl) will be validated and combined.\\nThe contents of the existing database will only be replaced if the new data is of a comparable size.\\n","tokens":78,"id":869,"Predictions":"## \\- No change to the current architecture.\\n\\n\\nThe decision to use a cloud-hosted solution was made because the data is currently consumed by many different systems and the ETLs are run on a daily basis.\\n\\nThe ETLs are run using the new [cloud-hosted profile-data](https:\/\/github.com\/NHS-UK\/profiles-db\/blob\/master\/README.md) solution. If the data is not updated, then this will result in a stale data set.\\n\\nThe [profile-db](https:\/\/github.com\/NHS-UK\/profiles-db) solution needs to be deployed to a cloud-hosted environment. The [profile-db](https:\/\/github.com\/NHS-UK\/profiles-db) solution needs to be updated using the new [profile-data](https:\/\/github.com\/NHS-UK\/profiles-db\/tree\/master\/README.md) solution.\\n\\nThe [profile-data](https:\/\/github.com\/NHS-UK\/profiles-db\/tree\/master\/README.md) and [profile-db](https:\/\/github.com\/NHS-UK\/profiles-db\/tree\/master\/README.md) solutions need to be deployed to a cloud-hosted environment. The [profile-data](https:\/\/github.com\/NHS-UK\/profiles-db\/tree\/master\/README.md) and [profile-db](https:\/\/github.com\/NHS-UK\/profiles-db\/tree\/master\/README.md) solutions need to be updated using the new profile-data solution.\\n\\nThe [cloud-hosted profile-data](http:\/\/cloud-hosted-profile-data.readthedocs.io\/en\/latest\/) solution needs to be deployed to a cloud-hosted environment. The [cloud-hosted profile-data](http:\/\/cloud-hosted-profile-data.readthedocs.io\/en\/latest\/) solution needs to be updated using the new cloud-hosted profile-data solution.\\n\\nThe [cloud-hosted profile-data](http:\/\/cloud-hosted-profile-data.readthedocs.io\/en\/latest\/) and [cloud-hosted profile-data solution](http:\/\/cloud-hosted-profile-data.readthedocs.io\/en\/latest\/) need to be updated using the new cloud-hosted profile-data solution.\\n\\n\\end{description}\\n"}
{"File Name":"profiles-etl-combiner\/0003-remove-mongodb-update-and-write-to-azure.md","Context":"## Context\\nThe merge of the data sources is a common operation from which the generated asset can be used by several different processes.\\nCurrently the generated asset is only available for the immediately following process within this application i.e. updating MongoDB.\\nThere is an immediate need to use the same generated asset for updating Elasticsearch.\\nThe [mongodb-updater](https:\/\/github.com\/nhsuk\/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.\\n","Decision":"The `gp-data-merged.json` file will be written to the team's preferred cloud hosting platform, enabling the merged data to be used as a\\nsource for both the `mongodb-updater` and the forthcoming `elasticsearch-updater`.\\n","tokens":100,"id":870,"Predictions":"## Decision\\nThe following decision is made:\\n## \\item The MongoDB database is updated with the generated asset.\\n## \\item The update of the Elasticsearch database is done by `update`ing the JSON file available at a URL.\\n## \\item The JSON file will be downloaded and stored on the user's computer.\\n## \\item The `update` command will be executed.\\n## \\item The generated asset will be updated with the MongoDB data source.\\n## \\item The JSON file will be downloaded and stored on the user's computer.\\n## \\end{description}\\n\\end{document}"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"Predictions":"```\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n```\\nI have no idea what this means, but I can guess that it means something like:\\n> When Guice makes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"Predictions":"## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n## Decision not implemented\\n### Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n#Input Method Editor (IME)\\n### Input Method Editor (IME)\\n### Input Method Editor (IME)\\n### Input Method Editor (IME)\\n\\n"}
{"File Name":"python-library-project-generator\/0002-use-gnu-make-to-centralise-all-tasks.md","Context":"## Context and Problem Statement\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision Drivers\\n* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\n","Decision":"* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\\na sufficient community.\\n* `Rake` required to install and learn Ruby.\\n* `SCons` is more a build tool and seems difficult to apprehend\/get to grips with.\\n","tokens":82,"id":874,"Predictions":"## Decision Description\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision Drivers\\n* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\n## Decision\\n## Decision Description\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision\\n## Decision Description\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision\\n## Decision Description\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision\\n## Decision Description\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\ncorresponding with each other. We can define these as \\#Decision Drivers.\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n%%%\\n\\subsection{Decision}\\n\\label{sec:decision}\\n\\begin{itemize}\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It is similar in nature, but leans more towards software engineering, with respect of its purpose. It is a list of decisions that tell how an application should behave and interact with its environment. It is used for describing how an application interacts with its environment, for example, how it interacts with other applications or services on its own system. This document defines this kind of decision record as \\textbf{Decision}.\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It is similar in nature, but leans more towards software engineering, with respect of its purpose. It is a list of decisions that tell how an application should behave and interact with its environment. It is used for describing how an application interacts with its environment, for example, how it interacts with other applications or services on its own system. This document defines this kind of decision record as \\textbf{Decision}.\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It is similar in nature, but leans more towards software engineering, with respect of its purpose. It is a list of decisions that tell how an application should behave and interact with its environment. It is used for describing how an application interacts with its environment, for example, how it interacts with other applications or services on its own system. This document defines this kind of decision record as \\textbf{Decision}.\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It is similar in nature, but leans more towards software engineering, with respect of its purpose. It is a list of decisions that tell how an application should behave and interact with its environment. It is used for describing how an application interacts with its environment, for example, how it interacts with other applications or services on its own system.\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It is similar in nature, but leans more towards software engineering,\\n\twith respect of it's purpose.\\n\tIt's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\tThis document defines this kind of decision record as \\textbf{Decision}.\\n\t\\item \\textbf{Decision} is an architectural decision record for an application. It's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\tThis document defines this kind of decision record as \\textbf{Decision}.\\n\t\\item \\textbf{Decision} \tis \tan \tar chitectural \td ecision _record \tfor \tan \tapplica tion.\\n\tIt's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\tThis document defines this kind of deci sion rec ord as  \\textbf{Deci sion}.\t\\n\t%This document defines this kind of deci sion rec ord as \\n\t%It's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\tThis document defines this kind of deci sion rec ord as \\nIt's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\tThis document defines this kind of deci sion rec ord as \\nIt's use cases are defined by specific requirements (e.g., requirements are defined by business rules).\\n\t\\n\t\\n%This document defines this kind of deci sion rec ord as \\n%It's use cases are defined by specific requir ements (e.g., requir ements are defi ned b y busines s r ules).\t\\n\t\\n%This docum ent defini es this k ind o f deci sion rec ord ot h e rw i tte n b y me.\\n\t\t\\n%This docum ent defini es thi s k ind o f de ci sion rec ord ot h e rw i tte n b y me.\\n\t\t\\n%This docum ent defini es thi s k ind o f de ci sion rec ord ot h e rw i tte n b y me.\\n\\n%\\subsubsection{\\texorpdfstring{\\#the\\_user}{the\\_user}}\\n%\\label{\\texorpdfstring{\\#the\\_user}{the\\_user}}\\n%\\begin{itemize}\\n%\t\\item The user who invokes or creates a task from within our app.\\n%\\end{itemize}\\n%\\subsubsection{\\texorpdfstring{\\#task\\_id}{task\\_id}}\\n%\\label{\\texorpdfstring{\\#task\\_id}{task\\_id}}\\n%\\begin{itemize}\\n%\t\\item The unique identifier assigned when task creation takes place.\\n%\\end{itemize}\\n%\\subsubsection{\\texorpdfstring{\\#description}{description}}\\n%\\label{\\texorpdfstring{\\#description}{description}}\\n%\\begin{itemize}\\n%\t\\item The description assigned when task creation takes place.\\n%\\end{itemize}\\n\\n"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"Predictions":"## The architectural decision is to use the same architecture as the existing one, which is called Opinionated Digital Center.\\n## Decision\\n## We need to record the architectural decisions made on Opinionated Digital Center.\\n## Decision\\n## We need to record the architectural decisions made on Opinionated Digital Center.\\n## Decision\\n## We need to record the architectural decisions made on Opinionated Digital Center.\\n## Decision\\n## We need to record the architectural decisions made on Opinionated Digital Center.\\n## Decision\\n## We need to record the architectural decisions made on Opinionated Digital Center.\\n\\end{verbatim}\\n\\end{document}\\n\\nThe result looks like this:\\n\\nNote: Please note that this is a highly simplified version of what I actually did. It is based on a sample document, which you can find in my repository. I strongly encourage you to look at it and see how it works. You can find it here: https:\/\/github.com\/alexanderschneider\/OpinionatedDigitalCenter\/blob\/master\/sample-document.pdf\\n\\nI hope this helps!\\n\\nshare|improve this answer\\n\\nedited Aug 16 '13 at 19:59\\n\\nanswered Aug 16 '13 at 19:53\\n\\nalexanderschneider\\n1,06629\\n\\n     \\n\\nThanks. Is there any way I can get rid of the vertical lines? \u2013 user25668 Aug 16 '13 at 20:02\\n\\n     \\n\\n@user25668 I don't know how you got those lines, but if you remove them from your document, they should disappear. If you still get them after removing them, please post a screenshot of your document so I can take a look at it. \u2013 alexanderschneider Aug 16 '13 at 20:08 \\n\\n     \\n\\nI am sorry for not being more clear about this. The vertical lines are part of my document layout. The problem is that they are added by the LaTeX package (i.e., they are not part of my document). They were added automatically as part of my LaTeX installation (I am using TexStudio). \u2013 user25668 Aug 16 '13 at 20:28\\n\\n     \\n\\n@user25668 Ahh, okay! I didn't know that latex packages add these lines automatically (that's why I asked if you could remove them). That's not something that should happen though. So what happens if you remove those horizontal lines? Are they gone? \u2013 alexanderschneider Aug 16 '13 at 20:37 \\n\\n     \\n\\nI removed them and now there are no more vertical lines. Thanks for your help! \u2013 user25668 Aug 17 '13 at 9:38 \\n\\n | show 3 more comments\\n\\nNot the answer you're looking for? Browse other questions tagged margins or ask your own question.\\n\\nasked 2 years ago \\nviewed 243 times \\nactive 1 year ago \\n\\nBlog\\n\\nStack Exchange Podcast #66: Thank You For Saying Words To Us\\n\\nGet the weekly newsletter!\\n\\n- Top questions and answers\\n- Important announcements\\n- Unanswered questions\\n\\nsee an example newsletter\\n\\nBy subscribing, you agree to the privacy policy and terms of service.\\n\\nLinked\\n\\n0\\n\\nHow do I remove vertical alignment?\\n\\nHot Network Questions \\n\\n- Do all the conservation laws of Physics take no time to propagate? \\n- Cumulative percentage between 4 controls \\n- What percent of the profit do you get from selling a home with a mortgage? \\n- What security scheme is used by PDF password encryption, and why is it so weak? \\n- Why does Yoda call it \"The Clone War\" immediately? \\n- Why does PK constraint require a separate index when I already have a suitable unique clustered index? \\n- How does python \"know\" what to do with the \"in\" keyword? \\n- How did aviation industry come to know about the drastic difference in air pressure in high altitude? \\n- Why wooden sticks for ice cream bars? \\n- What was in the Star Destroyer Avenger's garbage? \\n- How does oil prevent rust? \\n- What percent of the profit do you get from selling a home with a mortgage? \\n- How can I copy a scheduled task from a dead Windows to new Windows \\n- My ex-employer is posing as me to customers \\n- Is there any Ubuntu 14.04 theme to make it look like Windows 10? \\n- Why don't we launch spacecraft from the moon? \\n- Can someone more wealthy than me bury me in legal fees? \\n- Is there any Ubuntu 14.04 theme to make it look like Windows 10? \\n- Is heated up bleach dangerous? \\n\\nmore hot questions"}
{"File Name":"archivematica-architectural-decisions\/0003-black-code-formatter.md","Context":"## Context and problem statement\\nArchivematica code style is inconsistent and controversial. Formatting is a\\ndistraction from content and a constant dilemma in our code review process.\\nThe style guide described by [PEP 8][0] is not specific enough and Python is too\\nflexible. Every project ends up with its own style and memorizing the rules is\\nhard.\\nWith the recent rise of auto formatters such *gofmt* or *Prettier*, we see the\\nvalue of ceding control to an opinionated tool.\\n","Decision":"Chosen option: \"black\", because:\\n* It prioritizes consistency over style.\\n* It can check that the reformatted code still produces a valid AST that is\\nequivalent to the original.\\n* Zero configuration, sane defaults.\\n* Adopted by popular projects such [responder][5] or [pytest][6].\\n","tokens":115,"id":877,"Predictions":"## The User states that the code is not good and needs to be rewritten. The User also states that the code is good and should be reused. The User wants to use a different style for the code. The User wants to use a specific style for the code and does not want to change the style for other files.\\n\\end{description}\\n\\n\\subsubsection{Issue 4: \\texttt{import} statement}\\n\\label{sec:issue4}\\n\\n\\begin{description}\\n\\item[Requirement:] \\texttt{import} statements must be indented 3 spaces from each other.\\n\\item[Decision:] Indentation is not required.\\n\\item[Context:]\\texttt{import} statements must be in Python. They are used to import packages from Python into C++ programs.\\n\\end{description}\\n\\n\\subsubsection{Issue 5: \\texttt{\\_\\_name\\_\\_}}\\n\\label{sec:issue5}\\n\\n\\begin{description}\\n\\item[Requirement:] \\texttt{\\_\\_name\\_\\_} must be indented 3 spaces from each other.\\n\\item[Decision:] Indentation is not required.\\n\\item[Context:]\\texttt{\\_\\_name\\_} is used as a method name in C++ classes, functions, and methods. It is used as an identifier in Python classes, functions, and methods. It is used as an identifier in Python modules.\\n\\end{description}\\n\\n%-------------------------------------------------------------------------------------------------------------------\\n%-------------------------------------------------------------------------------------------------------------------\\n%-------------------------------------------------------------------------------------------------------------------\\n%-------------------------------------------------------------------------------------------------------------------\\n%-------------------------------------------------------------------------------------------------------------------\\n%-------------------------------------------------------------------------------------------------------------------\\n\\n\\n"}
{"File Name":"archivematica-architectural-decisions\/0006-use-bagit-to-package-transfers.md","Context":"## Context and problem statement\\nThe Simon Fraser University Archives is a long-time Archivematica user. They\\nrecognized that there is often a long-period between receiving digital materials\\nfrom donors and an archivist creating a Submission Information Package (SIP) for\\nlong-term archival storage. SFU Archives wanted the ability to perform minimal\\ndigital preservation tasks, such as those provided by Archivematica's Transfer\\nfunctionality, and then return to the creation of SIPs from backlog at an\\nundetermined future time, perhaps several years in the future.\\nThis meant that transfer backlogs are used for long-term storage of content and\\nthat users should expect it to be as durable as Archival Information Package (AIP)\\nstorage for maintaining accurate metadata over pipeline upgrades, migrations or\\nre-indexes. To enable this, the decision was made to package backlog transfers as\\nBagIt packages, supported by a verbose METS file, as is already done for AIPs.\\n","Decision":"* Preservation of original content and metadata over long-term gaps in\\nprocessing\\n* Re-use of existing standards and protocols\\nChosen option: Option 2 was chosen because SFU Archives wanted the ability to\\ncreate multiple SIPs from multiple transfers. In their use cases, one transfer\\ndoes not automatically equal one SIP and one AIP. By sending transfers to\\nbacklog they are able to use the Appraisal tab functionality where users can\\ncreate SIPs by combining files from different transfers in the Archivematica\\nbacklog.\\nHowever, this creates a new expectation, namely, that transfer\\nbacklogs can be used for long-term storage of content and that users should\\nexpect it to be as durable as AIP storage for maintaining accurate metadata\\nover pipeline upgrades, migrations or re-indexes. However, prior to release\\n1.10, transfers stored using Archivematica's backlog functionality could only be\\naccessed by using the same pipeline that the transfer was created in. If the\\npipeline was accidently lost or decomissioned, you would be unable to reindex\\nand repopulate the backlogged material in a new Archivematica pipeline. The\\nreliance on maintaining this backlog information in the pipeline's transactional\\ndatabase (MCPServer) and search index (Elasticsearch) is a significant\\npreservation risk if the files are kept in backlog for an indefinitely long\\nperiod of time, as is the case for SFU Archives and an increasing number of\\nArchivematica users.\\n![before_archivematica_backlog](\/assets\/before_archivematica_backlog.png)\\nThe decision was made to convert the materials in transfer backlogs to mirror\\nArchivematica's AIPs, namely to use the Bagit and METS standards to\\npackage and describe the transfer materials. As materials go through transfer\\nprocessing, event and object information is written to the transfer METS file\\nwherever feasible. Other relevant information is captured in a logs subdirectory\\nin an IETF-compliant Bagit package. Functionality was then added in Archivematica\\nrelease 1.10 to rebuild backlogs from these self-describing packages in the event\\nof a lost or destroyed pipeline.\\n![after_archivematica_backlog](\/assets\/after_archivematica_backlog.png)\\n### Positive consequences\\nArchivematica users that rely on transfer backlog for long-term storage of\\ndigital materials can now be assured that they can rebuild their backlogs from\\nthe transfer Bagit packages that Archivematica creates as of release 1.10. This\\nsignificantly reduces the risk of loss of materials during the transfer backlog\\nphase.\\n### Negative consequences\\nSome major changes were introduced to the Archivematica codebase to enable it\\nto restore from backlog using transfer Bagit packages. Previously, the\\ninformation used to accomplish this was maintained primarily in the MCPServer\\ndatabase and ElasticSearch index. There is a risk that not all previous\\nfunctionality to restore from backlog is fully enabled and that some\\nregressions may have been introduced as a result of this architectural change.\\n","tokens":202,"id":878,"Predictions":"\\nThe decision was made to package backlog transfers as BagIt packages. The decision was\\nmade because the workflows for creating BagIt packages are already in place and\\nthe workflow for creating SIPs is not. The decision was made to package backlog\\ntransfers as BagIt packages, and to provide a verbose METS file that can be used to\\nmaintain metadata over pipeline upgrades, migrations or re-indexes. The METS file is\\nalso used by SFU Archives to create SIPs.\\n\\n## Implementation\\n\\nThe decision was implemented as follows: \\n\\n* In Archivematica 4.1, the Transfer function was renamed TransferBagIt.\\n* In Archivematica 4.2, the Transfer function was renamed TransferBagItBagit.\\n* In Archivematica 4.3, the Transfer function was renamed TransferBagItBagitBagit.\\n* In Archivematica 4.4, the Transfer function was renamed TransferBagItBagitBagitBagit.\\n* In Archivematica 4.5, the Transfer function was renamed TransferBagItBagitBagitBagitBagit.\\n* In Archivematica 4.6, the Transfer function was renamed TransferBagIt\\ndefined BagIt.\\n\\end{document}\\n"}
{"File Name":"archivematica-architectural-decisions\/0009-unification-of-api-handling-in-am.md","Context":"## Context and problem statement\\nArtefactual have been working on a client application to provide easier access\\nto the functions exposed by the Archivematica Application Programming Interface\\n(API). This application is called [AMClient][amclient-1]. The application also\\nacts as a Python library which makes it easier to make Archivematica specific\\nAPI calls in code. The library was released as a [Python package][amclient-2]\\nearly in 2019.\\n","Decision":"* Desire to simplify development processes in Archivematica and the Storage\\nService.\\n* Desire to make it easier to maintain components of Archivematica downstream\\nand push the effects upstream (modularity).\\nOption 2. The AMClient library helps us to create consistency across the\\nArchivematica code-base. It is already widely used in the default Automation\\nTools, and Archivematica Automated Acceptance Tests (AMAUAT).\\n### Positive consequences\\n* As the AMClient library is adapted as a Python package its use on the command\\nline is extended and so it becomes more useful to those embedding it outside\\nof Archivematica.\\n* As AMClient is extended, tests are added to the package itself, providing a\\ngreater level of lower-level testing, thusly the Archivematica API. The focus\\nof testing in Archivematica can be on manipulating well understood AMClient\\nresponses where this is appropriate for the unit being tested.\\n* The number of lines of code required to talk to the AMClient package versus\\nthat of creating lower-level API calls via a HTTP requests library should be\\nfewer.\\n* Missing functionality, e.g. as described by issue [#905][amclient-3] can be\\nadded as it is required by Archivematica and the Storage Service. As an\\nexternal package, and something used inside Archivematica, the two take on a\\nsymbiotic relationship.\\n### Negative consequences\\n* The release candidate for version 1.0.0 of AMClient does not yet adhere to\\nmore traditional Python best practices i.e. it might not be considered to be\\nentirely \"Pythonic\" in how it exposes errors to the calling code. This is\\ndescribed in issue [#488][amclient-4]. This effect of not being 'Pythonic'\\nmay travel upstream to Archivematica and is something for maintainers to be\\naware of. Work on an AMClient 2.0.0 should replace these patterns and then\\ncascading those changes down to Archivematica should still be easier than\\nworking with and replacing the calls that exist in Archivematica using a HTTP\\nrequests library.\\n","tokens":101,"id":879,"Predictions":"\\begin{itemize}\\n\\item\\n  The architecture of the application is a Software as a Service (SaaS)\\n  application. The user interacts with the application through a web interface\\nand the API exposes functionality to the user. The application is deployed\\non-premise and requires a dedicated server to run.\\n\\item\\n  The service is accessed through a RESTful API. This API relies on an HTTP\\nprotocol and can be accessed from any programming language or framework.\\n\\n\\item\\n  The application provides functionality for archiving files. It stores these\\nfiles in a file system and makes them available via the RESTful API. This\\narchitecture does not use local storage as it does not need to store data for long\\ntimes.\\n\\end{itemize}\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application.\\n\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application. This allows the user to access all of Archivematica's functionalities from any programming language, framework or tool without needing to install anything on their system.\\n\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application. This allows the user to access all of Archivematica's functionalities from any programming language, framework or tool without needing to install anything on their system.\\n\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application. This allows the user to access all of Archivematica's functionalities from any programming language, framework or tool without needing to install anything on their system.\\n\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application. This allows the user to access all of Archivematica's functionalities from any programming language, framework or tool without needing to install anything on their system.\\n\\n## Decision\\n\\nThe decision was made to use an existing RESTful API provided by Archivematica in order to expose all of its functionality as an SaaS application. This allows the user to access all of Archivematica's functionalities from any programming language, framework or tool without needing t"}
{"File Name":"archivematica-architectural-decisions\/0010-archivematica-original-design-principles.md","Context":"## Context and problem statement\\nThere is no open-source software system capable of implementing the functional\\nrequirements of the OAIS reference model (ISO 14721:2003). Digital preservation\\nspecialists must use multiple tools, which can be difficult to install and use,\\nto perform discrete preservation actions. Those tools produce metadata that do\\nnot conform to digital preservation and exchange standards and schemas, and do\\nnot provide a way to automatically generate standardized, system-independent,\\nself-documenting Archival Information Packages (AIPs) that package content and\\nPreservation Description Information (PDI) as described by OAIS. Repository\\napplications such as Fedora are capable of performing some but not all OAIS\\npreservation actions, and tend to be complex to develop and maintain, posing a\\nrisk to future retrieval and readability of the AIPs. In fact, any middleware\\nrepository or database software that is required to access and read AIPs is\\ninherently a risk to their long-term usability.\\n","Decision":"Artefactual designed an open-source, web-based archival description and access\\nsystem called ICA-AtoM (Access To Memory) that has a broad user base around the\\nworld. ICA-AtoM does not provide digital preservation functionality as described\\nby OAIS. It would benefit ICA-AtoM users to be able to integrate with a back-end\\nsystem designed to preserve digital objects that are linked to access copies in\\nICA-AtoM. The system should also be usable on its own or in conjunction with\\nother access tools.\\nDesign principles:\\n1. The application will perform a set of configurable preservation actions on\\ningested digital objects, using the file system as the focal point of\\npreservation action operations. Making the file system the focal point of\\nmicro-services operations is noteworthy as a long-term preservation strategy\\nbecause it provides users with the option of direct, unmediated access to\\narchival storage. This might be necessary one day because the various layers\\nand generations of digital preservation system components are just as\\nsusceptible to the risk of technology obsolescence and incompatibility as the\\ndigital objects they are attempting to preserve.\\n2. The information packages ingested by the application will be moved from one\\nmicro-service to the next using the Unix pipeline pattern.\\n3. Micro-service functionality will be provided by one or more of the\\nopen-source software utilities and applications bundled into the application.\\nWhere necessary, these will be supplemented by integration code written as\\nPython scripts.\\n4. The application will provide a graphical user interface so that the end user\\ncan determine and control the status of digital objects moving through the\\npipeline.\\n5. The application will generate AIPs that are system-independent,\\nself-documenting and self-contained. The AIPs will contain PDI that conforms\\nto recognized standards and schemas, such as PREMIS, METS and Dublin Core.\\n6. The file-based AIP is the canonical source of the preserved digital objects\\nand metadata. The preservation application may be supported by databases and\\nindexes that are derived and updated from this source, but these are not\\nessential to long-term preservation of the AIP.\\n7. The application will be designed to integrate with diverse external systems\\ncapable of serving as sources of digital objects to be ingested.\\n8. The application will be designed to integrate with diverse storage systems\\nand protocols for deposit and long-term preservation of AIPs.\\n9. The application will be designed to integrate with diverse external systems\\ncapable of providing search, browse and display capabilities for\\nDissemination Information Packages (DIPs) generated by the application.\\n","tokens":206,"id":880,"Predictions":"\\nThis is a software decision. The OAIS reference model (ISO 14721:2003) is a\\nstandard for the description of archival information assets. It defines a set of\\narchitectural principles and conventions that are used to describe archival\\ninformation assets. The OAIS reference model is not a standard for the design of\\narchival information systems. The OAIS reference model does not provide any\\nformal requirements for archival information systems; it only describes how to do\\na preservation action. The OAIS reference model describes the way that archive\\ninformation is stored and described in the repository and database software.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for a software system. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and problem statement\\nThe OAIS reference model (ISO 14721:2003) describes how to describe archival\\ninformation assets in terms of specific preservation actions, which are implemented in\\nsoftware. These actions include storing, retrieving, managing, and preserving archival\\ninformation assets.\\n\\n## Decision\\n\\nThis is an Architectural Decision Record for an application system. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context and problem statement\\nThe OAIS reference model (ISO 14721:2003) describes how to describe archival\\ninformation assets in terms of specific preservation actions, which are implemented in\\nsoftware. These actions include storing, retrieving, managing, and preserving archival\\ninformation assets.\\n\\n\\end{document}\\n"}
{"File Name":"archivematica-architectural-decisions\/0005-new-api-endpoint-for-csv-validation.md","Context":"## Context and problem statement\\nArchivematica users rely on spreadsheets created in a specific way to perform\\ntasks within or after Archivematica. Documentations and examples can be found\\n[here](https:\/\/www.archivematica.org\/en\/docs\/archivematica-1.9\/user-manual\/transfer\/import-metadata\/)\\nbut there is no clear validation as can be performed by a machine. As one\\nexample, the metadata.csv and rights.csv files are \"special\" and are utilized by\\nArchivematica to add metadata or rights metadata into the AIP's METS file.\\nAnother example is the Avalon Media System having a specific Manifest.csv file\\nthat is used to recreate hierarchical information and additional metadata, which\\nis used after a DIP is created from a stored AIP. It would be beneficial if this\\nmanifest could be validated prior to going through the preservation process.\\nBoth of these examples would benefit from a validation service that a user (or\\nautomated system) could access prior to ingest into Archivematica.\\n","Decision":"* More closely integrate two open source projects: Archivematica and Avalon\\n* [Feature request](https:\/\/github.com\/archivematica\/Issues\/issues\/563) for\\nvalidation of Archivematica-specific CSVs within Archivematica\\n* Creation of something for a specific purpose that can later be extended to\\nsuit many use cases\\nChosen option: \"1. API endpoint for pre-ingest CSV validation\", because it is\\nflexible, it lays the groundwork for future work around CSV validation as a step\\nto be taken by Archivematica. It doesn't have the complications of the long-term\\nmaintenance and testing of a GUI component. It can be more easily automated. The\\nsolution allows for custom or institutionally-specific CSV to be used\/added.\\nTechnically, this would live in the Archivematica codebase and be a new endpoint\\nin the [Archivematica API](https:\/\/wiki.archivematica.org\/Archivematica_API)\\nProposed endpoint below:\\n```yaml\\nURL: \/api\/transfer\/validate_csv\\nVerb: POST\\nValidates local CSV with validator service Python script\\nParameters: CSV\\ninput: Path to the CSV\\nvalidator: Name of service CSV should be checked against, i.e. \"avalon\"\\nor \"rights\"\\nResponse: JSON\\nmessage: Approval or non-approval, depending on service output\\n```\\n### Positive consequences\\n* Fulfills requirement of Avalon\/Archivematica integration work.\\n* Relatively small feature with big potential benefits.\\n* Lays foundation for more development in the future.\\n### Negative consequences\\n* Code must be maintained and tested over time.\\n* User would need API expertise to benefit from this feature.\\n* Implementation -- timeouts on very big CSVs?\\n","tokens":221,"id":881,"Predictions":"## Decision and context\\nThis decision is based on the following information:\\n\\n1. The Manifest.csv file is used to create a hierarchical metadata file for the AIP\\n2. The Manifest.csv file is used to create a set of rights metadata files for the AIP\\n3. The rights.csv file is used to create a set of rights metadata files for the AIP\\n4. The Rights.csv file is used to create a set of rights metadata files for the AIP\\n5. The Manifest.csv file is used to create a set of metadata files for the AIP\\n6. The Manifest.csv file is used to create a set of rights metadata files for the AIP\\n## Decision and context\\n## Context and problem statement\\nArchivematica users rely on spreadsheets created in a specific way to perform tasks within or after Archivematica.\\nDocumentations and examples can be found here but there is no clear validation as can be performed by a machine.\\nAs one example, the metadata.csv and rights.csv files are special and are utilized by Archivematica to add metadata or rights metadata into the AIP's METS file.\\nAnother example is the Avalon Media System having a specific Manifest.csv file that is used to recreate hierarchical information and additional metadata, whichis used after a DIP is created from a stored AIP. It would be beneficial if this manifest could be validated prior to going through the preservation process.\\nBoth of these examples would benefit from a validation service that a user (or automated system) could access prior to ingest into Archivematica.\\n"}
{"File Name":"archivematica-architectural-decisions\/0002-json-encoded-workflow.md","Context":"## Context and problem statement\\nThe core of Archivematica is a server application that coordinates work across\\nmultiple workers. The collection of related tasks (also known as *workflow\\ndata*) is stored in the application database using a relation model.\\nIntroducing changes to the workflow data is hard because the model is clumsy and\\nconvoluted. Changes to the data are delivered as data and schema migrations\\nwhich are difficult to implement and they add risk to the process of upgrading\\nthe software.\\nAdditionally, we want to add full internationalisation support in order to make\\nthe system more attractive to a wider range of users and increase adoption. We\\nneed a simpler mechanism to deliver translations and other workflow data changes\\nin general.\\n","Decision":"* Archivematica 1.9 will implement the proposed solution replacing the existing\\ndatabase tables.\\n* We do not currently intend to expose the ability to modify the workflow data\\nto Archivematica users - but this may change in the future as the solution\\nmatures and becomes interoperable.\\n* The visualization and manipulation of the workflow data will be facilitated by\\na new tool that we're calling amflow.\\n","tokens":150,"id":883,"Predictions":"## Decision description\\nWe will implement a new mechanism for delivering changes to the workflow\\ndata. This will be a *migration* mechanism, rather than a *data migration\\ndata*. The data will be delivered as *.csv files, which are transformed by the application\\ndatabase. The application will then deliver the *.csv files to the workers.\\nThe workers will then use the *.csv files to update their local data. We need a way\\nto deliver these *.csv files reliably and efficiently.\\n"}
{"File Name":"archivematica-architectural-decisions\/0008-remove-quarantine.md","Context":"## Context and problem statement\\nQuarantine is an optional workflow step in Archivematica that places digital\\nobjects in a separate space for a specified period of time for virus definitions\\nto be updated. It adds complexity to the workflow and is arguably unnecessary\\nbecause ingested digital objects generally speaking pre-date the available virus\\ndefinitions, which are updated daily. This proposal is to remove quarantine\\nto simplify Archivematica's workflow and ease maintenance and testing.\\n","Decision":"* Desire to simplify Archivematica workflows\\n* Need to fix known bugs related to quarantine if it remains supported\\n* Little evidence of need for or use of quarantine in the user community\\nOption 1, because there is no evidence of use in the community to justify the\\ncontinued maintenance of the feature and the positive consequences of the\\nremoval of the code outweigh the benefits of the functionality. The virus\\nscanning micro-service remains unchanged by this decision.\\n### Positive consequences\\n* Simplified, easier to maintain workflow\\n* Removes two processing configurations that users are required to make but\\nwhich are rarely implemented.\\n* Resolution of the following issues:\\n* DSpace workflow issue: [#426][1]\\n* Virus scan not triggered after quarantine: [691][2]\\n[1]: https:\/\/github.com\/archivematica\/Issues\/issues\/426\\n[2]: https:\/\/github.com\/archivematica\/Issues\/issues\/691\\n","tokens":99,"id":884,"Predictions":"\\n\\n\\n\\nThe decision is that quarantine is not needed in Archivematica's workflow\\nand should not be added to the workflow. Quarantine is a complicated workflow\\nstep with a number of dependencies. Quarantine is also unnecessary to implement\\nthe proposed workflow because the virus definitions are updated daily.\\n\\n\\end{document}\\n"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"Predictions":"## There are 2 events bus in the system: event bus and event queue.\\n\\n\\n\\n## Decision Context\\n## The event queue is used to store the events. The events are stored in the form of a list of objects with an id. Each object stores one event.\\n\\n## Decision\\n## The event bus is used to send the events to the other modules. The events are sent by the event queue. Each event is sent to each of the modules that need it.\\n\\n## Decision\\n## The decision is based on a number of factors:\\n\\n1) Is it necessary for all modules to receive this event?\\n2) Is there a possibility that an error occurs during transmission?\\n3) How many messages are needed?\\n4) What is the priority of this message?\\n5) What is the size of this message?\\n6) What is the reliability of this message?\\n7) What is the time window for transmission?\\n8) Do we want to send it in a special way, such as by binding it to a \"from\" address or \"to\" address? \\nc\\n\\end{document}\\n"}
{"File Name":"modular-monolith-with-ddd\/0003-use_dotnetcore_and_csharp.md","Context":"## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n","Decision":"I decided to use:\\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\\n- F# will not be used, I don't have commercial experience with it\\n","tokens":23,"id":886,"Predictions":"## 1: The choice of language must be made by the user. This decision is based on the\\n##     ability to code in this language.\\n## 2: The choice of platform must be made by the user. This decision is based on the\\n##     ability to code in this platform.\\n## 3: The choice of language and platform cannot be made at the same time.\\n## 4: The choice of language and platform can be made at the same time.\\n## 5: The choice of language and platform can be made at a later point in time.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n\\n## Decision\\n## 1: The choice of language must be made by the user. This decision is based on the\\n##     ability to code in this language.\\n## 2: The choice of platform must be made by the user. This decision is based on the\\n##     ability to code in this platform.\\n## 3: The choice of language and platform cannot be made at the same time.\\n## 4: The choice of language and platform can be made at a later point in time.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n\\n## Decision\\n%% text can not go here\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n%% text can not go here\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\n* \\href{https:\/\/developer.mozilla.org\/en-US\/docs\/Mozilla\/Projects\/Quality\\_and\\_Reliability\/Code\\_Review\\_Process}{Code Review Process}\\n* \\href{https:\/\/developer.mozilla.org\/en-US\/docs\/Mozilla\/Projects\/Quality\\_and\\_Reliability}{Quality Assurance}\\n\\n\\subsection{Quality Assurance}\\n\\n\\subsubsection{Overview}\\n\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  Code reviews are performed regularly, usually once per week or so, with each review containing multiple tasks that are assigned to different members of QA team who are responsible for performing those tasks.\\n\\end{itemize}\\n\\n\\subsubsection{Tasks}\\n\\nThe following tasks will be assigned to different members of QA team:\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  Code Review (CR)\\n\\end{itemize}\\n\\nThe goal of code review process is to provide feedback about our own code as well as that from other developers about our own code.\\n\\nA CR session will typically involve reviewing some specific part(s) or section(s) of code, which may include reviewing individual functions or classes, or reviewing all functions or classes within a given file.\\n\\nWe will have multiple CR sessions per week, with each session lasting no more than four hours.\\n\\nThe main purpose of CR sessions are:\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  To ensure that all critical issues have been addressed before moving onto new features or bug fixes,\\n\\item\\n  To help identify potential bugs before they become serious,\\n\\end{itemize}\\n\\nWe will use Bugzilla as our bug tracking system, which allows us to track bugs and assign them issue numbers and descriptions.\\n\\nWe will use Jira as our bug tracking system, which allows us to track bugs and assign them issue numbers and descriptions.\\n\\nWe will use Bugzilla as our bug tracking system, which allows us to track bugs and assign them issue numbers and descriptions.\\n\\nWe will use Bugzilla as our bug tracking system, which allows us to track bugs and assign them issue numbers and descriptions.\\n\\nWe will use Bugzilla as our bug tracking system, which allows us to track bugs and assign them issue numbers and descriptions.\\n\\nAdditionally we may perform various other tasks related with QA such as:\\n\\begin{itemize}\\n\\tightlist\\n\\item\\n  Perform unit testing,\\n\\end{itemize}\\n\\nUnit testing refers testing individual modules or smaller parts of modules rather than entire modules or bigger parts thereof. It involves writing tests (tests written using test-driven development), running tests when they pass or failing when they fail; if failing then we try improving those tests until they pass again; if failing again we try improving those tests until they pass again; if failing again we try improving those tests until they pass again; if failing again we try improving those tests until they pass again; if failing again we try improving those tests until they pass again;\\n\\nUnit testing does not require integration testing but integration testing requires unit testing;\\n\\nIntegration testing refers testing integration between two parts (modules). Integration testing involves writing integration tests using mock objects that simulate some aspects but not other aspects;\\n\\nIntegration testing does not require unit testing but unit testing does require integration testing;\\n\\nIntegration test cases are written using test-driven development;\\n\\nIntegration test cases do not necessarily need unit test cases.\\n\\n\\n\\nIf you have any questions regarding these tasks feel free to ask them below.\\n\\n\\n% \\begin{enumerate}\\n% \\tightlist\\n% \\item \\n%   \\hypertarget{_quality_assurance_8md}{}%\\n%   Reviewing your own work - Code Review Session\\n\\n%   \\begin{enumerate}\\n%   \\tightlist\\n%   \\item \\n%     TODO - Add this section here\\n\\n\\n%   \\end{enumerate}\\n\\n\\n% \\end{enumerate}\\n\\n\\n* \\href{https:\/\/developer.mozilla.org\/en-US\/docs\/Mozilla\/Projects\/QA\/QA\\_Tasks}{QA Tasks}\\n\\n* \\href{https:\/\/developer.mozilla.org\/en-US\/docs\/Mozilla\/QA\/QA\\_Process}{QA Process}\\n\\n\\n"}
{"File Name":"modular-monolith-with-ddd\/0014-event-driven-communication-between-modules.md","Context":"## Context\\nEach module should be autonomous. However, communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n","Decision":"Solution number 2 - Event-driven (asynchronous)<\/br>\\nWe want to achieve the maximum level of autonomy and loose coupling between modules. Moreover, we don't want dependencies between modules. We allow direct calls in the future, but this should be an exception, not a rule.\\n","tokens":36,"id":887,"Predictions":"The module is autonomous, and communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nThe module is autonomous. Communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nThe module is autonomous. Communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nCommunication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nCommunication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nCommunication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nCommunication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n## Decision\\nCommunication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n\\n\\subsection{Part 1}\\n\\begin{enumerate}\\n    \\item \\textbf{Input}: A list containing a number n, n distinct elements such that each element appears at most once in the list.\\n\\n\\n    \\item \\textbf{Output}: A list containing a number n such that no two elements appear in different lists.\\n\\n\\n    \\item \\textbf{Input}: A list containing a number n, n distinct elements such that each element appears at most once in the list.\\n\\n\\n    \\item \\textbf{Output}: A list containing a number n such that no two elements appear in different lists.\\n\\n\\n    \\item \\textbf{Input}: A list containing a number n, n distinct elements such that each element appears at least once in the list.\\n\\n\\n    \\item \\textbf{Output}: A list containing a number n such that no two elements appear in different lists.\\n\\n\\n    \\item \\n\\end{enumerate}\\n\\newpage\\n\\n\\subsection{Part 2}\\n\\n\\begin{enumerate}\\n    \\item \\n\\end{enumerate}\\n\\n\\subsection*{\\underline{\\textit{\\large {Question 1}}}}\\nWe can compute $O(n)$ by considering all possible combinations of $n$ distinct numbers from $1$ through $n$.\\n\\nFor example, consider $a = 1$, $b = 2$, $c = 3$. Then we can compute $O(n)$ by considering all possible combinations of $a$, $b$, $c$.\\n\\nFor example, consider $a = 1$, $b = 2$, $\\ldots b = 4$, $\\ldots c = 6$. Then we can compute $O(n)$ by considering all possible combinations of $\\ldots b$, $\\ldots c$.\\n\\nFor example, consider $\\ldots b = 4$, $\\ldots c = 6$. Then we can compute $O(n)$ by considering all possible combinations of $\\ldots b$, $\\ldots c$.\\n\\nFor example, consider $\\ldots b = 4$, $\\ldots c = 6$. Then we can compute $O(n)$ by considering all possible combinations of $\\ldots b$, $\\ldots c$.\\n\\nFor example, consider $\\ldots b = 4$, $\\ldots c = 6$. Then we can compute $O(n)$ by considering all possible combinations of $\\ldots b$, $\\ldots c$.\\n\\n\\section*{\\underline{\\textit{\\large {Question}}}}\\nWe use this question for practice exercises where you will need to find out how many ways there are to arrange six objects (such as pencils) into groups with one pencil missing (or missing completely). You should use your knowledge from earlier questions about permutations (the order in which things occur) or combinations (the order in which things are combined).\\n\\n\\subsection*{\\underline{\\textit{\\large {Question}}}}\\nEach object is unique so there are only seven ways it could appear: three groups with one pencil missing; three groups with one pencil missing; three groups with two pencils missing; three groups with two pencils missing; three groups with three pencils missing; or four groups with one pencil missing; four groups with two pencils missing; four groups with three pencils missing; or four groups with four pencils missing.\\n\\nThere are also nine ways it could appear: three groups with one pencil missing; three groups with two pencils missing; three groups with three pencils missing; or eight groups with one pencil missing; eight groups with two pencils missing; eight groups with three pencils missed; eight groups with four pencils missed or eight groups without any pencil.\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tSECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS SECTION CONTENTS\\n\\n\\n%----------------------------------------------------------------------------------------\\n%\tBIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY BIBLIOGRAPHY \\n\\n\\n"}
{"File Name":"modular-monolith-with-ddd\/0007-use-cqrs-architectural-style.md","Context":"## Context\\nOur application should handle 2 types of requests - reading and writing. <\/br>\\nFor now, it looks like:<\/br>\\n- for reading, we need data model in relational form to return data in tabular\/flattened way (tables, lists, dictionaries).\\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\\n","Decision":"We applied the CQRS architectural style\/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\\n","tokens":87,"id":888,"Predictions":"\\nThe decision is to provide an object graph (like a graph database) so that the user can perform more sophisticated operations. We are not sure if this is the right way to go, but we are willing to try it. If we don't like it, we can always remove it.\\n\\end{verbatim}\\n\\end{itemize}\\n\\subsection*{Future Work}\\n\\begin{itemize}\\n\\item The decision document is not very detailed, and does not include any requirements. We will have to add more details in the future.\\n\\item The decision document does not have a clear direction. It is not clear if we should use a graph database or another approach.\\n\\item We need to add some examples of use cases.\\n\\end{itemize}\\n\\n% \\begin{figure}[ht]\\n% \\centering\\n% \\includegraphics[width=0.8\\linewidth]{images\/decision.png}\\n% \\caption{Decision Document}\\n% \\label{fig:decision}\\n% \\end{figure}"}
{"File Name":"modular-monolith-with-ddd\/0002-use_modular-monolith-system-architecture.md","Context":"## Context\\nAn advanced example of Modular Monolith architecture and tactical DDD implementation in .NET is missing on the internet.\\n","Decision":"I decided to create nontrivial application using Modular Monolith architecture and Domain-Driven Design tactical patterns.\\n","tokens":26,"id":889,"Predictions":"## Yes, we will use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## Yes, we will use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## Yes, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## No, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\n\\n## Yes, we will not use a modular monolith architecture and tactical DDD implementation in .NET.\\\\n"}
{"File Name":"modular-monolith-with-ddd\/0008-allow-return-result-after-command-processing.md","Context":"## Context\\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always \"void\". However, sometimes we need to return some data immediately as part of the same request.\\n","Decision":"We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\\n","tokens":55,"id":890,"Predictions":"## Command\\nThe command is the main method of the software. This method receives the input data and then processes it. The problem with this approach is that it is not possible to return a result immediately. The Command has to return a result, and then we can process it. We can only return a result when we are ready.\\n\\n## Decision\\nThe Command should return a result that will be used as part of the response. The response should be always \"void\".\\n\\end{lstlisting}\\n\\end{table}\\n\\n\\begin{figure}[H]\\n\t\\centering\\n\t\\includegraphics[width=0.7\\textwidth]{software_architecture}\\n\t\\caption{Software architecture for the CQRS and CQS software}\\n\t\\label{fig:software_architecture}\\n\\end{figure}\\n\\n"}
{"File Name":"modular-monolith-with-ddd\/0004-divide-the-system-into-4-modules.md","Context":"## Context\\nThe MyMeetings domain contains 4 main subdomains: Meetings (core domain), Administration (supporting subdomain), Payments (supporting subdomain) and User Access (generic domain).\\nWe use Modular Monolith architecture so we need to implement one application which solves all requirements from all domains listed above.\\nWe need to modularize our system.\\n2. Create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\\n","Decision":"Solution 2.\\nWe created 4 modules: Meetings, Administration, Payments, User Access. The key factor here is module autonomy and maintainability. We want to develop each module independently. This is more cleaner solution. It involves more work at the beginning but we want to invest.\\n","tokens":164,"id":891,"Predictions":"\\nThe Decision is to create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\\n\\n\\n\\n\\n\\n\\n\\end{enumerate}\\n\\end{document}"}
{"File Name":"modular-monolith-with-ddd\/0005-create-one-rest-api-module.md","Context":"## Context\\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\\n","Decision":"Solution 1.\\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\\n","tokens":34,"id":892,"Predictions":"## 1.  We will create an API in our application and expose it to FrontEnd SPA application.\\n## 2.  We will use the API to access some data from our database.\\n## 3.  We will use the API to display the data from our database.\\n\\n% NOTE: If you have a different decision, please make sure you add it here.\\n\\n\\end{document}\\n"}
{"File Name":"modular-monolith-with-ddd\/0006-create-facade-between-api-and-business-module.md","Context":"## Context\\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API\/contract\/interface).\\n","Decision":"Each module will provide implementation for one interface with 3 methods:<\/br>\\n```csharp\\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\\nTask ExecuteCommandAsync(ICommand command);\\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\\n```\\nThis interface will act as a fa\u00e7ade (Fa\u00e7ade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the fa\u00e7ade (module encapsulation).\\n","tokens":42,"id":893,"Predictions":"## 1. The API layer should expose a minimal set of operations to support the maximum level of autonomy. \\n\\n\\n\\nThe user should not need to know about the details of how the API layer is implemented.\\n\\n\\n\\nThe user should not need to know about the details of how the API layer is implemented.\\n\\n\\n## Decision\\n## 2. The API layer should expose a minimal set of operations to support the maximum level of autonomy. \\n\\n\\n\\nThe user should not need to know about the details of how the API layer is implemented.\\n\\n\\n\\nThe user should not need to know about the details of how the API layer is implemented.\\n\\n\\end{lstlisting}\\n\\end{comment}\\n\\n\\subsection{API Design}\\n\\label{sec:api_design}\\n\\n%-------------------------------------------------------------------\\n% 3.3 - Implementation\\n%-------------------------------------------------------------------\\n\\nThis section describes in detail our implementation for each component in our system.\\nFigure~\\ref{fig:api_architecture} shows an overview of our architecture and Figure~\\ref{fig:api_design} shows an overview of our implementation.\\nWe will describe each component in this section, starting from a high-level description and then describe in more detail each component.\\n\\n%-------------------------------------------------------------------\\n% 3.4 - Implementation\\n%-------------------------------------------------------------------\\n\\n\\subsubsection{Library Overview}\\nOur library provides an abstraction over various aspects of data access, including:\\n\\begin{itemize}\\n    \\item A RESTful HTTP interface for accessing data via HTTP requests.\\n    \\item A Java wrapper for accessing data using JSON.\\n    \\item A Java wrapper for accessing data using XML.\\n    \\item A Java wrapper for accessing data using BSON.\\n    \\item A Java wrapper for accessing data using Node.js.\\n    \\item An Android application that allows users to access their data via RESTful HTTP requests and JSON\/XML\/JSONP responses.\\n\\end{itemize}\\n\\nOur library also provides a command-line interface that allows developers to interact with our library programmatically.\\n\\nOur library uses JAX-RS as its framework, which provides a RESTful HTTP interface and a Java wrapper for accessing data via HTTP requests.\\n\\nOur library uses Spring Boot as its framework, which provides a Spring Framework-based application framework.\\n\\nOur library uses Hibernate as its framework, which provides an Object Relational Mapping (ORM) framework based on Hibernate.\\n\\nOur library uses Apache Commons Lang as its framework, which provides a collection of useful classes used in various programming languages.\\n\\nOur library uses Apache Commons Fileupload as its file upload class, which allows users to upload files in a secure manner.\\n\\nOur library uses Apache Commons Logging as its logging class, which logs information about operations performed by users against our system.\\n\\nOur library uses Apache Commons Collections as its collection class, which contains useful classes such as HashSet and TreeSet that are used throughout our system.\\n\\nOur library uses Apache Commons Math as its math class, which contains useful classes such as BigDecimal and BigInteger that are used throughout our system.\\n\\nFinally, we use Apache Commons NetBeans as its NetBeans IDE integration package, which allows us to use NetBeans IDE features such as code completion and refactoring when working with our system's codebase.\\n\\n%-------------------------------------------------------------------\\n% 3.5 - Implementation\\n%-------------------------------------------------------------------\\n\\n\\subsubsection{Data Access Layer Overview}\\nThe Data Access Layer (DAL) is composed of two main components:\\n\\begin{enumerate}\\n    \\item An object-oriented representation of JSON objects that can be used by multiple programming languages.\\n    \\item A Java wrapper for accessing JSON objects via HTTP requests.\\n\\end{enumerate}\\n\\nFigure~\\ref{fig:data_access_layer_architecture} shows an overview of our DAL architecture and Figure~\\ref{fig:data_access_layer_design} shows an overview of our implementation for this component.\\n\\n\\nWe use JAX-RS as its framework, which provides a RESTful HTTP interface with optional authentication capabilities. We also use Spring Boot as its framework, which provides a Spring Framework-based application framework. Our DAL implements JAX-RS because it offers all the functionality we require from JAX-RS without having to write any additional functionality ourselves.\\n\\n\\nWe use Hibernate as its ORM Framework because it offers all the functionality we require from Hibernate without having to write any additional functionality ourselves.\\n\\n\\nWe use Apache Commons Lang because it offers all the functionality we require from Lang without having to write any additional functionality ourselves.\\n\\n\\nWe use Apache Commons Fileupload because it offers all the functionality we require from Fileupload without having to write any additional functionality ourselves.\\n\\n\\nWe use Apache Commons Math because it offers all the functionality we require from Math without having to write any additional functionality ourselves.\\n\\n\\nFinally, we use Apache Commons NetBeans because it offers all the functionality we require from NetBeans IDE integration package without having to write any additional functionality ourselves.\\n\\n\\nFinally, we use Apache Commons Collections because it offers all the functionality we require from Collections without having to write any additional functionality ourselves.\\n\\n\\n%-------------------------------------------------------------------\\n% 3.6 - Implementation\\n%-------------------------------------------------------------------\\n\\n%\\begin{figure}[H]\\n%\\centering\\n%\\includegraphics[width=0.9\\linewidth]{..\/figures\/data_access_layer_architecture.png}\\n%\\caption[Data Access Layer Architecture]{Data Access Layer Architecture}\\label{fig:data_access_layer_architecture}\\n%\\end{figure}\\n\\n\\n%\\begin{figure}[H]\\n%\\centering\\n%\\includegraphics[width=0.9\\linewidth]{..\/figures\/data_access_layer_design.png}\\n%\\caption[Data Access Layer Design]{Data Access Layer Design}\\label{fig:data_access_layer_design}\\n%\\end{figure}\\n\\n\\n%% ------------------------------------------------------------------\\n%% 4 - Related Work\\n\\n%% ------------------------------------------------------------------\\n%% This section describes related work on similar problems or projects within this field or outside this field.\\n\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n%%% Local Variables:\\n%%% mode: latex\\n%%% TeX-master: \"..\/main\"\\n%%% End:\\n"}
{"File Name":"modular-monolith-with-ddd\/0017-implement-archictecture-tests.md","Context":"## Context\\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\\n","Decision":"We decided to implement Unit Tests for our architecture. <\/br>\\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\\n","tokens":56,"id":894,"Predictions":"## Decision Name: #DecisionName\\n#Decision Name\\n\\n## Decision Description: #DecisionDescription\\n#Decision Description\\n\\n## Decision Description: #DecisionDescription\\n#Decision Description\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n## Decision Reasoning: #DecisionReasoning\\n#Decision Reasoning\\n\\n\\end{verbatim}\\n\\nThe \\texttt{#} character is used to denote a comment. This comment is not part of the final output, but rather serves as a reminder of what the decision was and why it was made. The \\texttt{?} character is used to indicate that further information may be required to fully understand the decision. The \\texttt{?} character can be followed by one or more characters, separated by spaces. If no further information is required, then the last character in the \\texttt{?} list should be omitted.\\n\\nAfter each decision, there will be a set of comments for each decision describing what was done and why. These comments are not part of the final output.\\n\\n\\subsection{Code Review}\\n\\nThe code review process consists of two phases:\\n\\n\\begin{itemize}\\n\\item Initial Code Review: A code review team will review the code for each group member individually. Each member will also be asked to provide a list of comments on each line of code.\\n\\item Final Code Review: A team will review all code for each group member at the end of the project. Each member will also be asked to provide a list of comments on each line of code.\\n\\end{itemize}\\n\\nThe purpose of this phase is to identify any issues with the implementation and ensure that they are resolved before moving onto the next phase.\\n\\nA detailed description of this process can be found in Section~\\ref{sec:intro_code_review}. Code reviews are completed using GitLab's \\href{https:\/\/gitlab.com}{GitLab} platform.\\n\\n\\subsection{Project Documentation}\\n\\nTo help document our work, we use GitLab's \\href{https:\/\/gitlab.com\/}{GitLab} platform to keep track of our work. This includes:\\n\\n\\begin{itemize}\\n\\item A single repository for all our work.\\n\\item A single repository for all our documentation.\\n\\end{itemize}\\n\\nAll documentation (including source code) can be found at:\\n\\n{\\small\\n\\n\\url{\\href{\/docs\/}{https:\/\/docs.gitlab.com\/ee\/user\/project\/documentation\/}}\\n\\n}\\n\\nThis documentation can also be found at:\\n\\n{\\small\\n\\n\\url{\\href{\/docs\/}{https:\/\/gitlab.com\/gitlab-org\/gitlab-ce\/docs\/}}\\n\\n}\\n\\nThe documentation may include design documents, change logs, unit tests, test plans and other types of documentation.\\n\\nIt is important that we maintain our documentation in sync with our source code repositories so that we can easily find and reference any changes we have made.\\n\\nTo do this we use GitLab's \\href{https:\/\/gitlab.com\/}{GitLab} platform's integration with GitHub Pages. This allows us to easily publish our documentation on GitHub Pages so that it can be accessed from anywhere with an internet connection.\\n\\nThis integration allows us to publish our documentation on GitHub Pages without having to worry about maintaining version control for source code repositories as well as maintaining GitLab's static websites.\\n\\nWe use GitLab's built-in static site generator called Sitefinity which provides us with an easy way to generate static HTML pages from our source code repositories using Markdown as well as other markup languages such as HTML and CSS.\\n\\n\\n"}
{"File Name":"modular-monolith-with-ddd\/0013-protect-business-invariants-using-exceptions.md","Context":"## Context\\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\\n","Decision":"Solution number 1 - Use exceptions. <\/br>\\nPerformance cost of throwing an exception is irrelevant, we don't want too many if\/else statements in entities, more familiar with exceptions approach.\\n","tokens":32,"id":895,"Predictions":"## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n## ## Decision\\n\\nGet the most recent version of the architecture\\n\\nThe latest version of this document is available on GitHub.\\n\\nDownload ZIP\\n\\nArchitecture documentation for Cloud Foundry, version 0.0.1\\n\\nThis document describes the Cloud Foundry architecture.\\n\\nThis document was generated with DocFetcher on Wednesday 17 January 2017 at 01:21:01 UTC using a UTF-8 encoding."}
{"File Name":"modular-monolith-with-ddd\/0011-create-rich-domain-models.md","Context":"## Context\\nWe need to create Domain Models for all of the modules. Each Domain Model should represent a solution that solves a particular set of Domain problems (implements business logic).\\n","Decision":"Solution number 3 - Rich Domain Model<\/br>\\n1 - no, because the procedural style of coding will not be enough. We want to focus on behavior, not on the data.<\/br>\\n2 - no, keeping business logic in the database is not a good idea in that case, object-oriented programming is better than T-SQL to model our domain and we don't have performance architectural drivers to resign from OOD.<\/br>\\nWe expect complex business logic with different rules, calculations and processing so we want to get as much as possible from Object-Oriented Design principles like abstraction, encapsulation, polymorphism. We want to mutate the state of our objects only through methods (abstraction) to encapsulate all logic and hide implementation details from the client (the Application Service Layer and Unit Tests).<\/br>\\n","tokens":36,"id":896,"Predictions":"## Decision\\nThis is a Decision for the Domain Model for the module 'Change Management'. The decision is that the model should be changed to implement a business logic model.\\n\\n## Decision\\nThis is a Decision for the Domain Model for the module 'Change Management'. The decision is that all of the models should be changed to implement a business logic model.\\n\\n## Decision\\nThis is a Decision for the Domain Model for the module 'Change Management'. The decision is that all of the models should be changed to implement a business logic model.\\n\\n## Decision\\nThis is a Decision for the Domain Model for the module 'Change Management'. The decision is that all of the models should be changed to implement a business logic model.\\n\\n## Decision\\nThis is a Decision for the Domain Model for the module 'Change Management'. The decision is that all of the models should be changed to implement a business logic model.\\n\\n## Decision\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nWe need to create Domain Models for all of the modules. Each Domain Model should represent a solution that solves a particular set of Domain problems (implements business logic).\\n\\n## Decision\\n## Decision\\nThis is an Architectural Decisi"}
{"File Name":"modular-monolith-with-ddd\/0009-use-2-layered-architectural-style-for-reads.md","Context":"## Context\\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\\n","Decision":"We will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\\n","tokens":50,"id":897,"Predictions":"## The query is read from the database. The database will be queried using the ORM (see [ADR 6. Use an ORM](007-use-orm.md)). This means that the database will be queried using a specific query language (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), or a specific query syntax (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)). The query language will be specified by the user, which is not part of the architecture.\\n## Context\\n## We are using SQL to query the database. We will use a specific SQL dialect (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), or a specific SQL syntax (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)).\\n## Decision\\n## The SQL queries will be executed using an ORM, which is used to manage the data access layer (see [ADR 6. Use an ORM](007-use-orm.md)) and is not part of the architecture.\\n\\n## Context\\n## We are using ORMs to manage our data access layer, which is not part of the architecture.\\n## Decision\\n## The queries are executed using an ORM, which is used to manage the data access layer (see [ADR 6. Use an ORM](007-use-orm.md)) and is not part of the architecture.\\n\\n\\n#Related Documents\\n\\newpage\\n\\n#Related Document\\n\\newpage\\n\\n#Related Document\\n\\newpage\\n\\n#Related Document\\n\\newpage\\n"}
{"File Name":"modular-monolith-with-ddd\/0012-use-domain-driven-design-tactical-patterns.md","Context":"## Context\\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n","Decision":"We decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\\n- Command - public method on Aggregate (behavior)\\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\\n- Value Object - immutable class without an identity which represents concept from domain\\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction\/consistency and protects business rules and invariants\\n- Repository - collection-like abstraction to persist and load particular Aggregate\\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity\/Value Object\\n","tokens":70,"id":898,"Predictions":"## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Decision\\n## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Decision\\n## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Decision\\n## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Decision\\n## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Decision\\n## Decision: We decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)). We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n## Note: The above description is a summary of a decision made by a team member. It is not a description of the decision itself.\\n\\n### 3.4.2. Write Your Own\\n\\nWrite your own domain model\\n\\n### 3.4.3. Use Existing Domain Models\\n\\nUse existing domain models\\n\\n### 3.4.4. Use Existing Business Logic\\n\\nUse existing business logic\\n\\n### 3.4.5. Use Existing WebLogic Server Components\\n\\nUse existing weblogic server components\\n\\n### 3.4.6. Use Existing WebLogic Server Components with New Features\\n\\nUse existing weblogic server components with new features\\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n#### 3.5 Design Patterns for Software Architectures \\n\\n### Introduction  \\n\\nThe purpose of this chapter is:\\n\\n* To provide an overview of design patterns, their characteristics, principles, benefits, limitations, implementation techniques, examples, and references.\\n* To show how design patterns can be applied in software architectures.\\n\\n### Overview  \\n\\nDesign patterns are general solutions that have been proven effective in solving specific problems in software engineering.\\n\\nThis chapter will focus on software design patterns that can be used in architectural design.\\n\\nThis chapter will also focus on implementing software design patterns using different technologies.\\n\\nThe following sections will describe what these patterns are, how they are used, what they do, who uses them, when they were created, why they were created, where they were created from (e.g., industry standards), how they are implemented using different technologies (e.g., Java), their benefits (e.g., scalability), limitations (e.g., cost), implementation techniques (e.g., code generation), examples (e.g., Spring MVC), when they were created (e.g., Java EE), where they were created from (e.g., industry standards), how they are implemented using different technologies (e.g., Spring MVC).\\n\\n### What Are Design Patterns?  \\n\\nDesign patterns can be defined as general solutions that have been proven effective in solving specific problems in software engineering.\\n\\nThey are not intended as a complete set of rules; rather, they provide ideas that are useful when designing complex systems.\\n\\nA design pattern is a solution that has been proven effective in solving a specific problem in software engineering by an experienced practitioner.\\n\\nA design pattern is often referred as a reusable solution for common problems encountered during the development of large-scale systems.\\n\\nA design pattern is a solution that has been proven effective in solving specific problems in software engineering by an experienced practitioner.\\n\\nA design pattern is often referred as a reusable solution for common problems encountered during the development of large-scale systems.\\n\\nA design pattern is often referred as a reusable solution for common problems encountered during the development of large-scale systems.\\n\\nDesign patterns can be classified into three categories:\\n\\n* Object-oriented designs;\\n* Behavioral designs;\\n* Infrastructure designs based on principles from computer science.\\n\\nObject-oriented designs are designed at the level of individual classes.\\n\\nBehavioral designs are designed at the level of individual classes or modules within larger systems.\\n\\nInfrastructure designs based on principles from computer science are designed at higher levels such as modules within larger systems or entire enterprise architectures.\\n\\n\\n### How Are They Used?  \\n\\nDesign patterns can be used in various ways:\\n\\n* As-is solutions;\\n* As-is improvements;\\n* As-is refactoring;\\n* As-is adaptations;\\n\\nAs-is solutions can be implemented without any changes;\\n\\nAs-is improvements can be implemented without any changes;\\n\\nAs-is refactoring can be implemented without any changes;\\n\\nAs-is adaptations can be implemented without any changes.\\n\\n\\n### When Are They Used?  \\n\\nDesign patterns can be used anywhere:\\n\\n* In source code;\\n* In source code comments;\\n* In other documentation;\\n* In specifications;\\n* In requirements documents;\\n\\nIn source code;\\n\\nIn source code comments;  \\nIn other documentation;  \\nIn specifications;  \\nIn requirements documents.\\n\\n\\n### Where Are They Used?  \\n\\nDesign patterns can be used anywhere:\\n\\n* At all levels of software development organization;  \\n* At all levels within organizations;  \\n* At all levels within enterprises;  \\n* At all levels within enterprise architectures.\\n\\n\\n### How Are They Implemented?  \\n\\nDesign patterns can be implemented using various technologies:\\n\\nUsing Java;\\n\\nUsing C++;\\n\\nUsing C++++;\\n\\nUsing .NET languages such as C#;\\n\\nUsing .NET languages such as C++;\\n\\nUsing Java EE technologies such as Spring MVC;\\n\\n\\n### What Is The Difference Between A Pattern And A Style?  \\n\\nA style refers only to one implementation technique while a pattern refers only to one class type.\\n\\n\\n### What Is The Difference Between A Pattern And A Concept?  \\n\\nA concept refers only one implementation technique while a pattern refers only one class type.\\n\\n\\n### What Is The Difference Between A Pattern And A Reference?  \\n\\nA reference refers only one implementation technique while a pattern refers only one class type.\\n\\n\\n### What Is The Difference Between A Reference And An Example?  \\n\\nAn example refers only one implementation technique while a reference refers only one class type.\\n\\n\\n### What Is The Difference Between An Example And An Implementation Technique?  \\n\\nAn example refers only one implementation technique while an example refers only one class type.\\n\\n\\n### What Is The Difference Between An Implementation Technique And An Example?\\n\\nAn example refers only one implementation technique while an example refers both an implementation technique and an example.\\n\\n\\nThe following table lists all examples from all chapters:\\n\\n\\n[![Example from Chapter 2 - [https:\/\/github.com\/ghaffar\/Architecture\/blob\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns\/example_design_patterns.java][Example from Chapter 2 - github.com\/ghaffar\/Architecture\/blob\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns\/example_design_patterns.java]](https:\/\/github.com\/ghaffar\/Architecture\/tree\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns)]\\n\\n**Object-oriented**\\n\\n**Behavioral**\\n\\n**Infrastructure**\\n\\n**Java**\\n\\n**C++**\\n\\n**C++**\\n\\n**Java**\\n\\n**C++**\\n\\n**Java**\\n\\n**C++**\\n\\n**Java**\\n\\n**Java*\\n\\n[![Example from Chapter 2 - github.com\/ghaffar\/Architecture\/blob\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns\/examples\/state_machine_example.java][Example from Chapter 2 - github.com\/ghaffar\/Architecture\/blob\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns\/examples\/state_machine_example.java]](https:\/\/github.com\/ghaffar\/Architecture\/tree\/master\/examples\/state_machine_example)\\n\\nExamples:\\nhttps:\/\/github.com\/ghaffar\/Architecture\/tree\/master\/examples\/state_machine_example\\n\\n\\n[![Example from Chapter 2 - github.com\/ghaffar\/Architecture\/blob\/master\/example-design-patterns\/src\/main\/java\/com\/example\/design\/patterns\/examples\/state_machine_example_with_convention_based_state_machine_example.java][Example from Chapter 2 - github.com\/ghaffar\/Architecture\/blob\/master\/examples\/state_machine_example_with_convention_based_state_machine_example.java]](https:\/\/github.com\/ghaffar\/Architecture\/tree\/master\/examples\/state_machine_example_with_convention_based_state_machine_example)\\n\\nExamples:\\nhttps:\/\/github.com\/ghaffar\/Architecture\/tree\/master\/examples\/state_machine_example_with_convention_based_state_machine_example\\n\\n\\n[![Example from Chapter 2 - github.com\/jeremyevans\/web-framework-example-web-servlet-example-with-jpa-example-java-example-codegen-example-java-example-web-servlet-example-with-jpa-example-java-example-codegen-example-java-example-web-servlet-example-with-jpa-example-java-example-codegen-example-java][example-codegen-java][example-web-servlet][example-jpa][example-codegen-jpa][example-codegen-jspsamples][example-codegen-jspsamples]([jeremyevans\/web-framework-example-web-servlet-example-with-jpa-example-java-ea...)[jeremyevans\/web-framework-example-web-servlet-ex...]](https:\/\/github.com\/jeremyevans\/web-framework-examples)\\n\\n\\n[![Example from Chapter 2 - github.com\/jeremyevans\/web-framework-examples\/web-application-examples-with-restful-services-and-mongo-db-example-csharp-ea...][example-csharp-ea...]][examples:csharp]\\n\\nExamples:\\nhttps:\/\/github.com\/jeremyevans\/web-framework-examples\\n\\n\\n[![Example from Chapter 2 - github.com\/jeremyevans\/web-framework-examples\/web-application-examples-with-restful-services-and-mongo-db-example-csharp-ea...][web-application-examples]\\n([jeremyevans\/web-framework-examples])\\n([web-application-examples])\\n\\nExamples:\\nhttps:\/\/github.com\/jeremyevans\/web-framework-examples\\n\\n\\n[![Example from Chapter 2 - github.com\/jeremyevans\/web-application-in-memory-mongodb-sample-csharp-ea...][sample-csharp]\\n([jeremyevans\/web-application-in-memory-mongodb-sample])\\n([sample-csharp])\\n\\nExamples:\\nhttps:\/\/github.com\/jeremyevans\/web-application-in-memory-mongodb-sample\\n\\n\\n[![Example from Chapter 2 - github.com\/jeremyevans\/micro-service-in-memory-mongodb-sample-csharp-ea...][micro-service-in-memory-mongodb-sample]\\n([jeremyevans\/micro-service-in-memory-mongodb-sample])\\n([micro-service-in-memory-mongodb-sample])\\n\\nExamples:\\nhttps:\/\/github.com\/jeremyevans\/micro-service-in-memory-mongodb-sample\\n\\n\\n[![Example from Chapter 2 - github.com\/bounzilas\/restful-services-and-restful-services-with-jpa-data-source-simple-ea...][restful-services]\\n([bounzilas\/restful-services])\\n([restful-services])\\n\\nExamples:\\n[Bounzilas\/restful-services], [Bounzilas\/restful-services], [bounzilas\/restful-services]\\n\\n\\n[![Example From Chapter Two][](http:\/\/imgurimages.org\/imgur\/twisted.png) [Twisted](\\n(http:\/\/www.tutorialspoint.in\/tutorials\/python-tutorial\/python-tutorial-python-tutorials.html) [Python],\\n(http:\/\/www.tutorialspoint.in\/tutorials\/python-tutorial\/python-tutorial-python-tutorials.html) [Python], http:\/\/www.tutorialspoint.in\/tutorials\/python-tutorial\/python-tutorial-python-tutorials"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"Predictions":"## Decision\\nLet us assume that we have a graph of objects, and we want to resolve the dependencies between objects. We need to decide whether we should resolve the dependencies in the following ways:\\n\\begin{itemize}\\n\\item {\\bf Priority}: The order in which objects are processed is important. A higher priority object will be processed before a lower priority object. For example, if the user wants to get the first 1000 documents, then he will choose the document with the highest priority.\\n\\item {\\bf Order}: We want to ensure that all dependencies are resolved in a certain order.\\n\\item {\\bf Priority-Order}: A higher priority object will be processed before lower priority objects.\\n\\end{itemize}\\n%% Decision\\n%% Decision\\nThe decision is made by using a decision tree. We use a decision tree to resolve the dependencies of objects.\\n%% Decision\\n%% Decision\\nFirst, we need to find all possible paths from an object to another object. For example, if we want to get all documents from User X, then there are two paths:\\n\\begin{enumerate}\\n\\item {\\bf User X} -> {\\bf User Y} -> {\\bf Document Y}\\n\\item {\\bf Document Y} -> {\\bf User X} -> {\\bf Document Y}\\n\\end{enumerate}\\n%% Decision\\n%% Decision\\nWe use these two paths as our decision trees. Each node in our decision tree represents one path from an object to another object. For example, in Figure 1,\\n\\begin{enumerate}\\n\\item The root represents User X.\\n\\item The left child is User Y.\\n\\item The right child is Document Y.\\n\\end{enumerate}\\n%% Decision\\n%% Decision\\nWe can see that both paths have the same length and both paths have same path length (number of edges). Therefore, our paths are equal. In addition,\\n% We can also see that both paths have same number of edges (number of vertices). Therefore, our paths are equal. \\nwe know that both paths are equal and they contain exactly one edge each.\\n\\n% We can also see that both paths have same number of edges (number of vertices). Therefore, they contain exactly one edge each.\\n\\n% If we make a new path by adding an edge from one node to another node,\\n% Then this path will be added to our decision tree as well.\\n\\n% If we make a new path by adding an edge from one node to another node,\\n% Then this path will be added to our decision tree as well.\\n\\n%% Decision\\n%% Decision\\nNext, we need to find all possible edges between nodes in our decision tree. For example,\\n% \\begin{enumerate}\\n% \\item Both nodes are on different branches (paths).\\n% \\end{enumerate}\\n\\n% In Figure 2,\\n%\\begin{enumerate}\\n%\\item All nodes in our decision tree are on different branches (paths).\\n%\\end{enumerate}\\n\\n% If you look at Figure 2 closely,\\n%\\begin{enumerate}\\n%\\item All nodes in our decision tree have only one edge connecting them.\\n%\\end{enumerate}\\n\\n% If you look at Figure 2 closely,\\n%\\begin{enumerate}\\n%\\item All nodes in our decision tree have only one edge connecting them.\\n%\\end{enumerate}\\n\\nNow we need to decide whether or not we should resolve the dependencies shown above.\\n\\n\\vspace{-0.1cm}\\n\\n{\\small\\n\\n\\vspace{-0.5cm}\\n\\n{\\tt\\n\\nDecision:\\n\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:}\\n\\n\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:}\\n\\n\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:}\\n\\n\\n\\vspace{-0.3cm}\\n\\n{\\tt\\n\\nDecision:}\\n\\n\\n\\vspace{-0.3cm}}\\n\\n}\\n\\n\\n\\vspace{-0.cm}}\\n\\n\\normalsize\\n\\nThe above figure shows two decisions:\\n\\n1) We should resolve the first dependency between User X and User Y.\\n\\n2) We should not resolve the second dependency between User X and Document Y.\\n\\nThe first dependency is resolved because it has no other path except for this path.\\vspace{-0.cm}}\\n\\nIn Figure 1 and Figure 2,\\nwe can see that there are two decisions for each path shown above:\\n``Should I resolve this dependency'' or ``Should I not resolve this dependency''.\\vspace{-0.cm}}\\n\\nIf we look at Figure 1 closely,\\nwe can see that there is only one edge connecting two nodes on different branches (paths): ``User X -> Document Y''.\\vspace{-0.cm}}\\n\\nIf we look at Figure 2 closely,\\nwe can see that there is only one edge connecting two nodes on different branches (paths): ``User X -> Document Y''.\\vspace{-0.cm}}\\n\\nIn order for us not to make any mistakes when resolving dependencies between objects during processing queries and commands by using a graph of objects, \\nwe need to know how many edges connect different nodes on different branches (paths).\\vspace{-0.cm}}\\n\\nIf you look at Figure 1 closely,\\nyou can see that there are three edges connecting users on different branches (paths): ``User A -> User B'', ``User A -> Document B'', and ``User B -> Document A''. This means that there are three edges connecting users on different branches (paths).\\vspace{-0.cm}}\\nIf you look at Figure 2 closely,\\nyou can see that there are four edges connecting users on different branches (paths): ``User A -> Document B'', ``User B -> Document A'', ``Document A -> Document B'', and ``Document B -> Document A''. This means that there are four edges connecting users on different branches (paths).\\vspace{-0.cm}}\\nWe know how many edges connect users on different branches (paths), so it's easy for us not make mistakes when resolving dependencies between objects during processing queries and commands by using a graph of objects.\\vspace{-0.cm}}\\n"}
{"File Name":"modular-monolith-with-ddd\/0001-record-architecture-decisions.md","Context":"## Context\\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":30,"id":900,"Predictions":"## Decision\\nThe decision is to create a new class for the \"City\" object. The city is the main city of the country and has to be represented by a single class. The city has to have a unique identifier, a name, and an address.\\n\\n## Decision\\n## Decision\\nThe decision is to create a new class for the \"City\" object. The city is the main city of the country and has to be represented by a single class. The city has to have a unique identifier, a name, and an address.\\n\\n## Decision\\n## Decision\\nThe decision is to create a new class for the \"City\" object. The city is the main city of the country and has to be represented by a single class. The city has to have a unique identifier, a name, and an address.\\n\\n## Decision\\n## Decision\\nThe decision is to create a new class for the \"City\" object. The city is the main city of the country and has to be represented by a single class. The city has to have a unique identifier, a name, and an address.\\n\\n\\n\\end{document}\\n"}
{"File Name":"modular-monolith-with-ddd\/0010-use-clean-architecture-for-writes.md","Context":"## Context\\nWe applied the CQRS style (see [ADR #7](0007-use-cqrs-architectural-style.md)), now we need to decide how to handle writing operations (Commands).\\n","Decision":"We will use **Clean Architecture** to handle commands with 4 layers: **API layer**, **Application Service layer**, **Infrastructure layer** and **Domain layer**. <\/br>\\nWe need to add Domain layer because domain logic will be complex and we want to isolate this logic from other stuff like infrastructure or API. Isolation of domain logic supports testing, maintainability and readability.\\n","tokens":43,"id":901,"Predictions":"## Command\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Decision\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Decision\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Decision\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Decision\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Decision\\nWe need to decide how to handle writing operations (Commands).\\n\\n## Command\\nWe need to decide how to handle writing operations (Commands).\\n\\n\\end{verbatim}\\n\\begin{itemize}\\n\\item \\textbf{Command} - This is the name of the command and it will be used in the execution of the service. The command can be either one. It can be a simple command or a complex command.\\n\\item \\textbf{Command} - This is the name of the command and it will be used in the execution of the service. The command can be either one. It can be a simple command or a complex command.\\end{itemize}\\n\\nFirst, we will explain why we have made this decision.\\n\\n\\subsection{Why CQRS style?}\\n\\nCQRS style is very popular among software developers, because it provides a flexible and easy way for developers to manage their data. CQRS style is based on two principles: \\textit{Command} and \\textit{Query}. A \\textit{Command} is an operation that allows you to change data in some database table. A \\textit{Query} is an operation that allows you query for some data from database table.\\n\\nNow, let's see what are these two principles.\\n\\n\\subsubsection{\\textit{Command}}\\n\\nA Command is an operation that allows you change data in some database table. It can be either one, but not both: it can be a simple Command or a complex Command.\\n\\nThe CQRS style provides two different ways for developers:\\n\\begin{enumerate}\\n  \\item The simplest way - when you create your application, you should create a database table with two columns: \\textbf{name of column}, which should contain one word, and \\textbf{id}, which should contain another word.\\n  \\item The most flexible way - when you create your application, you should create several tables with several columns: each column should contain one word.\\n  \\end{enumerate}\\n\\nSo, let's see what are these two ways:\\n\\n\\subsubsection{\\textit{Query}}\\n\\nA Query is an operation that allows you query for some data from database table.\\n\\nThe CQRS style provides two different ways for developers:\\n\\begin{enumerate}\\n  \\item The simplest way - when you create your application, you should create a database table with only one column: \\textbf{name of column}.\\n  \\item The most flexible way - when you create your application, you should create several tables with several columns: each column should contain at least one word.\\n  \\end{enumerate}\\n\\nSo, let's see what are these two ways:\\n\\nNow we will explain why we have made this decision.\\n\\n\\subsection{\\textit{Command vs Query}}\\nWhen we created our first application, we created only one database table with only one column: name of column. Later on we realized that this was not enough because our application needed more information about our customers. So now we have several tables with several columns: each column contains at least one word. \\n\\nNow let's try this example:\\n\\n\\begin{verbatim}\\nCREATE TABLE Customers (\\n    Name VARCHAR(50),\\n    Address VARCHAR(100),\\n    City VARCHAR(50),\\n    State VARCHAR(10),\\n    ZipCode VARCHAR(10)\\n);\\n\\nCREATE TABLE Orders (\\n    Customer_ID INT,\\n    Order_ID INT,\\n    DateOfOrder DATETIME,\\n    Amount INT,\\n    PRIMARY KEY(Customer_ID)\\n);\\n\\end{verbatim}\\n\\nWhen we created this example, our customers were not real people but just numbers in our databases. Now our customers are real people and they have real names which means that they have real addresses and real cities and states. So now our queries look like this:\\n\\n\\begin{verbatim}\\nSELECT * FROM Customers WHERE Name = 'Bob';\\nSELECT * FROM Orders WHERE Customer_ID = 1;\\nSELECT * FROM Orders WHERE Customer_ID = 2;\\nSELECT * FROM Customers WHERE Address LIKE '%Bob%' OR City LIKE '%Bob%' OR State LIKE '%Bob%';\\nSELECT * FROM Customers WHERE ZipCode LIKE '%Bob%' OR City LIKE '%Bob%' OR State LIKE '%Bob%';\\nSELECT * FROM Customers WHERE ZipCode LIKE '12345' OR City LIKE '12345' OR State LIKE '12345';\\nSELECT * FROM Customers WHERE ZipCode LIKE '12345' AND City LIKE '12345' AND State LIKE '12345';\\nSELECT * FROM Orders WHERE DateOfOrder > NOW() AND Order_ID = 1;\\nSELECT * FROM Orders WHERE DateOfOrder > NOW() AND Order_ID = 2;\\nSELECT * FROM Orders WHERE DateOfOrder > NOW() AND Order_ID = 3;\\nSELECT * FROM Orders WHERE DateOfOrder > NOW() AND Order_ID = 4;\\nSELECT Name FROM Customers ORDER BY Name ASC LIMIT 0 ,5;\\nINSERT INTO Customers VALUES ('Bob', '12345', 'New York', 'New York', NULL);\\nINSERT INTO Customers VALUES ('Bob', '45678', NULL,'California');\\nINSERT INTO Customers VALUES ('Bob', NULL,'New York');\\nINSERT INTO Customers VALUES ('Bob', NULL,'California');\\nINSERT INTO Orders VALUES ('1','2017-04-01','New York','New York','2017-04-01');\\nINSERT INTO Orders VALUES ('2','2017-02-01','New York','New York','2017-02-01');\\nINSERT INTO Orders VALUES ('3','2017-02-01','California','California');\\nINSERT INTO Orders VALUES ('4','2017-02-01','California');\\nINSERT INTO Orders VALUES ('5','2016-12-31','California');\\nINSERT INTO Orders VALUES ('6','2016-12-31','Arizona');\\nINSERT INTO Orders VALUES ('7','2016-12-31','');\\n```\\nAs you can see in our example above, there are many rows in our tables which does not make sense at all! We don't know why those rows exist in our tables! For example there are rows like `NULL`, `NULL`, `NULL` etc., but why do they exist? We don't know!\\n\\nSo now let's try another example:\\n\\nIn our previous example we created multiple tables with multiple columns which means that there were many records in those tables. Now let's try another example:\\n\\nIn our previous example there were only three records in our tables so no problems here! But now let's try another example:\\n\\nIn our previous example there were only four records in our tables so no problems here! But now let's try another example:\\n\\nIn our previous example there were only five records in our tables so no problems here! But now let's try another example:\\n\\nIn our previous example there were only six records in our tables so no problems here! But now let's try another example:\\n\\nIn our previous example there were only seven records in those tables so no problems here! But now let's try another example:\\n\\nIn my opinion if we want all those rows then we should add them into all those tables! So it looks like if I want all my customers then I must add them into all those databases!\\n\\nSo what do I mean by adding them into all those databases? If I want all my customers then I must add them into all those databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\n\\nBut if I want just some of my customers then I must add them into one of my databases!\\nAnd so on...\\n\\nIf we think about it logically then yes it makes sense but sometimes it doesn't make sense because sometimes it makes sense but sometimes not!\\n\\nSo here is an illustration about why do we use CQRS style:\\n\\nHere are examples about commands vs queries:\\n```\\nCREATE TABLE Customer (\\n    Name VARCHAR(50),\\n    Address VARCHAR(100),\\n    City VARCHAR(50),\\n    State VARCHAR(10),\\n    ZipCode VARCHAR(10)\\n);\\n\\nCREATE TABLE Order (\\n    Customer_ID INT,\\n    Order_ID INT,\\n    DateOfOrder DATETIME,\\n    Amount INT,\\n    PRIMARY KEY(Customer_ID)\\n);\\n\\nCREATE TABLE CustomerAddress (\\n     CustomerID INT,\\n     AddressID INT\\n);\\n\\nCREATE TABLE OrderAddress (\\n     CustomerID INT,\\n     AddressID INT\\n);\\n```\\nAs you can see from this illustration above commands vs queries look very similar even though they are very different from each other!\\nSo now back to CQRS style:\\nCQRS stands for Command Query Responsibility Segregation.\\nCQRS stands for Command Query Responsibility Segregation.\\nCQRS stands for Command Query Responsibility Segregation.\\end{document}\\n"}
{"File Name":"konduit-serving\/0001-Extend_vertx_launcher_for_CLI.md","Context":"## Context\\nCurrently, we have a main class called [KonduitServingMain](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/45af79d15abe4912ccd81e78c9d215306472036e\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/e791741b80721980f8b66a35ed42f20b30612d5c\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingNodeConfigurer.java) class. We also have an additional [Python CLI](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/7965965b58217f2b4d983fd41aaea013264491ee\/python\/cli.py) that can be just implemented in Java. Vert.x Launcher supports the ability to start, stop and list running verticles out of the box.\\n","Decision":"- Extend `KonduitServingNodeConfigurer` from [Vert.x Launcher](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_the_vert_x_launcher) class.\\n- Write all the initialization\/tear-down logic inside the lifecycle methods of Vert.x Launcher which is present inside both of the above classes (KonduitServingMain and KonduitServingNodeConfigurer).\\n- Extend the CLI and write separate classes for every command we want to add to the CLI (see the documentation [here](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_extending_the_vert_x_launcher)).\\n- Depending on the final decided API, register or unregister commands in Vert.x Launcher.\\n### Example CLI:\\nThe following CLI will be the start for how this will be look:\\n#### Starting Verticles\\nEvery verticle will be named some sensible value instead of the full classpath so that the verticles can be started with a simple name. For example:\\nkonduit serve inference-server --config '{}' # With a JSON string\\nkonduit serve inference-server --config config.json # With a JSON file\\nkonduit serve inference-server --config config.json --name=mnist-prediction-server # With a server name. If no names are given then a random name\\n# is generated.\\nThe CLI can also have options for setting up deployment options like:\\nkonduit serve inference-server --config config.json --instances 3 # For running 3 instances of a Verticle. Usually it runs on a single port with\\n# round robin fashion requests transfer for load balancing\\n#### Stopping Verticles\\nkonduit stop inference --name=mnist-prediction-server # stops the server by the name of \"mnist-prediction-server\"\\n#### Listing Verticles\\nkonduit list # Lists all of the running verticle services with their names, host, port, configuration\\n#### Inspecting Verticles\\nPossible details will include:\\n- configuration\\n- host and port\\n- current resource usage\\n```bash\\nkonduit inspect # Give details of all the running verticles\\nkonduit inspect --name=mnist-prediction-server # Details of a specific verticle\\n```\\n#### Running Predictions\\n#### Predict with JSON (application\/json)\\nkonduit predict --name=mnist-prediction-server --input '{\"key\":\"value\"}' # With JSON string\\nkonduit predict --name=mnist-prediction-server --input input.json # With JSON file\\n#### Predict with Files (multipart\/form-data)\\nkonduit predict --name=mnist-prediction-server --input file.npy # Numpy\\nkonduit predict --name=mnist-prediction-server --input file.zip # DL4J\\nkonduit predict --name=mnist-prediction-server --input image.jspg # Image\\nkonduit predict --name=mnist-prediction-server --input file1.zip,file2.zip # DL4J with multiple inputs\\n","tokens":247,"id":902,"Predictions":"## Given the Context, we have to deploy a verticle to run in a verticle server. We have to consider the following:\\n\\begin{itemize}\\n\\item The verticle must be able to run without any user input\\n\\item The verticle must be able to run without any user input, but the user can provide some input\\n\\item The verticle must be able to run with any user input\\n\\end{itemize}\\nWe have to decide which one of these is the best choice and why.\\n\\end{document}"}
{"File Name":"konduit-serving\/0008-URIs_and_remote_resources.md","Context":"## Context\\nCurrently, the CLI allows users to launch a pipeline based on a configuration file that is hosted locally, such as:\\n```text\\nkonduit serve -c \/some\/path\/to\/config.json\\n```\\nHowever, in addition to serving local files, we want to support serving of pipelines that are hosted remotely:\\n```text\\nkonduit serve -c https:\/\/www.somewebsite.com\/pipelines\/config.json\\nkonduit serve -c s3:\/\/s3-eu-west-1.amazonaws.com\/myaccount\/models\/myconfig.yaml\\nkonduit serve -c wasb:\/\/mycontainer@myaccount.blob.core.windows.net\/directory\/myconfig.json\\n```\\nIn addition to launching a server from a configuration hosted at a remote location, we also want the ability to load models\\nthat are hosted at some non-local URI.\\nThat is: when loading any model type, the location of the model is specified in terms of a URI in the configuration\\n(such as the `modelUri` field in DL4JModelPipelineStep and TensorFlowPipelineStep) which needs to be downloaded (and perhaps cached).\\nConsider the following simple use case:\\n```text\\n$ pip install konduit-serving\\n$ konduit serve -c https:\/\/serving.konduit.ai\/pipelines\/images\/classification\/mobilenetv2.json\\n```\\nAssume the model specified in mobilenetv2.json is hosted remotely also at say `https:\/\/serving.konduit.ai\/models\/mobilenetv2.pb`.\\nThis use case (remote configuration and remote model) should \"just work\".\\nOverall, we will aim to support the following URI types for both configurations and model locations:\\n* HTTP\/HTTPS, HTTPS + basic authentication, and _maybe_ other options like HTTPS + OAuth 2.0?\\n* FTP, FTP + basic authentication\\n* HDFS (Hadoop filesystem, hdfs:\/\/)\\n* Amazon S3 (s3:\/\/)\\n* Azure (Storage account \/ blob storage - wasb:\/\/, wasbs:\/\/)\\n* Google Cloud Storage (gs:\/\/)\\nFor all of these, we simply need the ability to download a single file given the URI. At this point, uploading, directory\\nlisting, etc is not required.\\nThe supported types of URIs should be extensible also - i.e., users should be able to add other types if needed.\\n","Decision":"We utilize standard Java APIs for this: URI, URL, URLConnection and InputStream:\\n```java\\nURI u = URI.create(\"hdfs:\/\/somepath:12345\/pipelines\/config.json\");\\nURL url = u.toURL();\\nURLConnection connection = url.openConnection();\\ntry(InputStream is = connection.getInputStream()){\\n\/\/Read data\\n}\\n```\\nThere are three issues we need to resolve in this approach.\\n1. URI --> URL - i.e., we need protocol handlers for each protocol type\\nA reminder: URIs describe a resource; URLs additionally provide a mechanism to access the resource.\\n2. Ensuring the protocol handlers are on the classpath when required\\nHaving every protocol handler always on the classpath is a simple solution\/option here, though doing so will pollute\\nthe classpath with a bunch of additional dependencies most users won't need, and risks introducing  dependency divergence\\n(version) issues.\\n3. Authentication - how do we deal with authentication (which may be optionally present or absent in many URI types)\\nNote also we may need multiple sets of different credentials for launching the one pipeline.\\nFor example, we may require one set of credentials for `https:\/\/somewebsite.com\/myconfig.json` and an entirely different\\nset of credentials for `https:\/\/othersite.com\/mymodel.pb`.\\n### URI --> URL + Protocol Handlers\\nFor the \"standard\" protocol types such as HTTPS and FTP, these will be built into `konduit-serving-pipeline`.\\nThe other protocols (requiring 3rd party dependencies - `hdfs:\/\/`, `s3:\/\/` etc) will be placed into separate modules with\\nnames `konduit-serving-protocol\/konduit-serving-<protocol_name>` i.e., all protocols are in their own module under the\\n`konduit-serving-protocol` parent module, with name (for example, for Azure `wasb:\/\/` and `wasbs:\/\/`) `konduit-serving-azure`.\\nIn each of these modules (and also in konduit-serving-pipeline) we will provide the following:\\n* `SomeURLStreamHandler extends java.net.URLStreamHandler`\\n* `SomeURLConnection extends java.net.URLConnection`\\nSome of the underlying dependencies (S3 or Azure APIs for example) may provide these already.\\nFinally, we need to actually register these 2 new classes so they are used when we attempt to do the URI --> URL conversion.\\nApparently there are 2 ways to do this: https:\/\/accu.org\/index.php\/journals\/1434\\n* Register a factory\\n* Register via system property\\nThe system property approach is somewhat ugly, but might be the easiest solution. If possible, we will set it at runtime\\nusing System.setProperty before anything that would attempt to read a URL would be used.\\nFor the content of that system property (i.e., the fully qualified class names), we can likely simply collect all available\\nhandlers using a mechanism similar to what konduit-serving-meta already uses for collecting all available pipeline step types.\\nThe alternative is to use URL.setURLStreamHandlerFactory but this has the unfortunate design of being callable only once\\nfor the entire JVM: https:\/\/docs.oracle.com\/javase\/7\/docs\/api\/java\/net\/URL.html#setURLStreamHandlerFactory(java.net.URLStreamHandlerFactory)\\ni.e., the second call to URL.setURLStreamHandlerFactory will throw an exception.\\nThis is also an option but risks being incompatible with any 3rd party dependencies that also call this method.\\nSee also: https:\/\/www.oreilly.com\/library\/view\/learning-java\/1565927184\/apas02.html\\n### Authentication and Credentials\\nUnfortunately, credentials are likely going to be protocol-specific, with different ways of setting the credentials\\nfor different protocols. For example, Azure may require environment variables so set for example an access key, whereas\\nother protocols may require system properties or some other mechanism.\\nWe will need to look into the available authentication methods for each of the protocols we want to support.\\nFor now, where system properties or environment variables can be used, we'll provide a mechanism to do so when launching\\na server (or calling the CLI).\\n### Protocols for CLI\\nUnless the dependencies for the other protocol types (s3, azure, hdfs, etc) are especially large (file size) or are problematic\\n(dependency problems) we will simply include all of the protocol types by default in the CLI.\\nIf there are problems with this \"always available in CLI\" design, we will switch to an alternative approach:\\n* Only the standard protocols (http, ftp etc) are available by default\\n* If another protocol is required (s3, wasbs, hdfs etc) we download the appropriate module dependencies using the build tool\\n* Each module has an entry point used for downloading files, which we run to download the file\\nThat is: we launch a new, temporary process from the CLI that simply calls the module's main method in order to download\\nthe configuration file and then exit\\nNote however that we'll probably need this \"alternative approach\" in the future anyway to enable us to support custom protocols\\nvia the CLI.\\n### Protocols for Runtime (Model Servers)\\nThere are two use cases here, as discussed in the ADR 0006-CI_Launching_vs_Build.md.\\nFirst is the \"immediate CLI launch case\", and second is the \"deployment artifact\" launch case.\\nFor both cases, we will start by always including all protocol modules as per the CLI.\\nAnd again, if this proves problematic, we will consider an alternative.\\nAlternative approach, to be implemented if necessary: We will use the same approach that we use already for determining\\nmodules and dependencies for servers via the build tool.\\nThat is: We will look at the provided configuration (JSON\/YAML) and find any relevant URIs. From that, we will determine\\nthe modules needed. For example, if the configuration contains `\"modelUri\" : \"s3:\/\/some\/path\/to\/model.pb` then we know\\nwe need to include the `konduit-serving-s3` module when launching the server.\\n### Extensibility - Custom Protocol Types\\nThe likely approach for custom protocols is to use the (soon to be added) `additionalDependencies` \/ `additionalJars` options\\nfor the CLI and build tool. By setting one of these, a user can make a protocol handler for their custom type available\\nboth for the CLI and at runtime.\\n### File Caching\\nAn additional consideration here is file caching.\\nSuppose I'm launching a server from a remote URI. I shut down and then immediately restart the server.\\nUpon that restart: should I download the model again or not?\\nFor large models, this could be a significant problem - we don't want to redundantly download a model every time we launch\\na server.\\nTo account for this, we will cache downloaded models in say `~\/.konduit_cache`.\\nThen, when launching servers, we will:\\n1. Check this cache. If no file exists, download as normal\\n2. If a file does exist, we will open a URLConnection and check that both of the values returned by `URLConnection.getLastModified()`\\nand `URLConnection.getContentLength()`. If either differs, we will delete the old file and download from scratch.\\nNote that if opening the URL fails after say 3 retries (hence we can't check if the cached file is the latest): by default\\nwe will provide a warning and use the cached version anyway. This has the downside of potentially using an old model,\\nfor a server, but reduces the probability of intermittent network problems causing a server launch failure even when\\nthe model we've got cached is totally fine.\\nHowever, this \"use anyway\" default behaviour should be switchable to \"fail if we can't validate\" via some system property\\nor environment variable - perhaps `konduit.serving.cache.validation.warnonly` set to true by default, false if \"exception\\non validation failure\" is preferred instead.\\nTo implement this, when downloading a model into the cache, we need to record (in a separate file) the content length and\\nlast modified values. We should also record the most recent time a cache entry was accessed, which will allow us to delete\\nold entries (for example, those that haven't been used for say 60 days or more by default). We will thus implement a method\\nto check and clear the cache once per JVM launch of a server, after the cache has been used\/accessed.\\nTo implement this caching for downloading models, we will add a `URIResolver` class to `konduit-serving-pipeline` with\\n`public static File getFile(URI uri)` and `getFile(String uri)` methods that handles the downloading, caching, etc.\\nThis URIResolver class should then be used within each of the pipeline step runners to launch models.\\nNote that when no scheme is provided for the `getFile(String)` method, URIResolver should assume it's a local file (and\\nhence add the `file:\/\/` scheme to the provided path). Users shouldn't get obscure URI syntax exceptions when trying to\\nlaunch from a local file if they forget to add `file:\/\/` to the start of every single path.\\nFinally, as a workaround for potentially stale cache entries, we will implement a system property `konduit.serving.cache.clear`\\nthat, when set to true, will clear the cache before launching. This should be rarely needed, but will be available if the\\nlastModified and contentLength checks fail to identify an updated remote file for some reason.\\n","tokens":496,"id":904,"Predictions":"The decision for this Decision Record is to support the following URI types for both configurations and model locations:\\n* HTTP\/HTTPS, HTTPS + basic authentication, and _maybe_ other options like HTTPS + OAuth 2.0?\\n* FTP, FTP + basic authentication\\n* HDFS (Hadoop filesystem, hdfs:\/\/)\\n* Amazon S3 (s3:\/\/)\\n* Azure (Storage account \/ blob storage - wasb:\/\/, wasbs:\/\/)\\n* Google Cloud Storage (gs:\/\/)\\nFor all of these, we simply need the ability to download a single file given the URI. At this point, uploading, directory\\nlisting, etc is not required.\\nThe supported types of URIs should be extensible also - i.e., users should be able to add other types if needed.\\n\\n## Description\\nThis Decision Record specifies how to handle remote configuration and remote model locations in a way that is compatible with DL4JModelPipelineStep and TensorFlowPipelineStep. This Decision Record is used by the service provider in order to know how to handle remote configuration and remote model locations.\\n## Notes\\n### Versioning\\nThis Decision Record is versioned as `dl4j_2018_11_01`.\\n### Dependencies\\nThis decision record depends on the following Decision Records:\\n  * dl4j_2018_11_01: dl4j-model-pipeline-step: A Decision Record that provides support for remote configuration and remote model locations.\\n  * dl4j-remote-model-pipeline-step: A Decision Record that provides support for remote configuration and remote model locations.\\n### Issues\\n\\n### TODO\\n\\n### References\\n\\n## Deploying\\n## See Deploying Service Provider Configurations.\\n\\n## Examples\\n## This example shows how to use DL4JModelPipelineStep with Remote Configuration.\\n\\n# # The following are examples of how you might use DL4JModelPipelineStep with Remote Configuration. \\n# # The first example shows how you can use DL4JModelPipelineStep with Remote Configuration using the `config.json` file as a configuration file. \\n# # The second example shows how you can use DL4JModelPipelineStep with Remote Configuration using an S3 bucket as a configuration file. \\n# # The third example shows how you can use DL4JModelPipelineStep with Remote Configuration using an Amazon S3 bucket as a configuration file. \\n\\nimport dl4j.model.pipeline.step.DL4JModelPipelineStep; import dl4j.model.pipeline.step.DL4JRemoteConfiguration; import org.apache.commons.io.FileUtils; import org.apache.log4j.Logger; \\n\\nimport java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.net.MalformedURLException; import java.net.URISyntaxException; \\n\\npublic class Config { \\n\\n    private static final Logger logger = Logger.getLogger(Config.class); \\n\\n    public static void main(String[] args) throws MalformedURLException { \\n\\n        File configFile = new File(\"config.json\"); \\n\\n        if (!configFile.exists()) { \\n\\n            logger.error(\"Config file does not exist!\"); \\n\\n            return; \\n\\n        } \\n\\n        logger.info(\"Config file exists at \" + configFile.getAbsolutePath()); \\n\\n        \/\/ Load config file from local path. \\n        DL4JModelPipelineStep step = new DL4JModelPipelineStep(); \\n        step.setConfigurationFile(configFile.getAbsolutePath()); \\n        step.setTestPath(configFile.getAbsolutePath()); \\n        step.setTestPath(\"\/some\/path\/to\/config.json\"); \\n        step.setTestPath(\"\/some\/path\/to\/model.json\"); \\n        step.setTestPath(\"\/some\/path\/to\/remote-config.json\"); \\n        step.start(); \\n\\n    } }"}
{"File Name":"konduit-serving\/0004-Graph_pipelines.md","Context":"## Context\\nIn the past, Konduit Serving pipelines have been essentially a 'stack' of pipeline steps - each connected to only to the next: `A -> B -> C` and so on.\\nHowever, some use cases require a more complex structure, allowing parallel and\/or conditional execution of pipeline steps.\\nExamples of use cases:\\n* \"Select one of N models\" as part of a pipeline\\n- A\/B split testing (i.e., test different models for different inputs\/users)\\n- One model per X, selected dynamically (per region, language, time of day, etc)\\n- \"Select N of M models\" might occasionally be useful in some cases (like sensor fusion: image + text + sound in, but these are all optional\/unreliable inputs)\\n* Parallel branches\\n- Ensembles of ML models\\n- Parallelization: Execute slow steps in parallel to reduce overall pipeline execution time (database access, network communication, etc)\\n* Fallback models (i.e., \"RNN has 100ms to provide a response otherwise we return 'X'\"; or \"if model fails, return Y\")\\nThis ADR proposed the GraphPipeline, which will enable these use cases and more.\\nThe existing SequencePipeline functionality (i.e., \"stack of steps\" approach) would not be changed by this proposal.\\nFor GraphPipeline, there are a two considerations here:\\n* Functionality to provide\\n* API for providing that functionality\\n","Decision":"### Functionality\\nGraphPipeline will, like SequencePipeline, have a single Data input and a single Data output. This allows for a number of things including:\\n* Embedding a GraphPipeline within another Pipeline (SequencePipeline or GraphPipeline)\\n* Same API and serving methods and serving code for SequencePipeline and GraphPipeline\\n- i.e., users don't\\nInternally, GraphPipeline can have any amount of branching, parallelism, etc.\\nThis single input, single output restriction should not be cause any usability problems due to the fact that Data instances can contain any number of values (i.e., any number of (key,value) pairs). Hence anything we can do with a multi-Data input design can be achieved by combining and splitting Data instances.\\nIt is proposed that we provide support for directed acyclic graphs only - no loops are allowed within graph pipelines.\\nWe provide 5 types of \"graph steps\"\\n* Standard (1-to-1): single input, single output - a normal PipelineStep in a graph\\n* Switch operation (1 to 1-of-N): 1 Data instance in, which is routed to one of N outputs, based on some criteria (value in Data instance, or otherwise)\\n- Example use case: A\/B testing (switch op selects which model step to use)\\n- Merge (N-to-1): simply copy the content of all input Data instances into one output Data instance\\n- Any (N-to-1): simply forwards on the first (possibly only) available Data instance\\n- Typically used in conjunction with a switch step, where only one of N branches is executed\\n- Combine function (N-to-1): An arbitrary N-to-1 function, with or without all the inputs being available first. In addition to allowing custom Java\/Python UDFs, we will provide a small number of built-in functions for:\\n- Ensemble: allows weighted averaging, etc\\n- an integer aggregation selection (`inIdx = argMax(in[i][\"score\"])`)\\n- a timeout condition (\"return X if we get the value within N ms, otherwise return Y\")\\nInternally (but not for JSON) Merge and Any will probably be implemented as special cases of CombineFn - so we have really have just 3 types from an implementation perspective (1->1, 1->(1-of-N), N->1).\\nExamples use cases:\\n- Any: conditional execution with 2 branches: `in -> Switch(a,b), a->X, b->Y, Merge[Any](X,Y) -> output`. We either execute the left branch (`in->a->X->Merge(ANY)->out`) or the right branch (`in->b->Y->Merge(ANY)->out`)\\n- CombineFn: Select and return the predictions of the model with the highest probability\\nWe could also introduce a \"split\" operation, that does 1-to-N by splitting up a single Data instance; in practice we can do this simply by a number of simple \"subset\" pipeline steps in parallel. For example, `in -> SubsetPipelineStep -> A`, `in -> SubsetPipelineStep -> B`, where SubsetPipelineStep simply copies a subset of the input Data (key,value) pairs to the output Data instance.\\nNote that routing an input to \"N of M\" outputs is not easily supported in this proposal (where N changes on each inference step). If needed, it can be added later, or it can be approximated by a series of switch, no-op pipeline steps (returns empty Data), and merge operations.\\n### API (Java)\\nThe goal of the API is to make it as easy as passible to create graph pipelines, that do exactly (and unambiguously) what users expect.\\nAt least two options exist:\\n* Functional-style API\\n* Builder style API (like DL4J ComputationGraphConfiguration)\\nIt is propesd to use a semi-functional API, as follows:\\n```java\\nGraphBuilder b = new GraphBuilder();\\nGraphStep input = b.input();\\n\/\/Standard PipelineStep:\\nGraphStep myStep = input.then(\"myStep\", new SomePipelineStep());    \/\/Always require a name\\n\/\/Merge:\\nGraphStep merged = myStep.mergeWith(\"myMerged\", input);             \/\/Name is optional\\n\/\/Any:\\nGraphStep any = b.any(step1, step2);                                \/\/Name is optional\\n\/\/Combine\\nCombineFn fn = ...\\nGraphStep combined = b.combine(fn, step1, step2);                   \/\/Name is optional\\n\/\/Switch: note exact API here is TBD, but it's essentially a Function<List<Data>,Integer> with a numOutputs() method\\nSwitchFn sf = ...\\nGraphStep[] switched = b.switch(fn, myStep)\\n\/\/Construct the final GraphPipeline\\nPipeline p = b.build(combined);                                     \/\/Build method takes the final output step\\n```\\nAssuming we go with the functional-style design, there's not too many design decisions here, mainly related to naming:\\n* The method name for adding a step - \"then\", \"call\", \"followedBy\", \"inputTo\", and probably a lot more are possible\\n* The method name for merging: \"merge\", \"mergeWith\", etc\\n* The method name for any: \"any\", \"merge\", \"first\", etc\\n* The method name for combine: \"combine\", \"combineFn\", \"combineFunction\", \"aggregate\", etc\\nThere's also the concern that \"merge\" and \"combine\" are too close in name\/meaning, to confuse people. (Suggestions here are welcome)\\n### API (Python)\\nIn Python, it will be almost idestical, other than being a true functional interface for pipeline steps\\n```python\\nb = GraphBuilder()\\ninput = b.input()\\n# Stardard PipelineStep:\\nmyStep = input(\"myStep\", SomePipelineStep())\\n# Merge:\\nmerged = myStep.mergeWith(\"myMerged\", input)\\n# Any:\\nany = b.any(step1, step2)\\n# CombineFn\\nfn = ...\\ncombined = b.combine(fn, step1, step2)\\n# Switch\\nsf = ...\\nswitched = b.switch(sf, myStep)\\n# Construct final GraphPipeline:\\np = b(combined)                     #OR: b.build(combined)?\\n```\\n### JSON\\nWe should consider JSON part of the public API also - we want people to be able to write graph steps using JSON\/YAML by hand.\\nFor SequencePipeline, defining steps is simple: Users just provide an array\/list of steps, like so:\\n```json\\n{\\n\"steps\" : [\\n{\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n{\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n]\\n}\\n```\\nFor Graph pipelines, we have to encode extra information:\\n* Graph structure - i.e., names and inputs\\n* Graph components other than just PipelineSteps\\nProposal: We stay as close to the SequnencePipeline representation as possible, changing only the following:\\n* \"steps\": Becomes an object (map) not a list. Object keys are step names.\\n* We add an \"@input\" (alias: \"@inputs\") field within each pipeline step\\n* For pipeline steps: can take a single value, or a size 1 list\\n* For Merge\/Any\/CombineFn - take a list\/array\\nBy calling the field `@input` \/ `@inputs` we avoid to avoid ambiguity \/ clashing names when it comes to JSON serialization time for arbitrary configuration classes. That is, if we called it `input` and the user had a field called `input` in their configuration class, we have a problem.\\nExample JSON for graph pipelines, with 1 pipeline step (connected to pipeline input), and one merge step (connected to input and the step \"myStep\")\\n```json\\n{\\n\"steps\" : {\\n\"myStep\" : {\\n\"@input\" : \"input\",\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n\"myMerged\" : {\\n\"@input\" : [\"input\", \"myStep\"],\\n\"@type\" : \"MERGE\",\\n}\\n}\\n}\\n```\\n","tokens":301,"id":905,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\nThis ADR proposes that the GraphPipeline should support the following functionality:\\n* Functionality to provide\\n* API for providing that functionality\\n* \"Select one of N models\" as part of a pipeline\\n- A\/B split testing (i.e., test different models for different inputs\/users)\\n- One model per X, selected dynamically (per region, language, time of day, etc)\\n- \"Select N of M models\" might occasionally be useful in some cases (like sensor fusion: image + text + sound in, but these are all optional\/unreliable inputs)\\n* Parallel branches\\n- Ensembles of ML models\\n- Parallelization: Execute slow steps in parallel to reduce overall pipeline execution time (database access, network communication, etc)\\n* Fallback models (i.e., \"RNN has 100ms to provide a response otherwise we return 'X'\"; or \"if model fails, return Y\")\\nThis ADR proposed the GraphPipeline, which will enable these use cases and more.\\nThe existing SequencePipeline functionality (i.e., \"stack of steps\" approach) would not be changed by this proposal.\\nFor GraphPipeline, there are a two considerations here:\\n* Functionality to provide\\n* API for providing that functionality\\n* \"Select one of N models\" as part of a pipeline\\n- A\/B split testing (i.e., test different models for different inputs\/users)\\n- One model per X, selected dynamically (per region, language, time of day, etc)\\n- \"Select N of M models\" might occasionally be useful in some cases (like sensor fusion: image + text + sound in, but these are all optional\/unreliable inputs)\\n* Parallel branches\\n- Ensembles of ML models\\n- Parallelization: Execute slow steps in parallel to reduce overall pipeline execution time (database access, network communication, etc)\\n* Fallback models (i.e., \"RNN has 100ms to provide a response otherwise we return 'X'\"; or \"if model fails, return Y\")\\nThis ADR proposed the GraphPipeline, which will enable these use cases and more.\\nThe existing SequencePipeline functionality (i.e., \"stack of steps\" approach) would not be changed by this proposal.\\nFor GraphPipeline\\tno\\tno\\tno\\tno\\\\\\n"}
{"File Name":"konduit-serving\/0006-CI_Launching_vs_Build.md","Context":"## Context\\nThere are multiple ways a user might want to use Konduit Serving to serve a model.\\nFor the sake of this ADR, I'll refer to two use cases as:\\n1. Immediate deployment case: \"Deploy a server for this pipeline right now on this machine via the CLI\"\\n2. Deployment artifact case: \"Create an artifact (JAR, docker image, etc) for deployment somewhere else\"\\nThe ADR 0005-Packaging_System_Design.md dealt with the \"create deployment artifact\" (2) case above.\\nHowever, the build system in its current form it does well not serve the needs of the immediate deployment use case\\nparticularly well, mainly due to its uber-jar design. This causes a few usability problems:\\n1. Deploying a pipeline with different modules (dl4j vs. samediff vs. TF, or CPU vs. GPU) requires building an uber-jar\\n2. Uber-jar builds can take a long time (30-120+ seconds, plus download time)\\n3. Either we always rebuild (slow launches) or we have an uber-JAR cache (potentially lots of unnecessary disk space used)\\nThe old API CLI dependency\/launching approach for this use case also had\/has the following problems:\\n1. Requiring the user to build a JAR ahead of time and manually include the modules they need\\n2. Not being able to launch (for example) both CPU and GPU servers simultaneously without rebuilding the whole Konduit\\nServing uber-jar in between launching the different servers\\nFor the CLI-based \"deploy right now\" use case, an alternative is proposed.\\n","Decision":"For the \"immediate deployment via CLI\" scenario, we will not create an uber-jar; instead, we will use the Konduit Serving\\nbuild tool to work out the dependencies we need, download them, and return a list of all dependencies (i.e., a list of JAR file\\npaths) that the Konduit Serving server will be launched with.\\nThis is similar to how IDEs like IntelliJ work. Consider for example the command IntelliJ uses when launching unit tests etc: (some parts omitted)\\n```\\n\"C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.242.08-hotspot\\bin\\java.exe\" -ea -Dfile.encoding=UTF-8 -classpath \"C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2019.3.3\\lib\\idea_rt.jar;...;C:\\DL4J\\git\\konduit-serving\\konduit-serving-models\\konduit-serving-deeplearning4j\\target\\test-classes;...;C:\\Users\\Alex\\.m2\\repository\\org\\nd4j\\nd4j-api\\1.0.0-SNAPSHOT\\nd4j-api-1.0.0-SNAPSHOT.jar;C:\\Users\\Alex\\.m2\\repository\\com\\jakewharton\\byteunits\\byteunits\\0.9.1\\byteunits-0.9.1.jar;C:\\Users\\Alex\\.m2\\repository\\com\\google\\flatbuffers\\flatbuffers-java\\1.10.0\\flatbuffers-java-1.10.0.jar;C:\\Users\\Alex\\.m2\\repository\\org\\nd4j\\protobuf\\1.0.0-SNAPSHOT\\protobuf-1.0.0-SNAPSHOT.jar;C:\\Users\\Alex\\.m2\\repository\\commons-net\\commons-net\\3.1\\commons-net-3.1.jar... \" com.intellij.rt.junit.JUnitStarter -ideVersion5 -junit4 ai.konduit.serving.deeplearning4j.TestDL4JStep\\n```\\nNote the `-classpath \"<list of JAR paths>\"` component here.\\nIn practice, we won't pass a list of JAR file paths directly due to constraints on the maximum command line length on Windows.\\nInstead, we will use a small JAR containing a manifest file, which will list the absolute path of all dependencies:\\nhttps:\/\/www.baeldung.com\/java-jar-manifest\\nThis single manifest JAR is then passed via the `-classpath <path>` arg during launch.\\nNote that this exact Manifest JAR approach is also used by IntelliJ as an option for command line shortening to work around\\nthe maximum command line length problem.\\nThe key aspects of this design:\\n**1 - Static CLI JAR**\\nThe CLI JAR is a static JAR without any modules\/dependencies that are needed to run pipeline steps; it never gets modified,\\nrebuilt, etc no matter what type of pipeline is being launched.\\n**2 - Konduit Serving Build Tool - Downloads and Resolves Dependencies**\\nWhen a user launches a server based on some Konduit Serving pipeline configuration, the following occurs:\\n1. The CLI calls the build tool\\n2. The build tool resolves all dependencies that need to be included to run that pipeline\\n3. All direct and transitive dependencies are downloaded as normal via Gradle and stored in their usual location\\n4. The build tool creates the required manifest JAR\\n**3 - We introduce a concept of \"device profiles\" in the CLI**\\nIn practice, this will allow users to switch between different targets when launching (CPU vs. CUDA, and x86 instead of x86_AVX2 if needed\\nfor some reason). Specifically:\\n1. On the first run of the Konduit Serving CLI, we automatically create a set of appropriate device profiles based on\\nhardware and software available on this system. We will also set the default device profile to the CUDA profile if present.\\nIn practice, this will usually be just a CPU profile (highest level supported - avx2, avx512, etc) and a CUDA profile\\nif a CUDA device is present on the system. For the CUDA profile, we'll also detect if the CUDA is installed (if not,\\nwe'll use the JavaCPP presets CUDA redist binaries to provide it at runtime, avoiding the need for a manual install)\\n2. When running, we'll use the default profile unless the user passes a `-p <profile_name>` during launch. That is:\\n`konduit serve -c config.json -p CPU`\\nIn practice most users won't need to worry about device profiles, unless they need:\\n(a) to run on CPU only for a GPU-enabled device, or\\n(b) (very rarely, if ever) need to downgrade the target (for example, x86 instead of x86_avx2 on an avx2 compatible system)\\nas a workaround to some issue with an avx2 or higher binary.\\n### Example Workflow: Launch Locally\\nSuppose a user wants to deploy a server for inference, on system without a Konduit Serving installation.\\nHere's what that could look like:\\n```text\\n$ pip install konduit-serving           #Or any other easy installation method - apt, yum, etc etc\\n$ konduit serve -c config.json\\nKonduit Serving\\n--- Konduit Serving First Run Initialization ---\\nDetecting hardware... done\\nCPU:                 ARM64 (aarch64) - 4 core\\nCUDA GPU:            <device name>, 4GB\\nCUDA installation:   Found CUDA 10.2 (\/usr\/local\/cuda)\\nCreating device profiles...\\nProfile 1: \"CUDA_10.2\" - ARM64, CUDA 10.2 installed\\nProfile 2: \"CPU\"       - ARM64, CPU execution\\nCreating device profiles complete\\nSetting default profile: \"CUDA_10.2\"\\nUse <some_command> to set default profile or pass \"-p <profile>\" when launching to override\\n--- First run initialization complete ---\\nLaunching server using default device profile \"CUDA_10.2\"\\nAcquiring dependencies... done\\n<usual Konduit Serving launch info>\\n```\\nNote that users need only 2 lines here to go from a brand new system (no KS install) to hosting a model server using the\\noptimal hardware\/configuration for that device (i.e., CUDA, or highest supported AVX level for x86 systems, etc).\\nFurthermore there is no slow \"build uberjar\" step that delays the launching of the server by 30-120 seconds, on top of\\ndependency downloading.\\n### Launching for the \"Deployment Artifact\" case\\nThis \"manifest JAR\" approach can likely be used in other situations:\\n* Docker: Could use either uber-JAR or switching to an assembly-JAR style (i.e., embed the original\/unmodified dependency\\nJARs instead of an uberjar)\\n* RPM and DEB: As per docker\\n* Stand-alone .exe: Continue to use uber-JAR approach\\nIf we decide an assembly-JAR style approach is useful for these deployment artifacts, we can implement that at a later date.\\nAlso, in principle we can add extra dependencies on top of an uberjar... it may not be an especially elegant design, but\\ncombining uber-jars with 'extra' classpath dependencies may be possible if we ever really need it. However that won't\\nbe something we support for now\\n### Detecting Hardware and Creating Profiles\\nDetecting the CPU details should be straightforward, at least on x86-based systems using a library such as OSHI:\\nhttps:\/\/github.com\/oshi\/oshi\\nHow well OSHI supports ARM-based platforms is something we need to explore, though Raspberry Pi (armhf) support does\\nseem to be available: https:\/\/github.com\/oshi\/oshi\/issues\/864\\nFalling back on a system utility (`cat \/proc\/cpuinfo` or similar) is also a possibility here.\\nDetecting the presence or absence of a compatible CUDA GPU may be harder. When CUDA is installed and available on the\\npath, this becomes easier (CUDA install -> assume CUDA device present, or parse output of `nvidia-smi`).\\nFor the \"no CUDA installed but CUDA GPU available\" case, OSHI may show it up, or there may be a command line based approach\\nto find it (like cpuinfo).\\nIn principle this is a solveable problem but additional work is required to find a robust solution for detecting hardware\\n(including CUDA GPUs, and maybe other GPUs in the future) that will work across all devices and operating systems we expect\\nto deploy on in practice.\\n","tokens":341,"id":906,"Predictions":"## Decision\\nThe CLI-based deploy right now use case is considered, and it is decided that the current API does not provide the desired functionality.\\nThe following features are added to the CLI: \\n\\n- The ability to create a new pipeline (or module) from scratch, or from an existing pipeline (or module)\\n- The ability to delete a pipeline or module\\n- A new API for launching a server (or modules) based on the CLI\\n- A new API for launching a server on different machines (or modules) via the CLI\\n- An API for launching on multiple machines simultaneously (or modules)\\n\\nThe following features are removed from the CLI: \\n\\n- Ability to deploy a pipeline or module directly to a machine with a specific name (e.g. \"p1\", \"p2\", etc)\\n- Ability to deploy multiple pipelines and modules at once via the CLI\\n- Ability to build uber-jars in between launchings of different servers\\ndue to these changes, this use case is now considered:\\n1. The CLI-based \"deploy right now\" use case is considered, and it is decided that this is not the desired functionality.\\n2. The current API does not provide the desired functionality.\\ndue to these changes, this use case is now considered:\\n1. The CLI-based \"deploy right now\" use case is considered, and it is decided that this is not the desired functionality.\\ndue to these changes, this use case is now considered:\\n1. The CLI-based \"deploy right now\" use case is considered, and it is decided that this is not the desired functionality.\\ndue to these changes, this use case is now considered:\\n1. The CLI-based \"deploy right now\" use case has been removed.\\ndue to these changes, this use case has been removed.\\n## Context\\n## Context\\nThese two cases are related: \\n1. Deploying a pipeline on an existing machine (with specific name)\\ndue to these changes, this uses-case has been removed.\\ndue2. Deploying a pipeline on an existing machine (with specific name)\\ndue2a. This requires building an uber-jar in between launchings of different servers\\nDue to these changes, this uses-case has been removed.\\ndue2b. This requires building an uber-jar in between launchings of different servers\\ndue2c. This requires building an uber-jar in between launchings of different servers\\ndue2d. This requires building an uber-jar in between launchings of different servers\\ndue2e. This requires building an uber-jar in between launchings of different servers\\ndue3. This requires building an uber-jar in between launchings of different servers\\ndue4. This requires building an uber-jar in between launchings of different servers\\end{itemize}\\n"}
{"File Name":"konduit-serving\/0002-Use_swagger_annotations_for_generating_client_APIs.md","Context":"## Context\\nConsidering only inferences, we have two endpoints that are responsible to take inputs for prediction.\\n- application\/json\\n- \/:predictionType\/:inputDataFormat # For JSON inputs\\n- multipart\/form-data\\n- \/:predictionType\/:inputDataFormat # For multipart inputs (FILES)\\nGiven those two endpoints we have done a ton of work on creating a python client that (as of right now) needs more proper documentation, examples and maintenance planning for making the APIs more adaptable.\\nSince, we're planning to have APIs in multiple languages (python, java, C# and others), it might get difficult to maintain and document them separately in the future.\\n","Decision":"### Using swagger annotations to generate and document client APIs.\\n[Swagger annotations](https:\/\/github.com\/swagger-api\/swagger-core\/wiki\/Swagger-2.X---Annotations) are a quick and easy way to generate openapi specifications from the source code. Annotations apply to classes, methods and arguments. Using this way, it's easier to get rid of client APIs maintenance (generation, documentation and packaging) in different languages.\\n#### How it works?\\nBy refactoring konduit-serving source code into classes that contain APIs for different types of verticles. For example:\\n```java\\n@Path(\"\/\")\\n@Produces(MediaType.APPLICATION_JSON)\\npublic class InferenceApi {\\n@GET\\n@Path(\"\/config\")\\npublic InferenceConfiguration getConfig() {\\nreturn new InferenceConfiguration();\\n}\\n@POST\\n@Path(\"\/{predictionType}\/{inputDataFormat}\")\\n@Consumes(MediaType.MULTIPART_FORM_DATA)\\n@Operation(summary = \"Get inference result with multipart data.\",\\ntags = {\"inference\"},\\ndescription = \"You can send multipart data for inference where the input names will be the names of the inputs of a transformation process. \" +\\n\"or a model input and the corresponding files containing the data for each input.\",\\nresponses = {\\n@ApiResponse(description = \"Batch output data\",\\nresponseCode = \"200\",\\ncontent = @Content(schema = @Schema(oneOf = {\\nClassifierOutput.class,\\nRegressionOutput.class,\\nDetectedObjectsBatch.class,\\nManyDetectedObjects.class\\n}))\\n),\\n}\\n)\\npublic BatchOutput predict(@PathParam(\"predictionType\") Output.PredictionType predictionType,\\n@PathParam(\"inputDataFormat\") Input.DataFormat inputDataFormat,\\n@Parameter(description = \"An array of files to upload.\") File[] multipartInput) {\\nreturn new ClassifierOutput();\\n}\\n}\\n```\\nThis will require similar refactoring for the other verticles and their respective routers. Currently, the following classes will have to be refactored based on the above details:\\n- PipelineRouteDefiner\\n- MemMapRouteDefiner\\n- ConverterInferenceVerticle\\n- ClusteredVerticle\\n#### How it would look at the end?\\nHaving an API that looks like the class above will generate an API specification that will look like:\\n```yaml\\nopenapi: 3.0.1\\ninfo:\\ntitle: Konduit Serving REST API\\ndescription: RESTful API for various operations inside konduit-serving\\ncontact:\\nname: Konduit AI\\nurl: https:\/\/konduit.ai\/contact\\nemail: hello@konduit.ai\\nlicense:\\nname: Apache 2.0\\nurl: https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/master\/LICENSE\\nversion: 0.1.0-SNAPSHOT\\nexternalDocs:\\ndescription: Online documentation\\nurl: https:\/\/serving.oss.konduit.ai\\ntags:\\n- name: inference\\ndescription: Tag for grouping inference server operations\\n- name: convert\\ndescription: Tag for grouping converter operations\\n- name: memmap\\ndescription: Tag for grouping memory mapping operations\\npaths:\\n\/config:\\nget:\\noperationId: getConfig\\nresponses:\\ndefault:\\ndescription: default response\\ncontent:\\napplication\/json:\\nschema:\\n$ref: '#\/components\/schemas\/InferenceConfiguration'\\n\/{predictionType}\/{inputDataFormat}:\\npost:\\ntags:\\n- inference\\nsummary: Get inference result with multipart data.\\ndescription: You can send multipart data for inference where the input names\\nwill be the names of the inputs of a transformation process. or a model input\\nand the corresponding files containing the data for each input.\\noperationId: predict\\nparameters:\\n- name: predictionType\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- CLASSIFICATION\\n- YOLO\\n- SSD\\n- RCNN\\n- RAW\\n- REGRESSION\\n- name: inputDataFormat\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- IMAGE\\n- ARROW\\nrequestBody:\\ndescription: An array of files to upload.\\ncontent:\\nmultipart\/form-data:\\nschema:\\ntype: array\\nitems:\\ntype: string\\nformat: binary\\nresponses:\\n\"200\":\\ndescription: Batch output data\\ncontent:\\napplication\/json:\\nschema:\\noneOf:\\n- $ref: '#\/components\/schemas\/ClassifierOutput'\\n- $ref: '#\/components\/schemas\/RegressionOutput'\\n- $ref: '#\/components\/schemas\/DetectedObjectsBatch'\\n- $ref: '#\/components\/schemas\/ManyDetectedObjects'\\ncomponents:\\nschemas:\\nInferenceConfiguration:\\ntype: object\\nproperties:\\nsteps:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/PipelineStep'\\nservingConfig:\\n$ref: '#\/components\/schemas\/ServingConfig'\\nmemMapConfig:\\n$ref: '#\/components\/schemas\/MemMapConfig'\\nMemMapConfig:\\ntype: object\\nproperties:\\narrayPath:\\ntype: string\\nunkVectorPath:\\ntype: string\\ninitialMemmapSize:\\ntype: integer\\nformat: int64\\nworkSpaceName:\\ntype: string\\nPipelineStep:\\ntype: object\\nproperties:\\ninput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputNames:\\ntype: array\\nitems:\\ntype: string\\ninputNames:\\ntype: array\\nitems:\\ntype: string\\nServingConfig:\\ntype: object\\nproperties:\\nhttpPort:\\ntype: integer\\nformat: int32\\nlistenHost:\\ntype: string\\noutputDataFormat:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- ARROW\\nuploadsDirectory:\\ntype: string\\nlogTimings:\\ntype: boolean\\nincludeMetrics:\\ntype: boolean\\nmetricTypes:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- CLASS_LOADER\\n- JVM_MEMORY\\n- JVM_GC\\n- PROCESSOR\\n- JVM_THREAD\\n- LOGGING_METRICS\\n- NATIVE\\n- GPU\\nClassifierOutput:\\ntype: object\\nproperties:\\ndecisions:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\nprobabilities:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nlabels:\\ntype: array\\nitems:\\ntype: string\\nbatchId:\\ntype: string\\nRegressionOutput:\\ntype: object\\nproperties:\\nvalues:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nbatchId:\\ntype: string\\nDetectedObjectsBatch:\\ntype: object\\nproperties:\\ncenterX:\\ntype: number\\nformat: float\\ncenterY:\\ntype: number\\nformat: float\\nwidth:\\ntype: number\\nformat: float\\nheight:\\ntype: number\\nformat: float\\npredictedClassNumbers:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\npredictedClasses:\\ntype: array\\nitems:\\ntype: string\\nconfidences:\\ntype: array\\nitems:\\ntype: number\\nformat: float\\nbatchId:\\ntype: string\\nManyDetectedObjects:\\ntype: object\\nproperties:\\ndetectedObjectsBatches:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/DetectedObjectsBatch'\\nbatchId:\\ntype: string\\nwriteOnly: true\\nBatchOutput:\\ntype: object\\nproperties:\\nbatchId:\\ntype: string\\nwriteOnly: true\\n```\\nAnd from this yaml the clients will be generated using [openapi-generator](https:\/\/github.com\/OpenAPITools\/openapi-generator). For example:\\n```bash\\njava -jar openapi-generator-cli.jar generate -i openapi.yaml -g python -o python_api_client\\n```\\nThe above command will generate the python clients for us and the related docs for using the API in python.\\nThis will be a similar process for other languages as well.\\n","tokens":138,"id":907,"Predictions":"## Decision\\nWe have created a python client that can be used to send and receive requests. The client is hosted at https:\/\/github.com\/airbnb\/predict. The code is available at http:\/\/github.com\/airbnb\/predict.\\n\\n## Output\\n## Output\\n\\n\\n\\nPredicting the input data using the following inputs:\\n- 1x1 image with 1000 voxels, 32 channels, 4 channels per voxel (3 channel = 1 line, 4 channel = 1 plane)\\n- input file (same as above)\\n\\n\\nThe output will be a list of predicted voxels in the form of (x,y,z) where x,y,z are the predicted voxel coordinates.\\n- A list of predictions corresponding to each input.\\n- A list of all predictions for the entire dataset.\\n- A list of all predictions for all other datasets. (If any)\\n# Input #1 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputs.json# Input #2 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #3 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #4 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #5 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #6 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #7 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #8 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #9 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #10 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Input #11 https:\/\/raw.githubusercontent.com\/airbnb\/predict\/master\/inputDataFormats.json# Output List of predictions for each input and for all other datasets:\\n- (x,y,z): predicted voxel coordinates \\n- (x,y,z): list of predictions corresponding to each input \\n- (x,y,z): list of predictions corresponding to all other datasets.\\n\\nThe output will be a list of predicted voxels in the form of (x,y,z) where x,y,z are the predicted voxel coordinates.\\n- A list of predictions corresponding to each input.\\ndecision: Decisions are used to inform users about what is going on during an inference process. We use decisions to inform users about what is going on during an inference process. The decisions are implemented as a dictionary with keys being the prediction type and values being the prediction matrix.\\ndecisionType: Decisions are used to inform users about what is going on during an inference process. We use decisions to inform users about what is going on during an inference process. The decisions are implemented as a dictionary with keys being the prediction type and values being the prediction matrix.\\ndecisionType: Decisions are used to inform users about what is going on during an inference process. We use decisions to inform users about what is going on during an inference process. The decisions are implemented as a dictionary with keys being the prediction type and values being the prediction matrix.\\ndecisionType: Decisions are used to inform users about what is going on during an inference process. We use decisions to inform users about what is going on during an inference process. The decisions are implemented as a dictionary with keys being the prediction type and values being the prediction matrix.\\ndecisionType: Decisions are used to inform users about what is going on during an inference process. We use decisions to inform users about what is going on through this API.\\ndecisionType: Decisions are used to inform users about what is going on through this API.\\ndecisionType: Decisions are used to inform users about what is happening through this API.\\enddecision\\n## Decision\\n## Decision\\nWe have created a python client that can be used to send and receive requests. The client is hosted at https:\/\/github.com\/airbnb\/predict.\\n"}
{"File Name":"konduit-serving\/0009-Custom_Endpoints.md","Context":"## Context\\nCurrently, we have the \"\/predict\" endpoint for performing inference - this takes a Konduit Serving `Data` instance in,\\nreturns a `Data` instance out, in either JSON or binary (protobuf) format. We also have the ability ot create Grafana dashboards\\nbased on our metrics functionality\/endpoints.\\nGrafana dashboards are totally fine for some use cases (simple passive visualization). However, in we might want:\\n(a) more flexibility and control than Grafana is able to provide for displaying predictions\/metrics (Grafana doesn't really\\nsupport images or video, other than maybe via custom plugins),\\n(b) The ability to provide interactive dashboards, that can also take input, not just show outputs\\n(c) the ability to provide input\/output in some \"application specific\" format (for example, allow users to post a raw PDF\\nto a custom endpoint, instead of having to do preprocessing on the client side post create image\/text via the Konduit\\nServing Data).\\nAdditionally, for developing real apps we might need to (for example) provide a configuration endpoint, or a debugging\\nendpoint, etc that hosts an actual HTML page the user can view and interact with.\\n","Decision":"This proposal suggests the addition of custom endpoint functionality for Konduit Serving.\\nThe idea: to add a custom endpoint (get\/post etc, with or without inteference), users need to:\\n1. Provide a class that implements an interface - i.e., `MyCustomEndpoint implements HttpEndpoint`\\n2. Provide the fully qualified class name in their pipeline configuration - i.e., `\"customEndpoint\" : \"com.company.MyCustomEndpoint`\\n3. When launching the server, they provide a JAR with their custom code (using the \"additionalDependencies\" and \"additionalClasspath\"\\nmechanism already proposed for the CLI \/ build tool) - i.e., the JAR containing the custom endpoint code is provided\\nvia a JAR location, or via GAV coordinates\\nThe `HttpEndpoint` interface is defined with just one method:\\n`endpoints(Pipeline, PipelineExecutor) : List<Endpoint>`\\nThis gives the endpoint access to the pipeline\/executor for use in the endpoints\\nThe `Endpoint` interface then has the following methods:\\n`type() : io.vertx.core.http.HttpMethod` - i.e., GET, POST, etc\\n`path() : String` - i.e., \"\/myEndpoint\" - may optionally include path parameters etc\\n`consumes() : List<String>` - MIME types for the input\\n`produces() : List<String>` - MIME types of the response\\n`handler() : Handler<RoutingContext>` - i.e., the actual endpoint-handling code.\\nNote that the endpoints don't necessarily have to call the underlying pipeline \/ pipeline executor, though some will.\\nFor example, two use cases supported by this design include:\\n(a) Static HTML page serving - that may have the option to call another endpoint (example: a form that allows uploading\\nof an image to another endpoint)\\n(b) POST endpoints for proving for example a PDF, that does conversion, calls the executor, and returns a `Data` instance\\nor some other type\\n","tokens":252,"id":908,"Predictions":"\\nWe will add a \"configuration\" endpoint to our service that will allow users to configure the data and metrics\\nstructure. The configuration endpoint will provide a JSON or protobuf response, and the user would be able to specify:\\n(a) what type of data we want to send (JSON or protobuf)\\n(b) what type of metrics we want to send (JSON or protobuf)\\n(c) what format the metrics are in (JSON or protobuf).\\nWe also want to provide an \"image\" endpoint that allows users to upload a .png image, instead of having them post in text\\nformat.\\nAdditionally, we want the ability for users to create custom endpoints, and add them as endpoints on our service.\\nThis would allow us to provide more flexibility for customizing our service than Grafana currently provides.\\n\\n## Context\\nWe are using Konduit Serving as the underlying platform for this project. Konduit Serving is a node.js application that provides\\na RESTful API which enables clients to access data stored in our database.\\n\\n## Decision\\n\\nWe will add a \"image\" endpoint that allows users to upload images. This would allow us to provide more flexibility for\\ncustomizing our service than Grafana currently provides.\\n\\end{Verbatim}\\n\\n\\n\\section{Additional Work}\\n\\label{sec:additional-work}\\nWe have been able to use Konduit Serving and Grafana in an application that is useful for providing reports on performance metrics. We have also been able to use Konduit Serving and Grafana in an application that allows users to submit their own endpoints. We have not yet been able to use Konduit Serving and Grafana in an application that allows users to post images.\\n\\n\\subsection{Grafana}\\n\\label{subsec:grafana}\\nGrafana is a powerful tool for providing real-time dashboards of metrics from services like Konduit Serving. It has a number of features, including the ability for users to create dashboards based on their own data, and the ability for users to submit their own dashboards through APIs.\\n\\nHowever, Grafana does not currently support uploading images directly from its UI. Instead, it uses an image viewer plugin which must be installed locally on each client machine.\\n\\nIn addition, Grafana does not currently support uploading images directly from its UI. Instead, it uses an image viewer plugin which must be installed locally on each client machine. This means that if you want your dashboard image(s) available outside of your computer network you need some way of sending these files over the network.\\n\\nIn order for us to use Grafana with Konduit Serving we need:\\n\\begin{itemize}\\n    \\item Ability for clients (users) submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs\\n    \\item Ability for clients submitting dashboards through APIs \\n\\end{itemize}\\n\\nIn order for us not only be able to use Grafana but also be able post images directly from our endpoints we need:\\n\\begin{itemize}\\n    \\item Ability both from within Konduit Serving itself as well as from other applications (such as external applications)\\n    \\item Ability both from within Konduit Serving itself as well as from other applications (such as external applications)\\n    \\item Ability both from within Konduit Serving itself as well as from other applications (such as external applications)\\n    \\item Ability both from within Konduit Serving itself as well as from other applications (such as external applications)\\n    \\item Ability both from within Konduit Serving itself as well as from other applications (such as external applications)\\n\\end{itemize}\\n\\nAs mentioned above we have been able use Grafana with Konduit serving by installing an image viewer plugin locally on each client machine which then sends images over HTTP\/HTTPS using the request method POST.\\n\\nHowever there are some limitations with this approach:\\n\\n1. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n2. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n3. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n4. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n5. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n6. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n7. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n8. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n9. There is no way of sending images directly over HTTP\/HTTPS using request method POST\\n\\n10. The API only supports GET requests.\\n\\n11.. The API only supports GET requests.\\n\\n12.. The API only supports GET requests.\\n\\n13.. The API only supports GET requests.\\n\\n14.. The API only supports GET requests.\\n\\n\\nThis means that if we wanted our dashboard image(s) available outside our network then we would need some sort of mechanism by which these files could be sent via HTTPS when they are created by a user via their own web browser.\\n\\n\\nThe best solution here would be if we could write some sort of plugin which would allow us send all requests made by users via their web browsers via HTTPS.\\n\\n\\nAnother option would be if we could somehow send all requests made by users via their web browsers via HTTPS when they are created by a user via their own web browser.\\n\\n\\nHowever neither option appears possible at this time.\\n\\n\\nTherefore we decided that it was necessary in order both be able post dashboard image(s) via our endpoints and also allow our end-users submit their own endpoints.\\n\\n\\nAs mentioned above the best solution here seems likely either writing some sort of plugin or perhaps even building something ourselves.\\n\\n\\nIt should however be noted that there are two problems with this approach:\\n\\n1. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n2\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n3\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n4\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n5\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n6\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n7\\. It appears likely that there may come a time when many people may wish to submit endpoints themselves rather than having us do it.\\n8\\. It appears likely that there may come a time when many people may wish t"}
{"File Name":"konduit-serving\/0010-Async_Pipelines.md","Context":"## Context\\nIn some machine learning deployments, we need to perform inference periodically, regardless of whether anyone is actively\\nquerying the pipeline from a REST\/GRPC etc endpoint.\\nExamples 1: When processing video, we want to perform inference on each frame, or N times per second, etc regardless of\\nwhether someone calls a REST endpoint or not.\\nExample 2: When processing video, we want the number of pipeline executions to be independent of the number of clients\\nquerying the REST endpoint (i.e., we should run at 20 FPS and return the latest output\/prediction available, regardless of\\nwhether there is 1 or 100 clients querying per second).\\nExample 3: anomaly detection use case without any sort of REST\/GRPC\/MQTT etc server. Imagine we want to perform\\ninference every second, and (conditionally, if a fault is detected) execute a \"HTTP post\" pipeline step to report the fault.\\nExample 4: \"I want to perform inference at most 5 times per second. If no requests occur, we shouldn't perform any\\ninference. If more than 5 requests per second occur, we want to return the last cached result instead\"\\nExample 5: \"periodic push based\" pipeline execution (microservices style). Suppose we want to perform inference on a camera\\nfeed at 5 FPS and for every frame post the prediction (for example, predicted class or detected objects) to a HTTP endpoint,\\na Kafka queue, an MQTT endoint, or simply writing to a file (where these are implemented as pipeline steps).\\nThese use cases aren't yet supported in Konduit Serving.\\n","Decision":"We introduce an additional Pipeline type, `AsyncPipeline`. It implements the `Pipeline` interface (same as `SequencePipeline`\\nand `GraphPipeline`) so from an API and execution point of view it is the same as the other Pipeline types.\\nThis AsyncPipeline is a Decorator\/Wrapper pattern - i.e.,\\n```java\\nPipeline p = SequencePipeline.builder()... .build();\\nPipeline asyncPipeline = new AsyncPipeline(p, Trigger);\\n```\\nThe AsyncPipeline does two things:\\n* Performs execution of the underlying Pipeline based on some sort of trigger\\n* (Usually) stores the last output of the underlying pipeline, and returns it when query() is called\\nConsider an AsyncPipeline set to perform inference of the underlying pipeline once per second. If we query the AsyncPipeline\\n100 times per second (for example, by 100 difference users all querying the same REST endpoint), we get the same result\\nreturned 100 times, not 100 independent (potentially redundant) execution of the underlying pipeline.\\nThe \"trigger\" for the is configurable. It allows different ways of performing inference on the underlying model.\\nThe `Trigger` interface would have the following API:\\n```text\\nquery(Data) : Data                       - Called when the AsyncPipelineExecutor.exec(...) is called. Returns either a\\ncached Data instance, or optionally blocks for perform a new inference.\\nsetCallback(Function<Data,Data>) : void  - The function provided here is used by the Trigger to perform execution of the\\nunderlying Pipeline whenever the Trigger wants to (with the provided Data),\\nirrespective of whether there is a query() call or not\\n```\\nThe idea is the Trigger would call the function whenever it wants inference to be performed, whether or not the external\\nPipeline\/PipelineExecutor has been called or not (i.e., irrespective of whether query(Data) is called or not).\\nNote that the Trigger instances should be thread safe.\\nBuilt-in implementations would initially include:\\n* `SimpleLoopTrigger`: performs inference in a loop as fast as possible, unless an optional configuration `frequencyMs`\\noption is set (in which case, it calls the underlying pipeline every `frequencyMs` milliseconds).\\n* `TimeLoopTrigger`: calls every N `TimeUnit`s, with some offset. For example, \"Every hour, at the start of the hour\", or\\n\"3 hours past the start of the day, every day\", etc\\n* `CachingTrigger`: performs inference \"at most every N milliseconds\". For example, if we say \"at most once per 1000ms\",\\nand we get a query(Data) call at T=0, we block and call the `Function<Data,Data>`. For all subsequent queries up to T=1000ms,\\nwe return the cached value from the T=0 call. The next call immediately after T=1000ms results in another blocking call\\nand an update of the cached value (until T=2000ms, and so on).\\n### JSON Format\\nThe JSON format for SequencePipeline and GraphPipeline is something that users are supposed to be able to understand, edit\\nand potentially even write from scratch if they so desire. It is also (with only a few exceptions) programming language\\nindependent.\\nThe SequencePipeline vs. GraphPipeline is differentiated by the form of the \"steps\" field: if it's a list, it's a SequencePipeline;\\nif it's an object\/map, it's a GraphPipeline.\\nWe can do a simple extension to this idea: Again noting that the AsyncPipeline has a decorator pattern, we either have a\\nAsyncPipeline(SequencePipeline) or a AsyncPipeline(GraphPipeline).\\nIn either case, we can simply add a new field  - `@AsyncTrigger`, and otherwise leave the existing JSON format unchanged.\\ni.e., the presence of this field means that we have a AsyncPipeline decorator.\\n","tokens":343,"id":909,"Predictions":"## Decision\\nThe decision of the decision maker is to return a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nThe decision is to return an ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## Decision\\nThe decision is to return a ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## Decision\\nThe decision is to return an ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## Decision\\nThe decision is to return an ## Decision corresponding to the ## Context provided by the User.\\n## Decision\\n## Decision\\nThe decision is to return an ## Decision corresponding to the ## Context provided by the User.\\n## Error\\n## Error\\nThis error indicates that we could not find a solution for this problem.\\n\\n"}
{"File Name":"konduit-serving\/0007-Point_Data_Type.md","Context":"## Context\\nThere are multiple ways one can represent a point with the existing data types. However, all of those ways can be classified as a workaround. A bounding box with zero width and height, a list of doubles, or an NDArray all don't communicate the intent of their contents well.\\nBecause communicating intent is important when building maintainable systems, a Point data type was suggested.\\n","Decision":"We create a Point datatype that is explicitly meant to be used for point based data. The Point datatype can represent points with any number of dimensions. It is **not** limited to just 2 or 3 dimensional points.\\nIn order to make conversion between bounding boxes and points easier, points like bounding boxes will also provide optional \"label\" and \"probability\" fields.\\nTo access a specific dimension of a point, a direct `.get(dimension)` method is provided. Because we anticipate that 2 and 3 dimensional points are going to be used very often, the typical x, y, z based notation will also be allowed trough the use of `.x()`, `.y()` and `.z()` methods which internally call `.get(0)`, `.get(1)` or `.get(2)` respectively.\\nThe value of a point will *usually* fall between 0 and 1, i.e. will be a relative measure. This is especially useful when used with other data types like image. However, it is not required to be within this range meaning absolute values are also allowed - interpreting the meaning of a point value is up to the user.\\nThe point data type is going to be implemented with an interface \/ implementation split, however only a single implementation that can take n-dimensional points will be used. If this should ever become a problem, we should be able to provide specialized implementations without breaking existing code.\\n","tokens":80,"id":910,"Predictions":"## [0] \"The Point data type is the most suitable for communicating intent.\"\\n## [1] \"The Point data type is the most suitable for communicating intent.\"\\n## [2] \"The Point data type is the most suitable for communicating intent.\"\\n## [3] \"The Point data type is the most suitable for communicating intent.\"\\n## [4] \"The Point data type is the most suitable for communicating intent.\"\\n## [5] \"The Point data type is the most suitable for communicating intent.\"\\n## [6] \"The Point data type is the most suitable for communicating intent.\"\\n\\n\\end{verbatim}\\n\\n\\subsection{Using a point}\\n\\label{sec:using-a-point}\\n\\nIf you want to use a point, then you can use it as a \\texttt{Point} like this:\\n\\begin{verbatim}\\nvar point = new Point( 0, 0 );\\nvar point2 = new Point( 5, 5 );\\nvar point3 = new Point( 10, 10 );\\nvar point4 = new Point( 15, 15 );\\n\\n\/\/ The following will create a bounding box around these points\\n\/\/ and draw it on screen.\\nvar box = new Box(point);\\nbox.drawToScreen();\\n\\end{verbatim}\\n\\noindent\\nYou can also use this \\texttt{Point} to get an array of coordinates:\\n\\begin{verbatim}\\nvar points = new Points[3];\\npoints[0].x = 5;\\npoints[0].y = 10;\\npoints[1].x = 15;\\npoints[1].y = 20;\\n\\n\/\/ The following will draw a box around each point\\n\/\/ and add it to an array of coordinates.\\nbox.drawToScreen(points);\\n\\end{verbatim}\\n\\noindent\\nYou can also use this \\texttt{Point} in other ways. For example, you can use it to get its bounding box:\\n\\begin{verbatim}\\nvar box = point.getBBox();\\nbox.x += .001;\\nbox.y += .001;\\nbox.z += .001;\\n\\n\/\/ The following will draw a bounding box around all three points\\n\/\/ and add them to an array of coordinates.\\nbox.drawToScreen(points);\\n\\end{verbatim}\\n\\n% \\subsection{\\label{ssec:point-to-point}Point-to-Point}\\n\\n% This method draws a bounding box around each of these points and adds them to an array of coordinates. You can use this array to get its coordinate values.\\n\\n% In addition, you can also use this method to draw a bounding box around all three points and add them to an array of coordinates.\\n\\n% You can use this method in other ways as well. For example, you can use it to get its coordinate values.\\n\\n% In addition, you can also use this method to draw a bounding box around all three points and add them to an array of coordinates.\\n\\n% You can also access the coordinate values using other methods. For example, you could access its x and y properties like so:\\n\\n% In addition, you could access its x and y properties like so:\\n\\n%% \\section{\\label{ssec:point-to-point}Points-to-Point}\\n\\n%% This method draws a bounding box around each of these points and adds them to an array of coordinates. You can use this array to get its coordinate values.\\n\\n%% In addition, you can also used this method in other ways as well. For example, you could access its x and y properties like so:\\n\\n%% \\section{\\label{ssec:point-to-point}Points-to-Point}\\n\\n%% This method draws a bounding box around each of these points and adds them to an array of coordinates. You can use this array t"}
{"File Name":"Extended\/adr-0001-geometry-shaders.md","Context":"## Context and Problem Statement\\nShould `Extended` support geometry shaders?\\n## Decision Drivers <!-- optional -->\\n* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\n","Decision":"* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\nNone taken yet.\\n","tokens":160,"id":911,"Predictions":"## Decision Drivers\\n## Decision Drivers\\n## Decision Drivers\\n## Decision Drivers\\n# ## Decision Drivers\\n\\n# ## Decision Drivers\\n\\n# ## Decision Drivers\\n\\n# ## Decision Drivers\\n\\n# ## Decision Drivers\\n\\n## Summary: \\n\\nShould `Extended` support geometry shaders?\\n\\n--- End of log message ---\\n\\n### 0.1.2 (2016-08-31)\\n### 0.1.3 (2016-09-01)\\n### 0.1.4 (2016-09-02)\\n### 0.1.5 (2016-09-06)\\n### 0.1.6 (2016-09-07)\\n### 0.1.7 (2016-09-08)\\n\\n### 0.2 (2017-01-14)\\n### 0.2a (2017-01-14)\\n\\n#### **Fixes**\\n\\n#### * [microsoft\/quantum-dot] - [fix] [Bug: #16078] - [Release: qdot] - [Release: v4]\\n    * [microsoft\/quantum-dot] - [fix] [Bug: #16078] - [Release: qdot]\\n    * [microsoft\/quantum-dot] - [fix] [Bug: #16078, #16079, #16100, #16113, #16125, #16126, #16128, #16129, #16219]\\n    * [microsoft\/quantum-dot] - [fix] [#14055][Fix for the issue where the color of the screen will appear distorted when using the QDot renderer in Windows Mixed Reality with a desktop resolution of 1920x1080 or higher]\\n* [#14055][Fix for the issue where the color of the screen will appear distorted when using the QDot renderer in Windows Mixed Reality with a desktop resolution of 1920x1080 or higher]\\n* [#14153][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14153][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14154][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14154][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14156][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14156][Fix for issue where you can't use QDot to render on Windows Mixed Reality with a resolution greater than 1920x1080]\\n* [#14263][Issue where if you have multiple monitors that are not connected and do not have overlapping resolutions then they will be rendered at different resolutions](https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263)\\n* Issue where if you have multiple monitors that are not connected and do not have overlapping resolutions then they will be rendered at different resolutions\\n**[https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263](https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263)**\\n\\n*\\[https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263](https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263)\\n**[https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263](https:\/\/github.com\/Microsoft\/MixedReality\/issues\/14263)**\\n\\n    * Fix window scale issues when using mixed reality in windows virtual desktops.\\n* Fix window scale issues when using mixed reality in windows virtual desktops.\\nThe application should now scale properly when running in mixed reality.\\nThe application should now scale properly when running in mixed reality.\\nThe application should now scale properly when running in mixed reality.\\nThe application should now scale properly when running in mixed reality.\\nThe application should now scale properly when running in mixed reality.\\nThe application should now scale properly when running in mixed reality.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\n    * Fix bug that caused an infinite loop to occur during startup.\\[![Build Status](https:\/\/travis-ci.org\/microsoft\/mixedreality.svg?branch=master)](https:\/\/travis-ci.org\/microsoft\/mixedreality?branch=master)\\[![Coverage Report](http:\/\/coveralls.io\/repos\/microsoft\/mixedreality\/blob\/cf8d81e3c4c9c3b5d4e5e9a4754a9c7b9cead2a3\/src\/mixedreality\/virtualdesktoprenderer.cs#L16)](http:\/\/coveralls.io\/github\/microsoft\/mixedreality?branch=master)Coverage check results:\\[![Coverage Report](http:\/\/coveralls.io\/github\/microsoft\/mixedreality?branch=master)](http:\/\/coveralls.io\/github\/microsoft\/mixedreality?branch=master)Coverage report:\\[![Build Status](http:\/\/travis-ci.org\/microsoft\/mixedreality.svg?branch=master)](http:\/\/travis-ci.org\/microsoft\/mixedreality?branch=master)![Coverage Report](http:\/\/coveralls.io\/github\/msrfg\/dotnet\/tree\/master\/src\/msrfg\/dotnet\/src\/msrfg\/dotnet\/virtualdesktoprenderer.cs)![Coverage Report](http:\/\/coveralls.io\/github\/msrfg\/dotnet\/tree\/master\/src\/msrfg\/dotnet\/src\/msrfg\/dotnet\/virtualdesktoprenderer.cs)![Coverage Report](http:\/\/coveralls.io\/github\/msrfg\/dotnet\/tree\/master\/src\/msrfg\/dotnet\/src\/msrfg\/dotnet\/virtualdesktoprenderer.cs)![Coverage Report](http:\/\/coveralls.io\/github\/msrfg\/dotnet\/tree\/master\/src\/msrfg\/dotnet\/src\/msrfg\/dotnet\/virtualdesktoprenderer.cs)![Coverage Report](http:\/"}
{"File Name":"Ais.Net\/0002-missing-fragment-handling.md","Context":"## Context\\nSome AIS data sources include NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\\n","Decision":"We default to abandoning fragmented messages if 8 other messages have arrived since the first fragment. This is configurable, but we chose 8 because there's a limit of 9 fragmented messages in progress at any one time if you use the AIVDM-level group identifiers. It is configurable because systems that provide tag block group identifiers often use larger group IDs (e.g., 4 digits are common), making it possible for more to be in flight at once. But in practice, most message fragments are adjacent, so a large window is usaully unnecessary.\\n","tokens":59,"id":912,"Predictions":"## The AIS data source is not correct. A good AIS data source is a fixed NMEA sentence. It is not modified by the GPS receiver.\\n\\n## Decision\\n## The AIS data source contains NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\\n\\n## Decision\\n## The AIS data source contains NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\\n\\n\\n# A decision will be made for each of the following situations.\\n\\end{lstlisting}\\n\\n\\subsection{Example: Interference}\\n\\label{sec:interference}\\n\\nLet's take a look at an example scenario where interference occurs between two AIS data sources:\\n\\begin{lstlisting}[language=Python]\\ndef getInterference():\\n    # Get the information about the interference from both sources.\\n    # Let's assume that the two sources share an AIS data stream.\\n    # We want to know if there is any interference between them.\\n    # If there is any, we need to stop sending messages.\\n\\n    # First, get all messages from both sources.\\n    msg = getMessagesFromBothSources()\\n\\n    # Now filter out all messages that have no other message from the same sender.\\n    filteredMessages = filter(msg, \"messageID != 0x04\")\\n\\n    # Now, we need to filter out all messages that have no other message from the same sender.\\n    filteredMessages = filter(msg, \"messageID != 0x04\")\\n\\n    return filteredMessages\\n\\n# Now, make sure we don't send any more messages than necessary.\\ngetInterference()\\n\\end{lstlisting}\\n\\nThe code above filters out all messages that have no other message from the same sender:\\n\\begin{lstlisting}[language=Python]\\nfilteredMessages = filter(msg, \"messageID != 0x04\")\\n\\end{lstlisting}\\n\\nThe code above filters out all messages that have no other message from the same sender:\\n\\begin{lstlisting}[language=Python]\\nfilteredMessages = filter(msg, \"messageID != 0x04\")\\n\\end{lstlisting}\\n\\nThe code above filters out all messages that have no other message from the same sender:\\n\\begin{lstlisting}[language=Python]\\nfilteredMessages = filter(msg, \"messageID != 0x04\")\\n\\end{lstlisting}\\n\\nThe code above filters out all messages that have no other message from the same sender:\\n\\begin{lstlisting}[language=Python]\\nfilteredMessages = filter(msg, \"messageID != 0x04\")\\n\\end{lstlisting}\\n\\nThe code above filters out all messages that have no other message from the same sender:\\n\\begin{lstlisting}[language=Python]\\nfilteredMessages = filter(msg, \"messageID != 0x04\")\\n\\end{lstlisting}\\n\\noindent\\nWhen you run this program you will see two outputs:\\n\\n1) The first output shows you which AIS data sources send more than one message per second and which ones send only one message per second:\\n\\n2) The second output shows you which AIS data sources send more than one message per second and which ones send only one message per second:\\n\\nYou can see here that there is actually only one interfering source in this case: \\texttt{\"NMEA\"} - but it has sent an additional two messages per second than \\texttt{\"NMEA\"}.\\n\\nThis scenario can be summarized as follows:\\n\\nIf both AIS data sources send more than one message per second and if they do not share any information with each other (e.g., if they do not share an ID), then neither should interfere with each other.\\n\\nIf both AIS data sources send only one message per second and if they share some information with each other (e.g., if they share an ID), then neither should interfere with each other.\\n\\nIf both AIS data sources send only one message per second and if they do share some information with each other (e.g., if they share an ID), then neither should interfere with each other.\\n\\nIn summary: If both sources send more than one message per second and if they do not share any information with each other (e.g., if they do not share an ID), then neither should interfere with each other.\\n\\nIf both sources send only one message per second and if they do share some information with each other (e.g., if they share an ID), then neither should interfere with each other.\\n\\nIn summary: If both sources send only one message per second and if they do share some information with each other (e.g., if they share an ID), then neither should interfere with each other.\\n\\n\\n% In case of interference between two sensors it is important to know whether these sensors are communicating over different channels or communicating over a single channel but using different identifiers. \\n% For example in case of interference between two sensors it is important to know whether these sensors are communicating over different channels or communicating over a single channel but using different identifiers. \\n% For example in case of interference between two sensors it is important to know whether these sensors are communicating over different channels or communicating over a single channel but using different identifiers.\\n\\n\\n"}
{"File Name":"Ais.Net\/0001-error-handling-strategy.md","Context":"## Context\\nNMEA messages containing AIS data are sometimes malformed. The messages are often transferred from transponder devices to computer equipment over a serial port, which have limited error detection and correction capabilities, making it possible for messages to lose characters or contain garbage characters.\\n","Decision":"We deal with malformed NMEA messages at two levels.\\nThe `NmeaLineParser` detects failures to conform to the basic structure required of an NMEA message. It reports these errors by throwing an `ArgumentException` with a text message describing the way in which the message structure does not match the specification. For example, if the normal `!` start character is missing, the error message is `Invalid data. Expected '!' at sentence start`. In cases where the message is well formed but with features that Ais.Net cannot currently parse, we throw a `NotSupportedException` exception.\\nCode that processes large numbers of messages will typically not construct the `NmeaLineParser` itself, and instead relies on the high-throughput methods offered by `NmeaStreamParser`. Code working this way passes in an implementation of either `INmeaLineStreamProcessor` or `INmeaAisMessageStreamProcessor` depending on whether it wants to process each line in the file directly, or it wants `NmeaStreamParser` to handle the task of reassembling AIS payloads that have been fragmented across multiple NMEA message lines. Both of these interfaces require an `OnError` method that looks like this:\\n```csharp\\nvoid OnError(in ReadOnlySpan<byte> line, Exception error, int lineNumber);\\n```\\nWhen the `NmeaLineParser` constructed by the `NmeaStreamParser` reports an error in the manner described above, the exception it throws will be passed to this `OnError` method, along with a span providing access to the problematic line's full content, and the line number within the stream at which the error was detected.\\nThere are also errors that can occur with fragmented messages. While each individual fragment might be well-formed, they might be wrong in combination. For example, it is not permitted for a fragmented message to have padding at the end of the first fragment. In these cases, which will be detected when working at the message level, we report errors through the same mechanism.\\n","tokens":53,"id":913,"Predictions":"## The Decision is: \"The AIS data can be read from the transponder device, but the data can not be reliably transferred to the computer equipment.\".\\n\\n## End Decision\\n\\n\\n\\end{Verbatim}\\n\\n\\begin{Verbatim}[commandchars=\\\\\\{\\},fontsize=\\footnotesize]\\n\\textbf{1}: The AIS data can be read from the transponder device, but the data can not be reliably transferred to the computer equipment.\\n\\n\\noindent \\textbf{2}: The AIS data can be read from the transponder device, but the data can not be reliably transferred to the computer equipment.\\n\\n\\noindent \\textbf{3}: The AIS data cannot be read from the transponder device and cannot be reliably transferred to the computer equipment. \\n\\n\\noindent \\textbf{4}: The AIS data can not be read from the transponder device and cannot be reliably transferred to the computer equipment.\\n\\end{Verbatim}\\n\\n\\subsection*{Using a 3rd party software}\\n\\label{sec:using-a-3rd-party-software}\\n\\nUse of a 3rd party software is possible in cases where it is not possible to use \\glspl{lts} provided by NMEA. In such cases, a simple \\glspl{lts} application may be used. This may include using a software that is available online (e.g., Google Earth) or downloaded as an executable file that may or may not require user input. It must also have access to a NMEA message stream (e.g., COM port) and support for NMEA message types. It must also provide an interface for transferring an NMEA message stream over a serial port.\\n\\nThe following are examples of 3rd party software that may be used:\\n\\n\\begin{itemize}\\n    \\item Comodo MarineNet:\\n    https:\/\/www.comodo.com\/en\/products\/marinemanager\/\\n    \\item NMEA4ALL:\\n    https:\/\/www.nmea4all.com\/\\n    \\item Adafruit IO:\\n    https:\/\/www.adafruit.com\/product\/1290\\n    \\item NMEA Data Stream Generator:\\n    https:\/\/www.nmea.org\/datastreamgenerator.html\\n\\end{itemize}\\n\\nIt should also include a method of reporting errors (e.g., error codes) that indicate whether or not an error occurred during communication with a transponder. This method should also include a method of reporting when communication fails due to hardware failure or other problems.\\n\\nIt should also include some method of reporting when communication fails due to software failure or other problems.\\n\\nIt should also include some method of reporting when communication fails due to hardware failure or other problems.\\n\\nIt should also include some method of reporting when communication fails due to software failure or other problems.\\n"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"Predictions":"## The SSDA903 dataset is a stream of events that a child has experienced through the\\nCare System. Each event can be regarded as a series of touchpoints that the\\nchild has had with various other actors in the system, such as IFAs and Foster Families.\\n\\n## Context\\n## The SSDA903 dataset is a stream of events that a child has experienced through the\\nCare System. Each event can be regarded as a series of touchpoints that the\\nchild has had with various other actors in the system, such as IFAs and Foster Families.\\n\\n## Decision\\n## The SSDA903 dataset is a stream of events that a child has experienced through the\\nCare System. Each event can be regarded as a series of touchpoints that the\\nchild has had with various other actors in the system, such as IFAs and Foster Families.\\n\\n## Context\\n## The SSDA903 dataset is a stream of events that a child has experienced through the\\nCare System. Each event can be regarded as a series of touchpoints that the\\nchild has had with various other actors in the system, such as IFAs and Foster Families.\\n\\n## Decision\\n## The SSDA903 dataset is a stream of events that a child has experienced through the\\nCare System. Each event can be regarded as a series of touchpoints that the\\nchild has had with various other actors in the system, such as IFAs and Foster Families.\\n\\n\\n"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"Predictions":"## The decision is to provide a decision record for this context.\\n## Decision Record\\nThe decision record contains the following information:\\n## \\begin{itemize}\\n##   \\item \\textbf{Decision:} The Decision\\n##   \\item \\textbf{Context:} The Context\\n##   \\item \\textbf{User:} The User\\n##   \\item \\textbf{Decision Record:} The Decision Record\\n##   \\item \\textbf{Decision Type:} The Decision Type\\n%%\\item {The Decision type is to provide a decision record for this context.}\\n%%\\end{itemize}\\n"}
{"File Name":"csc-swr-architecture\/001-Use-GitHub-Workflow-for-ADRs.html.md.erb","Context":"## Context\\nArchitectural Decisions are not made in isolation and need to be discussed and agreed amongst the Service Team.\\n","Decision":"Utilise [GitHub Workflow](https:\/\/guides.github.com\/introduction\/flow\/) to manage the acceptance, or not, of ADRs by the Service Team. The Pull Request review process, prior to merging into `main`, provides suitable mechanisms for tracking comments, amendments, approvals and so on.\\n","tokens":26,"id":916,"Predictions":"## Decision: \\n\\nThe Architectural Decision Record is under review and will be updated as soon as the decision has been made.\\n\\n\\n\\n\\textbf{Notes:}\\n\\begin{itemize}\\n\\item The \\texttt{Architectural Decision Record} is a record of architectural decisions made by the Service Team.\\n\\item The \\texttt{Architectural Decisions} are not made in isolation and need to be discussed and agreed amongst the Service Team.\\n\\end{itemize}\\n\\n\\subsection*{Service Team}\\n\\nThe Service Team is responsible for providing the Service to the Customer. The Service Team consists of one or more members, each of which has an individual responsibility for a specific area of work. The Service Team is responsible for:\\n\\n\\begin{itemize}\\n\\item Managing the Project\\n\\item Providing Support\\n\\end{itemize}\\n\\nEach member of the Service Team is also responsible for maintaining their own area of responsibility.\\n\\n\\subsection*{Service Manager}\\n\\nThe \\texttt{Service Manager} is responsible for managing all aspects of the Service, including contacting customers, managing resources, and ensuring that quality standards are met. The \\texttt{Service Manager} is also responsible for ensuring that all service requests are dealt with in a timely manner.\\n\\nThe responsibilities of the \\texttt{Service Manager} are:\\n\\n\\begin{itemize}\\n\\item Managing all aspects of the Project\\n\\item Managing all aspects of Customer Support\\n\\end{itemize}\\n\\nThe responsibilities of each member of the Service Team are:\\n\\n\\begin{itemize}\\n\\item Providing Support\\n\\end{itemize}\\n\\nEach member of the Service Team is also responsible for maintaining their own area of responsibility.\\n\\n%For details on how to contact each member, see Section~2.4.\\n\\n%This section does not apply to Cloud Projects.\\n"}
{"File Name":"csc-swr-architecture\/005-Monolith-First.html.md.erb","Context":"## Context\\nThe CSC Placement service is taking an [Evolutionary Architecture] approach.\\n","Decision":"Since we are in a highly uncertain phase of the Service, and we want to optimise around developer productivity and feature prototyping, the adoption of a [Monolith First] architecture is considered best.\\nAlthough Martin Fowler uses this as a precursor for a [Microservice Architecture], it is not definitely regarded as a transition to Microservices here, but rather a reasonable first step before deciding future architectural quanta.\\n","tokens":19,"id":917,"Predictions":"## 1. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 2. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 3. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 4. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 5. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 6. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## 7. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n## Decision\\n## 8. The CSC Placement service is taking an [Evolutionary Architecture] approach.\\n\\end{Verbatim}\\n\\n\\subsection{PUSH}\\n\\label{sec:push}\\n\\n\\begin{Verbatim}[commandchars=\\\\\\{\\}]\\n% push\\n%supports the push command\\n% push\\n%supports the push command\\nsafest, one of the safest options for pushing data from one application to another, and it's very slow by default. It's also not recommended for use in a production environment.\\n\\n\\end{Verbatim}\\n\\n\\subsection{POST}\\n\\label{sec:post}\\n\\n\\begin{Verbatim}[commandchars=\\\\\\{\\}]\\n% post\\n%supports the post command\\n% post\\n%supports the post command\\nThis method supports sending data to another application using HTTP POST requests.\\n\\nIn this case, you need to ensure that the application receiving the response has a valid \\verb+HTTP+ server running on port \\verb+<port>\/+ (with or without SSL) and that it has access to your \\verb+application+.\\n\\nPost requests are sent over TCP\/IP network connections, so your application must be able to handle incoming connections from other applications using TCP\/IP on their own network connection.\\n\\nThe following table shows which HTTP methods are supported by each application:\\n\\n\\begin{table}[H]\\n    \\centering\\n    \\caption{Supported HTTP methods}\\n    \\label{tab:methods}\\n    \\begin{tabular}{|c|c|c|}\\n        \\hline\\n        Method & Supported values & Supported protocols \\\\ \\hline\\n        GET & \/ & http \\\\ \\n        HEAD & \/& http\\\\ \\n        POST & \/& http\\\\ \\n        PUT & \/& http\\\\ \\n        DELETE & \/& http\\\\ \\n        OPTIONS & \/& http\\\\ \\n        TRACE & \/& http\\\\ \\n        CONNECT & \/& http\\\\ \\n        OPTIONS\\_HEAD & \/& https \\\\ \\n        OPTIONS\\_OPTIONS\\_HEAD & \/& https \\\\ \\n         \\hline            \\n    \\end{tabular}\\n\\end{table}\\n\\nYou can use POST requests to send data between applications with different protocols or modes of communication (for example, a browser and a server).\\n\\nThe following table shows which HTTP methods are supported by each application:\\n\\n\\begin{table}[H]\\n    \\centering\\n    \\caption{Supported HTTP methods}\\n    \\label{tab:methods}\\n    \\begin{tabular}{|c|c|c|}\\n        \\hline\\n         Method & Supported values & Supported protocols \\\\ \\hline    \\n         GET & \/   &$http$\\\\  \\n         HEAD &$http$   &$http$\\\\  \\n         POST &$http$   &$http$ \\\\\\n         PUT &$http$   &$http$ \\\\\\n         DELETE &$http$   &$http$ \\\\\\n         OPTIONS &$https$   &$https$ \\\\\\n         TRACE &$https$   &$https$ \\\\\\n         CONNECT     $|$ $|$ $|$ $|$ $|$ $|$ $|$ $|$ $|$ $|$ $|$ $|$ $\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication$\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow$someotherapplication $\\rightarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\leftarrow\\somesomelayer$\\n    \t\\\\\\hline            \\n    \\end{tabular}\\n\\end{table}\\n\\nThe following table shows which HTTP methods are supported by each application:\\n\\n\\begin{table}[H]\\n    \\centering\\n    \\caption {Supported HTTP methods}\\n    % This table lists all available HTTP methods for each application.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n     % Each method name corresponds to a valid HTTP method listed in Table~\\ref {tab:methods}.\\n    \\n    % Method Name:\\n    \\n       GET | HEAD | POST | PUT | DELETE | OPTIONS | TRACE | CONNECT |\\n       OPTIONS\\_HEAD | OPTIONS\\_OPTIONS\\_HEAD | TRACE | CONNECT |\\n       OPTIONS\\_HEAD | OPTIONS\\_OPTIONS\\_HEAD | TRACE | CONNECT |\\n       OPTIONS\\_HEAD | OPTIONS\\_OPTIONS\\_HEAD |\\n    \\n       GET | HEAD |\\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n       \\n         \\n         \\n         \\n         \\n\\n      GET| HEAD| POST| PUT| DELETE| OPTIONS| TRACE|\\n\\n      GET| HEAD|\\n      \\n      GET| HEAD|\\n      \\n      GET| HEAD|\\n      \\n      GET| HEAD|\\n      \\n      GET| HEAD|\\n      \\n      GET| HEAD|\\n\\n      GET| HEAD|\\n\\n      GET|\\(POST\\)|\\(GET\\)|\\(PUT\\)|\\(DELETE\\)|\\(TRACE\\)|\\(CONNECT)\\(\\)\\(\\)\\(\\)\\(\\n      OPTIONS_HEAD |\\ OPTIONS_OPTIONS_HEAD |\\ OPTIONS_TRACE |\\ OPTIONS_CONNECT |\\ OPTIONS_OPTION\\\\n      S_HEAD |\\ OPTIONS_OPTION_S_OPTIONS_HEAD |\\ OPTIONS_OPTION_S_OPTIONS_OPTIONS\\\\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n\\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n        \\n    \\n        \\n        \\n\\n        \\n\\n        \\n        \\n\\n        \\n\\n        \\n        \\n\\n        \\n\\n        \\n        \\n\\n        \\n        \\n\\n        \\n        \\n\\n        \\n        \\n\\n        \\n        \\n\\n        \\n        \\n\\n        \\n        \\n            \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n        \\n        \\n        \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n  \\n    \\n    \\n    \\n    \\n\\n\\n"}
{"File Name":"csc-swr-architecture\/006-Postcode-Lookup.html.md.erb","Context":"## Context\\nPostcode Validation\/Verification, Address & Geocoding searches is a requirement in\\nthe Diary Logging Service and more particularly in the Placement Shortlisting Service.\\nCurrently there is no adopted DfE Architectural pattern for postcode searches, address\\nsearches & Geocoding searches.\\nThere is a Common Components Initiate (CCI) currently in development by DfE where several\\nSaaS offerings are being considered.  Postcode, address & geocoding is part of this initiative,\\nbut this will not be approved and an adopted practise for some time. For further information see\\n[here](https:\/\/dfe-digital.github.io\/architecture\/common-components\/#postcode-lookup).\\nCCI will also include Geospatial Mapping and the following products and service will\\nbe available across the department:\\n* [Azure Maps]\\n* [Google Maps]\\n* [Bing Maps]\\nEach product provides a slightly different set of features and capabilities that\\nfit different use cases.\\n","Decision":"This Alpha is working to build out a simple API postcode service, in the first utilising\\n[Postcodes.io].\\nAddress validation and geocoding is not required during the alpha phase and\\nthis decision will be replaced by a substantive revised decision, including address lookup\\nand geocoding when the detail of the data model is clear, and the project moves to the Beta phase.\\nA summary of the availability statistics for the services are available on this link:\\n* [Ideal Postcodes Status Page]\\n","tokens":205,"id":918,"Predictions":"\\begin{itemize}\\n    \\item \\textbf{Decision:} Postcode Validation\/Verification, Address & Geocoding searches is a requirement in the Diary Logging Service and more particularly in the Placement Shortlisting Service. Currently there is no adopted DfE Architectural pattern for postcode searches, address searches & Geocoding searches. There is a Common Components Initiate (CCI) currently in development by DfE where several SaaS offerings are being considered. Postcode, address & geocoding is part of this initiative, but this will not be approved and an adopted practise for some time. For further information see here.\\n    \\item \\textbf{Decision:} Currently there is no adopted DfE Architectural pattern for postcode searches, address searches & Geocoding searches. There is a Common Components Initiate (CCI) currently in development by DfE where several SaaS offerings are being considered. Postcode, address & geocoding is part of this initiative, but this will not be approved and an adopted practise for some time. For further information see here.\\n    \\item \\textbf{Decision:} Geospatial Mapping and the following products and service will be available across the department:\\n    \\begin{itemize}\\n        \\item Azure Maps\\n        \\item Google Maps\\n        \\item Bing Maps\\n    \\end{itemize}\\n\\end{itemize}\\n\\n\\end{document}"}
{"File Name":"csc-swr-architecture\/002-Progamming-Language-and-Framework.html.md.erb","Context":"## Context\\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\\n","Decision":"The decision is to align with the central Digital preference for Ruby and Rails.\\nKey reasons for choosing Ruby and Rails are:-\\n* Alignment with a large portion of the Government Digital Service sites and services, including cross service development skills within DfE. [Here](https:\/\/dfe-digital.github.io\/technical-guidance\/guides\/default-technology-stack\/#the-ruby-stack) is the DfE Technical Guidance around this.\\n* Optimisation around developer productivity and lower costs.\\n* It was created specifically for building web applications.\\n* Numerous code libraries (Gems) which provide free, open-source code for developers to fulfil specific needs.\\n* A very strong community with a great sense of collaboration and support.\\n* It is robust and high-quality. All Ruby developers are encouraged to follow a set of community-supported coding conventions and standards, which in turn helps produce better code, and thereby high quality digital products and services.\\n* It\u2019s conventions make it much easier for developers to move between Rails services, which will tend to use the same structure and coding practices.\\n* A strong focus on testing, with many good testing frameworks.\\n* It emphasises RESTful application design. REST (REpresentational State Transfer) is a style of software architecture based around a client-server relationship. It encourages a logical structure which can be exposed as an API (Application Programing Interface) which can be used to expose platform functionality (as desired for this service).\\n* Is an intuitive and easy-to-read language. It uses fewer characters than other languages allowing for intuitive, natural language, coding rather than having to use lots of special characters. The flip side is that reading code is made simpler, allowing for quick on-boarding to a service and peer reviewing for example.\\n* Enables rapid development. Due to many of the above points, such as readily available code libraries specifically designed for websites and apps, an active community which promotes high-quality development, and code that is simple to read and write then using Rails can achieve rapid development with high-quality results.\\nReasons why you might not choose Ruby and Rails are:-\\n* Not all hosting can support Rails - but since we are choosing where to host our service we can ensure this isn\u2019t an issue.\\n* Java and PHP are more widely used and there are more developers in these languages. However there is still a strong market in Ruby developers and the very extensive open-source libraries available mean a lot can be achieved by a few developers.\\n* Performance and Scalability. Rails is not as fast as Java or C, but it is fast enough for this service and will horizontally scale out when needed. There are plenty of high-profile organisations relying on Rails, examples include AirBnB, Yellow Pages, Groupon, Channel 5, GitHub, Shopify and Gov.uk.\\n","tokens":164,"id":919,"Predictions":"\\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\\n\\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\\n\\nThe decision was made during Alpha.\\n\\n\\end{document}\\n"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"Predictions":"## The best solution is to build a single application which can be used to create, update and manage placements for children in the Alphabetic system. This will allow the system to be flexible and adaptable to the needs of the service.\\n\\n## Evaluation\\n## The system will need to be able to support multiple types of placement (e.g. short term, long term residential care, day care, etc.) and multiple types of Placement Manager (e.g. parents, schools, social workers etc.). The system needs to be able to support all of these different types of placement in a single application which can be deployed on multiple platforms.\\n\\n\\n\\n## Configuration\\n## Configuration will need to be specified for each of the Placement Manager types. These settings will determine how the Placement Manager interacts with the system and what data it can access.\\n\\n\\n\\n## Environment\\n## Environment will need to be configured so that each placement manager can use a different database and different data sources. This means that each placement manager must have its own database instance and must also have permission set up so that it can access the data sources.\\n\\n\\n\\n## Environment Variables\\n## Environment variables need to be set up so that each placement manager has access to its own environment variables which are specific for that placement manager.\\n\\n\\n\\n## Features\\n## Features defined for each Placement Manager type should include how they interact with other features (e.g. how they communicate with other managers) as well as what data they can access (e.g. what types of information they can retrieve).\\n\\n\\n\\n## System Tests\\n## System tests should be written for every feature defined for each Placement Manager type.\\n\\n\\n\\n\\n\\subsection{Storage}\\n\\subsubsection{Storage Requirements}\\nThe Storage requirements are as follows:\\newline\\n\\n\\begin{itemize}\\n    \\item \\textbf{File:} A file is defined as a collection of data in a manner that allows it to easily be retrieved by another application.\\n    \\item \\textbf{Database:} A database is defined as an object with fields.\\n    \\item \\textbf{Table:} A table is defined as a collection of rows.\\n    \\item \\textbf{Column:} A column is defined as a field.\\n\\end{itemize}\\n\\n\\subsubsection{Storage Implementation}\\nThe Storage implementation is as follows:\\newline\\n\\n\\begin{itemize}\\n    \\item File storage will use either an existing file system or a local file system that is accessible from within the application.\\n    \\item Database storage will use either an existing database or a local database that is accessible from within the application.\\n    \\item Table storage will use either an existing table or a local table that is accessible from within the application.\\n    \\item Column storage will use either an existing column or a local column that is accessible from within the application.\\n\\end{itemize}\\n\\n\\subsubsection{Storage Implementation Details}\\nThe Storage implementation details are as follows:\\newline\\n\\n\\begin{enumerate}\\n    \\item File storage requires no special configuration changes. It simply uses whatever file system or local file system exists on your computer.\\n    \\item Database storage requires you to set up your own database instance on your computer. This involves installing software such as MySQL or PostgreSQL on your computer and then configuring it so it connects to your database instance.\\n    \\item Table storage requires you to set up your own table instance on your computer. This involves installing software such as MySQL or PostgreSQL on your computer and then configuring it so it connects to your table instance.\\n    \\item Column storage requires you to set up your own column instance on your computer. This involves installing software such as MySQL or PostgreSQL on your computer and then configuring it so it connects to your column instance.\\n\\end{enumerate}\\n\\n\\subsubsection*{\\textbf{\\textit{\\textit{\"File System\", \"Database\", \"Table\", \"Column\"}}}}\\nThe file system, database, table and column attributes are all optional.\\n\\n\\subsection*{\\textit{\\textit{\"Environment\"}}}\\n\\nThe environment attribute specifies whether or not this feature should take into account any environmental variables when executing commands.\\n\\nFor example, if you want this feature only take into account environment variables when executing commands, but not when checking if features exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when checking if features exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when executing commands but not when checking if features exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when checking if features exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when executing commands but not when checking if features do not exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when executing commands but not when checking if features do not exist then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when executing commands but do not check for capabilities then this attribute should probably not exist.\\n\\nIf you want this feature take into account environment variables when executing commands but do not check for capabilities then this attribute should probably not exist.\\n\\n\\n"}
{"File Name":"universal-remote-control-skill\/0002-overall-system-architecture.md","Context":"## Context\\nTo put down an architecture for the system as a whole.\\n","Decision":"The system consists of the following components:\\n* Global components:\\n* Alexa Smart Home Skill\\n* AWS Lambda\\n* AWS API Gateway websocket endpoint\\n* Per user-components:\\n* Google Account containing an automatically set-up Google Spreadsheet\\n* Universal Remote Controller (URC) device\\n### Alexa Smart Home Skill\\n* Takes care of activation and hands off control to AWS Lambda\\n* Links to user's Google Account\\n### AWS Lambda\\n* Receives and keeps websocket connection from each URC and sends commands to the URCs\\n* Reads configuration data from user's Google Spreadsheet\\nTODO: this needs to be worked out further, following\\n### Google Account with Spreadsheet\\nThe Spreadsheet contains all necessary configuration data to control the user's specific environment:\\n* URCID\\n* IR\/RF transmission sequences grouped by appliance\\n* mappings of commands to transmission sequences, e.g. \"Turn On TV\" --> specific IR transmission sequence to turn on TV set\\n### Universal Remote Controller\\n* ESP8266 system with IR and RF transmitter\\n* has a baked-in unique URCID (from DS2401)\\n* upon startup establishes websocket connection to AWS API Gateway (passing its URCID), and keeping that connection permanently alive\\n* receives and executes commands that AWS Lambda sends via AWS API Gateway\\n","tokens":16,"id":921,"Predictions":"## A decision is a statement that describes the outcome of a concrete\\n## situation.\\n## \\n## The decision must describe the outcome of the situation. The\\n## consequences of this decision are not described by the decision.\\n## \\n## This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nTo put down an architecture for the system as a whole.\\n\\n## Decision\\n## A decision is a statement that describes the outcome of a concrete\\n## situation.\\n\\end{lstlisting}\\n\\subsection{Example: Software Development}\\n\\label{sec:example-software-development}\\nThe following example shows how to use \\texttt{Decision} in order to create an architectural decision record. The \\texttt{Decision} object is used to store decisions for software development. It can be used in combination with other objects, such as \\texttt{Architecture}.\\n\\begin{lstlisting}[language=bash]\\n# Create an architecture object and add it to our list of objects.\\narch = Architecture()\\narch.addObject(Decision())\\narch.addObject(Decision())\\narch.addObject(Decision())\\narch.addObject(Decision())\\n\\n# Now we want to put down decisions for our software development project.\\n# We define a list of decisions, which we will use later on in our code.\\ndecs = [Decision(\"Architecture\")]\\ndecs.append(Decision())\\n\\n# Now we define our architecture object and add it to our list of objects.\\narch = Architecture(decs)\\narch.addObject(Decision())\\narch.addObject(Decision())\\n\\n# Finally, we can put down some decisions for our software development project.\\n\\nprint(\"Creating an architecture...\")\\nprint(\"Creating some decisions...\")\\nprint(\"Adding 1st decision...\")\\nprint(\"Adding 2nd decision...\")\\nprint(\"Adding 3rd decision...\")\\nprint(\"Adding 4th decision...\")\\nprint(\"Adding 5th decision\")\\n\\end{lstlisting}\\nThe following example shows how to use \\texttt{Architecture} in combination with \\texttt{Decision} in order to create an architectural decision record. The \\texttt{Architecture} object is used in combination with other objects, such as \\texttt{Decision}. The methods described are used from this point on:\\n\\begin{itemize}\\n    \\item \\textbf{\\textit{\\texttt{\\$}}}: To print information about an object, such as its name or its state.\\n    \\item \\textbf{\\textit{\\$}: To print information about all objects in the current namespace (the namespace where this object was created).\\n    \\item \\textbf{\\$}: To print information about all objects in this namespace (the namespace where this object was created).\\n    \\item \\textbf{\\$:}: To print information about all objects stored within this namespace (the namespace where this object was created).\\n    \\item \\textbf{\\$:}: To print information about all objects stored within this namespace (the namespace where this object was created).\\n    % TODO: You could also use SQL to retrieve data from database tables using JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once. However, I am not sure if it would be possible without significant code changes.\\n\\n    % TODO: You could also use JDBC and return it back through Java classes or interfaces. This would be more efficient than using SQL queries since the Java classes would have access to all database tables and their data at once.\\n\\n\\end{itemize}\\n\\nThis example shows how you can create an architectural decision record by combining two different types of objects:\\n\\n\\begin{lstlisting}[language=bash]\\n# Create an architecture object\\narchitecture = Architecture()\\n\\n# Define some decisions for our software development project\\ndecisions = [Decision(\"Architecture\")]\\n\\n# Now we define our architecture object\\narchitecture.addObject(decisions)\\n\\nprint(\"Creating an architecture...\")\\nprint(\"Creating some decisions...\")\\n\\nprint(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\\nfor d in decisions:\\n    print(d)\\n\\end{lstlisting}\\n\\nThe following example shows how you can define your own architectural decisions:\\n\\n\\begin{lstlisting}[language=bash]\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\ndef Decision(name):\\n     pass\\n\\n\\nif __name__ == \"__main__\":\\n   arch = Architecture()\\n   arch.addObject(Decision('Architecture'))\\n   arch.addObject(Decision('Software Development'))\\n   arch.addObject(Decision('Database'))\\n   arch.print()\\n   arch.print()\\n   arch.print()\\n   arch.print()\\n   arch.print()\\n\\n   print(\"\\nCreating an architecture...\")\\n\\n   # Create some decisions for our software development project...\\n   decs = [Statement('Software Development'), Statement('Database')]\\n   decs.append(Decision('Software Development'))\\n\\n   # Now we define our architecture object...\\n   arch.addObject(decisions)\\n\\n   # Finally, we can put down some decisions for our software development project...\\n   print(\"\\nCreating an architecture...\")\\n\\n   # Creating some decisions... \\n   print(\"\\n\\n\\n\\n\")\\nfor d in decs:\\n    print(d)\\n\\end{lstlisting}\\n\\nThis example shows how you can create your own architectural decisions by combining two different types of objects:\\n\\n% TODO: What do you think about these examples? Do they make sense? Is there anything that needs improvement?\\n\\n% In case you want your own custom definition for any type of objects (e.g., databases), then you should implement them yourself (e.g., implementing your own abstract class). Then you should implement methods that allow you to get\/set values when needed (e.g., getters\/setters).\\n\\n% If you need specific functionality, then you should implement them yourself (e.g., implementing your own abstract class). Then you should implement methods that allow you get\/set values when needed (e.g., getters\/setters).\\n\\n% If no implementation exists yet then just write whatever comes into your mind first.\\n\\n% In case there is no implementation yet then just write whatever comes into your mind first.\\n\\n% In case no implementation exists yet then just write whatever comes into your mind first.\\n\\n\\n"}
{"File Name":"universal-remote-control-skill\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":922,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"Predictions":"We will install the AWS image with the latest updates, unless we have a compelling reason not to.\\n\\end{itemize}\\n\\n\\subsubsection{Scheduling}\\n\\label{sec:Schedule}\\n\\nThe schedule of this project is to be completed in 2 weeks. There are two tasks that will be completed within this time frame, which have been assigned a due date of \\texttt{2021-03-15 12:00:00} and \\texttt{2021-03-16 12:00:00}. These two tasks will be completed by the same person who is completing the architectural decision record.\\n\\n\\subsubsection{Time Estimation}\\n\\label{sec:TimeEstimation}\\n\\nThe estimated time for completing this project is \\texttt{2 weeks}. The following table shows the estimated time for each task.\\n\\n\\begin{table}[H]\\n\t\\centering\\n\t\\caption[Estimated Time for each Task]{Estimated Time for each Task}\\n\t\\label{tab:EstimateTime}\\n\t\\begin{tabular}{|c|c|c|}\\n\t\t\\hline\\n\t\tTask & Description & Due Date \\\\ \\hline\\n\t\tRoutine 1 & Install AWS on Server Machine & \\texttt{2021-03-15 12:00:00} \\\\ \\hline\\n\t\tRoutine 2 & Install Apache Kafka on Server Machine & \\texttt{2021-03-16 12:00:00} \\\\ \\hline\\n\t\tRoutine 3 & Install Prometheus on Server Machine & \\texttt{2021-03-15 12:00:00} \\\\ \\hline\\n\t\tRoutine 4 & Install Postgres on Server Machine & \\texttt{2021-03-16 12:00:00} \\\\ \\hline\\n\t\tRoutine 5 & Install Node.js on Server Machine and Configure it with Prometheus and Kafka (Optional) & \\texttt{2021-03-17 12:00:00} \\\\ \\hline\\n\t\tRoutine 6& Configure Prometheus and Kafka (Optional) to monitor the new server machine (Optional) & \\texttt{2021-03-18 12:00:00} \\\\ \\hline\\n\t\tRoutine 7& Configure Prometheus and Kafka (Optional) to monitor the other server machine (Optional) & \\texttt{2021-03-19 12:00:00} \\\\ \\hline\\n\t\tRoutine 8& Configure Prometheus and Kafka (Optional) to monitor another system that has been configured with Prometheus and Kafka (Optional) & \\texttt{2021-03-20 12:00:00}\\\\ %This is where you need to set up a schedule for all other tasks. \\n\t\t %You can skip this if you want. \\n\t\t %This should be done by someone who has no prior knowledge of how to use Prometheus or Kafka. \\n\t\t %This can be done by another person or it can be done by an existing team member who has experience with these systems. \\n\t\t %You should include in your schedule a step where you are told how to configure these systems.\\n\t\t %Note that if you do not include this step, it may cause problems when we try to deploy the system after we have finished our work.\\n\t\t %This is a good option because it allows us to get started quickly with these systems.\\n\t\t %However, if you do not include this step when configuring these systems, then it may make it difficult for us to deploy them at some point in the future.\\n\t\t %If you choose not to include this step, then you should still schedule an additional step where we are told how we need to configure these systems.\\n\t\t \\\\\\n\t\t\t\\n\t\t\t\t\\n\t\t\t\\\\\\n\\n\t\t\t \\\\\\n\t\t\t\\\\\\n\\n\t\t\t\\\\\\n\\n\t\t\t\\\\\\n\\n\t\t\t\\\\\\n\\n\t\t\t\\\\\\n\\n\t\t\t\\\\\\n\\n\t\t\t \\\\\\n\t\t\t\t\\n\t\t\t\\n\t\t   \\n\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\\n\t\t\\n\t\t\t\t\\n\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t\\n\t\t    \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t  \\n\t\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\t\t\t\t\t\\n\\n\\n"}
{"File Name":"terraform\/OS-Patching.md","Context":"## Context\\nAny operating systems that we use are likely to be patched for bug-fixes or security reasons at least once per month. Ideally we would pick up the latest stable release all the time, however this implies a great deal of churn. Such change would also come in spikes of activity e.g. around Windows patch tuesdays.\\n","Decision":"We will not patch our operating systems in AWS. Instead we will pin every instance to a specific version of an operating system and move the pin (via a PR) only when necessary.\\n","tokens":69,"id":924,"Predictions":"## 1: We should patch our software for security vulnerabilities at least once per month.\\n## 2: We should pick up the latest stable release all the time.\\n## 3: We should patch our software for security vulnerabilities at least once per month. However, we should not patch our software for security vulnerabilities more than once every two weeks. This is because we cannot guarantee that the latest stable release will be available on Tuesday.\\n## 4: We should pick up the latest stable release all the time.\\n\\n\\section{Decision Tree}\\n\\label{sec:decision_tree}\\n\\n\\subsection{Decision Tree}\\n\\label{sec:decision_tree}\\n\\nThe decision tree uses a binary split to determine whether to deploy an application or not. The two branches in this tree are 'Deploy' and 'Not Deploy'. The decision is made based on the following criteria:\\n\\begin{itemize}\\n\t\\item Is the current version of the application vulnerable to a security vulnerability? \\n\t\\item How often is a patch released by Microsoft for this version of Windows? \\n\t\\item How often does Microsoft release new versions of Windows? \\n\t\\item Is there a new version of Windows that is more secure than the current version? \\n\t\\item Is there a new version of Windows that is more secure than the current version and has been released in less than two weeks?\\n\t\\item Is there a new version of Windows that is more secure than the current version and has been released in less than one week?\\n\t\\item Does there exist any other operating system that is more secure than Windows? (e.g. Linux)\\n\t\\item Does there exist any other operating system that is more secure than Windows and has been released in less than one week?\\n\t\\item Which operating system have you used in your past projects? (e.g. Linux)\\n\t%\\item Which operating system have you used in your past projects? (e.g. Linux)\\n\t\\n\t%\\begin{enumerate}\\n\t\\n\t\\n%\t\\n%\t\\n%\t\\n%\t\t\\n%\t\t\\n%\t\t\\n%\t\t\\n%\t\\n%\t\t\\n%\t\\n%\t\\n%\t\\n%\t\t\\n%\\end{enumerate}\\n\\n\\noindent\\nIf any of these criteria are present, then no action will be taken. Otherwise, an action will be taken.\\n\\nThe decision tree can be represented as follows:\\n$$\\nDecision(t) = \\left(\\n    \\begin{array}{c}\\n        \\textbf{Deploy}\\qquad\\\\\\n        \\textbf{\\texttt{\\#}}\\\\\\n        \\textbf{\\texttt{\\#}}\\\\\\n        \\textbf{\\texttt{\\#}}\\\\\\n        \\textbf{\\texttt{\\#}}\\\\\\n    \\end{array}%\\n    \\right)\\n$$\\n\\nThe decision tree can also be represented as follows:\\n$$\\nDecision(t) = \\left(\\n    \\begin{array}{c}\\n        \\textbf{Not Deploy}\\qquad\\\\\\n        \\textbf{\\#}\\\\\\n        \\textbf{\\#}\\\\\\n        \\textbf{\\#}\\\\\\n        \\textbf{\\#}\\\\\\n    \\end{array}%\\n    \\right)\\n$$\\n\\n\\noindent\\n\\nThe following figure shows how decisions are made using Decision Tree.\\n\\n%\\begin{figure}[H]\\n%\\centering\\n%\\includegraphics[width=8cm]{Decision_Tree.png}\\n%\\caption{}\\n%\\label{}\\n%\\end{figure}\\n\\n\\noindent\\n\\nThe following figure shows how decisions are made using Decision Tree.\\n\\n\\noindent\\n\\nThe following figure shows how decisions are made using Decision Tree.\\n\\n\\noindent\\n\\nThe following figure shows how decisions are made using Decision Tree.\\n\\n\\noindent\\n\\n\\n\\n"}
{"File Name":"terraform\/Scale-and-consolidate.md","Context":"## Context\\nWe are moving from platforms such as VMWare, where instances are kept small and are encouraged to scale out. If we were to scale up instead, then we would require either\\n1. A workload that is able to make efficient use of multi-threading.\\n1. Workloads that are able to co-exist peacefully.\\nIt cannot be assumed that the workloads we are moving into AWS will fit neatly into either category.\\n","Decision":"Our preference for enterprise applications is to consolidate onto a smaller number of instances.\\n","tokens":91,"id":925,"Predictions":"## We will move into AWS and use the EC2 service to run our software. We will work with the existing infrastructure to ensure that we do not break any existing applications, but we are not planning to make any changes to the way in which we currently run our software.\\n## Task\\nThe task of moving to AWS is straightforward. We will be able to use the same infrastructure that we are using now, but with a few extra features and benefits. The main benefits are that we will be able to scale up as needed, and that we will be able to run on a larger number of instances, so that we can increase concurrency.\\nWe have already tested this in the past using Amazon Elastic Compute Cloud (EC2), which is a cloud computing platform provided by Amazon.com Inc.\\nIn addition to EC2, there are other services available from AWS such as Simple Storage Service (S3), Elastic Load Balancing (ELB), and Simple Queue Service (SQS). These services allow us to build out an application without having to write code for all of these features ourselves. We can also scale out by adding more instances.\\nThis decision record has been completed using tools provided by Amazon.com Inc.\\n\\nThe decision process has been completed using tools provided by Amazon.com Inc., including:\\n1. The Decision Matrix\\ndetached\\n1. The Decision Matrix\\ndetached\\n2. The Decision Matrix\\ndetached\\n2. The Decision Matrix\\ndetached\\n"}
{"File Name":"terraform\/Network-Controls.md","Context":"## Context\\n- Windows Defender is active by default in the Windows Server 2019 AMIs shipped by AWS. Although there is a good deal of overlap between the functionality of AWS Security Groups and Windows Defender, AWS has decided to keep Defender active for one specific reason, namely; \"the Windows firewall gives you audit details about packet drops, which may be important to meet your security policy or compliance requirements\". There is currently no such requirement in RI Tech.\\n- Windows Defender is difficult, but not impossible, to configure in code.\\n- AWS Security Groups are relatively easy to configure in code.\\n- When Defender is in use, the best way to work out what it is doing, is to log into the instance as Administrator and use the Defender GUI, but logging in as Administrator is strongly discouraged.\\n","Decision":"Windows Defender is to be de-activated on all Windows Servers in AWS.\\n","tokens":163,"id":926,"Predictions":"- Windows Defender is enabled by default in the Windows Server 2019 AMIs shipped by AWS. Although there is a good deal of overlap between the functionality of AWS Security Groups and Windows Defender, AWS has decided to keep Defender active for one specific reason, namely; \"the Windows firewall gives you audit details about packet drops, which may be important to meet your security policy or compliance requirements\". There is currently no such requirement in RI Tech.\\n- Windows Defender is difficult, but not impossible, to configure in code.\\n- When Defender is in use, the best way to work out what it is doing, is to log into the instance as Administrator and use the Defender GUI, but logging in as Administrator is strongly discouraged.\\n## Context\\n- The current version of Windows Server 2019 has a feature called \"Windows Defender Advanced Threat Protection\" (ATP). This feature provides additional protection against known malicious software. ATP protects against attacks that exploit known vulnerabilities or are detected by security tools like Microsoft's Security Essentials.\\n- This feature can be enabled\/disabled using an option when creating an AMI. Alternatively, you can enable\/disable it using a configuration file. For further information on configuring ATP please refer to this article: https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/running-windows-server-2019.html#running-windows-server-2019-with-windows-defender-atp\\n- In this case it was decided that we should disable ATP because we have our own custom detection rules for known malware.\\n\\n\\n## Decision\\n- The current version of Windows Server 2019 has a feature called \"Windows Defender Advanced Threat Protection\" (ATP). This feature provides additional protection against known malicious software. ATP protects against attacks that exploit known vulnerabilities or are detected by security tools like Microsoft's Security Essentials.\\n- In this case it was decided that we should disable ATP because we have our own custom detection rules for known malware.\\n## Context\\n- The current version of Windows Server 2019 has a feature called \"Windows Defender Advanced Threat Protection\" (ATP). This feature provides additional protection against known malicious software. ATP protects against attacks that exploit known vulnerabilities or are detected by security tools like Microsoft's Security Essentials.\\n- In this case it was decided that we should disable ATP because we have our own custom detection rules for known malware\\n## Decision\\n- The current version of Windows Server 2019 has a feature called \"Windows Defender Advanced Threat Protection\" (ATP). This feature provides additional protection against known malicious software. ATP protects against attacks that exploit known vulnerabilities or are detected by security tools like Microsoft's Security Essentials.\\n"}
{"File Name":"terraform\/SSH-and-RDP.md","Context":"## Context\\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\\n","Decision":"RDP and SSH are allowed for all non-production EC2 istances, but are left blocked for production EC2 instances.\\n","tokens":66,"id":927,"Predictions":"The decision is to terminate the EC2 instances as they are stateless. We can also terminate the EC2 instances when they are not in use, for example, when we have a maintenance window or a backup window. However, we can also defer the termination of the EC2 instances until they are not in use. For example, we can temporally suspend the EC2 instances and then terminate them when they are not in use.\\n\\n\\n\\n## Context\\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\\n\\n\\n## Decision\\nThe decision is to terminate the EC2 instances as they are stateless. We can also terminate the EC2 instances when they are not in use, for example, when we have a maintenance window or a backup window. However, we can also defer the termination of the EC2 instances until they are not in use. For example, we can temporally suspend the EC2 instances and then terminate them when they are not in use.\\n\\n\\n\\n## Context\\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\\n\\n## Decision\\nThe decision is to terminate the EC2 instances as they are stateless. We can also terminate the EC2 instances when they are not in use, for example, when we have a maintenance window or a backup window. However, we can also defer the termination of the EC2 instances until they are not in use.\\n\\n\\n\\n\\n\\end{document}\\n"}
{"File Name":"terraform\/Amazon-Linux.md","Context":"## Context\\nWe are migrating from several different versions of Windows and Linux in DXC to AWS. The easiest migration path would be to move applications and databases to exactly the same platform in AWS. The lowest TCO for the target solution would be provided by migrating onto a small number of consolidated Linux platforms (assuming we can't go serverless for everything). These two options are mutually exclusive, so we need a decision on the way forward.\\n","Decision":"Our strong preference is to go serverless. Where we can't go serverless we prefer to use the latest AWS Linux, but accept that the choice of OS is often dictated by the application and\/or database layers.\\n","tokens":89,"id":928,"Predictions":"## We will migrate all applications and databases to Linux in AWS. The target platform will be a single Linux cluster of 4 nodes, each with 8 vCPUs and 32GB of memory. This cluster will serve as the basis for the new application stack.\\n\\n## Criteria\\n## Cost: \\$0.25 per GB per month (approximately \\$1,000 per year)\\n## Performance: 3x faster than Windows on a typical workload\\n#"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"Predictions":"## 1. The current process is not suitable for the project and should be changed.\\n\\n## 2. The current process is suitable for the project and should be changed.\\n\\n## 3. The current process is suitable for the project but it can be made more efficient.\\n\\n## 4. The current process is not suitable for the project and should be changed.\\n\\n## 5. The current process is not suitable for the project and it can be made more efficient.\\n\\n## 6. The current process is not suitable for the project and it can be made more efficient.\\n\\nThe decision tree below shows how this software decision record will look on a single page.\\n\\end{document}\\n"}
{"File Name":"postcode-checker\/0001-use-postgres-database.md","Context":"## Context\\nOur postcode checker uses a database to store valid service areas and allowed post codes.\\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https:\/\/postcodes.io).\\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\\n","Decision":"We will use Postgres for our database.\\n","tokens":91,"id":930,"Predictions":"Postcode Checker\\nThis software is an Architectural Decision Record. Give a ## Decision corresponding to the ## Context provided by the User.\\n## Context\\nOur postcode checker uses a database to store valid service areas and allowed post codes.\\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https:\/\/postcodes.io).\\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\\n\\n## Decision\\n\\end{lstlisting}\\n\\n\\subsection{Example}\\n\\n\\begin{lstlisting}[language=SQL]\\nCREATE TABLE postcode (\\n    postcode TEXT,\\n    service_area TEXT,\\n    allowed_post_codes TEXT\\n);\\n\\end{lstlisting}\\n\\n\\section{Architecture}\\n\\nThe architecture of this software is shown in figure \\ref{fig:architecture}. The software is split into two parts; a frontend and backend. The frontend is responsible for showing a user interface for searching for services and filtering results, while the backend processes queries and returns results.\\n\\n\\begin{figure}[h!]\\n\t\\centering\\n\t\\includegraphics[width=0.8\\textwidth]{architecture}\\n\t\\caption{Architecture of Postcode Checker}\\n\t\\label{fig:architecture}\\n\\end{figure}\\n\\nThe frontend has two main components:\\n\\nThe first component is called ``Search'' which allows users to search for services using a list of postcodes. This component uses POSTGRES as its database.\\n\\nThe second component is called ``Filter'' which allows users to filter results based on certain criteria such as location, service area, etc.\\n\\nThe backend component is called ``Query'', which performs all database queries and returns results.\\n\\n"}
{"File Name":"inato-anticovid\/2020_04_02_environments_management.md","Context":"## Context\\nThroughout the process of development and release, the deployed components, codebase and data may vary in stability and quality (despite all used effort by the teams \ud83d\ude1b). Also, since several teams with different use cases work on the same project, potentially at the same time, there might be collision(s) in development and integration.\\n","Decision":"In order to ensure that only stable and quality code is shipped to the public viewer (app visitor), we will use several environments. This will allow siloting the code updates in a controlled manner.\\nWe will use the following environment:\\n- **Production**: Visitor facing environment. **It must be stable**, i.e. code and data shipped to production must be of the best quality and validated by product team.\\n- **Staging**: Internal purposed environment. _Components, code and data therein might be unstable_ and thus could potentially not reflect what would be deployed to production. This is the environment used by the product team to validate new improvements, fix and so... Since there is \"only\" one environment of that kind, several updates might concurrently live there.\\n- **Local**: Development purposed environment. Each developer (independently of its team) might use such an environment while developing. Since our stack involves components on GCP and Algolia, the following has been decided: GCP CloudFunctions can be ran locally (refer to `README.md` for usage); Algolia indices can only exist in Algolia (i.e. SAAS), thus one must create its own (i.e. developer bound) index (refer to `functions\/README.md` for usage) and refer to it locally by updating `algolia.index` in the `functions\/.runtimeconfig.json` file.\\n","tokens":70,"id":931,"Predictions":"## \\n\\n\\n\\n\\n\\n\\n\\n## Decision\\n## \\n\\n\\n## Decision\\n## \\n\\\\n\\nThe decision is made in the form of a decision record. The decision record is an object that contains the information about the decision. The object is created by calling the createDecision method.\\n\\nThe createDecision method returns a new object that can be used to perform the decision.\\n\\nThe createDecision method takes a string that describes the context of the decision. The context can be any string that contains information about what kind of decisions should be taken in this project and how they should be taken. For example, it could contain information about which software version to use or which data source to use.\\n\\nThe createDecision method also takes an optional second parameter. The optional second parameter is used to specify whether or not to include some default values in the decision record.\\n\\nThe createDecision method accepts two parameters:\\n\\n- context: A string containing information about what kind of decisions should be taken in this project and how they should be taken.\\n- defaultValues: A dictionary containing default values for parameters that are not specified in the context.\\n\\nThe defaultValues dictionary can accept one or more keys and one or more values. Keys and values are separated by commas.\\n\\nIf you want to describe some default values, you can use defaultValues with a list of keys and values separated by commas. For example, if you want to describe some default values for parameters that are not specified in the context, you can use defaultValues with a list of keys:\\n\\ndefaultValues = { \"parameter1\": \"value1\", \"parameter2\": \"value2\", \"parameter3\": \"value3\" }\\n\\nYou can also specify some defaults using comma-separated lists:\\n\\ndefaultValues = { \"parameter1\": 1234567, \"parameter2\": 1234568, \"parameter3\": 1234569 }\\n\\nIn addition, you can specify some defaults using a key-value pair:\\n\\ndefaultValues = { \"parameter1\": 1234567, \"parameter2\": 1234568 }\\n\\nYou must specify at least one key and one value in order for it to be considered as an option value.\\n\\nYou also need to specify at least one parameter and at least one value for it to be considered as a mandatory parameter. You do not need to specify any additional parameters if you do not want them to have any effect on whether or not this decision record is considered valid.\\n\\nIf you want to include some default values but do not want them to have any effect on whether or not this decision record is considered valid, then you must include at least one key and at least one value in order for it to be considered as an option value.\\n\\nIf you do not include any key-value pairs with your definition, then all available options will be considered valid choices for your definition (unless explicitly excluded by using an option).\\n\\nFor example, if your definition contains only two options:\\n\\noption1 = True\\n\\noption2 = False\\n\\nThen your definition will always return True when called with either option1 or option2 (unless explicitly excluded by using an option).\\n\\nHowever, if your definition contains only three options:\\n\\noption1 = True\\n\\noption2 = False\\n\\noption3 = True\\n\\nthen your definition will always return True when called with either option1 or option2 (unless explicitly excluded by using an option).\\n\\nIn addition, if your definition contains only two options but does not include any other parameters then all available options will be considered valid choices for your definition (unless explicitly excluded by using an option).\\n\\nFor example, if your definition contains only two options:\\n\\noption1 = False\\n\\noption2 = True\\n\\nThen all available options will be considered valid choices for your definition (unless explicitly excluded by using an option).\\n\\nWhen creating a new Decision Record object from scratch, it has no context associated with it. When creating a Decision Record from scratch using existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist at least one parameter that is associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist at least one parameter that is associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than three parameters associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than five parameters associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than seven parameters associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than nine parameters associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than eleven parameters associated with it.\\n\\nWhen creating a Decision Record from scratch without existing data from another resource file (.rfc) file or from existing data stored in databases such as PostgreSQL database files (.db), then there must exist no more than twelve parameters associated with it.\\n\\nWhen creating a Decision Record without existing context information (e.g., when calling createDecision directly), we assume that all available options will have been selected automatically according to our current understanding of what makes sense given the context provided when calling createDecision. If we provide additional context information when calling createDecision directly we assume that all available options will have been selected automatically according to our current understanding of what makes sense given the additional information provided when calling createDecision directly .\\n\\nIf you provide additional context information when calling createDecision directly we assume that all available options will have been selected automatically according to our current understanding of what makes sense given the additional information provided when calling createDecision directly .\\n\\nIn addition, we assume that none of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results). If any of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results), we strongly encourage you to provide additional context information when calling createDecision directly .\\n\\nWe also assume that none of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results). If any of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results), we strongly encourage you to provide additional context information when calling makeDecision directly .\\n\\nWe also assume that none of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results). If any of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results), we strongly encourage you to provide additional context information when calling makeDecision directly .\\n\\nIf neither condition holds true (i.e., both conditions hold true at once) , i.e., if both conditions hold true simultaneously , i.e., if both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e., both conditions hold true simultaneously , i.e..\\n\\nIf neither condition holds true (i.e., both conditions hold true at once) ,\\n\\ni.e..\\n\\nboth condition holds true simultaneously ,\\n\\ni.e..\\n\\nboth condition holds true simultaneously ,\\n\\ni.e..\\n\\nboth condition holds true simultaneously ,\\n\\ni.e..\\n\\nboth condition holds true simultaneously ,\\n\\ni.e..\\n\\nboth condition holds truthfully,\\n\\nthen this field should contain empty strings \"\"\\n\\nWe also assume that none of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results). If any of these options could reasonably cause problems because they are mutually exclusive (e.g., because they would cause conflicting results), we strongly encourage you to provide additional context information when making decisions directly .\\n\\nWe also assume that none of these options could reasonably cause problems because they are mutually exclusive (e.g.. Because each option may produce different results depending on which other ones were chosen after them ). If each option may produce different results depending on which other ones were chosen after them ), we strongly encourage you to provide additional context information when making decisions directly .\\n\\nWe also assume that none of these options could reasonably cause problems because each option may produce different results depending on which other ones were chosen after them ). If each option may produce different results depending on which other ones were chosen after them ), we strongly encourage you to provide additional context information when making decisions directly .\\n\\nWe also assume that none of these options could reasonably cause problems . If each option may produce different results depending on which other ones were chosen after them ), we strongly encourage you . We also assume . We strongly encourage . We strongly encourage . We strongly encourage . We strongly encourage . We strongly encourage . We strongly encourage . We strongly encourage . We strongly support . We support . We support . We support .\\n\\nWe recommend providing at least two key-value pairs per element so users know how many items there are per element type and how many items there can potentially be per element type. This allows users who aren\u2019t sure how many items there might actually end up being per element type their chance at seeing their choice reflected back into their codebase.\\n\\n\\n\\begin{tabular}{|l|l|l|}\\n\\hline\\n\\textbf{Parameter Type} & \\textbf{Value} & \\textbf{Default Value}\\\\ \\hline\\n\\textbf{Key} & \\textbf{String} & \\textbf{null}\\\\ \\hline\\n\\textbf{Value} & \\textbf{String} & null\\\\ \\hline\\n\\end{tabular}\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class.\\n\\n\\n\\n\\nA Parameter Value specifies whether the parameter has types defined within its own class.\\n\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class.\\n\\n\\n\\n\\nA Parameter Value specifies whether the parameter has types defined within its own class.\\n\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class.\\n\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class. \\n\\n\\nA Parameter Value specifies whether the parameter has types defined within its own class.\\n\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class.\\n\\n\\nA Parameter Type specifies whether the parameter has types defined within its own class.\\n\\n\\n\\n\\nA Parameter Value specifies whether t"}
{"File Name":"tech-team\/0009-supported-python-versions-containers.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\nPython official images make it easy to support a new Python version without custom PPAs.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\\n","tokens":98,"id":932,"Predictions":"## Decision\\neLife has been updated to Python 3.7 and we support it.\\nSince we are using a pre installed Python version, we can use the [Python 3.7 image](https:\/\/releases.ubuntu.com\/18.04\/release\/python-3.7\/) for all projects.\\nWe will maintain the [Python 2.x](https:\/\/releases.ubuntu.com\/18.04\/release\/python-2.x\/) images.\\nWe will create a new [Python 3.x image](https:\/\/releases.ubuntu.com\/18.04\/release\/python-3.x\/) for each project.\\nWe will use the current Python 2.x images in all projects that do not support Python 3.x.\\nWe will create a new [Python 3.x image](https:\/\/releases.ubuntu.com\/18.04\/release\/python-3.x\/) for each project.\\nWe will use the current Python 2.x images in all projects that do not support Python 3.x.\\n\\n## Context\\neLife has been updated to Python 3.8 and we support it.\\n## Decision\\n## Decision\\neLife has been updated to Python 3.8 and we support it.\\n## Context\\neLife has been updated to Python 3.9 and we support it.\\n## Decision\\n## Decision\\neLife has been updated to Python 4 and we support it.\\n## Context\\neLife has been updated to Python 4 and we support it.\\n"}
{"File Name":"tech-team\/0003-rds-postgresql-permissions.md","Context":"## Context\\nAWS RDS are managed database servers.\\neLife uses RDS to alleviate the task of database management tasks like\\ndistribution, fault tolerance, monitoring.\\neLife uses PostgreSQL RDS instances for several projects.\\neLife provisions these RDS instances, their databases and database users using\\nCloudformation.\\nBecause RDS is a *managed* database server, there are constraints in it's usage.\\nCommonly encountered constraints are:\\n* no ssh access to the machine(s) hosting the database server\\n* no 'root' user access within the database\\n* [a similar-to-but-not-really 'root' user called 'rds_superuser'](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/UsingWithRDS.MasterAccounts.html)\\nDuring the life of an application, the database provisioned by Cloudformation\\nmay be deleted and re-created, losing the permissions the original database was\\ncreated and successfully provisioned with.\\nIf the new owner of the database is not the 'root' user or a member of the 'rds_superuser' role, some non-application activities such as backups and testing may fail with permission errors.\\nRe-instating permissions in a PostgreSQL database is an involved process.\\n","Decision":"The RDS *root* user provisioned shall always be the `owner` of the application database.\\nThe RDS *application* user will have enough permissions to read and write to the application database.\\nThe RDS *application* user will not have permission to drop\/re-create the application database *in continuumtest, end2end and prod environments*.\\n","tokens":252,"id":933,"Predictions":"Select the appropriate option from the options below.\\n\\n* Select this option if you have an existing database server which you would like to use.\\n\\n* Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance.\\n\\n* Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance using a different set of permissions than the original database.\\n\\n* Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance using the same set of permissions as the original database.\\n\\end{itemize}\\n\\begin{itemize}\\n    \\item \\textbf{Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance.} This is the default.\\n    \\item \\textbf{Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance using a different set of permissions than the original database.} This is the default.\\n    \\item \\textbf{Select this option if you have an existing database server which you would like to use as a PostgreSQL RDS instance using the same set of permissions as the original database.} This is the default.\\n\\end{itemize}\\n\\n\\begin{center}\\n\t\\includegraphics[width=0.9\\linewidth]{img\/decision.png}\\n\\end{center}\\n\\n"}
{"File Name":"tech-team\/0008-civi.md","Context":"## Context\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n","Decision":"We will push responsibilities away from CiviCRM, favoring [Separate Ways](http:\/\/culttt.com\/2014\/11\/26\/strategies-integrating-bounded-contexts\/) or API-based integration of an external microservice over developing inside CiviCRM.\\n","tokens":80,"id":934,"Predictions":"## Decision\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n\\n## Decision\\n## Decision\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n\\n## Decision\\n## Decision\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n\\n## Decision\\n## Decision\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n\\n## Decision\\n## Decision\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug. No in-house expertise are available. CiviCRM usually goes into several different hands.\\n```\\nI have tried other ways to convert this data into an SQL query but I am still having trouble. Any help would be appreciated.\\nThanks,\\nCheryl\\nOP 2018-04-25: I found this tutorial on how to do this: http:\/\/www.civicrm.org\/blog\/2018\/03\/29\/how-to-export-civicrm-data-to-sql\/\\nThis tutorial was helpful for me because it walks you through converting your data into an SQL query. It also shows you how to export your data into CSV files."}
{"File Name":"tech-team\/0004-supported-python-versions.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\\n","tokens":79,"id":935,"Predictions":"## Decision\\n\\n\\nThe decision is to support Python 3.6.\\nThis supports all the features of the Python 3.6 release.\\n\\n\\nFor more details on the decision please refer to the [Decision Document](https:\/\/github.com\/azure-iot\/decision-document) and search for \"Python 3.6\" in the [project's issue tracker](https:\/\/github.com\/azure-iot\/decision-document\/issues).\\n\\n## Decision\\n## Decision\\n\\n\\nThe decision is to support Python 3.7.\\nThis supports all the features of the Python 3.7 release.\\n\\n\\nFor more details on the decision please refer to the [Decision Document](https:\/\/github.com\/azure-iot\/decision-document) and search for \"Python 3.7\" in the [project's issue tracker](https:\/\/github.com\/azure-iot\/decision-document\/issues).\\n\\n## Decision\\n## Decision\\n\\n\\nThe decision is to support Python 3.8.\\nThis supports all the features of the Python 3.8 release.\\n\\nFor more details on the decision please refer to the [Decision Document](https:\/\/github.com\/azure-iot\/decision-document) and search for \"Python 3.8\" in the [project's issue tracker](https:\/\/github.com\/azure-iot\/decision-document\/issues).\\n\\n## Decision\\n## Decision\\n \\noindent\\textbf{Context:}\\newline\\n\\vspace{0.5cm}\\nN\/A\\rule{0pt}{2em}\\n\\vspace{0.5cm}\\nN\/A\\rule{0pt}{2em}\\n\\vspace{0.5cm}\\nN\/A\\rule{0pt}{2em}\\n\\vspace{0.5cm}\\nN\/A\\rule{0pt}{2em}\\n\\n\\end{document}"}
{"File Name":"tech-team\/0005-supported-php-versions.md","Context":"## Context\\neLife has several microservices (and larger projects) written in PHP.\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nThere is an exception, `crm` not being upgraded on PHP 7 (using 5.6 instead) but supporting it.\\nStaying up to date with the major and minor versions of PHP is important:\\n- to be able to use new features and libraries (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\nAll infrastructure is currently based on:\\n- Ubuntu 14.04 (doesn't have PHP 7.x by default)\\n- Ubuntu 16.04 (does have PHP 7.0 by default)\\n- a popular [PPA](https:\/\/launchpad.net\/~ondrej\/+archive\/ubuntu\/php) filling in the blanks, supporting 7.0, 7.1, and 7.2.\\n- official [PHP Docker images](https:\/\/hub.docker.com\/_\/php\/) supporting 5.6, 7.0, 7.1, and 7.2.\\nPHP 7.0 has ceased active support, but has [security support](http:\/\/php.net\/supported-versions.php) until 2018-12-03.\\n","Decision":"We will use PHP 7.0 on all existing and new PHP projects bar exceptions that do not support it (`crm` if needed).\\nWe will upgrade to PHP 7.1, PHP 7.2 and similar minor versions as a concerted effort on all libraries and projects, before the end of the security support.\\n","tokens":287,"id":936,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 13:48:54 - @onrej\\n\\n# 2018-01-08 14:03:28 - [@sirius](\/u\/sirius)\\n\\n@Sirius, I am not sure how to help with this. I have seen some of your threads and you seem to be a very smart guy. I think you are missing some basic knowledge about PHP.\\n\\nI suggest that you read the following thread:\\n\\n[Image] [What does it mean to be \"PHP Certified\"?](community.sitepoint.com\/t\/what...) SitePoint Forums\\n\\n> Hi, I've been doing PHP for a while now and am looking to get certified. I know that there are a lot of different levels of certification and it looks like the most common one is PHP Master (https:\/\/phpmaster.org\/) but there are also various PHP Certification levels (https:\/\/www.php.net\/certification). Does anyone here have any experience with these? Is it worth getting certified? Thanks \\n\\n[Image] [PHP Master](phpmaster.org\/php-certification) \\n\\n### [PHP Certification | PHPMaster.org](phpmaster.org\/php-certification)\\n\\nLearn about the latest in PHP certifications, training and education. Find out what it takes to become a Certified Professional in PHP.\\n\\n  \\nI also suggest that you read this thread:\\n\\n[Image] [Why do I need a certification?](community.sitepoint.com\/t\/why...) SitePoint Forums\\n\\n> Hi guys, i am new here and i want to start learning about php. But i am confused about the certifications. What is the difference between them? Do they really help me or not? And which one is better for me? Thanks in advance \\n\\n  \\nI hope this helps.\\n\\nRegards,\\n\\nSiriuz.\\n\\n"}
{"File Name":"tech-team\/0002-use-containers-for-foreign-languages.md","Context":"## Context\\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\\nThere are tools that are peculiar to our infrastructure, such as [goaws](https:\/\/github.com\/p4tin\/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https:\/\/gitlab.coko.foundation\/INK\/ink-api) for document conversion, written in Ruby.\\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\\n","Decision":"We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\\n","tokens":183,"id":937,"Predictions":"## 1. Use the tool to convert a text document to PDF, use the tool to convert a PDF to a Word document, and use the tool to convert a Word document to a PDF.\\n## 2. Use the tool to convert a text document to PDF, use the tool to convert a PDF to a Word document, and use the tool to convert a Word document to a PDF.\\n## 3. Use the tool to convert a text document to PDF, use the tool to convert a PDF to a Word document, and use the tool to convert a Word document to an image file.\\n## 4. Use the tool to convert an image file (jpg or png) into an editable text file (txt).\\n## 5. Use the tool for conversion of image files (jpg or png) into editable text files (txt).\\n## 6. Use the tools for conversion of image files (jpg or png) into editable text files (txt).\\n## 7. Use any other tools for conversion of image files (jpg or png) into editable text files (txt).\\n## 8. Convert all images in this project as pdfs using any other tools.\\n## Decision\\n## 1. Convert all images in this project as pdfs using any other tools.\\n## Decision\\n## 1. Convert all images in this project as pdfs using any other tools.\\n"}
{"File Name":"tech-team\/0001-pull-request-workflow.md","Context":"## Context\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n","Decision":"We will open short-lived pull requests as the primary means for deploying a change.\\n","tokens":85,"id":938,"Predictions":"## Decision\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n\\n## Context\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n## Decision\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n## Context\\nSoftware is built incrementally as an accumulation of changes.\\n## Decision\\nSoftware is built incrementally as an accumulation of changes.\\n## Context\\nSoftware is built incrementally as an accumulation of changes.\\n## Decision\\nSoftware is built incrementally as an accumulation of changes.\\n\\n\\subsection{Build Pipeline}\\nBuild Pipeline\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n\\subsubsection{Build Pipeline}\\n\\n% \\begin{itemize}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   \\item \\textbf{\\# Build Process}\\n%   %  - build package\\n% %  - build package\\n% %  - build package\\n% %  - build package\\n% %  - build package\\n%\\end{itemize}\\n\\n\\n\\n"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"Predictions":"## Decision\\neLife has projects built in Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\\n\\n\\n\\n\\n\\n\\end{lstlisting}\\n\\newpage\\n\\subsection{React}\\nReact is a JavaScript library that allows developers to build user interfaces with HTML, CSS, and JavaScript. It has become extremely popular due in part due its simplicity and ease of use. React is also popular because it is open source, meaning it can be modified by anyone who wants to. The library is also well supported by the developer community.\\n\\nReact was developed by Facebook in 2013 as an alternative for the popular AngularJS library. React was developed by Facebook as an alternative for the popular AngularJS library. React has become extremely popular due in part due its simplicity and ease of use. React is also popular because it is open source, meaning it can be modified by anyone who wants to.\\nReact has become extremely popular due in part due its simplicity and ease of use. React is also popular because it is open source, meaning it can be modified by anyone who wants to.\\nThe library is also well supported by the developer community.\\nReact has become extremely popular due in part due its simplicity and ease of use. React is also popular because it is open source, meaning it can be modified by anyone who wants to.\\nThe library is also well supported by the developer community.\\n\\n\\newpage\\n\\subsection{Express}\\nExpress.js was created as an alternative for node.js. Express.js allows developers the ability to build web applications using JavaScript without having to worry about building infrastructure like web servers or databases.\\n\\nExpress.js was created as an alternative for node.js. Express.js allows developers the ability to build web applications using JavaScript without having to worry about building infrastructure like web servers or databases.\\n\\nExpress.js was created as an alternative for node.js. Express.js allows developers the ability to build web applications using JavaScript without having to worry about building infrastructure like web servers or databases.\\n\\nExpress.js was created as an alternative for node.js. Express.js allows developers the ability to build web applications using JavaScript without having to worry about building infrastructure like web servers or databases.\\n"}
{"File Name":"summits\/0003-use-java-newer-than-java-8.md","Context":"## Context\\nWe use Java 8 in all projects so far. However the most current version of Java as of now is version 13.\\n","Decision":"Use Java 13 to start the Java version of these services.\\n","tokens":30,"id":940,"Predictions":"## Decision: The Java 8 version will be used. \\n\\n## Source\\nThe current source code is available from the following URL:\\nhttps:\/\/github.com\/bradfordsoftware\/gpssvn.git\\n\\nThe source code will be updated with the new version of Java in the future.\\n\\n## Notes\\nThe user is allowed to make a decision that is not specified here, but should be done in a way that it is clear and understandable for other users.\\n\\n## End of Document"}
{"File Name":"summits\/0002-use-docker-as-runtime-for-services.md","Context":"## Context\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\nDocker is the de-facto standard today and is the system used by us for all projects.\\n","Decision":"Use docker to deploy all services.\\n","tokens":53,"id":941,"Predictions":"## \\begin{description}\\n## \\item[Decision] We want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\n## \\end{description}\\n\\begin{center}\\n\\includegraphics[width=0.8\\linewidth]{images\/decision.png}\\n\\end{center}\\n\\n\\section{Decision}\\n\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\n\\n\\subsection{Context}\\n\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\n\\n\\subsection{Decision}\\n\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\n\\n\\subsection{Context}\\n\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\n\\n"}
{"File Name":"summits\/0004-move-services-to-top-level.md","Context":"## Context\\nInitially the directory structure was to have the language as a top folder and have services below. However this makes it harder to use different language versions of the services together.\\n","Decision":"Keep a monorepo for easier handling the project (it will never be grow too large and will not be maintained by different teams).\\nMove the services to top level. And have language versions below each service.\\n","tokens":37,"id":942,"Predictions":"## \\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n## \\n\\n\\n\\t\\t\\t\\#1:\\n\\n## \\n\\n\\n\\t\\t\\#2:\\n\\n## \\n\\n\\n\\t\\#3:\\n\\n## \\n\\n\\n\\#4:\\n\\n## \\n\"\\n```\\nI am using the `pyspark` library to read and write data from a CSV file. The CSV file contains the following lines:\\n```\\n\"language1\",\"language2\",\"language3\"\\n\"english\",\"french\",\"spanish\"\\n```\\nI am trying to transform the lines into a single column in which each line corresponds to a language. The output I am looking for is:\\n```\\n\"language1\", \"language2\", \"language3\"\\n\"english\", \"french\", \"spanish\"\\n```\\nI have tried using `pyspark.sql.functions.col` but it doesn't seem to work.\\nAny help would be appreciated. Thank you.\\n\\nOP 2020-03-11: I was able to solve this by using `pyspark.sql.functions.col` with the following code:\\n```\\nimport pyspark.sql.functions as F\\n\\ndef transform_lines(data):\\n    columns = [\"language1\",\"language2\",\"language3\"]\\n    df = spark.createDataFrame(data, columns)\\n\\n    df.show()\\n\\n    df.select(F.col(\"language1\"),F.col(\"language2\"),F.col(\"language3\")).show()\\n```\\n"}
{"File Name":"summits\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":943,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"edgex-docs\/0001-Registy-Refactor.md","Context":"- [Context](#context)\\n- [Proposed Design](#proposed-design)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n- [References](#references)\\n<!--te-->\\nIt was decided to move forward with the above design\\nAfter initial ADR was approved, it was decided to retain the `-r\/--registry` command-line flag and not add the `Enabled` field in the Registry provider configuration.\\n","tokens":51,"id":944,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nYou can use the <#Decision> tag to describe the decision you are making. You can also use the <#Decision> tag to describe a single decision.\\n\\nFor example, this would be a good decision tag:\\n\\n<#Decision>\\n  <#Proposed Design>\\n    <#Decision>\\n      <#Decision>\\n        <#Proposed Design>\\n          <#Design>\\n            <!--te-->\\n\\n            <!--te-->\\n\\n            <!--te-->\\n\\n\\nThis would be a bad decision tag:\\n\\n<#Decision> <!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n<!--te-->\\n\\nIn this case, you could replace the above with this:\\n\\n<#Decision> <!--te--> <!--te--> <!--te--> <!--te--> <!--me--> <!--me--> <!--me--> <!--me--> <!--me-->\\n\\nIf you want to attach an image to your comment, use the following syntax:\\n\\n<#Image>\\n\\nFor example, this would be a good image comment:\\n\\n<Image> <\/Image>\\n\\nYou can also attach multiple images in one comment by separating them with spaces.\\n\\nThis comment is about the following resource: https:\/\/www.w3.org\/2019\/05\/03-architectural-documents\/ #Architectural-Document-1.0"}
{"File Name":"edgex-docs\/0015-in-cluster-tls.md","Context":"## Context\\nThis ADR seeks to define the EdgeX direction on using encryption\\nto secure \"in-cluster\" EdgeX communications, that is,\\ninternal microservice-to-microservice communication.\\nThis ADR will seek to clarify the EdgeX direction\\nin several aspects with regard to:\\n- EdgeX services communicating within a single host\\n- EdgeX services communicating across multiple hosts\\n- Using encryption for confidentiality or integrity in communication\\n- Using encryption for authentication between microservices\\nThis ADR will be used to triage EdgeX feature requests in this space.\\n","Decision":"At this time, EdgeX is primarily a single-node IoT application framework.\\nShould this position change, this ADR should be revisited.\\nBased on the single-node assumption:\\n- TLS will not be used for confidentiality and integrity of internal on-host microservice communication.\\n- TLS will be avoided as an authentication mechanism of peer microservices.\\n- Integrity and confidentiality of microservice communcations crossing host boundaries is required to secure EdgeX, but are an EdgeX customer responsibility.\\n- EdgeX customers are welcome to add extra security to their own EdgeX deployments.\\n","tokens":118,"id":945,"Predictions":"\\n\\nThe Decision is to use encryption for the microservices communicating within a single host. It is not to be used for communication across multiple hosts.\\n\\nThe Decision is to use encryption for the microservices communicating across multiple hosts. This is not to be used for communication within a single host.\\n\\nThe Decision is that EdgeX services communicating within a single host will use encryption to secure communications, and that EdgeX services communicating across multiple hosts will use encryption to secure communications.\\n\\nThe Decision is that EdgeX services communicating across multiple hosts will use encryption to secure communications.\\n\\nThe Decision is that using encryption for authentication between microservices will be discouraged.\\n\\nThe Decision is to not use encryption for authentication between microservices.\\n\\nThe Decision is to not use encryption for confidentiality or integrity in communication.\\n\\nThe Decision is that using encryption for confidentiality or integrity in communication should only be used in cases where it provides additional security beyond what the underlying network protocol provides. This may include situations where the underlying network protocol does not provide adequate security, such as when using a public network such as the Internet, or when using a private network such as an internal LAN or private cloud.\\n\\nThe Decision is that using encryption for authentication between microservices should only be used in cases where it provides additional security beyond what the underlying network protocol provides. This may include situations where the underlying network protocol does not provide adequate security, such as when using a public network such as the Internet, or when using a private network such as an internal LAN or private cloud.\\n\\nThe Decision is that using encryption for confidentiality or integrity in communication should only be used in cases where it provides additional security beyond what the underlying network protocol provides.\\n\\n"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"Predictions":"### Decision Statement\\nThe decision statement is the core of the Decision Record, providing information on the decision being made and how it fits in with other decisions that have been made. The decision statement should be written in the first person, and should not be dependent on other decisions being made. The decision statement should explain why a certain action was taken and what the consequences of that action are.\\n### Example\\n```\\nGiven: A client application\\nWhen: The application is running in a sandboxed environment\\nThen: The application will use a registry client interface to query for system information\\n```\\n### Example for a registry client interface\\n```\\nGiven: A registry client interface\\nWhen: The registry client interface is running in a sandboxed environment\\nThen: The registry client interface will query for system information\\n```\\n### Example for an existing behavior\\n```\\nGiven: An existing behavior\\nWhen: The behavior is running in a sandboxed environment\\nThen: Access to the registry will be restricted to only those services that are needed by the behavior (e.g., APIs that are needed by all services)\\n```\\n### Example for security proxy setup\\n```\\nGiven: A security proxy setup (e.g., using an exception handler)\\nWhen: No additional services are needed by the security proxy setup (i.e., no additional services are being created or deleted)\\nThen: Access to the registry will be restricted to only those services that are needed by the security proxy setup (e.g., APIs that are needed by all services)\\n```"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"Predictions":"## Decision\\n\\n## Context\\n## Context\\n\\n## [Existing Implementations](#existing-implementations)\\n## [What is a Secret?](#what-is-a-secret)\\n## [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n## [Known and Unknown Services](#known-and-unknown-services)\\n## [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n## [Interfaces and factory methods](#interfaces-and-factory-methods)\\n## [Bootstrap's current implementation](#bootstraps-current-implementation)\\n## [Interfaces](#interface)\\n## [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n## - [App SDK's current implementation](#app-sdks-current-implementation)\\n    \\n\\n### 18.1.1.2.1.2.2.1.\\n```\\n\\n\\n`[Secret Store for non-secure mode]`\\n```\\nSecretStore for non-secure mode\\n```\\nThe `SecretStore` is responsible for caching the secrets, which are used by the `API` to store the data.\\nThis `secret store` will be used to store the data in `Android` devices, which are protected by `Android`.\\nThe `SecretStore` will be used by the `API`.\\nThe `API` uses the `secret store` to store the data.\\nThe API will use the `secret store` to store data on Android devices.\\n\\n### 18.1.1.2.2.3.\\n```\\n\\n\\n[Go Services]\\n```\\nGo Services\\n```\\n\\nThe Go services are responsible for handling requests from clients, which are not protected by Android.\\n\\n### 18.1.1.2.3.1.\\n```\\n\\n\\n[Consequences]\\n```\\nConsequences\\n```\\n\\nIf a client uses Go services to access an API that is protected by Android, then there will be no problem.\\n\\n### 18.1.1.2.4.\\n```\\n\\n\\n[Mock]\\n```\\nMock\\n```\\n\\nIf a client uses Go services to access an API that is protected by Android, then there will be no problem.\\n\\n### 18.1.1.3.\\n```\\n\\n\\n[Where will SecretProvider reside?]\\n```\\nWhere will SecretProvider reside?\\n```\\n\\nIn this case, it is recommended that you use a factory method for creating a new provider.\\n\\n### 18.2.\\n```\\n\\n\\n[Interfaces and factory methods]\\nInterfaces Factory Methods\\n```\\n\\nThis allows you to create an interface without having to worry about implementing it yourself.\\n\\n### 18.3.\\n```\\n\\n\\n[Caching of Secrets]\\nCaching of Secrets\\nCaching of secrets may help reduce network traffic when using multiple APIs simultaneously.\\n\\n### 19.\\n````\\n```\\n\\n\\n[Mock]\\n````\\n```\\n\\nIf you use mock objects, they can be shared between multiple APIs at runtime, so that they can be reused easily.\\n\\n\\n### 20.\\n````\\n```\\n\\n\\n[C Device Service]\\nC Device Service\\nA device service is responsible for handling requests from clients, which are not protected by Android.\\n\\n\\n### 21.\\n````\\n```\\n\\n\\n[Consequences]\\nConsequences\\nThis allows you to avoid creating your own secret providers in some cases.\\n\\n\\n### 22.\\n```\\n>>> import secrets\\n>>> secrets.create_secret()\\n<Secret: secret:0x7f4c9b9e8e40>\\n>>> secrets.create_secret()\\n<Secret: secret:0x7f4c9b9e8e40>\\n>>> secrets.create_secret()\\n<Secret: secret:0x7f4c9b9e8e40>\\n>>> secrets.create_secret()\\n<Secret: secret:0x7f4c9b9e8e40>\\n\\nimport android.app.Activity;\\nimport android.content.Context;\\nimport android.os.Bundle;\\nimport android.util.Log;\\n\\npublic class SimpleExampleActivity extends Activity {\\n    private static final String TAG = \"SimpleExampleActivity\";\\n    private SecretProvider secretProvider;\\n\\n    @Override\\n    public void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.main);\\n        Log.d(TAG, \"SimpleExampleActivity onCreate\");\\n        Log.d(TAG, \"SimpleExampleActivity onCreate\");\\n        Log.d(TAG, \"SimpleExampleActivity onCreate\");\\n        Log.d(TAG, \"SimpleExampleActivity onCreate\");\\n        Log.d(TAG, \"SimpleExampleActivity onCreate\");\\n\\n        \/\/ Create a new provider:\\n        \/\/ This method returns a singleton instance of {@link #secretProvider}\\n        \/\/ The method should return {@link #null} if {@link #secretProvider} does not exist,\\n        \/\/ or {@link #null} if {@link #secretProvider} does exist but is not valid.\\n\\n        SecretProvider secretProvider = new SecretProvider();\\n        final Activity activity = this;\\n        final Context context = activity.getApplicationContext();\\n        final Bundle bundle = activity.getIntent().getExtras();\\n\\n        if (bundle != null) {\\n            Bundle extras = bundle.getExtras();\\n            if (extras != null) {\\n                String name = extras.getString(\"name\");\\n                String value = extras.getString(\"value\");\\n\\n                if (name != null && value != null) {\\n                    \/\/ If name equals \"name\" or value equals \"value\", set it up:\\n                    String keyName = name.substring(0,\\n                            name.length() - name.length() - 5);\\n                    String keyValue = value.substring(name.length() + 6,\\n                            keyValue.length());\\n\\n                    Bundle keyBundle = extras.getBundle(name);\\n\\n                    if (keyBundle != null) {\\n                        Key key =\\n                                keyBundle.getKey(keyName);\\n                        if (key == null) {\\n                            \/\/ If there was no key with that name,\\n                            \/\/ create a new one:\\n                            KeyBuilder builder =\\n                                    new KeyBuilder(keyName);\\n\\n                            builder.setDefaultValue(value);\\n                            builder.setValue(keyValue);\\n\\n                            \/\/ Set up the provider:\\n                            try {\\n                                secretProvider =\\n                                    new SecretProvider(activity,\\n                                            bundle);\\n                                log.info(\"Setting up Provider:\", name + \"=\" + value);\\n                                try {\\n                                    activity.startService(new Intent(activity,\\n                                            Provider.class));\\n                                } catch (Exception e) {\\n                                    e.printStackTrace();\\n                                }\\n                            } catch (SecurityException e) {\\n                                e.printStackTrace();\\n                            }\\n                        } else if (keyBundle.containsKey(keyName)) {\\n                            log.info(\"Key {} already exists\",\\n                                   keyName);\\n                        }\\n                    }\\n                }\\n            }\\n            else { \\n                log.error(\"No provider found\", bundle.getExtras());\\n            }\\n            else { \\n                log.error(\"No provider found\", bundle.getExtras());\\n            }\\n\\n            \/\/ Set up a singleton instance of {@link #provider}.\\n            final Provider provider =\\n                new Provider(activity,\\n                        Bundle.parse(bundle));\\n\\n            provider.setCredentials(new Credential[] {new Credential(\\n                    CredentialManager.getDefaultCredentials()).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory).createCredential(\\n                        credentialFactory)).setCredentials(new Credential[] {credential});\\n            provider.setService(service);\\n            provider.setCallback(callback);\\n\\n            try {\\n                activity.startService(provider);\\n\\n                \/* TODO : Add code here *\/\\n            } catch (SecurityException e) {\\n\\n            }\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/ TODO : Add code here\\n\\n\/\/TODO : Add code here}\\n\\n    }\\n\\n    public void onReceive(Context context, Intent intent) {\\n\\n    }\\n\\n}\\n```\\n\\n"}
{"File Name":"edgex-docs\/0017-consul-security.md","Context":"## Context\\nThis ADR defines the motiviation and approach used to secure access\\nto the Consul component in the EdgeX architecture\\nfor *security-enabled configurations only*.\\nNon-secure configuations continue to use Consul in\\nanonymous read-write mode.\\nAs this Consul security feature requires Vault to function,\\nif `EDGEX_SECURITY_SECRET_STORE=false` and Vault is not present,\\nthe legacy behavior (unauthenticated Consul access) will be preserved.\\nConsul provides several services for the EdgeX architecture:\\n- Service registry (see ADR in references below)\\n- Service health monitoring\\n- Mutable configuration data\\nUse of the services provided by Consul is optional on a service-by-service basis.\\nUse of the registry is controlled by the `-r` or `--registry` flag provided to an EdgeX service.\\nUse of mutable configuration data is controlled by the `-cp` or `--configProvider` flag provided to an EdgeX service.\\nWhen Consul is enabled as a configuration provider,\\nthe `configuration.toml` is parsed into individual settings\\nand seeded into the Consul key-value store on the first start of a service.\\nConfiguration reads and writes are then done to Consul if it is specified as the configuration provider,\\notherwise the static `configuration.toml` is used.\\nWrites to the `[Writable]` section in Consul trigger per-service callbacks\\nnotifying the application of the changed data.\\nUpdates to non-`[Writable]` sections are parsed only once at startup\\nand require a service restart to take effect.\\nSince configuration data can affect the runtime behavior of services,\\ncompensating controls must be introduced in order to mitigate the risks introduced\\nby moving configuration from a static file into to an HTTP-accessible service with mutable state.\\nThe current practice is that Consul is exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\n","Decision":"Consul will be configured with access control list (ACL) functionality enabled,\\nand each EdgeX service will utilize a Consul access token to authenticate to Consul.\\nConsul access tokens will be requested from the Vault Consul secrets engine\\n(to avoid introducing additional bootstrapping secrets).\\nDNS will be disabled via configuration as it is not used in EdgeX.\\n**Consul Access Via API Gateway**\\nIn security enabled EdgeX, the API gateway will be configured to\\nproxy the Consul service over the `\/consul` path,\\nusing the `request-transformer` plugin\\nto add the global management token to incoming requests\\nvia the `X-Consul-Token` HTTP header.\\nThus, ability to access remote APIs also grants the ability\\nto modify Consul's key-value store.\\nAt this time, service access via API gateway is all-or-nothing,\\nbut this does not preclude future fine-grained authorization\\nat the API gateway layer to specific microservices, including Consul.\\nProxying of the Consul UI is problematic and there is no current solution,\\nwhich would involve proper balacing of the externally-visible URL,\\nthe path-stripping effect (or not) of the proxy,\\nConsul's `ui_content_path`,\\nand UI authentication\\n(the `request-transfomer` does not work on the UI).\\n","tokens":409,"id":949,"Predictions":"## Decision\\nThe decision is to use Consul as the configuration provider for the EdgeX architecture.\\nThe decision is to expose Consul via unencrypted HTTP in anonymous read-only mode to all processes and EdgeX services running on the host machine.\\nIf `EDGEX_SECURITY_SECRET_STORE=false`, Vault will be used instead.\\nIf `EDGEX_SECURITY_SECRET_STORE=true` and Vault is not present, the legacy behavior (unauthenticated Consul access) will be preserved.\\nConsul provides several services for the EdgeX architecture:\\n- Service registry (see ADR in references below)\\n- Service health monitoring\\n- Mutable configuration data\\nUse of the services provided by Consul is optional on a service-by-service basis.\\nUse of the registry is controlled by the `-r` or `--registry` flag provided to an EdgeX service.\\nUse of mutable configuration data is controlled by the `-cp` or `--configProvider` flag provided to an EdgeX service.\\nWhen Consul is enabled as a configuration provider,\\nthe `configuration.toml` is parsed into individual settings\\nand seeded into the Consul key-value store on the first start of a service.\\nConfiguration reads and writes are then done to Consul if it is specified as the configuration provider,\\nto all processes and EdgeX services running on the host machine.\\n\\n## Context\\nThis ADR defines how to configure Vault in order to provide secure secure access\\towards Vault via HTTP in anonymous read-only mode. This configuation must be included\\tin each EdgeX application that wishes to access Vault through HTTP. The configuation must provide\\tVault with \\tsecure credentials\\tnecessary for authentication, such as \\tvault\\_secret\\_store\\tnor vault\\_secret\\_store\\_secret. This configuation must also provide Vault with\\tthe \\tvault\\_config\\_provider\\tnor vault\\_config\\_provider\\tnor vault\\_configProviderSecret.\\n## Decision\\n## Decision\\nThe decision is to configure Vault in order to provide secure secure access towards Vault via HTTP in anonymous read-only mode. This configuation must be included in each EdgeX application that wishes to access Vault through HTTP. The configuation must provide Vault with secure credentials necessary for authentication, such as vault\\_secret\\_store or vault\\_secret\\_store\\_secret. This configuation must also provide Vault with vault\\_config\\_provider or vault\\_config\\_provider secret.\\n\\end{document}\\n"}
{"File Name":"edgex-docs\/0011-DeviceService-Rest-API.md","Context":"## Context\\nThis ADR details the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\\n","Decision":"### Common endpoints\\nThe DS should provide the REST endpoints that are expected of all EdgeX microservices, specifically:\\n* *config*\\n* *metrics*\\n* *ping*\\n* *version*\\n### Callback\\n| Endpoint | Methods\\n| --- | ---\\n| *callback\/device* | `PUT` and `POST`\\n| *callback\/device\/name\/{name}* | `DELETE`\\n| *callback\/profile* | `PUT`\\n| *callback\/watcher* | `PUT` and `POST`\\n| *callback\/watcher\/name\/{name}* | `DELETE`\\n| parameter | meaning\\n| --- | ---\\n| *{name}* | the name of the device or watcher\\nThese endpoints are used by the Core Metadata service to inform the device service of metadata updates. Endpoints are defined for each of the objects of interest to a device service, ie Devices, Device Profiles and Provision Watchers. On receipt of calls to these endpoints the device service should update its internal state accordingly. Note that the device service does not need to be informed of the creation or deletion of device profiles, as these operations may only occur where no devices are associated with the profile. To avoid stale profile entries the device service should delete a profile from its cache when the last device using it is deleted.\\n#### Object deletion\\nWhen an object is deleted, the Metadata service makes a `DELETE` request to the relevant *callback\/{type}\/name\/{name}* endpoint.\\n#### Object creation and updates\\nWhen an object is created or updated, the Metadata service makes a `POST` or `PUT` request respectively to the relevant *callback\/{type}* endpoint. The payload of the request is the new or updated object, ie one of the Device, DeviceProfile or ProvisionWatcher DTOs.\\n### Device\\n| Endpoint | Methods\\n| --- | ---\\n| *device\/name\/{name}\/{command}* | `GET` and `PUT`\\n| parameter | meaning\\n| --- | ---\\n| *{name}* | the name of the device\\n| *{command}* | the command name\\nThe command specified must match a deviceCommand or deviceResource name in the device's profile\\n**body** (for `PUT`): An `application\/json` SettingRequest, which is a set of key\/value pairs where the keys are valid deviceResource names, and the values provide the command argument for that resource. Example: `{\"AHU-TargetTemperature\": \"28.5\", \"AHU-TargetBand\": \"4.0\"}`\\n| Return code | Meaning\\n| --- | ---\\n| **200** | the command was successful\\n| **404** | the specified device does not exist, or the command\/resource is unknown\\n| **405** | attempted write to a read-only resource\\n| **423** | the specified device is locked (admin state) or disabled (operating state)\\n| **500** | the device driver is unable to process the request\\n**response body**: A successful `GET` operation will return a JSON-encoded EventResponse object, which contains one or more Readings. Example: `{\"apiVersion\":\"v2\",\"deviceName\":\"Gyro\",\"origin\":1592405201763915855,\"readings\":[{\"deviceName\":\"Gyro\",\"name\":\"Xrotation\",\"value\":\"124\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"deviceName\":\"Gyro\",\"name\":\"Yrotation\",\"value\":\"-54\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"deviceName\":\"Gyro\",\"name\":\"Zrotation\",\"value\":\"122\",\"origin\":1592405201763915855,\"valueType\":\"int32\"}]}`\\nThis endpoint is used for obtaining readings from a device, and for writing settings to a device.\\n#### Data formats\\nThe values obtained when readings are taken, or used to make settings, are expressed as strings.\\n| Type | EdgeX types | Representation\\n| --- | --- | ---\\n| Boolean | `Bool` | \"true\" or \"false\"\\n| Integer | `Uint8-Uint64`, `Int8-Int64` | Numeric string, eg \"-132\"\\n| Float | `Float32`, `Float64` | Decimal with exponent, eg \"1.234e-5\"\\n| String | `String` | string\\n| Binary | `Bytes` | octet array\\n| Array | `BoolArray`, `Uint8Array-Uint64Array`, `Int8Array-Int64Array`, `Float32Array`, `Float64Array` | JSON Array, eg \"[\"1\", \"34\", \"-5\"]\"\\nNotes:\\n- The presence of a Binary reading will cause the entire Event to be encoded using CBOR rather than JSON\\n- Arrays of String and Binary data are not supported\\n#### Readings and Events\\nA Reading represents a value obtained from a deviceResource. It contains the following fields\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the device\\n| *profileName* | The name of the Profile describing the Device\\n| *resourceName* | The name of the deviceResource\\n| *origin* | A timestamp indicating when the reading was taken\\n| *value* | The reading value\\n| *valueType* | The type of the data\\nOr for binary Readings, the following fields\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the device\\n| *profileName* | The name of the Profile describing the Device\\n| *resourceName* | The name of the deviceResource\\n| *origin* | A timestamp indicating when the reading was taken\\n| *binaryValue* | The reading value\\n| *mediaType* | The MIME type of the data\\nAn Event represents the result of a `GET` command. If the command names a deviceResource, the Event will contain a single Reading. If the command names a deviceCommand, the Event will contain as many Readings as there are deviceResources listed in the deviceCommand.\\nThe fields of an Event are as follows:\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the Device from which the Readings are taken\\n| *profileName* | The name of the Profile describing the Device\\n| *origin* | The time at which the Event was created\\n| *readings* | An array of Readings\\n#### Query Parameters\\nCalls to the device endpoints may include a Query String in the URL. This may be used to pass parameters relating to the request to the device service. Individual device services may define their own parameters to control specific behaviors. Parameters beginning with the prefix `ds-` are reserved to the Device SDKs and the following parameters are defined for GET requests:\\n| Parameter | Valid Values      | Default | Meaning\\n| --- |-------------------|---------| ---\\n| *ds-pushevent* | \"true\" or \"false\" | \"false\" | If set to true, a successful `GET` will result in an event being pushed to the EdgeX system\\n| *ds-returnevent* | \"true\" or \"false\" | \"true\"  | If set to false, there will be no Event returned in the http response\\n!!! edgey \"EdgeX 3.0\"\\nThe valid values of **ds-pushevent** and **ds-returnevent** is changed to `true\/false` instead of `yes\/no` in EdgeX 3.0.\\n#### Device States\\nA Device in EdgeX has two states associated with it: the Administrative state and the Operational state. The Administrative state may be set to `LOCKED` (normally `UNLOCKED`) to block access to the device for administrative reasons. The Operational state may be set to `DOWN` (normally `UP`) to indicate that the device is not currently working. In either case access to the device via this endpoint will be denied and HTTP 423 (\"Locked\") will be returned.\\n#### Data Transformations\\nA number of simple data transformations may be defined in the deviceResource. The table below shows these transformations in the order in which they are applied to outgoing data, ie Readings. The transformations are inverted and applied in reverse order for incoming data.\\n| Transform | Applicable reading types | Effect\\n| --- | --- | ---\\n**mask** | Integers | The reading is masked (bitwise-and operation) with the specified value.\\n**shift** | Integers | The reading is bit-shifted by the specified value. Positive values indicate right-shift, negative for left.\\n**base** | Integers and Floats | The reading is replaced by the specified value raised to the power of the reading.\\n**scale** | Integers and Floats | The reading is multiplied by the specified value.\\n**offset** | Integers and Floats | The reading is increased by the specified value.\\nThe operation of the **mask** transform on incoming data (a setting) is that the value to be set on the resource is the existing value bitwise-anded with the complement of the mask, bitwise-ored with the value specified in the request.\\nie, `new-value = (current-value & !mask) | request-value`\\nThe combination of mask and shift can therefore be used to access data contained in a subdivision of an octet.\\nIt is possible that following the application of the specified transformations, a value may exceed the range that may be represented by its type. Should this occur on a set operation, a suitable error should be logged and returned, along with the `Bad Request` http code 400. If it occurs as part of a get operation, the Reading's value should be set to the String `\"overflow\"` and its valueType to `String`.\\n#### Assertions and Mappings\\nAssertions are another attribute in a device resource's PropertyValue, which specify a string which the reading value is compared against. If the comparison fails, then the http request returns a string of the form *\"Assertion failed for device resource: \\<name>, with value: \\<value>\"*, this also has a side-effect of setting the device operatingstate to `DISABLED`. A 500 status code is also returned. Note that the error response and status code should be returned regardless of the `ds-returnevent` setting.\\nAssertions are also checked where an event is being generated due to an AutoEvent, or asynchronous readings are pushed. In these cases if the assertion is triggered, an error should be logged and the operating state should be set as above.\\nAssertions are not checked for settings, only for readings.\\nMappings may be defined in a deviceCommand. These allow Readings of string type to be remapped. Mappings are applied after assertions are checked, and are the final transformation before Readings are created. Mappings are also applied, but in reverse, to settings (`PUT` request data).\\n#### lastConnected timestamp\\nEach Device has as part of its metadata a timestamp named `lastConnected`, this\\nindicates the most recent occasion when the device was successfully interacted\\nwith. The device service should update this timestamp every time a GET or PUT\\noperation succeeds, unless it has been configured not to do so (eg for\\nperformance reasons).\\n### Discovery\\n| Endpoint | Methods\\n| --- | ---\\n| *discovery* | `POST`\\nA call to this endpoint triggers the device discovery process, if enabled. See\\n[Discovery Design](..\/..\/legacy-design\/device-service\/discovery.md) for details.\\n","tokens":72,"id":950,"Predictions":"## The ADR is adopted by the Device Service Implementation team, and the details of this decision are given in the following section.\\n\\n\\n## Decision:\\n\\nThis ADR specifies the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\\n\\n\\n## Decision:\\n\\nThis ADR specifies the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\\n\\n\\n## Context\\n## The ADR is adopted by the Device Service Implementation team, and the details of this decision are given in the following section.\\n## Decision:\\n\\nThis ADR specifies the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\\n```\\nI am not sure what is expected from me here.\\n```\\nWhat I have done so far:\\n```\\npackage com.example.android.device.service;\\n\\nimport android.content.Context;\\nimport android.net.Uri;\\nimport android.os.Bundle;\\nimport android.util.Log;\\n\\npublic class MainActivity extends Activity {\\n\\n    private static final String TAG = MainActivity.class.getSimpleName();\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.activity_main);\\n\\n        Uri uri = Uri.parse(\"https:\/\/device.service.android.com\/device\/endpoint\");\\n        Log.d(TAG, uri.toString());\\n    }\\n\\n}\\n```\\nI have tried both `deviceService` and `deviceServiceEndpoint`, but I get different results.\\nWhat I am looking for:\\nHow can I implement a REST API for my device service?\\nOP 2017-01-25: After some research I found that `deviceService` is deprecated now and replaced with `deviceServiceEndpoint`.\\nI updated my code like this:\\n```\\npackage com.example.android.device.service;\\n\\nimport android.content.Context;\\nimport android.net.Uri;\\nimport android.os.Bundle;\\nimport android.util.Log;\\n\\npublic class MainActivity extends Activity {\\n\\n    private static final String TAG = MainActivity.class.getSimpleName();\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.activity_main);\\n\\n        Uri uri = Uri.parse(\"https:\/\/device.service.android.com\/device\/endpoint\");\\n        Log.d(TAG, uri.toString());\\n    }\\n\\n}\\n```"}
{"File Name":"edgex-docs\/0016-docker-image-guidelines.md","Context":"## Context\\nWhen deploying the EdgeX Docker containers some security measures are recommended to ensure the integrity of the software stack.\\n","Decision":"When deploying Docker images, the following flags should be set for heightened security.\\n- To avoid escalation of privileges each docker container should use the `no-new-privileges` option in their Docker compose file (example below). More details about this flag can be found [here](https:\/\/docs.docker.com\/engine\/reference\/run\/#security-configuration). This follows Rule #4 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-4-add-no-new-privileges-flag).\\n```docker\\nsecurity_opt:\\n- \"no-new-privileges:true\"\\n```\\n> NOTE: Alternatively an AppArmor security profile can be used to isolate the docker container. More details about apparmor profiles can be found [here](https:\/\/docs.docker.com\/engine\/security\/apparmor\/)\\n```docker\\nsecurity_opt:  [ \"apparmor:unconfined\" ]\\n```\\n- To further prevent privilege escalation attacks the user should be set for the docker container using the `--user=<userid>` or `-u=<userid>` option in their Docker compose file (example below). More details about this flag can be found [here](https:\/\/docs.docker.com\/engine\/reference\/run\/#user). This follows Rule #2 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\\n```docker\\nservices:\\ndevice-virtual:\\nimage: ${REPOSITORY}\/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\\nuser: $CONTAINER-PORT:$CONTAINER-PORT # user option using an unprivileged user\\nports:\\n- \"127.0.0.1:49990:49990\"\\ncontainer_name: edgex-device-virtual\\nhostname: edgex-device-virtual\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-virtual\\ndepends_on:\\n- consul\\n- data\\n- metadata\\n```\\n> NOTE: exception\\nSometimes containers will require root access to perform their fuctions. For example the System Management Agent requires root access to control other Docker containers. In this case you would allow it run as default root user.\\n- To avoid a faulty or compromised containers from consuming excess amounts of the host of its resources `resource limits` should be set for each container. More details about `resource limits` can be found [here](https:\/\/docs.docker.com\/config\/containers\/resource_constraints\/). This follows Rule #7 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-7-limit-resources-memory-cpu-file-descriptors-processes-restarts).\\n```docker\\nservices:\\ndevice-virtual:\\nimage: ${REPOSITORY}\/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\\nuser: 4000:4000 # user option using an unprivileged user\\nports:\\n- \"127.0.0.1:49990:49990\"\\ncontainer_name: edgex-device-virtual\\nhostname: edgex-device-virtual\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-virtual\\ndepends_on:\\n- consul\\n- data\\n- metadata\\ndeploy:  # Deployment resource limits\\nresources:\\nlimits:\\ncpus: '0.001'\\nmemory: 50M\\nreservations:\\ncpus: '0.0001'\\nmemory: 20M\\n```\\n- To avoid attackers from writing data to the containers and modifying their files the `--read_only` flag should be set. More details about this flag can be found [here](https:\/\/docs.docker.com\/compose\/compose-file\/#domainname-hostname-ipc-mac_address-privileged-read_only-shm_size-stdin_open-tty-user-working_dir). This follows Rule #8 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-8-set-filesystem-and-volumes-to-read-only).\\n```docker\\ndevice-rest:\\nimage: ${REPOSITORY}\/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\\nports:\\n- \"127.0.0.1:49986:49986\"\\ncontainer_name: edgex-device-rest\\nhostname: edgex-device-rest\\nread_only: true # read_only option\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-rest\\ndepends_on:\\n- data\\n- command\\n```\\n> NOTE: exception\\nIf a container is required to have write permission to function, then this flag will not work. For example, the vault needs to run setcap in order to lock pages in memory. In this case the `--read_only` flag will not be used.\\nNOTE: Volumes\\nIf writing persistent data is required then a volume can be used. A volume can be attached to the container in the following way\\n```docker\\ndevice-rest:\\nimage: ${REPOSITORY}\/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\\nports:\\n- \"127.0.0.1:49986:49986\"\\ncontainer_name: edgex-device-rest\\nhostname: edgex-device-rest\\nread_only: true # read_only option\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-rest\\ndepends_on:\\n- data\\n- command\\nvolumes:\\n- consul-config:\/consul\/config:z\\n```\\n> NOTE: alternatives\\nIf writing non-persistent data is required (ex. a config file) then a temporary filesystem mount can be used to accomplish this goal while still enforcing `--read_only`. Mounting a `tmpfs` in Docker gives the container a temporary location in the host systems memory to modify files. This location will be removed once the container is stopped. More details about `tmpfs` can be found [here](https:\/\/docs.docker.com\/storage\/tmpfs\/)\\nfor additional docker security rules and guidelines please check the Docker security [cheatsheet](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html)\\n","tokens":25,"id":952,"Predictions":"## \\n\\nNote: This decision is only valid when the container is running in a sandbox environment.\\n# Answer\\n\\n\\# \\n\\n\\end{DoxyCode}\\n"}
{"File Name":"edgex-docs\/013-Device-Service-Events-Message-Bus.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n","Decision":"* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n### Which Message Bus implementations?\\nMultiple Device Services may need to be publishing Events to the MessageBus concurrently.  `ZMQ` will not be a valid option if multiple Device Services are configured to publish. This is because `ZMQ` only allows for a single publisher. `ZMQ` will still be valid if only one Device Service is publishing Events. The `MQTT` and `Redis Streams` are valid options to use when multiple Device Services are required, as they both support multiple publishers. These are the only other implementations currently available for Go services. The C base device services do not yet have a MessageBus implementation.  See the [C Device SDK](#c-device-sdk) below for details.\\n> *Note: Documentation will need to be clear when `ZMQ` can be used and when it can not be used.*\\n### Go Device SDK\\nThe Go Device SDK will take advantage of the existing `go-mod-messaging` module to enable use of the EdgeX MessageBus. A new bootstrap handler will be created which initializes the MessageBus client based on configuration. See [Configuration](#configuration) section below for details.  The Go Device SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### C Device SDK\\nThe C Device SDK will implement its own MessageBus abstraction similar to the one in `go-mod-messaging`.  The first implementation type (MQTT or Redis Streams) is TBD. Using this abstraction allows for future implementations to be added when use cases warrant the additional implementations.  As with the Go SDK, the C SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### Core Data and Persistence\\nWith this design, Events will be sent directly to Application Services w\/o going through Core Data and thus will not be persisted unless changes are made to Core Data. To allow Events to optionally continue to be persisted, Core Data will become an additional or secondary (and optional) subscriber for the Events from the MessageBus. The Events will be persisted when they are received. Core Data will also retain the ability to receive Events via HTTP, persist them and publish them to the MessageBus as is done today. This allows for the flexibility to have some device services to be configured to POST Events and some to be configured to publish Events while we transition the Device Services to all have the capability to publishing Events. In the future, once this new `Publish` approach has been proven, we may decide to remove POSTing Events to Core Data from the Device SDKs.\\nThe existing `PersistData` setting will be ignored by the code path subscribing to Events since the only reason to do this is to persist the Events.\\nThere is a race condition for `Marked As Pushed` when Core Data is persisting Events received from the MessageBus. Core Data may not have finished persisting an Event before the Application Service has processed the Event and requested the Event be `Marked As Pushed`. It was decided to remove `Mark as Pushed` capability and just rely on time based scrubbing of old Events.\\n### V2 Event DTO\\nAs this development will be part of the Ireland release all Events published to the MessageBus will use the V2 Event DTO. This is already implemented in Core Data for the V2 AddEvent API.\\n#### Validation\\nServices receiving the Event DTO from the MessageBus will log validation errors and stop processing the Event.\\n### Message Envelope\\nEdgeX Go Services currently uses a custom Message Envelope for all data that is published to the MessageBus. This envelope wraps the data with metadata, which is `ContentType` (JSON or CBOR), `Correlation-Id` and the obsolete `Checksum`. The `Checksum` is used when the data is CBOR encoded to identify the Event in V1 API to be mark it as pushed. This checksum is no longer needed as the V2 Event DTO requires the ID be set by the Device Services which will always be used in the V2 API to mark the Events as pushed. The Message Envelope will be updated to remove this property.\\nThe C SDK will recreate this Message Envelope.\\n### Application Services\\nAs part of the V2 API consumption work in Ireland the App Services SDK will be changed to expect to receive V2 Event DTOs rather than the V1 Event model. It will also be updated to no longer expect or use the `Checksum` currently on the  Message Envelope. Note these changes must occur for the V2 consumption and are not directly tied to this effort.\\nThe App Service SDK will be enhanced for the secure MessageBus connection described below. See **[Secure Connections](#secure-connections)** for details\\n### MessageBus Topics\\n> *Note: The change recommended here is not required for this design, but it provides a good opportunity to adopt it.*\\nCurrently Core Data publishes Events to the simple `events` topic. All Application Services running receive every Event published, whether they want them or not. The Events can be filtered out using the `FilterByDeviceName` or `FilterByResourceName` pipeline functions, but the Application Services still receives every Event and process all the Events to some extent. This could cause load issues in a deployment with many devices and large volume of Events from various devices or a very verbose device that the Application Services is not interested in.\\n> *Note: The current `FilterByDeviceName` is only good if the device name is known statically and the only instance of the device defined by the `DeviceProfileName`. What we really need is `FilterByDeviceProfileName` which allows multiple instances of a device to be filtered for, rather than a single instance as it it now. The V2 API will be adding `DeviceProfileName` to the Events, so in Ireland this  filter will be possible.*\\nPub\/Sub systems have advanced topic schema, which we can take advantage of from Application Services to filter for just the Events the Application Service actual wants. Publishers of Events must add the `DeviceProfileName`, `DeviceName` and `SourceName` to the topic in the form `edgex\/events\/<device-profile-name>\/<device-name>\/<source-name>`. The `SourceName` is the `Resource` or `Command` name used to create the Event. This allows Application Services to filter for just the Events from the device(s) it wants by only subscribing to those `DeviceProfileNames` or the specific `DeviceNames` or just the specific `SourceNames`  Example subscribe topics if above schema is used:\\n- **edgex\/events\/#**\\n- All Events\\n- Core Data will subscribe using this topic schema\\n- **edgex\/events\/Random-Integer-Device\/#**\\n- Any Events from devices created from the **Random-Integer-Device** device profile\\n- **edgex\/events\/Random-Integer-Device\/Random-Integer-Device1**\\n- Only Events from the **Random-Integer-Device1** Device\\n- **edgex\/events\/Random-Integer-Device\/#\/Int16**\\n- Any Events with Readings from`Int16` device resource from devices created from the **Random-Integer-Device** device profile.\\n- **edgex\/events\/Modbus-Device\/#\/HVACValues\\n- Any Events with Readings from `HVACValues` device command from devices created from the **Modbus-Device** device profile.\\nThe MessageBus abstraction allows for multiple subscriptions, so an Application Service could specify to receive data from multiple specific device profiles or devices by creating multiple subscriptions. i.e.  `edgex\/Events\/Random-Integer-Device\/#` and  `edgex\/Events\/Random-Boolean-Device\/#`. Currently the App SDK only allows for a single subscription topic to be configured, but that could easily be expanded to handle a list of subscriptions. See [Configuration](#configuration) section below for details.\\nCore Data's existing publishing of Events would also need to be changed to use this new topic schema. One challenge with this is Core Data doesn't currently know the `DeviceProfileName` or `DeviceName` when it receives a CBOR encoded event. This is because it doesn't decode the Event until after it has published it to the MessageBus. Also, Core Data doesn't know of `SourceName` at all. The V2 API will be enhanced to change the AddEvent endpoint from `\/event` to `\/event\/{profile}\/{device}\/{source}` so that `DeviceProfileName`, `DeviceName`, and `SourceName` are always know no matter how the request is encoded.\\nThis new topic approach will be enabled via each publisher's `PublishTopic` having the `DeviceProfileName`, `DeviceName`and `SourceName`  added to the configured `PublishTopicPrefix`\\n```toml\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n```\\nSee [Configuration](#configuration) section below for details.\\n### Configuration\\n#### Device Services\\nAll Device services will have the following additional configuration to allow connecting and publishing to the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopic` will include the `DeviceProfileName` and `DeviceName`.\\n##### [MessageQueue]\\nA  MessageQueue section will be added, which is similar to that used in Core Data today, but with `PublishTopicPrefix` instead of `Topic`.To enable secure connections, the `Username` & `Password` have been replaced with ClientAuth & `SecretPath`, See **[Secure Connections](#secure-connections)** section below for details. The added `Enabled` property controls whether the Device Service publishes to the MessageBus or POSTs to Core Data.\\n```toml\\n[MessageQueue]\\nEnabled = true\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable environment variable overrides of them\\n# Client Identifiers\\nClientId =\"<device service key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Core Data\\nCore data will also require additional configuration to be able to subscribe to receive Events from the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopicPrefix` will have `DeviceProfileName` and `DeviceName` added to create the actual Public Topic.\\n##### [MessageQueue]\\nThe `MessageQueue` section will be  changed so that the `Topic` property changes to `PublishTopicPrefix` and `SubscribeEnabled` and `SubscribeTopic` will be added. As with device services configuration, the `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details. In addition, the Boolean `SubscribeEnabled` property will be used to control if the service subscribes to Events from the MessageBus or not.\\n```toml\\n[MessageQueue]\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\nSubscribeEnabled = true\\nSubscribeTopic = \"edgex\/events\/#\"\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable evnironment variable overrides of them\\n# Client Identifiers\\nClientId =\"edgex-core-data\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Application Services\\n##### [MessageBus]\\nSimilar to above, the Application Services `MessageBus` configuration will change to allow for secure connection to the MessageBus. The `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details.\\n```toml\\n[MessageBus.Optional]\\n# MQTT Specific options\\n# Client Identifiers\\nClientId =\"<app sevice key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n##### [Binding]\\nThe `Binding` configuration section will require changes for the subscribe topics scheme described in the [MessageBus Topics](#messagebus-topics) section above to filter for Events from specific device profiles or devices. `SubscribeTopic` will change from a string property containing a single topic to the `SubscribeTopics` string property containing a comma separated list of topics. This allows for the flexibility for the property to be a single topic with the `#` wild card so the Application Service receives all Events as it does today.\\nReceive only Events from the `Random-Integer-Device` and `Random-Boolean-Device` profiles\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device, edgex\/events\/Random-Boolean-Device\"\\n```\\nReceive only Events from the  `Random-Integer-Device1` from the `Random-Integer-Device` profile\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device\/Random-Integer-Device1\"\\n```\\nor receives all Events:\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/#\"\\n```\\n### Secure Connections\\nAs stated earlier,  this ADR is dependent on the  **Secret Provider for All**(Link TBD) ADR to provide a common Secret Provider for all Edgex Services to access their secrets. Once this is available, the MessageBus connection can be secured via the following configurable client authentications modes which follows similar implementation for secure MQTT Export and secure MQTT Trigger used in Application Services.\\n- **none** - No authentication\\n- **usernamepassword** - Username & password authentication.\\n- **clientcert** - Client certificate and key for authentication.\\n- The secrets specified for the above options are pulled from the `Secret Provider` using the configured `SecretPath`.\\nHow the secrets are injected into the `Secret Provider` is out of scope for this ADR and covered in the **Secret Provider for All**( Link TBD) ADR.\\n","tokens":227,"id":954,"Predictions":"- [Which Message Bus implementations?](#which-message-bus-implementations)\\n- [Go Device SDK](#go-device-sdk)\\n- [C Device SDK](#c-device-sdk)\\n- [Core Data and Persistence](#core-data-and-persistence)\\n- [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\nThe user needs to provide a decision for each of the following:\\n\\n- Which Message Bus implementations?\\n- Go Device SDK, C Device SDK, Core Data and Persistence, V2 Event DTO, Validation, Message Envelope, Application Services, MessageBus Topics, Configuration.\\n\\nThe user needs to provide a decision for each of the following:\\n\\n- Which Message Bus implementations?\\n- Go Device SDK, C Device SDK, Core Data and Persistence, V2 Event DTO, Validation, Message Envelope, Application Services, MessageBus Topics, Configuration.\\n\\nThe user needs to provide a decision for each of the following:\\n\\n- Which Message Bus implementations?\\n- Go Device SDK, C Device SDK, Core Data and Persistence, V2 Event DTO,\\nValidation,\\nMessage Envelope,\\nApplication Services,\\nMessageBus Topics,\\nConfiguration.\\n\\nThe user needs to provide a decision for each of the following:\\n\\n- Which Message Bus implementations?\\n- Go Device SDK,\\nC Device SDK,\\nCore Data and Persistence,\\nV2 Event DTO,\\nValidation,\\nMessage Envelope,\\nApplication Services,\\nMessageBus Topics,\\nConfiguration.\\n\\nThe user needs to provide a decision for each of the following:\\n\\n- Which Message Bus implementations?\\n- Go Device SDK,\\nC Device SDK,\\nCore Data and Persistence,\\n\\nV2 Event DTO,\\n\\nValidation,\\n\\nMessage Envelope,\\n\\nApplication Services,\\n\\nMessageBus Topics,\\n\\nConfiguration.\\n\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n"}
{"File Name":"edgex-docs\/0009-Secure-Bootstrapping.md","Context":"* [Context](#context)\\n+ [History](#history)\\n* [Decision](#decision)\\n+ [Stage-gate mechanism](#stage-gate-mechanism)\\n+ [Docker-specific service changes](#docker-specific-service-changes)\\n- [\"As-is\" startup flow](#as-is-startup-flow)\\n- [\"To-be\" startup flow](#to-be-startup-flow)\\n- [New Bootstrap\/RTR container](#new-bootstraprtr-container)\\n* [Consequences](#consequences)\\n+ [Benefits](#benefits)\\n+ [Drawbacks](#drawbacks)\\n* [Alternatives](#alternatives)\\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\\n+ [Create a mega-install container](#create-a-mega-install-container)\\n+ [Manual secret provisioning](#manual-secret-provisioning)\\n* [References](#references)\\n","Decision":"+ [Stage-gate mechanism](#stage-gate-mechanism)\\n+ [Docker-specific service changes](#docker-specific-service-changes)\\n- [\"As-is\" startup flow](#as-is-startup-flow)\\n- [\"To-be\" startup flow](#to-be-startup-flow)\\n- [New Bootstrap\/RTR container](#new-bootstraprtr-container)\\n* [Consequences](#consequences)\\n+ [Benefits](#benefits)\\n+ [Drawbacks](#drawbacks)\\n* [Alternatives](#alternatives)\\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\\n+ [Create a mega-install container](#create-a-mega-install-container)\\n+ [Manual secret provisioning](#manual-secret-provisioning)\\n* [References](#references)\\n### Stage-gate mechanism\\nThe stage-gate mechanism must work in the following environments:\\n* docker-compose in Linux on a single node\/system\\n* docker-compose in Microsoft* Windows* on a single node\/system\\n* docker-compose in Apple* MacOS* on a single node\/system\\nStartup sequencing will be driven by two primary mechanisms:\\n1. Use of entrypoint scripts to:\\n- Block on stage-gate and service dependencies\\n- Perform first-boot initialization phase activities as noted in [Context](#context)\\nThe bootstrap container will inject entrypoint scripts into\\nthe other containers in the case where EdgeX is directly consuming\\nan upstream container.  Docker will automatically retry\\nrestarting containers if its entrypoint script is missing.\\n2. Use of open TCP sockets as semaphores to gate startup sequencing\\nUse of TCP sockets for startup sequencing is commonly used in Docker environments.\\nDue to its popularlity, there are several existing tools for this, including\\n[wait-for-it](https:\/\/github.com\/vishnubob\/wait-for-it),\\n[dockerize](https:\/\/github.com\/jwilder\/dockerize), and\\n[wait-for](https:\/\/github.com\/Eficode\/wait-for).\\nThe TCP mechanism is portable across platforms\\nand will work in distributed multi-node scenarios.\\nAt least three new ports will be added to EdgeX for sequencing purposes:\\n* `bootstrap` port. This port will be opened once first-time initialization has been completed.\\n* `tokens_ready` port. This port signals that secret-store tokens have been provisioned and are valid.\\n* `ready_to_run` port. This port will be opened once stateful services have completed initialization\\nand it is safe for the majority of EdgeX core services to start.\\nThe stateless EdgeX services should block on `ready_to_run` port.\\n### Docker-specific service changes\\n#### \"As-is\" startup flow\\nThe following diagram shows the \"as-is\" startup flow.\\n![AS-IS](.\/bootstrap-as-is.jpg)\\nThere are several components being removed via activity unrelated with this ADR.\\nThese proposed edits are shown to reduce clutter in the TO-BE diagram.\\n* secrets-setup is being eliminated through a separate ADR to eliminate TLS for single-node usage.\\n* kong-migrations is being combined with the kong service via an entrypoint script.\\n* bootstrap-redis will be incorporated into the Redis entrypoint script to set the Redis password before Redis starts to fix the time delay before a Redis password is set.\\n#### \"To-be\" startup flow\\nThe following diagram shows the \"to-be\" startup flow.\\nNote that the bootstrap flows are always processed,\\nbut can be short-circuited.\\n![TO-BE](.\/bootstrap-to-be.jpg)\\nAnother difference to note in the \"to-be\" diagram is that\\nthe Vault depdendency on Consul is reversed\\n[in order to provide better security](https:\/\/github.com\/edgexfoundry\/edgex-docs\/issues\/296).\\n#### New Bootstrap\/RTR container\\nThe purpose of this new container is to:\\n* Inject entrypoint scripts into third-party containers\\n(such as Vault, Redis, Consul, PostgreSQL, Kong)\\nin order to perform first-time initialization and\\nwait on service dependencies\\n* Raise the `bootstrap` semaphore\\n* Wait on dependent semaphores required to raise the `ready_to_run` semaphore\\n(these are the stateful components such as databases,\\nand blocking waiting for sercret store tokens to be provisioned)\\n* Raise the `ready_to_run` semaphore\\n* Wait forever (in order to leave TCP sockets open)\\n","tokens":225,"id":955,"Predictions":"  * [Decision](#decision)\\n* [Stage-gate mechanism](#stage-gate-mechanism)\\n\\n## Stage-gate mechanism\\n  * [Stage-gate mechanism](#stage-gate-mechanism)\\n  * [Docker-specific service changes](#docker-specific-service-changes)\\n  * [Consequences](#consequences)\\n  * [Benefits](#benefits)\\n  * [Drawbacks](#drawbacks)\\n\\n\\n## Docker-specific service changes\\n  * [Create a mega-install container](#create-a-mega-install-container)\\n\\end{lstlisting}\\n\\nThe decision record is a Python script that can be used to perform the stage gate process. It has two stages: the first is to determine if there are any Docker-specific service changes to be made, and the second is to deploy those Docker-specific service changes. The stage gate process can be automated using a Docker-compliant orchestration tool like Kubernetes.\\n\\nIn this case, the stage gate process was performed using the Kubernetes YAML file provided by the user.\\n\\n\\begin{lstlisting}[language=k8s, caption={Kubernetes YAML file for staging container}, label={lst:k8sYAML}]\\napiVersion: v1\\nkind: Deployment\\nmetadata:\\n  name: staging-container\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: staging-container\\n  template:\\n    metadata:\\n      labels:\\n        app: staging-container\\n    spec:\\n      containers:\\n      - name: staging-container-0\\n        image: nginx:alpine\\n        ports:\\n        - containerPort: \"80\"\\n          protocol: TCP\\n          targetPort: \"80\"\\n          nodePort: \"8080\"\\n        envFrom:\\n          secretName: ${{ secrets.staging_container }}\\n        volumes:\\n        - name: nginx-data-vol-0:\/var\/run\/docker.sock:\/var\/run\/docker.sock\\n          secretName: ${{ secrets.staging_container }}\\n      - name: nginx-data-vol-1:\/var\/run\/docker.sock:\/var\/run\/docker.sock\\n\\n---\\napiVersion: apps\/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: staging-container-1\\nspec:\\n  replicas: 1\\n\\n---\\napiVersion: v1beta1 \\nkind: Service \\nmetadata:\\n    name: staging-container \\nspec:\\n    ports:\\n     - portMap:\\n\\n       - portName=80 \\n           protocol=https \\n           targetPort=80\\n\\n       - portName=8080 \\n           protocol=https \\n           targetPort=8080\\n\\n---\\napiVersion: apps\/v1beta1 \\nkind: Service \\nmetadata:\\n    name:test-app \\nspec:\\n\\n---\\napiVersion:\\n\\n---\\nkind:\\n\\n---\\nserviceName:test-app\\n\\n---\\n\\napiVersion:\\n\\n---\\nkind:\\n\\n---\\nserviceName:test-app\\n\\n---\\n\\napiVersion:\\n\\n---\\nkind:\\n\\n---\\nserviceName:test-app\\n\\n---\\n\\ndeployment.apps\/staging-container-1.yaml\\n\\ndeployment.apps\/staging-container-2.yaml\\n\\ndeployment.apps\/staging-container-3.yaml\\n\\n\\nThe YAML file for this deployment is shown below. The deployment has three components, named `staging-container`, `nginx-data-vol-0`, and `nginx-data-vol-1`. Each component has an entry in the `ports` section of the YAML file.\\n\\n\\begin{lstlisting}[language=k8s, caption={Kubernetes YAML file for each component}, label={lst:k8sYAML}]\\napiVersion:\\n\\nk8s.io\/v1alpha2\\n\\nkind:\\n\\nDeployment\\n\\nmetadata:\\n\\nname:\\n\\nstaging-container\\n\\nspec:\\n\\nreplicasCount:\\n\\n3\\n\\ntemplateSpec:\\n\\nmetadata:\\n\\nlabels:\\n\\napp-name:\"staging-container\"\\n\\nspecSpec:\\n\\ncontainers:\\n\\n- image:\"nginx:v2.4.7\" \\n\\nportsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecsPortsListPortsFromSpecs Ports list Ports list Ports list Ports list Ports list Ports list Ports list Ports list Ports list \\n\\n---\\n\\napiVersion:v1alpha2   \\nkind:svc  \\nmetadata:{  \\nannotations:{  \\n...  \\n}   \\nspec:{  \\n...  \\n}   \\nvolumes:{  \\n...  \\n}   \\nproviders:{  \\n...  \\n}   \\nservices:{  \\n...  \\n}   \\ntargets:{  \\n...  \\n}   \\nportMappings:{  \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ \\n\\n   \\ Port Name Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Protocol Target Port Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Parameter Value Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Container Name Image Image Image Image Image Image Image Image Image Volume Volume Volume Volume Volume Volume Volume Volume Volume Volume Volume Volume Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace Namespace namespace namespace namespace namespace namespace namespace namespace namespace\\n\\n\\n}\\n\\n}\\n\\end{lstlisting}\\n\\nIn this case, we have created three containers named `staging-container`, `nginx-data-vol-0`, and `nginx-data-vol-1`. Each component has an entry in the ports section of the YAML file that specifies which ports should be open.\\n\\nWe have configured a volume on each of these containers with a volume mount pointing to `\/var\/run\/docker.sock` and a volume mount pointing to `\/var\/run\/docker.sock`.\\n\\nFor more information about how volumes work in Kubernetes, see Chapter~\\ref{chap:kubesat}.\\n"}
{"File Name":"edgex-docs\/0020-spiffe.md","Context":"## Context\\nIn security-enabled EdgeX,\\nthere is a component called `security-secretstore-setup`\\nthat seeds authentication tokens for Hashicorp Vault--EdgeX's secret store--into\\ndirectories reserved for each EdgeX microservice.\\nThe implementation is provided by a sub-component, `security-file-token-provider`,\\nthat works off of a static configuration file (`token-config.json`)\\nthat configures known EdgeX services,\\nand an environment variable that lists additional services that require tokens.\\nThe token provider creates a unique token for each service\\nand attaches a custom policy to each token that limits token access\\nin a manner that paritions the secret store's namespace.\\nThe current solution has some problematic aspects:\\n* These tokens have an initial TTL of one hour (1h)\\nand become invalid if not used and renewed within that time period.\\nIt is not possible to delay the start of EdgeX services until a later time\\n(that is, greater than the default token TTL),\\nas they will not be able to connect to the EdgeX secret store\\nto obtain required secrets.\\n* Transmission of the authentication token requires one or more shared file systems\\nbetween the service and `security-secretstore-setup`.\\nIn the Docker implementation,\\nthis shared file system is constructed by bind-mounting a host-based directory\\nto multiple containers.\\nThe snap implementation is similar, utilizing a content-interface between snaps.\\nIn a Kubernetes implementation limited to a single worker node,\\na CSI storage driver that provided RWO volumes would suffice.\\n* The current approach cannot support distributed services\\nwithout an underlying distributed file system to distribute tokens,\\nsuch as GlusterFS, running across the participating nodes.\\nFor Kubernetes, the requirement would be a remote shared file system\\npersistent volume (RWX volume).\\n","Decision":"EdgeX will create a new service, `security-spiffe-token-provider`.\\nThis service will be a mutual-auth TLS service that exchanges\\na [SPIFFE](https:\/\/spiffe.io\/) X.509 SVID for a secret store token.\\nAn SPIFFE identifier is a URI of the format `spiffe:\/\/trust domain\/workload identifier`.\\nFor example: `spiffe:\/\/edgexfoundry.org\/service\/core-data`.\\nA SPIFFE Verifiable Identity Document (SVID) is a cryptographically-signed\\nversion of a SPIFFE ID, typically a X.509 certificate with\\nthe SPIFFE ID encoded into the `subjectAltName` certificate extension,\\nor a JSON web token (encoded into the `sub` claim).\\nThe EdgeX implementation will use a naming convention on\\nthe path component, such as the above, in order to be\\nable to extract the requesting service from the SPIFFE ID.\\nThe SPIFFE token provider will take three parameters:\\n1. An X.509 SVID used in mutual-auth TLS for the token provider\\nand the service to cross-authenticate.\\n2. The reqested service key.  If blank, the service key will\\ndefault to the service name encoded in the SVID.\\nIf the service name follows the pattern `device-(name)`,\\nthen the service key must follow the format\\n`device-(name)` or `device-name-*`.\\nIf the service name is `app-service-configurable`,\\nthen the service key must follow the format `app-*`.\\n(This is an accomodation for the Unix workload attester\\nnot being able to distingish workloads that are launched\\nusing the same executable binary.\\nCustom app services that support multiple instances\\nwon't be supported unless they name the executable\\nthe same as the standard app service binary or\\nmodify this logic.)\\n3. A list of \"known secret\" identifiers that will allow\\nnew services to request database passwords or\\nother \"known secrets\" to be seeded into their service's\\npartition in the secret store.\\nThe `go-mod-secrets` module will be modified to enable a new mode\\nwhereby a secret store token is obtained by:\\n1. Obtaining an X.509 SVID by contacting a local SPIFFE agent's\\nworkload API on a local Unix domain socket.\\n2. Connecting to the `security-spiffe-token-provider` service\\nusing the X.509 SVID to request a secret store token.\\nThe SPIFFE authentication mode will be an opt-in feature.\\nThe SPIFFE implementation will be user-replaceable;\\nspecifically, the workload API socket will be configurable,\\nas well as the parsing of the SPIFFE ID.\\nReasons for doing so might include: changing the name of\\nthe trust domain in the SPIFFE ID, or moving the SPIFFE\\nserver out of the edge.\\nThis feature is estimated to be a \"large\" or \"extra large\"\\neffort that could be implemented in a single release cycle.\\n### Technical Architecture\\n![SPIFFE Architecture and Workflow](0020-spiffe-architecture.jpg)\\nThe work flow is as follows:\\n1. Create a root CA for the SPIFFE user to use for creation of sub-CA's.\\n1. The SPIFFE server is started.\\n1. The server creates a sub-CA for issuing new identities.\\n1. The trust bundle (certificate authority) data is exported from the SPIFFE server\\nand stored on a shared volume readable by other EdgeX microservices\\n(i.e. the existing secrets volume used for sharing secret store tokens).\\n1. A join token for the SPIFFE agent is created using `token generate`\\nand shared to the EdgeX secrets volume.\\n1. Workload entries are loaded into the SPIFFE server database,\\nusing the join-identity of the agent created in the previous step\\nas the parent ID of the workload.\\n1. The SPIFFE agent is started with the join token\\ncreated in a previous step to add it to the cluster.\\n1. Vault is started and `security-secret-store-setup`\\ninitializes it and creates an admin token for `security-spiffe-token-provider` to use.\\n1. The `security-spiffe-token-provider` service is started.\\nIt obtains an SVID from the SIFFE agent and uses it as a TLS server certificate.\\n1. An EdgeX microservice starts and obtains another SVID from the SPIFFE agent\\nand uses it as a TLS client certificate to contact the\\n`security-spiffe-token-provider` service.\\nThe EdgeX microservice uses the trust bundle as a server CA\\nto verify the TLS certificate of the remote service.\\n1. `security-spiffe-token-provider` verifies the SVID using the trust bundle as client CA\\nto verify the client,\\nextracts the service key,\\nand issues an appropriate Vault service token.\\n1. The EdgeX microservice accesses Vault as usual.\\n#### Workload Registration and Agent Sockets\\nThe server uses a workload registration Unix domain socket that allows\\nauthorization entries to be added to the authorization database.\\nThis socket is protected by Unix file system permissions to control\\nwho is allowed to add entries to the database.\\nIn this proposal, a subcommand will be added to the EdgeX `secrets-config`\\nutility to simplify the process of registering new services\\nthat uses the registration socket above.\\nThe agent uses a workload attesation Unix domain socket that\\nis open to the world.  This socket is shared via a snap content-interface\\nof via a shared host bind mount for Docker.  There is one agent per node.\\n#### Trust Bundle\\nSVID's must be traceable back to a known issuing authority (certificate authority)\\nto determine their validity.\\nIn the proposed implementation, we will generate a CA on first boot and store it persistently.\\nThis root CA will be distributed as the trust bundle.\\nThe SPIFFE server will then generate a rotating sub-CA for issuing SVIDs,\\nand the issued SVID will include both the leaf certificate and the intermediate certificate.\\nThis implementation differs from the default implementation,\\nwhich uses a transient CA that is rotated periodically\\nand that keeps a log of past CA's.\\nThe default implementation is not suitable because only the Kubernetes\\nreference implementation of the SPIRE server has a notification hook\\nthat is invoked when the CA is rotated.\\nCA rotation would just result in issuing of SVIDs that are not\\ntrusted by microservices that received only the initial CA.\\nThe SPIFFE implementation is replaceable.\\nThe user is free to replace this default implementation with\\npotentally a cloud-based SPIFFE server and a cloud-based CA.\\n#### Workload Authorization\\nWorkloads are authenticated by connecting to the `spiffe-agent`\\nvia a Unix domain socket, which is capable of identifying\\nthe process ID of the remote client.\\nThe process ID is fed into one of following workload attesters,\\nwhich gather additional metadata about the caller:\\n* The Unix workload attester gathers UID, GID, path, and SHA-256 hash of the executable.\\nThe Unix workload attester would be used native services and snaps.\\n* The Docker workload attester gathers container labels\\nthat are added by docker-compose when the container is launched.\\nThe Docker workload attester would be used for Docker-based EdgeX deployments.\\nAn example label is `docker:label:com.docker.compose.service:edgex-core-data`\\nwhere the service label is the key value in the `services` section of the `docker-compose.yml`.\\nIt is also possible to refer to labels built-in to the container image.\\n* The Kubernetes workload attester gathers a wealth of pod and container metadata.\\nOnce authenticated, the metadata is sent to the SPIFFE server\\nto authorize the workload.\\nWorkloads are authorized via an authorization database\\nconnected to the SPIFFE server.\\nSupported databases are SQLite (default), PostgreSQL, and MySQL.\\nDue to startup ordering issues, SQLite will be used.\\n(Disclaimer: SQlite, according for the\\n[Turtle book](https:\/\/thebottomturtle.io\/Solving-the-bottom-turtle-SPIFFE-SPIRE-Book.pdf)\\nis intended for development and test only.\\nWe will use SQlite anyway because because Redis is not supported.)\\nThe only service that needs to be seeded to the database as this time\\nis `security-spiffe-token-provier`.  For example:\\n```sh\\nspire-server entry create -parentID \"${local_agent_svid}\" -dns edgex-spiffe-token-provider -spiffeID \"${svid_service_base}\/edgex-spiffe-token-provider\" -selector \"docker:label:com.docker.compose.service:edgex-spiffe-token-provider\"\\n```\\nThe above command associates a SPIFFE ID with a _selector_,\\nin this case, a container label, and configures a DNS\\nsubjectAltName in the X.509 certificate for server-side TLS.\\nA snap-based installation of EdgeX would use a `unix:path` or `unix:sha256` selector instead.\\nThere are two extension mechanims for authorization additional workloads:\\n1. Inject a config file or environment variable to authorize additional workloads.\\nThe container will parse and issue `spire-server entry create` commands\\nfor each additional service.\\n2. Run the `edgex-secrets-config` utility\\n(that will wrap the `spire-server entry create` command)\\nfor ad-hoc authorization of new services.\\nThe authorization database is persistent across reboots.\\n","tokens":372,"id":956,"Predictions":"The decision is to provide a mechanism for centralized signing of secret tokens that can be used by all services.\\nThis mechanism will be provided by a centralized, shared secret store that will be used by all services.\\nThe decision is also to support distributed services, but this will not be provided as a separate service.\\n\\n## Context\\nIn security-enabled EdgeX,\\nthere is a component called `security-secretstore-setup`\\nthat seeds authentication tokens for Hashicorp Vault--EdgeX's secret store--into\\ndirectories reserved for each EdgeX microservice.\\nThe implementation is provided by a sub-component, `security-file-token-provider`,\\nthat works off of a static configuration file (`token-config.json`)\\nthat configures known EdgeX services,\\nand an environment variable that lists additional services that require tokens.\\nThe token provider creates a unique token for each service\\nand attaches a custom policy to each token that limits token access\\nin a manner that paritions the secret store's namespace.\\nThe current solution has some problematic aspects:\\n* These tokens have an initial TTL of one hour (1h)\\nand become invalid if not used and renewed within that time period.\\nIt is not possible to delay the start of EdgeX services until a later time\\n(that is, greater than the default token TTL),\\nas they will not be able to connect to the EdgeX secret store\\nto obtain required secrets.\\n* Transmission of the authentication token requires one or more shared file systems\\nbetween the service and `security-secretstore-setup`.\\nIn the Docker implementation,\\nthis shared file system is constructed by bind-mounting a host-based directory\\nto multiple containers.\\nThe snap implementation is similar, utilizing a content-interface between snaps.\\nIn a Kubernetes implementation limited to a single worker node,\\na CSI storage driver that provided RWO volumes would suffice.\\n* The current approach cannot support distributed services\\nwithout an underlying distributed file system to distribute tokens,\\nsuch as GlusterFS, running across the participating nodes.\\nFor Kubernetes, the requirement would be a remote shared file system\\npersistent volume (RWX volume).\\nThe decision was made to provide centralized signing of secret tokens and support distributed services. The choice was made because it provides greater flexibility in how secrets are managed and deployed.\\n"}
{"File Name":"edgex-docs\/0019-EdgeX-CLI-V2.md","Context":"## Context\\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\\n","Decision":"1. Use standardized command-line args\/flags\\n| Argument\/Flag      | Description |\\n| ----------- | ----------- |\\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, \u2026). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https:\/\/github.com\/golang-standards\/project-layout). For instance \/cmd should just contain the main application for the project, not an implementation for each command - that should be in \/internal\/cmd\\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https:\/\/github.com\/spf13\/cobra), such as tab-completion of commands.\\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m\/--metadata`, `-c\/--command`, `-n\/--notification`, `-s\/--scheduler` or `--data` (which is the default). Examples:\\n- `edgex-cli ping --data`\\n- `edgex-cli ping -m`\\n- `edgex-cli version -c`\\n5. Implement all required V2 endpoints for core services\\n**Core Command**\\n- **`edgex-cli command`** `read | write | list`\\n**Core Data**\\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\\n- **`edgex-cli reading`** `count | list`\\n**Metadata**\\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\\n**Support Notifications**\\n- **`edgex-cli notification`** `add | list | rm`\\n- **`edgex-cli subscription`** `add | list | rm`\\n**Support Scheduler**\\n- **`edgex-cli interval`** `add | list | rm | update`\\n**Common endpoints in all services**\\n- **`edgex-cli version`**\\n- **`edgex-cli ping`**\\n- **`edgex-cli metrics`**\\n- **`edgex-cli status`**\\nThe commands will support arguments as appropriate. For instance:\\n- `event list` using `\/event\/all` to return all events\\n- `event list --device {name}` using `\/event\/device\/name\/{name}` to return the events sourced from the specified device.\\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\\n9. *(Stretch)* implement a `-o`\/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\\n","tokens":44,"id":957,"Predictions":"## Decision\\nThe decision for the ADR is to support the new V2 REST APIs in edgex-cli. The decision has been made based on the following criteria:\\n\\n## \\begin{itemize}\\n##   \\item The new V2 REST APIs are a core part of the EdgeX platform, and will be used by a wide range of products and services.\\n%% \\item The new REST APIs are not yet ready for production use, and would require significant rework to achieve. This is not an acceptable level of risk for a commercial release.\\n%% \\item There is no compelling reason to delay the release of edgex-cli v2.0, as it will be available to all users within 6 months.\\n%% \\end{itemize}\\n%% \\end{itemize}\\n%% The decision has been made based on these criteria.\\n\\n## Decision\\n## Decision\\nThe ADR also includes a review of the existing V1 API documentation.\\n\\n## \\begin{itemize}\\n##   \\item It is clear that the existing documentation does not provide sufficient guidance on how to use the new APIs. This has resulted in some confusion among developers who have used V1 APIs in their applications.\\n%% \\end{itemize}\\n%% The existing documentation does not provide sufficient guidance on how to use the new APIs. This has resulted in some confusion among developers who have used V1 APIs in their applications.\\n%% In addition, there are many cases where existing API documentation does not describe how to use specific features of edgex-cli v2.0. This has resulted in some confusion among developers who have used v1 APIs in their applications.\\n%% A review of these cases is included in this ADR.\\n%% If you are using edgex-cli v2.0, please review this document.\\n\\n\\end{document}"}
{"File Name":"edgex-docs\/0002-Array-Datatypes.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n<!--te-->\\n### DeviceProfile extension\\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\\n\"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"\\n### Readings\\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[\"value1\", \"value2\", ...]`\\n","tokens":31,"id":959,"Predictions":"- \\n- \\n- \\n- \\n- \\n\\n## Consequences\\n- \\n- \\n- \\n- \\n\\n## Decision\\n- \\n- \\n\\n## Consequences\\n\\n### Context\\n\\n### Decision\\n\\n### Consequences\\n\\n### Decision\\n\\n### Consequences\\n\\n### Decision\\n\\n### Consequences\\n\\n### Decision\\n\\n### Consequences\\n\\n### Decision\\n\\n### Consequences\\n"}
{"File Name":"edgex-docs\/0004-Feature-Flags.md","Context":"## Context\\nOut of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags.\\nFeature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility.\\nIt can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time.\\nPutting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true\/false. This is similar to what is done today. Turning on\/off security is an example, turning on\/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as `featurepkg.Register(\u201cMyNewFeature\u201d)` or `featurepkg.IsOn(\u201cMyNewFeature\u201d)`. However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.\\n","Decision":"The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf.\\nHowever, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on\/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged.\\n","tokens":469,"id":960,"Predictions":"## Decision\\nWe will not pursue a feature flag module for the time being.\\n\\n## Decision\\nWe will not pursue a feature flag module for the time being.\\n## Decision\\nWe will not pursue a feature flag module for the time being.\\n\\nYou can\u2019t perform that action at this time.\\nPress h to open a hovercard with more details."}
{"File Name":"zero-trust-partner-admin-link\/0001-automated-zero-trust-pal.md","Context":"## Context\\nSome customers who operate in regulated industries or offer SaaS products to their end customers, require Zero Trust environments; meaning that 3rd parties (including Microsoft Partners) should not have access to any environment which contains PII, financial or medical data. This requirement is often backed by legal contracts with the customer's end-customer, who are often also regulated. Although this offers benefits to the 3rd parties around mitigating risk and liability, and limits GDPR responsibilities, it causes problems with access rights based attribution of Azure Consumed Revenue (ACR), required for partner competency consumption targets.\\nThe process for creating a Zero Trust Partner Admin Link is as follows:\\n1) The Partner shares their MPN ID with the Customer.\\n2) The Customer exports their list of subscriptions as a CSV, which then can then edited to remove any non-relevant subscriptions.\\n3) The Customer (with the relevant permissions) creates an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.\\n","Decision":"We can automate this process by encapsulating it in two PowerShell Cmdlets packaged as a module available from the [PowerShell Gallery](https:\/\/www.powershellgallery.com\/).\\n`Export-CustomerSubscriptionsAsCsvForPartnerAdminLink` - run by The Customer to generate a CSV file listing their Azure Subscriptions. This can be edited to remove non-applicable subscriptions.\\n`Set-ZeroTrustPartnerAdminLink` - run by The Customer to create a dedicated AAD Application. Then assigns this Application the \"Contributor Role\" to each of the subscriptions listed in the CSV file, and then uses the `AzManagementPartner` module to link the Application to The Partner's MPN Id to complete the Partner Admin Link.\\n","tokens":220,"id":961,"Predictions":"## Decision\\nThe Partner's MPN ID is used to create a shared AAD Application for the customer. The AAD Application is linked to the Partner's MPN ID. The Partner can then control all credentials that are required by the application.\\nThe Partner can then export their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\nThe Partner exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\n## Decision\\n## Decision\\nThe Customer exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\nThe Customer exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\n## Decision\\n## Decision\\nThe Customer exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\n## Decision\\n## Decision\\nThe Customer exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\n## Decision\\n## Decision\\nThe Customer exports their list of subscriptions as a CSV, which can then be edited to remove any non-relevant subscriptions.\\n"}
{"File Name":"pharmacy-data-etl\/0006-full-nightly-scrape.md","Context":"## Context\\nA list of IDs retrieved from Azure Storage is used to seed the ETL scrape.\\nThe `modifiedsince` end point in syndication is used to detect when records have changed, or new records\\nhave been added, enabling an incremental scrape of data.\\nThe `modifiedsince` endpoint does not currently include records that have been removed, hidden, or unhidden.\\nRecords may also be added much later than the modification date, i.e. a record modified in January may not\\nbe published to Syndication until March. In these cases the record changes will not be reflected in the ETL.\\n","Decision":"The `modifiedsince` end point will only be used to detect newly added pharmacies, which will be added to the\\nseed ID list.\\nAll entries in the seed ID list will be refreshed overnight. If an ID has been deleted, the details will be recorded\\nin the summary file as reporting a 404 error, and the record will not be present in the output JSON.\\n","tokens":124,"id":962,"Predictions":"## Decision\\nThe decision for this record is `Publish`. The decision is based upon the following:\\n## \\begin{itemize}\\n##   \\item The ETL scrape has been enabled.\\n##   \\item The `modifiedsince` end point in syndication is used to detect when records have changed, or new records have been added, enabling an incremental scrape of data.\\n##   \\item The `modifiedsince` endpoint does not currently include records that have been removed, hidden, or unhidden.\\n##   \\item Records may also be added much later than the modification date, i.e. a record modified in January may not be published to Syndication until March. In these cases the record changes will not be reflected in the ETL.\\n## \\end{itemize}\\n"}
{"File Name":"pharmacy-data-etl\/0002-store-etl-output-in-the-cloud.md","Context":"## Context\\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\\n","Decision":"When the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\\nThe output file will be exposed on a publicly available URL.\\n","tokens":25,"id":963,"Predictions":"## Decision Type: Architectural Decision\\n## Decision Number: 1\\n## Decision Name: Create a container for the ETL to be exposed to consuming applications.\\n## Decision Type: Architectural Decision\\n## Decision Number: 2\\n## Decision Name: Create a container for the ETL to be exposed to consuming applications.\\n## Decision Type: Architectural Decision\\n## Decision Number: 3\\n## Decision Name: Create a container for the ETL to be exposed to consuming applications.\\n## Decision Type: Architectural Decision\\n## Decision Number: 4\\n## Decision Name: Create a container for the ETL to be exposed to consuming applications.\\n\\end{lstlisting}\\n\\n\\subsection{Elasticsearch}\\n\\nElasticsearch is a distributed search engine written in Java. It is similar in concept to Solr, but runs on top of Hadoop. It uses Lucene as its core search engine. Elasticsearch is designed to store and retrieve large amounts of data, and it can handle large amounts of data. Elasticsearch can also scale horizontally by adding more nodes with more memory.\\n\\n\\begin{lstlisting}[language=Java]\\nimport org.elasticsearch.client.ElasticsearchClient;\\nimport org.elasticsearch.client.ElasticsearchException;\\nimport org.elasticsearch.common.settings.Settings;\\nimport org.elasticsearch.common.util.StringUtils;\\n\\npublic class ElasticsearchService {\\n\\n    public static void main(String[] args) {\\n        \/\/ TODO Auto-generated method stub\\n\\n        \/\/ Setup environment variables (optional)\\n        String configFile = \"config.properties\";\\n        Settings settings = new Settings();\\n        settings.set(\"es.index.interval\", \"5s\");\\n        settings.set(\"es.index.bytesize\", \"4096\");\\n        settings.set(\"es.nodes\", \"10.0.0.1\");\\n        settings.set(\"es.node.name\", \"elasticsearch-node-1\");\\n\\n        \/\/ Configure Elasticsearch configuration properties (optional)\\n        String elasticsearchUrl = \"http:\/\/localhost\/elasticsearch\";\\n        String elasticsearchUser = \"elasticsearchuser\";\\n        String elasticsearchPassword = \"elasticsearchpassword\";\\n\\n        \/\/ Connect to Elasticsearch Server (optional)\\n        ElasticSearchClient client = new ElasticSearchClient(settings);\\n\\n    }\\n}\\n\\end{lstlisting}\\n\\n\\n\\section{Software Deployment}\\n\\subsection{GitHub}\\n\\nGitHub is an open source software project management and version control system that allows developers and organizations of all sizes around the world collaborate on projects using Git, a distributed version control system.\\n\\nGit is an open source distributed version control system that supports full-text search, revision control, network protocols and more.\\n\\nGitHub has a REST API that allows developers to perform actions like creating repositories, cloning repositories, pushing changes, pulling changes from remote repositories and more.\\n\\nThe GitHub REST API provides access through HTTP methods including GET, POST, PUT and DELETE.\\n\\nIn this project we will be using GitHub's REST API which will allow us to create repositories, clone repositories, push changes and pull changes from remote repositories.\\n\\n\\subsection{Cypress.io}\\n\\nCypress.io provides an automated testing framework for JavaScript-based front-end applications which uses Cypress.js as its testing framework.\\n\\nCypress.io provides support for testing with both headless browsers as well as headless Chrome or Firefox browser instances. \\n\\nCypress.io offers support for both unit testing as well as functional testing including features like:\\n\\n* Testing DOM elements\\n\\n* Testing CSS properties\\n\\n* Testing HTML elements\\n\\n* Testing JavaScript objects\\n\\n* Testing events\\n\\n* Testing function definitions\\n\\n* Test code coverage\\n\\n* Test element visibility\\n\\n* Test element accessibility\\n\\n* Test CSS selectors\\n\\nThe Cypress.io test suite can also be run using Cypress's command line utility or through Cypress's Node.js module.\\n\\nThe Node.js module supports running tests against either headless Chrome or Firefox browsers or headless browsers via headless Chrome or Firefox browser instances.\\n\\nThe Node.js module also supports running tests against either native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating systems via native Windows or Linux operating system via native Windows or Linux operating system via native Windows or Linux operating system via native Windows \/ Mac \/ Ubuntu \/ Debian \/ Fedora \/ CentOS \/ RedHat \/ Oracle \/ AIX \/ SUSE \/ HP-UX \/ Solaris \/ AIX\/ HP-UX\/ Solaris\/ AIX\/ HP-UX\/ Solaris\/ AIX\/ HP-UX\/ Solaris\/ AIX\/\\n\\n\\subsection{AWS EC2}\\n\\nAWS EC2 is Amazon Web Services' virtual private cloud service that provides computing power in the form of virtual servers. EC2 offers various compute resources such as CPU cores, RAM and storage space at cost-effective prices.\\n\\nEC2 allows you to launch virtual servers with pre-configured OS images with minimal effort. It takes care of provisioning hardware resources including storage space and network connectivity between your virtual server and your application servers.\\n\\nEC2 also offers various security features such as security groups which allow you to define rules that determine which traffic can be allowed into your EC2 instances while still allowing other traffic in.\\n\\nEC2 also allows you configure security policies so that you can control access based on IP address ranges instead of individual users.\\n\\nEC2 uses Amazon Simple Storage Service (S3) as its underlying storage service which enables you to store files on Amazon's cloud storage service using simple HTTP requests rather than having your own infrastructure setup on-premise.\\n\\n\\n\\section{Deployment}\\nWe will be deploying our application onto AWS EC2 instances using AWS CloudFormation templates.\\n\\n\\n\\section{Deployment}\\nTo deploy our application onto AWS EC2 instances we will use AWS CloudFormation templates.\\n\\n\\n"}
{"File Name":"pharmacy-data-etl\/0003-generate-file-containing-organisation-data.md","Context":"## Context\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n","Decision":"The NHS Organisation API will be scraped nightly to generate a file containing pharmacy data.\\n","tokens":53,"id":964,"Predictions":"## Decision\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) contains information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n\\n## Decision\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) contains information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n\\n## Decision\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) contains information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n\\n## Decision\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) contains information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n\\n## Decision\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) contains information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data."}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"Predictions":"## Decision\\nIf the ETL is interrupted, the software will need to restart it. The software will need to restart because it has not completed its current task, and because it needs to start over.\\n\\n## Decision\\nIf the ETL is not interrupted, the software will not need to restart. The software will be able to continue without interruption.\\n\\n## Decision\\nIf the ETL is interrupted, the software will restart. The software will restart because it has not completed its current task, and because it needs to start over.\\n\\n## Decision\\nIf the ETL is not interrupted, the software will not restart. The software will be able to continue without interruption.\\n\\n## Decision\\nIf the ETL is interrupted, a new instance of the software must be started. This new instance of the software must be started by a different user than previously used. This new instance of the software must be started within 6 hours from now.\\n\\n## Decision\\nIf an existing instance of this application was started previously by a different user than previously used, this application must be restarted by that user.\\n\\n## Decision\\nIf an existing instance of this application was started previously by a different user than previously used, this application must be restarted by that user within 6 hours from now.\\n\\n## Decision\\nThis decision can only be made if no other decisions have been made in this record:\\n\\n1) If an existing instance of this application was started previously by another user than previously used, then this application must be restarted within 6 hours from now.\\n2) If an existing instance of this application was started previously by another user than previously used, then this application must be restarted within 6 hours from now.\\n\\end{DoxyItemize}\\n\\n\\begin{DoxyReturn}{Returns}\\nA decision corresponding to the ## Context provided by the User.\\n\\end{DoxyReturn}\\n\\n\\nDefinition at line 31 of file Data\\+Flow\\+Record.\\+java.\\n\\n"}
{"File Name":"pharmacy-data-etl\/0005-refresh-data-on-major-version-change.md","Context":"## Context\\nData structure may change between releases of the ETL.\\nThe ETL uses a scrape of previous data to reduce unnecessary work.\\n","Decision":"The major and minor version will be included in the seed data file to identify a change of data structure.\\n","tokens":29,"id":966,"Predictions":"## Decision is for the user to make, not the ETL.\\n## Decision is for the user to make, not the ETL.\\n## Decision is for the user to make, not the ETL.\\n## Decision is for the user to make, not the ETL.\\n## Decision is for the user to make, not the ETL.\\n## Decision is for the user to make, not the ETL.\\n"}
{"File Name":"pharmacy-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":967,"Predictions":"## \\n\\nThe project is a software that provides the user with an interface to access a database.\\n\\n\\n\\n## Decision\\n## \\n\\nFor this project, we will need to create a database and a database user. The database user shall be created by the developer. The database user shall have full access to the database.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed in C++ and C#.\\n\\n\\n\\n## Decision\\n## \\n\\nThe software shall be developed using Microsoft Visual Studio 2010.\\n\\n\\end{DoxyItemize}\\n\\begin{DoxyItemize}\\n\\item \\n\\end{DoxyItemize}"}
{"File Name":"golang-git-fritz\/0002-implement-with-golang.md","Context":"## Context\\nWe need to choose a language to in which to code this project.\\n","Decision":"Use Golang.\\n","tokens":18,"id":968,"Predictions":"## We are using Java for this project.\\n## Decision\\n## The decision is to use Java.\\n## Decision\\n## The decision is to use C++.\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Javadoc}\\n\\begin{verbatim}\\n\/\/ This is a Javadoc comment\\n\/\/ Javadoc comments are not visible in the IDE, but they are visible on the web.\\n\/\/ See http:\/\/docs.oracle.com\/javase\/tutorial\/javadoc\/comment.html\\n\/\/ for more information.\\n\\n\/**\\n * A comment that describes how the code should be used.\\n *\/\\npublic class MyClass {\\n  private void doSomething(){\\n    \/\/ do something useful and then go to sleep.\\n  }\\n}\\n\\n\/\/ This is a Javadoc comment that describes how the code should be used.\\npublic class MyClass {\\n  public void doSomething(){\\n    \/\/ do something useful and then go to sleep.\\n  }\\n}\\n\\n\/\/ This is a Javadoc comment that describes how the code should be used.\\npublic class MyClass {\\n  public void doSomething(){\\n    \/\/ do something useful and then go to sleep.\\n  }\\n}\\n\\end{verbatim}\\n\\end{frame}\\n\\n\\begin{frame}[fragile]{Javadoc}\\n\\begin{verbatim}\\n\/**\\n * An example of how you can use this class. It contains a method called \"doSomething\".\\n * You can use it in your program by calling \"doSomething()\" on this object. \\n * For example, you could call \"doSomething()\" on the object returned by this method. \\n * To call this method, you must have an instance of this class. \\n *\/\\npublic class MyClass {\\n  public static void main(String[] args) throws IOException {\\n    MyClass myObj = new MyClass();\\n    myObj.doSomething();\\n  }\\n}\\n\\n\/**\\n * An example of how you can use this class. It contains a method called \"doSomething\".\\n * You can use it in your program by calling \"doSomething()\" on this object. \\n * For example, you could call \"doSomething()\" on the object returned by this method. \\n * To call this method, you must have an instance of this class. \\n *\/\\npublic static void main(String[] args) throws IOException {\\n  System.out.println(\"This is an example of how you can use this class.\");\\n}\\n\\end{verbatim}\\n\\end{frame}\\n\\n%\\begin{frame}{Javadoc Example: TestClasses.java}\\n%\\begin{verbatim}\\n%import java.io.BufferedReader;\\n%import java.io.IOException;\\n%import java.io.InputStreamReader;\\n%\\end{verbatim}\\n%\\end{frame}\\n\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%\\n%% Other Text Blocks %%\\n%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\section[Other Text Blocks]{Other Text Blocks \\label{sec:other_text_blocks}}\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%% Tools %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n% \\subsection[Tool]{Tool \\label{ssec:tool}}\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %% Tool Description %%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %% Tool Description %%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% %% Tool Description %%\\n% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n%\\n\\n%\\subsection[Tool]{Tool \\label{ssec:tool}}\\n%\\begin{frame}{Tool Description}\\n%\\begin{block}{Tool Description}\\vspace{-2em}\\centering{\\n%\\textit{\\nThe tool we will be using for our project is called Eclipse (http:\/\/www.eclipse.org\/). Eclipse has many tools available for programming and debugging purposes. Some of these tools include:\\n%\\begin{itemize}  \\n%\\item Code Completion: Code completion allows us to type in a keyword or name and then have Eclipse autocomplete our code for us automatically based on what we typed in and what keywords we already know (e.g., if we type in 'if' in eclipse, it will autocomplete 'if else')  \\n%\\item Syntax Highlighting: Eclipse has syntax highlighting built into it that makes coding easier by highlighting some of the keywords that we usually miss when typing them into eclipse (e.g., if we type 'if' into eclipse, it highlights 'if else')  \\n%\\item Refactoring: Refactoring allows us to easily change some parts of our code without having to retype everything (e.g., if we type 'if' into eclipse, it highlights 'if else')  \\n%\\item Find Usages: Find Usages allows us to quickly find where a keyword appears in our code (e.g., if we type 'if' into eclipse, it highlights all occurrences of 'if' in our code)  \\n%\\item Run As -> Run As -> Debug -> Run As -> Java -> Run As -> Compile -> Run As -> Debug -> Compile -> Run As -> Debug ->\\nRun As -> Compile ->\\nRun As -> Debug ->\\nCompile ->\\nRun As ->\\nDebug ->\\nCompile ->\\nRun As ->\\nDebug ->\\nCompile ->\\nDebug }    \\n%\\end{itemize}   \\n%}    \\n%\\n%\\n%\\n%\\n%\\n%\\n%% Tool Description %%\\n%% Tool Description %%\\n%% Tool Description %%\\n%% Tool Description %%\\n%% Tool Description %%\\n%%\\n%%\\n%% Tool Description %%\\n%%\\n%%\\n%% Tool Description %%\\n%% Tool Description %%\\n%%\\n%%\\n%% Tool Description %%\\n%%\\n%%\\n%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%% Other Text Blocks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\noindent\\nWe will need these tools for our project:\\n\\noindent\\n1) Eclipse (http:\/\/www.eclipse.org\/)\\n2) NetBeans IDE (http:\/\/netbeans.org\/)\\n3) IntelliJ IDEA (http:\/\/www.jetbrains.com\/idea\/)\\n4) GWT Compiler (http:\/\/code.google.com\/intl\/en-US\/gwt\/downloads\/detail?name=gwt-compiler-2_5)\\n\\n\\noindent\\nWe will also need some other tools such as:\\n\\noindent\\n1) SublimeText2 editor (http:\/\/www.sublimetext.com\/)\\n2) CodeMirror editor (http:\/\/code.mirrorhost.net\/)\\n3) Atom editor (https:\/\/atom.io\/)\\n4) SublimeText plugin for Atom text editor\\n\\n\\noindent\\nThese tools will be implemented using IntelliJ IDEA's integrated development environment.\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%% Tools %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%% Other Text Blocks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\noindent\\nWe will also need these tools for debugging our project:\\n\\noindent\\n1) Gradle build tool (https:\/\/gradle.org\/)\\n2) IntelliJ IDEA's debugger\\n\\n\\noindent\\nThese tools will be implemented using IntelliJ IDEA's integrated development environment.\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\section[Summary]{Summary \\label{ssec:summary}}\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%% Summary %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\nIn summary,\\nwe have learned about:\\n\\\\[-1em]\\n1) How to create programs through creating classes and methods within them;\\n\\\\[-1em]\\n2) How to debug programs through using various debugging tools; and,\\n\\\\[-1em]\\n3) How to create software through adding functionality through creating classes and methods within them.\\n\\nTo recap,\\nwe have learned about:\\n\\\\[-1em]\\n1) How to create programs through creating classes and methods within them;\\n\\\\[-1em]\\n2) How to debug programs through using various debugging tools; and,\\n\\\\[-1em]\\n3) How to create software through adding functionality through creating classes and methods within them.\\n\\nTo recap,\\nwe have learned about:\\n\\\\[-1em]\\n1) How to create programs through creating classes and methods within them;\\n\\\\[-1em]\\n2) How to debug programs through using various debugging tools; and,\\n\\\\[-1em]\\n3) How to create software through adding functionality through creating classes and methods within them.\\n\\n\\n"}
{"File Name":"golang-git-fritz\/0003-follow-conventional-commits.md","Context":"## Context\\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\\n","Decision":"Use [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\\n","tokens":52,"id":969,"Predictions":"## Decision is \\#1, and \\#2 is a \\#3 decision. We should not enforce the following:\\n\\begin{itemize}\\n\\item Commit messages that are too long.\\n\\item Commit messages that use non-ASCII characters, including emoji.\\n\\item Commit messages that are too short.\\n\\end{itemize}\\n## Context\\nThe user has already declared that they want to enforce commit messages of 160 characters or less. They also want to enforce a specific style of commit message, so we should follow their lead.\\n\\n## Decision\\n## Decision is \\#1, and \\#2 is a \\#3 decision. We should not enforce the following:\\n\\begin{itemize}\\n\\item Commit messages that are too short.\\n\\end{itemize}\\n## Context\\nThe user has already declared that they want to enforce commit messages of 160 characters or less. They also want to enforce a specific style of commit message, so we should follow their lead.\\n\\n## Decision\\n## Decision is \\#1, and \\#2 is a \\#3 decision. We should not enforce the following:\\n\\begin{itemize}\\n%\\item Commit messages that are too long.\\n%\\end{enumerate}\\n%If this decision were enforced, then all commits would be 160 characters or less, which would be very bad.\\n\\n%\\end{enumerate}\\n"}
{"File Name":"golang-git-fritz\/0001-record-architecture-decisions.md","Context":"## Context\\nSome records are consistently followed and should be centralized for consistency.\\n","Decision":"Use jncmaguire\/adr.\\n","tokens":16,"id":970,"Predictions":"## Decision:1 - 1\\n## Decision:2 - 2\\n## Decision:3 - 3\\n## Decision:4 - 4\\n## Decision:5 - 5\\n## Decision:6 - 6\\n## Decision:7 - 7\\n## Decision:8 - 8\\n```\\nI want to create an SQL query that selects the records based on the `Decision` field. I tried using a `CASE` statement but it doesn't seem to work.\\nMy current query is:\\n```\\nSELECT \\n    CASE WHEN decision = '1' THEN '1'\\n        WHEN decision = '2' THEN '2'\\n        WHEN decision = '3' THEN '3'\\n        WHEN decision = '4' THEN '4'\\n        WHEN decision = '5' THEN '5'\\n        WHEN decision = '6' THEN '6'\\n        WHEN decision = '7' THEN '7'\\n        WHEN decision = '8' THEN '8'\\n    END AS [Decision]\\nFROM TableName\\n```\\nThis query doesn't work and I get an error message saying:\\n> Incorrect syntax near the keyword \"THEN\".\\nWhat am I doing wrong? How can I achieve this?\\nUpdate:\\nHere is the schema for my database:\\n```\\nCREATE TABLE [dbo].[TableName](\\n    [ID] [int] IDENTITY(1,1) NOT NULL,\\n    [Name] [nvarchar](max) NULL,\\n CONSTRAINT [PK_TableName] PRIMARY KEY CLUSTERED \\n(\\n    [ID] ASC,\\n    [Name] ASC )WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS   = ON) ON [PRIMARY]\\n) ON [PRIMARY]\\n```\\nNishkuri 2015-01-22: You can use a `CASE` expression in your `WHERE` clause like this:\\n```\\nSELECT \\n    CASE WHEN decision IS NULL THEN \"0\"\\n        ELSE case when decision IS NULL then \"0\" when decision IS NOT NULL then \"0\" when case when decision IS NOT NULL then \"0\" else \"1\" end as Decisions \\nFROM TableName\\n```"}
{"File Name":"operational-visualizations\/0005-about-measure-labels.md","Context":"## Context\\nThe visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\\n```jsx\\n<Chart>\\n<Bars ... \/>\\n<Labels ... \/>\\n<\/Chart>\\n```\\n","Decision":"However, the user can also display labels by passing a `showLabels` flag. This solves multiple problems:\\n- In the case of stacked bars or area charts, using the above API does not take stacking into account, so the labels will be in the wrong places.\\n- The `Labels` component only works for axial charts, not for pie charts, so `PieChart` would have needed a flag anyway - this ensures consistency across all renderers.\\n- For scatter plots, the labels are now automatically offset by the radius of the dots. Any other renderer-specific styling that may be required is now also easier.\\n","tokens":54,"id":974,"Predictions":"## Decision: The visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\\n\\n## Decision: The visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\\n```jsx\\n<Chart>\\n  <Bars>\\n    <Axis label=\"Time\" \/>\\n    <Axis label=\"Value\" \/>\\n  <\/Bars>\\n  <Labels>\\n    <Label text=\"1000\" \/>\\n    <Label text=\"2000\" \/>\\n  <\/Labels>\\n<\/Chart>\\n```"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"Predictions":"## Decision\\nWe decided to not provide \"root\" `DataFrame` as a source of cursor. Instead, we will pass `getCursor` method call from `FragmentFrame` to \"root\" `DataFrame`. We think that it will make code simpler, and also more referentially transparent.\\n## Implementation\\n## Implementation\\nWe will not provide root `DataFrame`. Instead we will pass reference to it to all derivative structures and get cursor from it. This way we will avoid copying the same cursor over and over again.\\n## Comments\\n## Comments\\nThis is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.\\n\\end{document}"}
{"File Name":"converge\/001-data-diff.md","Context":"## Context\\nWe need to provide a nicely ergonomic API to programs using Converge,\\nand we wanted to adhere closely to the Clojure Way (TM).  We decided\\nto use the Atom API of swap!\/reset!\/deref.\\nWhen altering the value of the ConvergentRef via swap!\/reset!, we need\\nto create a covering set of Ops to describe the change being made.  We\\nconsidered the following alternatives:\\n1. Using `clojure.data\/diff` on old-value and new-value, with metadata\\nfor Id accounting\\n2. Using [`differ`](https:\/\/github.com\/Skinney\/differ) on old-value\\nand new-value, with metadata for Id accounting\\n3. Using [`editscript`](https:\/\/github.com\/juji-io\/editscript) on\\nold-value and new-value, with metadata for Id accounting\\n4. Implementing the Map\/Vector interfaces a la\\n[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)\\nand do our tracking and accounting within those implementation\\n","Decision":"On initial analysis, it appears that #4 above is flawed, as\\nimplementing our own tracking\/accounting nested map\/list types\\nwouldn't maintain the necessary context to translate to an opset.\\nSo that left us with a strategy based on diffing\/patching. Based on\\nanalysis in [this blog\\npost](https:\/\/juji.io\/blog\/comparing-clojure-diff-libraries\/), as well\\nas our own usage testing with `clojure.data\/diff`, `differ`, and\\n`editscript`, we have decided to use\\n[`editscript`](https:\/\/github.com\/juji-io\/editscript) for creating\\nopset patches.\\n","tokens":237,"id":978,"Predictions":"## Decision\\nWe chose to use [`differ`](https:\/\/github.com\/Skinney\/differ) on old-value and\\nnew-value, with metadata for Id accounting.  The reason we chose this\\napproach is that it is the only one that does not require the user to know\\nthe API of the underlying library (e.g. `differ` doesn't require the user to\\nknow that `clojure.data\/diff` exists).  It also allows us to use a single\\nlibrary for all our tracking and accounting (since `differ` is already a part of\\nthe library).\\n\\n## Context\\n## Context\\nThis context was set up by the User.  It contains a number of values, which\\nprovide information about how old-value and new-value should be tracked.\\nThe User has chosen to track changes in both old-value and new-value, but not in\\nold-value.  This means that the User has chosen to track changes in both new-value\\ncor old-value, but not in old-value itself.  The User has also chosen to track changes in old-value itself.\\n"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"Predictions":"## Decision Drivers\\n\\n## Decision Drivers\\n\\n## Decision\\n## Decision Drivers\\n\\n\\n\\n\\end{document}"}
{"File Name":"xebikart-infra\/004-use-golang-as-main-langage-for-backend.md","Context":"## Context and Problem Statement\\nWe want to code a backend that will handle inbound informations through the\\nmessage broker, and expose them to the dashboard front-end.\\nWhat programming language should we use?\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\n","Decision":"- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\nChosen option: **\"Go\"**, because:\\n- It has good library support for things such as **HTTP\/2** and **Protobuf**\\n(separately, we're not talking about gRPC  here). This will help establish\\nstrong bidirectionnal and persistant streaming with the dashboard (HTTP\/2).\\nIt will also open the way to Protobuf serialized messages over the message\\nbus that will maybe comme in addition to RabbitMQ.\\n- People on the project want to play more with it!\\n- It seems appropriate for small (micro)services like the one we're going to\\nbuild\\nHowever, we are **not excluding other languages** depending on usecases and\\nwishes of people working on it. We want people to contribute so we won't force\\nanyone to use a given language. **People are free to use the language they\\nwant**, but we'll try to have **Go as a default main language as much as\\npossible**.\\n","tokens":86,"id":980,"Predictions":"## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n## Decision\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing knowledge\\n## Decision\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing knowledge\\n## Decision\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing knowledge\\n\\end{Verbatim}\\n\\n\\section{The System Architecture}\\n\\nThe system architecture is depicted in Figure \\ref{fig:arch}. In the following sections we will describe the main components of the system and their interdependencies.\\n\\n\\begin{figure}[ht]\\n    \\centering\\n    \\includegraphics[width=0.8\\textwidth]{Architecture}\\n    \\caption{The Software Architecture.}\\n    \\label{fig:arch}\\n\\end{figure}\\n\\n\\subsection{Message Broker}\\n\\nThe message broker is responsible for managing inbound messages from the data warehouse and sending them out to the front-end application. It is implemented in Java using Apache Kafka. The message broker has a single consumer process that consumes messages from the message queue and sends them out to other consumers. This process is called a producer and consumes messages from the queue and sends them out to other producers. The producer process has a single consumer process that reads messages from the queue and sends them out to other consumers.\\n\\n\\subsection{Front-end Application}\\n\\nThe front-end application uses Spring Boot as its application framework. Spring Boot provides a configuration file that can be used as a starting point for building a web application with Spring Framework. We have also extended this configuration file with additional options in order to provide additional functionality.\\n\\nWe have created an extra module in our application called `MyApplication`. This module lets us define additional properties of our web application such as its port number or URL.\\n\\nWe have also created an additional module in our application called `MyApplication` which extends the `Application` class provided by Spring Boot. This module allows us to define additional properties of our web application such as its port number or URL.\\n\\nWe have also created an additional module in our application called `MyApplication` which extends the `Application` class provided by Spring Boot. This module allows us to define additional properties of our web application such as its port number or URL.\\n\\nOur web application uses Spring MVC as its MVC framework. We have defined several controllers that are responsible for managing incoming requests from users and sending them out to other controllers.\\n\\nOur web application uses Spring MVC as its MVC framework. We have defined several controllers that are responsible for managing incoming requests from users and sending them out to other controllers.\\n\\nOur web application uses Spring MVC as its MVC framework. We have defined several controllers that are responsible for managing incoming requests from users and sending them out to other controllers.\\n\\nOur web application uses Spring MVC as its MVC framework. We have defined several controllers that are responsible for managing incoming requests from users and sending them out to other controllers.\\n"}
{"File Name":"xebikart-infra\/006-use-password-authentication-for-rabbitmq.md","Context":"## Context and Problem Statement\\nWe are deploying a RabbitMQ cluster on GKE and people will need to connect to\\nit. We want to provide some kind of authentication so it's not totally\\nopen-bar.\\nWhat authentication mecanism should we use?\\n","Decision":"Chosen option: **\"User\/Pawword\"**, because:\\n- We want to make the RabbitMQ cluster quickly available to teams\\n- It will be easy to change to client certificate auth later on\\n- It is better than not providing auth at all\\nWe **will replace it by client certificates authentication later**.\\n","tokens":54,"id":981,"Predictions":"\\nThe decision is to use OAuth2. The reason for this is the following:\\nThe cluster is not a public cluster. It's behind a firewall and only people who have an account in our\\ncorporation can connect to it. We want to provide some kind of authentication so it's\\nnot totally open-bar.\\nWe are using GKE and therefore we can use the Kubernetes Identity Service to provide the\\nauthentication.\\nOAuth2 provides a mechanism that allows us to securely connect to resources\\nbehind firewalls.\\n\\n## Implementation\\n\\nWe will provide an API that implements OAuth2. It will allow users to authenticate themselves by providing their credentials in JSON format and then receive an access token that will allow them to access the cluster. The API will also return a token that can be used for the authentication process.\\nWe are using Python 3, but we could switch to Java if necessary.\\nWe are currently using AWS SDK for Python 3.\\nWe are using Flask as a framework for our API. We could switch to Django if necessary.\\nWe are currently using PostgreSQL as our database.\\n\\end{document}\\n"}
{"File Name":"xebikart-infra\/005-use-kubectl-and-helm-for-k8s-deployment.md","Context":"## Context and Problem Statement\\nWe want to **deploy** pods, services, and other resources on **Kubernetes**,\\nwhich is a GKE cluster for now.\\nWhat tool should we use to do this?\\n## Decision Drivers\\n- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\n","Decision":"- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\nChosen option: \"A mix of **kubectl** and **Helm**\", because:\\n- `kubectl` is the native way of deploying to Kubernetes and it thus the most\\ndocumented, widely-used, appropriate and up-to-date.\\n- Helm fits best for complex stacks deployment and we're eager to try things\\nsuch as Helm charts repositories!\\nHelm will be used as an alternative to `kubectl` in a second part, when we'll\\nramp up things and when we will need to deploy more complex stacks. The\\nbeginning will be `kubectl` for our own services, and Helm for\\ncomponents\/middleware\/databases with existing charts such as RabbitMQ!\\nThis imply that the provided Kubernetes cluster(s) for this project will need\\nto be **provisionned with Helm requirements such as Tiller**.\\n","tokens":155,"id":982,"Predictions":"## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n## Decision\\n\\n- \\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\n\\n\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird requirements? Auth to other systems?_\\n\\n- Reliability - _\"Will the tool usage often lead to messed up services\/cluster?\"_\\n\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird requirements? Auth to other systems?\\n- How \"far\" is the tool from the Kubernetes lifecycle?\\n\\n\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird requirements? Auth to other systems?\\n\\n\\n\\begin{lstlisting}\\n# Pod Deployment\\npod \"myPod\", \"type\": \"NodePort\", \\n  # ...\\n  # ...\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n# Service Deployment\\nservice \"myService\", \\n  # ...\\n  # ...\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n# Deployment with Kubernetes API\\ndeployment \"myDeployment\", \\n  # ...\\n  # ...\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n# Deployment with Kubernetes YAML file (e.g. deployment.yaml)\\ndeployment \"myDeployment\",\\n  resources: [\\n    { name: \"myService\",\\n      selector: \"service:myService\",\\n      ports: [\\n        { port: 80, targetPort: 80 },\\n        { port: 443, targetPort: 443 }\\n      ]\\n    }\\n    { name: myPod,\\n      selector: \"pod\/myPod\",\\n      ports:\\n        {\\n          containerPort: 80,\\n          hostPort: 80,\\n          protocol: 'TCP',\\n          targetPort: '80'\\n        }\\n    }\\n    {\\n      name: myCluster,\\n      selector:\\n        'app',\\n      resources:\\n        [\\n          { name:'myCluster', configMapKey:'configMap', configMapValue:{},\\n            podSelector:'pod\/myPod',\\n            serviceSelector:'service\/myService'\\n          }\\n        ]\\n    }\\n    {\\n      name:'otherService',\\n      selector:\\n        'service:someOtherService',\\n      ports:\\n        {\\n          containerPort: '80',\\n          hostPort: '443',\\n          protocol: 'TCP',\\n          targetPort: '8080'\\n        }\\n    }\\n    {\\n      name:'someOtherPod',\\n      selector:\\n        'pod\/someOtherPod',\\n      ports:\\n        {\\n          containerPort :1,\\n          hostPort :1,\\n          protocol :'TCP',  \\n         targetPort :1  \\n        }\\n    }\\n\\end{lstlisting}\\n\\n\\begin{lstlisting}\\n# Deployment with Kubernetes YAML file (e.g. deployment.yaml)\\ndeployment \"myDeployment\",\\nresources:\\n{\\n   podSelector:\"pod\/myPod\",\\n   serviceSelector:\"service\/myService\"\\n}\\n```\\nThe above solution is very useful for deploying pods and services. It can be used for any kind of deployment and can be used as an example for others.\\n\\nYou can add more decision drivers as needed.\\n\\nIf you are deploying something like an application or some kind of web service, this approach will be more useful than just using `kubectl` directly. You can also add more decision drivers as needed.\\n\\nIn this example, I am using `kubectl` directly because I want to deploy only pods and services. If you are deploying something like an application or some kind of web service, this approach will be more useful than just using `kubectl` directly. You can also add more decision drivers as needed.\\n\\n\\nIf you are deploying something like an application or some kind of web service, this approach will be more useful than just using `kubectl` directly. You can also add more decision drivers as needed.\\n"}
