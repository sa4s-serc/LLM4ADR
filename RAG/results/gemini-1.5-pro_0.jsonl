{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"text":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n\n##Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","MatchedIds":[1319,1303,1274,1302,1293],"Prediction":"## Decision\nThe \"Ingroup: <group>\" line shall be removed from error and warning messages. \n\nThis removal simplifies error messages, making them more concise and easier to understand for users. \n","GenTime":"2024-08-28 11:42:01"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"text":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n\n\n##Decision\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","MatchedIds":[1180,3159,3295,2181,885],"Prediction":"## Decision\n\nWe will use a naming convention for Exchanges that clearly identifies the purpose and involved components:\n\n**Format:** `<Source>.<Destination>.<Action>`\n\n**Where:**\n\n* **`<Source>`:**  Indicates the origin of the message (e.g., `BabelTower`, `ClientApp`, `ServiceA`).\n* **`<Destination>`:**  Indicates the intended recipient of the message (e.g., `Connector`, `ServiceB`, `QueueName`).\n* **`<Action>`:**  Describes the action associated with the message (e.g., `Events`, `Commands`, `Notifications`).\n\n**Examples:**\n\n* **`BabelTower.Connector.Commands`**: Used by BabelTower to send commands to the Connector service.\n* **`ClientApp.Connector.Events`**: Used by a Client Application to publish events to the Connector.\n* **`Connector.ServiceB.Notifications`**: Used by the Connector to send notifications to ServiceB.\n\n**Benefits:**\n\n* **Clarity:** The naming convention makes it self-explanatory what each Exchange is used for.\n* **Organization:**  Facilitates the organization and management of Exchanges as the system scales.\n* **Debugging:**  Easier to trace messages and identify potential issues.\n\nThis convention provides a flexible and scalable approach to naming Exchanges, ensuring clarity and maintainability within the messaging architecture. \n","GenTime":"2024-08-28 11:45:30"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"text":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n\n\n##Decision\nWe will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","MatchedIds":[5257,1173,4843,2520,2488],"Prediction":"## Decision\n\nWe will store the configuration in the browser's `localStorage`.\n\n### Reasons\n\n* **Simplicity:** localStorage provides a straightforward key-value storage mechanism directly accessible within the browser, eliminating the need for backend infrastructure or complex data synchronization logic for the pilot phase. \n* **Scope:** Given this is for a pilot, the amount of configuration data is likely small, making localStorage a suitable option.\n* **Performance:** Directly accessing configuration from `localStorage` will be faster than fetching it from a remote database.\n\n### Considerations\n\n* **Security:** If the configuration contains sensitive information, `localStorage` may not be suitable due to its vulnerability to XSS attacks. Consider alternative solutions or encryption for such data.\n* **Scalability:**  While sufficient for a pilot, if the configuration data grows significantly or needs to be shared across multiple devices, migrating to a backend database would be necessary for a production-ready solution.\n* **Data Persistence:** Be aware that users can clear their browser's `localStorage`. If data persistence across all scenarios is critical, a backend solution is recommended.\n\nThis approach allows for rapid prototyping and evaluation during the pilot phase.  We can re-evaluate the storage mechanism based on the pilot's learnings and the long-term requirements of the application.\n","GenTime":"2024-08-28 11:45:37"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"text":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n\n##Decision\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","MatchedIds":[1495,1494,4618,3692,847],"Prediction":"## Decision\nWe will abandon the \"front-house\/back-house\" split of a single agent. Instead, we will introduce two distinct types of agents: \n\n1. **User Agent:** This agent will reside on the user's client machine and will be responsible for holding the user's private keys, managing the source chain, running the app's ribosome, and interfacing with the UI. \n\n2. **HoloPort Agent:** This agent will reside on the HoloPort and will be responsible for handling DHT operations, routing, maintaining the world model, and acting as a proxy for the user agent. Importantly, the HoloPort Agent will have **its own set of keys**, separate from the user's keys. \n\nThis approach addresses the critical issue of requiring the user's keys for every network communication originating from the HoloPort. It allows for offline operation of the user agent, as the HoloPort agent can continue to participate in the network independently. The HoloPort agent can be seen as a delegate, acting on behalf of the user with a limited scope of permissions granted by the user.\n\nFurther research and design are needed to define the communication protocol between the User Agent and HoloPort Agent, including secure key management and delegation of permissions. \n","GenTime":"2024-08-28 11:45:44"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"text":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n\n##Decision\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","MatchedIds":[4388,4821,3722,979,1243],"Prediction":"## Decision\n\nWe will use **Airflow** to manage asynchronous tasks and workflows. \n\n**Reasoning:**\n\nWhile requiring us to manage the scheduler and a cache, Airflow offers the most compelling advantages:\n\n* **Mature and Stable:** Airflow has a large and active community, indicating stability and long-term support.\n* **Fits Execution Model:** Airflow's design aligns well with our need for both ad-hoc and scheduled tasks.\n* **Excellent UI:** Provides superior monitoring and management capabilities compared to other options.\n* **Python-based:** Integrates well with our existing technology stack.\n\n**Addressing Concerns:**\n\n* **Celery Dependency:**  While Airflow uses Celery, this is not a significant drawback. Celery is a robust and well-understood tool, and Airflow provides a higher-level abstraction for managing workflows.\n* **Scheduler and Cache Management:** We accept the operational overhead of managing the Airflow scheduler and cache as a trade-off for the framework's benefits.\n\n**Future Considerations:**\n\nWe will investigate and potentially adopt managed Airflow services as they mature to simplify operational overhead. \n","GenTime":"2024-08-28 11:45:50"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"text":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n\n\n##Decision\nThe [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","MatchedIds":[2316,4021,2261,3123,2520],"Prediction":"## Decision\n\nWe will store events in MongoDB using a consistent schema to ensure easy querying and processing. Each event will be stored as a separate document with the following structure:\n\n```json\n{\n  \"eventId\": \"uuid\", \/\/ Unique identifier for the event\n  \"eventType\": \"string\", \/\/ Type of event (e.g., \"userCreated\", \"orderPlaced\")\n  \"eventTimestamp\": \"datetime\", \/\/ Timestamp indicating when the event occurred\n  \"sourceSystem\": \"string\", \/\/ System where the event originated\n  \"correlationId\": \"uuid\", \/\/ Optional: ID to correlate events across multiple services\n  \"data\": {\n    \/\/ Event-specific data (can be nested)\n  }\n}\n```\n\n**Rationale:**\n\n* **Consistency:** A predefined schema ensures uniformity across all events, making it easier to query and analyze the data.\n* **Queryability:**  Fields like `eventType`, `eventTimestamp`, and nested fields within `data` allow for efficient querying based on different criteria.\n* **Scalability:** MongoDB's document-oriented nature allows for flexible schema evolution and easy scaling as the application grows.\n\n**Example:**\n\n```json\n{\n  \"eventId\": \"9b1deb4d-3b7d-4bad-9bdd-2b0d7b3dcb6d\",\n  \"eventType\": \"userCreated\",\n  \"eventTimestamp\": \"2023-10-26T10:00:00Z\",\n  \"sourceSystem\": \"authenticationService\",\n  \"data\": {\n    \"userId\": \"12345\",\n    \"username\": \"johndoe\",\n    \"email\": \"john.doe@example.com\"\n  }\n}\n```\n\n**Further Considerations:**\n\n* **Indexing:** Create indexes on frequently queried fields like `eventType` and `eventTimestamp` for improved query performance.\n* **Data Modeling:** Choose appropriate data types and structures within the `data` field based on the specific event and its payload.\n* **Schema Evolution:** Plan for schema changes over time and ensure backward compatibility with existing events. \n","GenTime":"2024-08-28 11:46:19"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"text":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n\n\n##Decision\nIn order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","MatchedIds":[2515,583,2471,4096,4642],"Prediction":"## Decision\n\nWe will use a **microservices architecture** deployed on **Google Kubernetes Engine (GKE)**, leveraging Google Cloud services for storage, search, and scalability.\n\n**Architecture:**\n\n* **Upload Service:** \n    * Handles image\/GIF uploads. \n    * Uses **Cloud Storage** buckets for storing original files.\n    * Generates thumbnails using **ImageMagick** or a similar library, stored separately in Cloud Storage.\n    * Publishes metadata (title, tags, upload timestamp, storage URLs) to a **Pub\/Sub** topic.\n* **Search Service:** \n    * Subscribes to the metadata Pub\/Sub topic.\n    * Indexes metadata into **Elasticsearch** for fast text-based search. \n* **Download Service:**\n    * Receives download requests.\n    * Fetches images from Cloud Storage based on provided IDs.\n    * Can leverage **Cloud CDN** for faster delivery of popular content.\n\n**Reasons:**\n\n* **Microservices on GKE:** Allows for independent scaling of components based on load. GKE provides managed Kubernetes, simplifying deployment and orchestration.\n* **Cloud Storage:** Scalable and cost-effective for storing large amounts of image\/GIF data.\n* **Elasticsearch:** Provides fast and feature-rich text search capabilities essential for searching by title and tags within the 500ms requirement.\n* **Pub\/Sub:** Decouples services, enabling asynchronous communication and improving fault tolerance.\n* **Google Cloud's Focus on Speed:** Services like Cloud CDN and the optimized networking within Google Cloud help achieve the desired search response time. \n\n**Benefits:**\n\n* **Scalability:** Easily handle increased traffic by scaling individual microservices and utilizing the inherent scalability of Google Cloud services.\n* **Performance:** Meet the 500ms search response time goal with Elasticsearch and potential CDN caching.\n* **Rapid Development:** Leverage managed services to focus on application logic rather than infrastructure management.\n* **Cost-Effectiveness:** Pay only for the resources consumed, scaling up or down based on demand. \n\n**Next Steps:**\n\n1. **Detailed Design:** Flesh out API contracts, database schemas, and service interactions.\n2. **Implementation:** Develop and test individual microservices.\n3. **Deployment:** Utilize Google Cloud Build and GKE for automated deployment pipelines.\n4. **Monitoring and Optimization:** Set up monitoring using Cloud Monitoring and logging with Cloud Logging to ensure performance and stability, making adjustments as needed. \n","GenTime":"2024-08-28 11:46:31"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"text":"## Context\\nWe need to choose a frontend framework.\\n\n\n##Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","MatchedIds":[1243,3715,3721,3718,472],"Prediction":"## Decision\nWe will use **React** as our frontend framework. \n\nReact's component-based architecture promotes code reusability and maintainability, which is crucial for building scalable and complex user interfaces. Its virtual DOM and efficient rendering algorithms contribute to a smooth and performant user experience. Additionally, React's vast ecosystem of libraries, tools, and its large, active community ensure we have ample resources and support for our development process. \n","GenTime":"2024-08-28 11:46:34"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"text":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n\n##Decision\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","MatchedIds":[5213,2410,4056,1784,996],"Prediction":"## Decision\n\nThe GovUK Notify \"Staging\" service is now classified as \"Live\". It will be treated as a live service with regards to monitoring, support, and any communication with users of the service. \n","GenTime":"2024-08-28 11:46:36"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"text":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n\n\n##Decision\nWe will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","MatchedIds":[280,2181,1610,4385,399],"Prediction":"## Decision \nWe will adopt the ISO 8601 standard, specifically as described in RFC 3339, for formatting date and time strings within our JSON payloads.\n\n### Reasons:\n* **Interoperability:**  ISO 8601 is a widely recognized international standard for date and time representation. Using this format enhances interoperability with various systems and programming languages, simplifying data exchange and reducing the risk of parsing errors. \n* **Clarity and Unambiguity:** The standard provides a clear and unambiguous format for representing date and time components, including timezone information. This reduces confusion and ensures consistent interpretation across different systems and developers.\n* **Community Adoption:** The widespread adoption of ISO 8601 within the JSON community further reinforces its suitability as the preferred choice for date-time formatting. \n* **Tooling and Library Support:** Numerous libraries and tools across different programming languages are readily available to parse and manipulate dates and times formatted according to ISO 8601.\n\nBy adhering to this standard, we aim to enhance the robustness, maintainability, and interoperability of our JSON data. \n","GenTime":"2024-08-28 11:46:42"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"text":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n\n\n##Decision\nChosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","MatchedIds":[2975,4644,160,798,1545],"Prediction":"## Decision\n\nChosen option: **Migrate entirely to Xlib.**\n\n### Positive Consequences\n\n- **Unified API:** Utilizing Xlib for all display management needs simplifies the codebase and avoids the hazards of mixing XCB and Xlib calls.\n- **Increased Functionality:** Xlib offers a more extensive set of functions compared to XCB, addressing the initial need for additional features.\n- **Mature and Well-documented:** Xlib is a mature and well-documented library with a large community, making it easier to find support and troubleshoot issues.\n\n### Negative Consequences\n\n- **Potential Performance Impact:** Xlib might have a slightly larger overhead compared to the lower-level XCB. However, this difference is often negligible in practice, especially when compared to the risks and complexities of maintaining a mixed system. \n- **Migration Effort:**  Migrating existing code from XCB to Xlib requires development time and effort.\n\n### Mitigation\n\n- Carefully plan and execute the migration process to minimize potential risks and downtime.\n- Thoroughly test the migrated codebase to ensure functionality and performance.\n-  Profile the application after migration to identify and address any potential performance bottlenecks.\n\n**Justification:** While XCB offers performance benefits, the hazards and complexity introduced by a mixed system outweigh the potential gains. Migrating entirely to Xlib provides a more stable, maintainable, and feature-rich solution, even with the slight potential performance trade-off and migration effort. \n","GenTime":"2024-08-28 11:46:50"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"text":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n\n\n##Decision\nWe will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","MatchedIds":[3784,1094,429,302,2516],"Prediction":"## Decision\n\nGiven the constraints of a greenfield project and the requirement for fast load times, we will employ a **microservices architecture style** with a strong emphasis on **asynchronous communication** and **performance optimization**.  \n\nThis decision is driven by the following factors:\n\n- **Greenfield Project**: Allows us to adopt a modern architecture without the limitations of legacy systems.\n- **Fast Load Times**: Microservices, with their smaller codebases and independent deployments, facilitate quicker startup times and optimized performance tuning for specific tasks.\n- **Quality Attributes**: Microservices inherently support scalability, maintainability, and independent development cycles, enabling us to deliver a high-quality application.\n\nAsynchronous communication patterns, such as message queues or event streaming, will be utilized wherever possible to minimize latency and enhance responsiveness. \n\nFurther architectural decisions, such as specific technologies and infrastructure choices, will be made as the project progresses and more detailed requirements become available. \n","GenTime":"2024-08-28 11:46:55"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"text":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n\n##Decision\nWe will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","MatchedIds":[2221,2515,2488,4859,2516],"Prediction":"## Decision\n\nWe will implement a service worker that acts as an intermediary between the frontend application and the backend API. This service worker will utilize IndexedDB to store user data, form submissions, and uploaded images locally.\n\n### Reasoning\n\n* **Loose Coupling:** A service worker decouples the frontend from the backend, enabling independent development and deployment. \n* **Offline Support:**  IndexedDB's offline capabilities ensure users can interact with the forms even without an internet connection. Data will be synced with the backend when connectivity is re-established.\n* **Data Persistence:** Unsubmitted forms and uploaded images stored in IndexedDB will be available for the user to recover, even if they close and reopen the application or browser.\n* **Image Handling:** IndexedDB's support for blob data allows us to store images locally before uploading them to the backend.\n* **Wide Support:**  IndexedDB boasts strong browser compatibility, ensuring a smooth experience for the majority of users.\n\n### High-Level Architecture\n\n1.  **Frontend Interaction:** The frontend application interacts with the service worker to perform data operations (read, write, update). \n2.  **Service Worker Logic:** \n    *   The service worker intercepts these requests and interacts with IndexedDB for local data management.\n    *   It synchronizes data with the backend API when online, handling potential conflicts gracefully. \n3.  **Backend API:**  The backend API provides the data endpoints for synchronization with the service worker.\n\n### Additional Considerations:\n\n* **Data Synchronization:** Implement a robust synchronization strategy to manage data consistency between IndexedDB and the backend, especially in scenarios like conflicting updates or intermittent connectivity.\n* **Security:** Securely store sensitive user data in IndexedDB using appropriate encryption techniques if required.\n* **Storage Limits:**  While IndexedDB offers decent storage, monitor usage and implement strategies for handling potential storage constraints, especially for large image uploads.\n","GenTime":"2024-08-28 11:47:05"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"text":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n\n##Decision\n* Chosen Alternative: creating scripts\\n","MatchedIds":[1173,4448,211,4358,4096],"Prediction":"## Decision\n\nWe will use a combination of the following approaches to manage credentials and environment-dependent information:\n\n**1. Secrets Management:**\n\n* **For sensitive information like API keys and database credentials:** Leverage a dedicated secrets management tool like HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault. These tools offer secure storage, access control, and rotation capabilities. \n* **Integrate the chosen tool with the deployment pipeline:** Inject secrets directly into the application environment at runtime, avoiding the need to hardcode them. This can be achieved through environment variables, volume mounts, or dedicated libraries provided by the secrets management tool.\n\n**2. Service Discovery:**\n\n* **For dynamic service addresses:** Implement a service discovery mechanism like Consul, etcd, or a cloud-provider specific solution (AWS Cloud Map, Azure DNS).\n* **Applications should discover and use service addresses dynamically:** Avoid hardcoding URLs and rely on service names resolved by the service discovery system.\n\n**3. Configuration Management:**\n\n* **For non-sensitive, environment-specific configurations:** Use configuration management tools like Spring Cloud Config Server, etcd, or environment variables.\n* **Externalize environment-dependent settings:**  Store these configurations separately from the application code and fetch them during application startup.\n\n**4. Infrastructure as Code:**\n\n* **Define and manage infrastructure in a declarative way:** Use tools like Terraform or AWS CloudFormation to provision and configure resources.\n* **Dynamically inject environment-specific information:** Pass configuration values like service addresses to applications during the deployment process.\n\n**Justification:**\n\nThis approach promotes security by separating sensitive information from the codebase, enhances flexibility through dynamic service discovery, and ensures consistency across different environments. By combining these methods, we can avoid hardcoding environment-specific details and ensure that applications have access to the necessary information at runtime. \n","GenTime":"2024-08-28 11:47:15"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"text":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n\n##Decision\n### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","MatchedIds":[1985,4354,810,4099,4119],"Prediction":"## Decision\n### Introduce Proof Operator\n\nInstead of using raw `[]byte` for Merkle proofs in `abci.ResponseQuery`, we introduce a new interface called `ProofOperator`. This interface defines the operations necessary to verify and combine Merkle proofs, allowing for greater flexibility and extensibility.\n\n```go\ntype ProofOperator interface {\n    \/\/ Name returns the name of the proof operator.\n    Name() string\n\n    \/\/ Apply applies the proof operator to the given key and value, returning the\n    \/\/ new value and proof.\n    Apply(key, value, proof []byte) ([]byte, []byte, error)\n}\n```\n\n### Proof Representation\n\nProofs will be represented as a list of `ProofOp` structures:\n\n```go\ntype ProofOp struct {\n    \/\/ Type is the type of proof operator.\n    Type string `json:\"type\"`\n\n    \/\/ Data is the data for the proof operator.\n    Data []byte `json:\"data\"`\n}\n```\n\nThe `Type` field identifies the specific `ProofOperator` implementation to be used. The `Data` field contains any data required by the operator, such as intermediate hashes or other proof elements.\n\n### Layered Proofs\n\nTo support multilayer proofs, each layer can be represented by a separate `ProofOp`. These `ProofOp`s can be chained together by including the output proof of one operator as the input proof for the next. This allows for the verification of complex proofs without requiring any intermediate decoding or re-encoding.\n\n### Third-party Proof Types\n\nThe `ProofOperator` interface enables the integration of third-party proof types. Developers can create custom implementations of the interface to support new proof formats and algorithms. This extensibility ensures that the system can evolve to accommodate future advancements in Merkle proof technology.\n\n### Migration\n\nExisting uses of `[]byte` for Merkle proofs in `abci.ResponseQuery` can be migrated to the new `ProofOp` format.  A default `ProofOperator` implementing the current simple Merkle proof format can be provided for backwards compatibility.\n\n\nBy adopting this approach, we achieve the following:\n\n* **Layering:** Seamlessly layer Merkle proofs without cumbersome decoding\/reencoding steps.\n* **Generalization:** Provide a standardized way to chain proofs, enabling complex verification procedures.\n* **Extensibility:**  Open the door for third-party proof types, ensuring future compatibility and innovation.\n\nThis change enhances the flexibility and power of Merkle proofs within the system, paving the way for more sophisticated and efficient verification mechanisms. \n","GenTime":"2024-08-28 11:47:28"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"text":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n\n\n##Decision\nWe use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","MatchedIds":[2520,1089,4061,1740,1239],"Prediction":"## Decision\n\nWe will host the database on **AWS RDS (Relational Database Service)**. \n\nThis decision is driven by the following factors:\n\n* **Managed Service:** RDS handles much of the database administration tasks like provisioning, backups, replication, and patching, freeing us to focus on application development.\n* **Scalability and Availability:** RDS allows for easy scaling of compute and storage resources, and offers multiple availability zone deployments for high availability.\n* **Cost-Effectiveness:**  While not the absolute cheapest option, RDS offers a good balance of cost and performance, with various pricing tiers to suit different needs.\n* **Integration:** As we are already considering AWS for other services, using RDS ensures seamless integration within the AWS ecosystem. \n\nWe will need to further decide on the specific database engine (e.g. MySQL, PostgreSQL, Aurora) based on the project's specific requirements and performance needs. \n","GenTime":"2024-08-28 11:47:34"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"text":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n\n##Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","MatchedIds":[4738,4695,1701,4644,2975],"Prediction":"## Decision\nChosen option: **Implement a SQL-like query language for custom dashboards.**\n\n### Reasons\n* **Familiarity:**  Many users building dashboards will be familiar with SQL or SQL-like syntax, making the learning curve less steep.\n* **Expressiveness:** SQL provides a powerful and flexible way to express complex queries, including joins, aggregations, subqueries, and more.\n* **Existing Resources:**  Leveraging a SQL-like language allows us to utilize existing parsing libraries, documentation, and potentially even tooling.\n\n### Features\n* **Basic SQL syntax:**  SELECT, FROM, WHERE, GROUP BY, ORDER BY, aggregations (COUNT, SUM, AVG, etc.)\n* **Data Source Abstraction:**  The query language will reference data sources and fields in an abstract way, allowing it to be adapted to different underlying data stores.\n* **Functions:**  Common functions for data manipulation and analysis (e.g., string operations, date\/time functions) will be supported.\n\n### Implementation Details\n* A parser and interpreter will be developed to translate the SQL-like queries into queries executable by the underlying data store.\n* A mechanism will be provided to map abstract data source and field names to the actual data model.\n* Security considerations will be addressed to prevent SQL injection attacks.\n\n### Positive Consequences\n* **Increased Flexibility:** Users can create highly customized dashboards with complex data filtering and aggregation.\n* **Faster Dashboard Development:** The expressive query language simplifies the process of defining intricate data views.\n* **Potential for Standardization:** This query language could potentially be used for other data analysis and reporting tasks within the application.\n\n### Negative Consequences\n* **Complexity:** Implementing a full-featured SQL-like language can be complex and time-consuming.\n* **Learning Curve:** While familiar to many, users unfamiliar with SQL will still need to learn the basics of the query language.\n\n### Alternatives Considered\n* **GUI-based Query Builder:** While user-friendly, these often lack the flexibility to express complex queries.\n* **Domain-Specific Language (DSL):** A custom DSL could be developed, but this increases the learning curve and development effort.\n\n### Next Steps\n* Define the grammar and syntax of the query language.\n* Choose a parsing library or framework.\n* Design the mapping between the abstract query language and the underlying data model.\n* Develop a prototype and gather feedback from potential users. \n","GenTime":"2024-08-28 11:47:45"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"text":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n\n\n##Decision\nAdopt Hexagonal Architecture for project.\\n","MatchedIds":[1094,429,885,4896,276],"Prediction":"## Decision\n\nWe will use the **Broker architectural style** for the SDARS application. \n\n### Reasoning\n\nThe Broker style promotes loose coupling and flexibility by introducing a central broker component responsible for routing and transforming messages between independent components. This approach aligns well with the requirement of enabling \"different communication ways\" among the three components of SDARS. \n\n### Implementation Details:\n\n1. **Components:** The three independent components of SDARS remain as separate modules, each with its defined functionalities.\n2. **Broker:** A central broker component will be introduced, responsible for:\n    * **Routing:** Directing messages between the correct sender and receiver components.\n    * **Transformation:** Potentially transforming message formats for compatibility if required.\n    * **Communication Protocols:** Supporting different communication methods (e.g., REST, message queues, WebSockets) based on the needs of each component.\n3. **Loose Coupling:** Components interact with each other indirectly through the broker, reducing dependencies and promoting modularity.\n\n### Benefits:\n\n* **Flexibility:** Easily add new communication methods or components without impacting existing ones by extending the broker's capabilities.\n* **Scalability:**  Components can be scaled independently based on their individual needs.\n* **Maintainability:** Simplifies maintenance and updates as changes in one component are less likely to ripple through the entire system.\n\n### Considerations:\n\n* **Single Point of Failure:** The broker can become a single point of failure, so implement redundancy or failover mechanisms.\n* **Complexity:** While offering flexibility, the broker can introduce complexity, especially if supporting many communication protocols or complex routing logic. \n","GenTime":"2024-08-28 11:47:53"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"text":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n\n\n##Decision\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","MatchedIds":[187,3171,4847,351,1739],"Prediction":"## Decision\n\nTo address the contention around the single Test Editor and unblock the pipeline, we will implement a \"dynamic environments\" strategy for the Editor in our test environment:\n\n**1. Dynamic Provisioning:**\n\n*  Instead of a fixed Test Editor, we will leverage infrastructure-as-code to spin up ephemeral Editor instances on demand for each pull request. \n*  These PR-specific Editors will have a unique URL or identifier.\n*  Necessary resources (servers, databases, etc.) will be created and configured automatically.\n\n**2. Automated Deployment:**\n\n*  The CI\/CD pipeline will automatically deploy the relevant code changes for a pull request to its corresponding Editor instance.\n\n**3. Parallel Testing:**\n\n*  Multiple developers can test their features concurrently in their isolated Editor environments without blocking each other or the live-production deployment.\n\n**4. Teardown:**\n\n*  Once testing is completed (either manually or after a defined timeframe), the dynamic Editor environment will be automatically torn down to release resources.\n\n**5. Pipeline Enhancement:**\n\n*  The deployment pipeline will be modified to deploy to the ephemeral Test Editors.\n*  A manual approval step will remain for promoting a tested version from a Test Editor to the Staging environment.\n\n**Diagram:**\n\n![alt text](0010-release-pipeline-decision.png \"Diagram of how the editor is released through the pipeline.\")\n\n**Benefits:**\n\n* **Increased Development Velocity:** Eliminates bottlenecks and allows for faster feedback loops.\n* **Improved Collaboration:** Developers can work independently on their features.\n* **Reduced Risk:** Issues are caught and addressed earlier in isolated environments.\n* **Efficient Resource Utilization:** Dynamic provisioning ensures resources are used only when needed.\n","GenTime":"2024-08-28 11:48:04"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"text":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n\n\n##Decision\nJest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","MatchedIds":[2273,452,4179,1258,4223],"Prediction":"## Decision\n\nWe will use Jest as our testing framework. While Lab is a viable option for testing, its small community leads to integration and configuration challenges, as seen with the Typescript coverage reporting issue. Ava, while having first-class Typescript support, suffers from performance limitations highlighted by respected community members. Jest, being a widely adopted and performant framework, provides a good balance of features, ease of use, and community support. \n","GenTime":"2024-08-28 11:48:07"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"text":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n\n##Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","MatchedIds":[157,381,802,1125,2655],"Prediction":"## Decision\n\nWe will use a **CSS Modules with TypeScript** approach for styling our components. This approach addresses all our decision drivers:\n\n**Performance:**\n\n* **Code splitting:** CSS Modules generate unique class names, eliminating the risk of style collisions and enabling efficient code splitting at the component level.\n* **Caching:** CSS Modules compile to static CSS files, allowing browsers to cache styles effectively.\n* **Minimal runtime overhead:** CSS Modules have minimal runtime overhead since they don't rely on JavaScript for style injection at runtime.\n\n**Design System Integration and Type Safety:**\n\n* We'll use a **shared design system package** (written in TypeScript) containing constants for colors, typography, spacing, etc. \n* This package can be imported into both our CSS Modules (using CSS variables or a preprocessor like Sass) and JavaScript logic, ensuring consistency.\n* TypeScript definitions for the design system package guarantee type safety across CSS and JavaScript.\n\n**Component Encapsulation and Reusability:**\n\n* CSS Modules' local scope by default enforces style encapsulation within components, preventing unintended style leaks.\n* Reusable components will not accept arbitrary styles\/classes, ensuring their integrity. \n* Customization will be achieved through well-defined component props, promoting predictable behavior.\n\n**Responsive Props and Atomic Layout:**\n\n* We will adopt a system for **responsive props**, allowing developers to specify different styles based on breakpoints defined in our design system.\n* These responsive props can be easily integrated into our components using CSS Modules and media queries.\n* For layout, we'll use a library of small, reusable, **atomic layout components** like `Flex`, `Grid`, `Box` etc. These components will come with their own predefined CSS Modules and be styled using the shared design system, ensuring consistency and ease of use.\n\n**Additional Considerations:**\n\n* **Tooling:**  We'll utilize a build process that bundles and optimizes CSS Modules, ensuring efficient delivery.\n* **Documentation:**  Clear documentation and examples will be provided to guide developers on using CSS Modules effectively with our design system.\n\nThis approach provides a robust, performant, and maintainable styling solution that aligns with our requirements for building a consistent and scalable design system.\n","GenTime":"2024-08-28 11:48:18"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"text":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n\n\n##Decision\nWe will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","MatchedIds":[452,2273,4223,3323,4179],"Prediction":"## Decision\n\nWe will use a combination of tools for a robust testing strategy:\n\n* **Jest:** As a versatile testing framework with excellent TypeScript and Node.js support, Jest will be our primary testing framework.\n* **React Testing Library (RTL):** For testing React components, RTL will ensure our tests focus on user interactions and component behavior rather than implementation details. \n* **Cypress or Playwright:** For comprehensive end-to-end testing that simulates real user interactions within a browser environment, we will utilize either Cypress or Playwright depending on specific project needs.\n* **Storybook:** To facilitate component isolation and visual testing during development, we will integrate Storybook into our workflow.\n\nThis combination of tools will enable us to write various types of tests, ensuring our code is well-tested and functions as intended across different levels of the application. \n","GenTime":"2024-08-28 11:48:24"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"text":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n\n\n##Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","MatchedIds":[885,2343,2791,2499,786],"Prediction":"## Decision\n\n**Chosen option:** SNS + SQS\n\n**Rationale:**\n\nWhile SNS alone offers a simple pub\/sub model suitable for immediate notifications,  the combination of SNS and SQS provides greater flexibility and robustness for microservice communication, especially when considering data synchronization scenarios:\n\n* **Guaranteed Delivery and Retry:** SQS queues offer durable message storage and retry mechanisms. This ensures messages are delivered even if the consuming microservice is temporarily unavailable, crucial for reliable data synchronization.\n* **Decoupling:**  SNS acts as a message router, decoupling the sender from knowing the details of the consumers. SQS queues further enhance this by allowing microservices to consume messages at their own pace, improving fault tolerance and independent scaling.\n* **Flexibility:** The combination supports both push-based (via SNS for immediate notifications) and pull-based (via SQS for asynchronous processing) consumption patterns, catering to diverse microservice needs.\n\n**Trade-offs:**\n\n* **Increased Complexity:** Integrating SQS introduces an additional component requiring configuration and management.\n* **Cost Implications:** While both services offer tiered pricing, using SNS+SQS can lead to slightly higher costs compared to SNS alone, depending on usage patterns.\n\n**Mitigation:**\n\n* Leverage Infrastructure-as-Code (IaC) tools like AWS CDK or Terraform to streamline the deployment and management of SNS and SQS resources.\n* Carefully analyze message volume and consumption patterns to optimize cost based on available SNS and SQS pricing tiers.\n\n**Conclusion:**\n\nFor robust and reliable microservice communication, particularly for critical data synchronization scenarios, the combination of SNS and SQS offers significant advantages over using SNS alone. While slightly more complex, the benefits of guaranteed delivery, decoupling, and flexibility outweigh the trade-offs. \n","GenTime":"2024-08-28 11:48:33"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"text":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n\n\n##Decision\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","MatchedIds":[872,1631,241,641,128],"Prediction":"## Decision\n\nWe will use **explicit library loading without a globally predefined default version.** This decision prioritizes clarity, maintainability, and flexibility for library consumers.\n\n**Rationale:**\n\n- **Explicitness & Version Awareness:** Requiring explicit imports with version specifications makes library usage transparent and promotes version awareness. Consumers understand which library version their pipelines rely on, reducing the risk of unexpected breakages due to implicit updates.\n- **Controlled Upgrades:**  This approach puts consumers in control of library upgrades. They can explicitly update the version in their pipeline code when desired, preventing unintentional adoption of breaking changes from new library versions.\n- **Flexibility with Release Channels:**  Explicit versioning enables consumers to leverage release channels (e.g., branches, tags) directly. They can pin their pipelines to specific versions or automatically receive updates by referencing a branch.\n- **Reduced Global Configuration:**  Avoiding a global default version simplifies the overall configuration and reduces potential conflicts or confusion. Each pipeline explicitly defines its dependencies.\n\n**Trade-offs:**\n\n- **Slightly More Verbose:** Explicit imports require a minor increase in code verbosity.\n- **Potential for Version Inconsistency:** Different pipelines might use different library versions, requiring more careful management of interoperability if pipelines share data or artifacts.\n\n**Mitigation:**\n\n- Encourage clear documentation and versioning practices for the library to help consumers make informed decisions about upgrades.\n- Provide tooling or automation to help consumers identify and manage library versions across their pipelines. \n\nThis approach balances ease of use with the crucial aspects of version control and maintainability, making it well-suited for a shared library. \n","GenTime":"2024-08-28 11:48:41"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"text":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n\n\n##Decision\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","MatchedIds":[4980,5188,1279,1193,1291],"Prediction":"## Decision\nIntroduce a dedicated configuration object to encapsulate the initialization parameters for presenters. \n\nInstead of passing individual parameters, pass this configuration object to the presenter's initializer. This improves code readability, maintainability, and reduces the likelihood of errors when adding or modifying initialization parameters in the future.\n\n**Example:**\n\n**Before:**\n\n```\n\/\/ Lengthy parameter list\nlet presenter = MyPresenter(view: view, service: service, analytics: analytics, logger: logger, ...) \n```\n\n**After:**\n\n```\n\/\/ Configuration object\nstruct PresenterConfiguration {\n  let view: ViewProtocol\n  let service: ServiceProtocol\n  let analytics: AnalyticsProtocol\n  let logger: LoggerProtocol\n  \/\/ ... other parameters\n}\n\n\/\/ Presenter initialization\nlet config = PresenterConfiguration(view: view, service: service, analytics: analytics, logger: logger, ...)\nlet presenter = MyPresenter(config: config) \n```\n\nThis approach provides several benefits:\n\n* **Improved Readability:** The code becomes more concise and easier to understand.\n* **Reduced Errors:**  Passing a single object minimizes the risk of passing parameters in the wrong order or with incorrect types.\n* **Easier Maintenance:** Adding or removing parameters in the future only requires modifying the configuration object, not every presenter initialization.\n* **Flexibility:**  Configuration objects can be easily extended with default values or optional parameters. \n","GenTime":"2024-08-28 11:48:49"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"text":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n\n##Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","MatchedIds":[1992,157,381,1193,4613],"Prediction":"## Decision\n\n**Components in FDS will not accept a `classNames` prop.** \n\nInstead, we will provide a robust API surface area through existing props and expand on them as needed. \n\n### Reasoning\n\nWhile a `classNames` prop offers flexibility, it undermines the integrity and predictability of FDS components. This approach leads to:\n\n- **Loss of Control:** Arbitrary class names make it difficult to maintain the intended design and behavior of components. \n- **Styling Conflicts:** External styles could clash with the component's internal styles, leading to unpredictable results. \n- **Reduced Reusability:** Components become less reusable when tightly coupled with external styles.\n\n### Alternatives Considered\n\n1. **`classNames` prop with limitations:** This would involve sanitizing or restricting the class names that can be passed in. However, this adds complexity and still doesn't fully address the underlying issues. \n\n### Positive Consequences\n\n- **Predictable Behavior:** Components will behave consistently, regardless of where they are used.\n- **Easier Maintenance:**  A controlled style interface simplifies updates and reduces the risk of regressions.\n- **Enhanced Reusability:** Components remain self-contained and easily reusable across different projects.\n\n### Negative Consequences\n\n- **Potentially Reduced Flexibility:**  Developers may have less freedom to customize the appearance of components. \n\n### Future Considerations\n\n- **Component-Specific Styling APIs:** If a genuine need for customization arises, we can explore component-specific styling props (e.g., a `variant` prop to switch between pre-defined styles). \n- **Composition over Inheritance:** Encourage the creation of new components by composing existing FDS components to achieve unique designs.\n\nThis decision prioritizes the long-term maintainability, consistency, and reliability of FDS components. We believe this approach will provide a solid foundation for building scalable and robust user interfaces. \n","GenTime":"2024-08-28 11:48:58"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"text":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n\n\n##Decision\nDe-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","MatchedIds":[4852,1927,2975,3290,3159],"Prediction":"## Decision\n\nWe will create an abstraction layer for logging within our applications. This will involve:\n\n* **Defining a common interface:** This interface will expose a consistent set of methods for logging messages with different severity levels (e.g., DEBUG, INFO, WARNING, ERROR). \n* **Implementing a core logging library:** This library will handle basic log formatting and provide a default output (e.g., stdout). \n* **Utilizing pluggable handlers:**  The core logging library will support various handlers that can be attached and configured to process log entries differently. \n    * Examples:\n        * **File Handler:** Writes logs to a specified file.\n        * **Remote Handler:** Forwards logs to a centralized logging system (e.g., Splunk, Graylog).\n        * **Filtering Handler:**  Filters log messages based on severity level, source module, or other criteria.\n        * **Formatting Handler:** Modifies the format of log entries (e.g., JSON, key-value pairs). \n\nThis approach offers several benefits:\n\n* **Consistency:** Developers will use a unified API for logging regardless of the underlying implementation.\n* **Flexibility:**  The system can be easily adapted to different logging backends and requirements by adding or configuring handlers.\n* **Testability:**  The abstraction layer allows for easy mocking and testing of logging functionality in isolation from specific handlers. \n","GenTime":"2024-08-28 11:49:05"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"text":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n\n\n##Decision\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","MatchedIds":[1606,779,3159,3845,138],"Prediction":"## Decision\nWe will implement an ETag-based mechanism using the following approach:\n\n1. **ETag Generation:**\n    * For every resource that is updated asynchronously, we will generate an ETag. \n    * This ETag will be a hash or a unique identifier representing the state of the resource in the projection.\n    * The ETag will be returned in the response header (e.g., `ETag: \"some-hash\"`) for any API request that modifies the resource.\n\n2. **Client-Side Caching and Validation:**\n    * The client (UI) will store the ETag received from the API response. \n    * For subsequent requests to fetch the resource, the client will include the stored ETag in the `If-None-Match` header.\n\n3. **Server-Side Validation:**\n    * The API will compare the received `If-None-Match` header with the current ETag of the resource in the projection.\n    * If the ETags match, the server will return an HTTP 304 (Not Modified) response. This tells the client that the cached data is still valid.\n    * If the ETags don't match, the server returns the updated resource representation with the new ETag, indicating the client's data is outdated.\n\n**Benefits:**\n\n* Reduces unnecessary data transfer by allowing the client to use its cached resource when the projection is not yet updated.\n* Improves user experience by preventing the display of stale data.\n* Provides a standard and efficient mechanism for cache validation.\n\n**Considerations:**\n\n*  Requires careful handling of ETag generation and synchronization to avoid data inconsistencies. \n*  Client-side logic is needed to handle different HTTP status codes (200, 304). \n","GenTime":"2024-08-28 11:49:15"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"text":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n\n\n##Decision\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","MatchedIds":[1318,1289,5227,1310,1278],"Prediction":"## Decision\nIntroduce a feature, either through command line arguments or APIs,\\nto only modify a single key. With this feature, tools should by default not copy any structure from the original configuration file.\n\nExample use cases:\n\n- Using `kdb set user:\/example\/key value`, the configuration file should afterwards only contain `user\/example\/key=value` no matter what was in the configuration file before.\n- For tools like `kdb meta-set`, the behavior should stay the same to not break expected behavior. They should explicitly opt-in to the new behavior if desired.\n\n- [Single Key Modifications](single_key_modifications.md) \n","GenTime":"2024-08-28 11:49:19"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"text":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n\n\n##Decision\nWe remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","MatchedIds":[2520,3657,4859,2221,1239],"Prediction":"## Decision\n\nWe will **not** be using CockroachDB for our persistence layer, despite its potential benefits. \n\n**Reasoning:**\n\nWhile CockroachDB offers appealing features like distributed scalability and resilience, the lack of full, production-ready support for Windows poses a significant risk.  Our current reliance on Windows infrastructure makes deployment and maintenance unpredictable. \n\nThis decision prioritizes:\n\n* **Stability:**  Relying on a fully supported platform for our core database ensures a more predictable and stable operational environment. \n* **Maintenance Overhead:**  Dealing with potential compatibility issues or limitations on Windows would introduce unnecessary complexity and overhead for our team.\n\n**Alternatives to Consider:**\n\n* **Re-evaluate Windows Dependency:** If CockroachDB's strengths are compelling enough, investigate the feasibility of transitioning parts of the infrastructure away from Windows. \n* **Explore Other Database Options:** Continue evaluating alternative database technologies that offer robust Windows support and align with the project's scalability and performance needs. \n\nThis decision will be revisited if our infrastructure requirements change or CockroachDB significantly improves its Windows support. \n","GenTime":"2024-08-28 11:49:25"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"text":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n\n\n##Decision\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","MatchedIds":[3853,3094,3657,779,4821],"Prediction":"## Decision\n\nWe will use the following rules to determine the appropriate `on_delete` behaviour for foreign keys:\n\n* **Default to `:restrict`**: This enforces the strictest level of referential integrity by default and helps prevent accidental data loss.\n* **Use `:cascade` for parent-child relationships**: When deleting a parent record, automatically delete its child records. This applies to cases like deleting a document and its associated editions.\n* **Use `:nullify` for optional relationships**: When the foreign key can be NULL, and deleting the referenced record doesn't invalidate the referencing record. Use this when it's acceptable for the relationship to be broken by setting the foreign key to NULL. \n* **Document deviations**: If `:restrict` is not appropriate, clearly document the rationale for using `:cascade` or `:nullify` for a specific foreign key constraint.\n\nThis approach ensures data integrity as a priority while allowing flexibility when necessary. Clear documentation of deviations helps maintain understanding and consistency across the codebase. \n","GenTime":"2024-08-28 11:49:32"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"text":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n\n\n##Decision\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","MatchedIds":[405,3637,899,3635,2963],"Prediction":"## Decision\n\nIntroduce a new rendering option for the AMF parser that controls how inlined external references are handled during emission:\n\n1. **Preserve Inline (Default):** This option maintains the current behavior, emitting the inlined content as is, even if no explicit link exists in the model.\n2. **Emit Link:** This option forces the emitter to generate a link to the external reference, even if the original model inlined the content. This link can be a:\n    - **Relative Link:**  Pointing to the external file's location relative to the base unit's location. This is suitable for scenarios where the files are expected to be co-located.\n    - **Absolute Link:** Utilizing an established URI scheme to reference the external file. This ensures resolvability regardless of file location.\n3. **Discard Inline:** This option omits the inlined content entirely during emission. This is useful when only the existence of the external reference is important, and the inlined content is deemed redundant.\n\nThis approach provides flexibility to users, allowing them to choose the behavior that best suits their needs when processing and presenting AMF models containing inlined external references. \n","GenTime":"2024-08-28 11:49:38"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"text":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n\n\n##Decision\nThe guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","MatchedIds":[5171,5172,5177,4245,3203],"Prediction":"## Decision\n\nTo maximize interoperability and flexibility within the PACE ecosystem, we adopt the following guiding principles for designing interfaces between different programs and components:\n\n1. **Language-agnostic communication:**  Communication between PACE components should rely on language-agnostic protocols and data formats. This will be achieved primarily through:\n    * **REST APIs:** For synchronous communication where a request-response pattern is suitable. \n    * **Message queues:** For asynchronous communication and event-driven architectures.\n    * **Standardized data formats:** JSON or other well-defined, language-agnostic formats for data exchange.\n\n2. **Well-defined interfaces:** Each component will expose well-defined APIs with clear specifications (e.g., using OpenAPI for REST APIs). This ensures that different teams can independently develop and maintain their components, relying on stable interfaces for integration. \n\n3. **Modularity and loose coupling:**  Components should be designed as modular units with minimal dependencies on each other. This facilitates independent development, testing, and deployment. \n\n4. **Documentation:**  All interfaces will be thoroughly documented, including usage examples, to facilitate integration and adoption by different teams.\n\nThese principles will guide the design and development of interfaces between PACE components, promoting interoperability, flexibility, and maintainability within the project. \n","GenTime":"2024-08-28 11:49:45"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"text":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n\n##Decision\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","MatchedIds":[3987,3416,4533,3852,2215],"Prediction":"## Decision\n\nDue to the inability to reliably verify file types through headers alone, we will **abandon the approach of client-side direct uploads to S3 with header-based validation**. \n\nTo mitigate the risk of malicious file uploads, we will implement one of the following alternatives:\n\n1. **Server-Side Upload and Validation:** Clients will upload files to our server, where we will perform rigorous file-type validation using libraries like `mimetypes` (basic) or more robust tools like `filetype` (Python) or similar libraries in other languages. This allows for content and not just header inspection. Only after successful validation will the file be uploaded to S3.\n\n2. **S3 Pre-Signed URLs with Restricted Functionality:** While we will still utilize S3 pre-signed URLs, we will restrict their functionality to only allow uploads of specific file types using the `x-amz-content-type` header condition. This adds an extra layer of security but may not be as robust as server-side validation.\n\n3. **Managed File Upload Service:** We will explore integrating a third-party service specializing in secure file uploads, such as Cloudinary or similar. These services often provide advanced security features like file type validation, virus scanning, and content moderation.\n\nThe choice between these options will depend on factors like the desired level of security, implementation complexity, and cost considerations. We will evaluate each option and make a final decision based on our specific needs and constraints. \n","GenTime":"2024-08-28 11:49:52"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"text":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n\n\n##Decision\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","MatchedIds":[649,1035,4761,3470,4271],"Prediction":"## Decision\nWe will use Java as the programming language for the DOS component and leverage the Spring framework for building RESTful web services. This combination offers a robust ecosystem, addresses many non-functional requirements out of the box, and provides access to mature libraries, ultimately facilitating a stable and maintainable solution for our collections platform. \n","GenTime":"2024-08-28 11:49:55"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"text":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n\n##Decision\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","MatchedIds":[577,4443,4645,4448,2397],"Prediction":"## Decision\n\nWe will decouple the authentication configuration from `panoptes-javascript-client` to allow for more flexible configuration options. \n\nInstead of relying solely on environment variables, the client will be modified to accept an authentication configuration object during initialization.  This object can be populated from various sources based on the consuming app's needs, such as:\n\n- **Configuration Files:** Applications can load configuration from files like `.env`, JSON, or YAML, allowing for easy management and version control of API endpoints and other settings.\n- **Server-Side Rendering:**  For Next.js applications and other server-side rendering scenarios, the configuration can be injected from the server-side environment and passed down to the client.\n- **Build-Time Configuration:** Similar to existing Single Page Applications, build tools can be used to inject configuration values at build time.\n\nThis approach provides the following advantages:\n\n- **Flexibility:** Developers gain flexibility in choosing how they configure the authentication client, adapting to different application architectures and deployment environments.\n- **Single Source of Truth:**  Configuration can be managed from a central location, reducing the risk of inconsistencies and errors.\n- **Improved Security:**  Sensitive information like API keys are not directly embedded in the client-side code, enhancing security.\n\n**Implementation Details:**\n\n- Introduce a new method or property in `panoptes-javascript-client` to accept an authentication configuration object.\n- Update the client's internal logic to use the provided configuration instead of directly accessing environment variables.\n- Maintain backward compatibility by providing a default configuration path or fallback mechanism for existing applications that rely on environment variables.\n- Document the new configuration options and provide clear examples of how to configure the client using different approaches.\n\nThis decoupling will improve the usability and maintainability of `panoptes-javascript-client` for various applications, particularly those with server-side rendering or complex configuration needs. \n","GenTime":"2024-08-28 11:50:04"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"text":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n\n\n##Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","MatchedIds":[5157,3421,583,4747,2397],"Prediction":"## Decision\n\n**Chosen option:** Auth0 (or a similar Identity-as-a-Service provider)\n\n**Reasoning:**\n\nGiven the decision drivers prioritizing ease of deployment, cost-effectiveness, and feature completeness, leveraging a cloud-based Identity-as-a-Service (IDaaS) solution like Auth0 presents a compelling option. Here's why:\n\n* **Ease of Deployment & Configuration:** Auth0 excels in this area, offering a streamlined setup process and intuitive management console. This significantly reduces the initial time and effort required to integrate identity management into the platform. \n* **Cost-Effectiveness:**  Auth0 operates on a tiered subscription model, often with a generous free tier suitable for smaller projects. This allows for cost optimization, particularly in the early stages, with the flexibility to scale as usage grows. \n* **Feature Richness:** Auth0 provides all the core features required for the BFI platform:\n    * User creation and registration workflows\n    * Comprehensive user management capabilities\n    * Secure authentication protocols (e.g., OpenID Connect, OAuth 2.0)\n    * Role-based authorization mechanisms for fine-grained access control\n\n**Alternatives Considered:**\n\n* **Self-Hosted Open-Source Solution:** While potentially offering greater control, this approach incurs higher setup and maintenance overheads, contradicting the drivers of ease and cost-effectiveness. \n* **Custom-Built Solution:** This option offers maximum customization but demands substantial development effort, time investment, and ongoing maintenance, making it less desirable given the project constraints.\n\n**Tradeoffs:**\n\nThe primary tradeoff with an IDaaS solution like Auth0 is a degree of vendor lock-in. However, the benefits of rapid deployment, reduced operational burden, and comprehensive feature set outweigh this concern, particularly for a non-core functionality like identity management.\n\n**Next Steps:**\n\n1. **Evaluate Auth0 (or similar IDaaS providers):**  Conduct a thorough evaluation of Auth0's features, pricing, and security practices. Consider alternative IDaaS providers like AWS Cognito or Azure AD B2C for comparison. \n2. **Proof of Concept:** Develop a small-scale proof of concept integrating Auth0 with the IIIF Universal Viewer auditing platform to validate its suitability.\n3. **Implementation:**  Integrate the chosen IDaaS solution into the platform, configure user roles and permissions, and thoroughly test the authentication and authorization flows. \n","GenTime":"2024-08-28 11:50:15"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"text":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n\n\n##Decision\nCopilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","MatchedIds":[2236,2968,2240,4538,631],"Prediction":"## Decision\n\nWe will adopt MCP as the primary mechanism for configuring Istio within our platform. This decision is based on the following factors:\n\n1. **Simplified Client Logic:** By moving configuration logic to the server (Pilot), we simplify the client-side components (Envoy proxies). This reduces the potential for client-side bugs and eases the burden of maintaining client-side code. \n\n2. **Centralized Configuration Management:** MCP provides a centralized and standardized way to manage Istio configurations. This simplifies the management of large and complex deployments.\n\n3. **Extensibility:** MCP provides a well-defined API for extending Istio's configuration capabilities. This allows us to easily integrate with other systems and adapt to future changes.\n\n4. **Improved Performance:** By offloading configuration logic to the server, MCP has the potential to improve performance by reducing the workload on client-side components.\n\n**Implementation Details:**\n\n* We will leverage Istio's built-in support for MCP to configure the Envoy proxies.\n* We will explore existing tools and frameworks for managing and distributing Istio configurations using MCP.\n* We will develop a strategy for migrating existing configuration mechanisms to MCP. \n\n**Potential Challenges:**\n\n* The adoption of MCP may introduce new complexities in terms of server-side management and configuration. \n* We will need to ensure that our existing monitoring and observability infrastructure is compatible with MCP.\n\nWe believe that the benefits of adopting MCP outweigh the potential challenges. This decision aligns with our goal of building a robust, scalable, and manageable service mesh infrastructure.\n","GenTime":"2024-08-28 11:50:23"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"text":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n\n##Decision\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","MatchedIds":[2410,4041,4841,1016,4747],"Prediction":"## Decision\n\nImplement a **data warehouse** to serve as the central repository for integrated data. \n\n**Rationale:**\n\n* A data warehouse is specifically designed for analytical reporting and can handle large volumes of historical data from multiple sources.\n* This approach decouples analytical workloads from transactional systems, improving performance and scalability of both. \n* Building a data pipeline to extract, transform, and load (ETL) data from source applications into the warehouse will allow for consistent and structured data storage.\n\n**Technology Choices:**\n\n* **Cloud-based data warehouse services** like Amazon Redshift, Google BigQuery, or Snowflake offer scalability, cost-effectiveness, and managed infrastructure.\n* **Open-source solutions** like Apache Hadoop or Spark provide flexibility and can be deployed on-premise or in the cloud.\n\n**Further Considerations:**\n\n* Data governance and security policies will be crucial for managing access and ensuring data quality. \n* Designing appropriate data models and schemas within the warehouse is vital for efficient querying and analysis. \n* Building dashboards and visualizations on top of the data warehouse will provide actionable insights to support the GOV.UK publishing workflow. \n","GenTime":"2024-08-28 11:50:29"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"text":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n\n\n##Decision\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","MatchedIds":[4618,4614,697,1193,36],"Prediction":"## Decision\n\nWe will adopt a phased approach to decoupling the repositories, prioritizing stability and minimizing disruption to development and hotfix releases. \n\n**Phase 1: Preparation and Isolation (4 weeks)**\n\n* **Branching Strategy:** Create separate, long-lived branches for each component being decoupled (`renderer`, `website`, `js-sdk-toolchain`) from the main `explorer` repository.\n* **Dependency Management:** Clearly identify all interdependencies between the components. Use a package manager (npm, yarn) to manage dependencies and versioning between the packages.  \n* **Testing and CI\/CD:** Update the CI\/CD pipeline to build, test, and publish each component independently. Implement robust integration testing to catch issues arising from the decoupling.\n\n**Phase 2: Incremental Rollout (8 weeks)**\n\n* **Website Decoupling:**  Prioritize decoupling the `website` first, as it is presumably less coupled than the renderer. Merge the `website` branch into the main branch and release it independently. \n* **Renderer Decoupling:** Decouple the `renderer` next. Closely monitor for any issues and roll back if necessary.\n* **ECS and Toolchain Decoupling:**  Decouple the `js-sdk-toolchain` last, ensuring seamless integration with the already decoupled `renderer` and `website`.\n\n**Phase 3: Continuous Improvement (Ongoing)**\n\n* **Documentation:** Maintain up-to-date documentation for the new repository structure, build process, and release procedures. \n* **Monitoring and Feedback:**  Continuously monitor the performance and stability of the decoupled components.  Gather feedback from developers and address any pain points that arise.\n\n**Hotfix Strategy:**\n\n* During the transition period, hotfixes will be applied to the relevant branch (`explorer`, `website`, `renderer`, or `js-sdk-toolchain`) and then merged into the main branch to maintain consistency.\n* Once a component is fully decoupled, hotfixes will be applied directly to the component's repository.\n\nThis phased approach allows us to  decouple the repositories iteratively, mitigating risks and ensuring minimal disruption to the development process. Regular communication and collaboration between teams are crucial throughout the entire process. \n","GenTime":"2024-08-28 11:50:41"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"text":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n\n\n##Decision\nWe will build and store OCI images built using Docker.\\n","MatchedIds":[1376,2968,74,923,4096],"Prediction":"## Decision\nWe will use the OCI image format for our container images. \n\nThis decision is based on the following factors:\n\n* **Kubernetes preference:** Kubernetes, our chosen deployment platform,  shows a clear preference for Docker\/OCI images.\n* **Industry trend:**  The industry is moving towards OCI as the standard, with rkt (a competing container runtime) also shifting its support to OCI.\n* **Docker familiarity:** Docker, and by extension the OCI format,  has wide adoption and understanding within GDS, potentially easing development and troubleshooting.\n* **Default runtime:** Docker being the default container runtime for Kubernetes simplifies integration and reduces potential compatibility issues. \n\nUsing the OCI format aligns with both the technical direction of the industry and our chosen platform, minimizing future compatibility risks.\n","GenTime":"2024-08-28 11:50:46"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"text":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n\n##Decision\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","MatchedIds":[2752,2723,3488,631,4937],"Prediction":"## Decision\n\nWe will adopt a **cloud-native architecture**, leveraging **serverless computing** and **managed services** from a leading public cloud provider (e.g., AWS, Azure, GCP) wherever possible. \n\nThis decision is driven by the following factors:\n\n* **Alignment with stated goals:** A cloud-native, serverless-first approach directly addresses the need for elasticity, resilience, pay-as-you-go pricing, and access to cutting-edge technologies.\n* **Focus on business value:** Utilizing managed services allows the team to focus on delivering core business value for the Digital Citizenship platform, rather than infrastructure management.\n* **Improved agility and time-to-market:** Serverless computing enables faster development cycles and quicker deployment of new features and updates.\n* **Enhanced security posture:** Public cloud providers invest heavily in security, offering best-in-class security features and compliance certifications. \n* **Reduced operational overhead:** Relying on managed services minimizes the operational burden and allows for efficient scaling based on demand.\n\nWhile we prioritize managed services and serverless, we acknowledge that some components might require custom solutions. In such cases, we will favor open-source solutions where feasible and evaluate building from scratch only when absolutely necessary.\n\nThis approach allows us to build a future-proof platform that is scalable, secure, and cost-effective while allowing the team to remain agile and responsive to evolving requirements.\n","GenTime":"2024-08-28 11:50:53"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"text":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n\n##Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","MatchedIds":[2397,4388,578,3722,745],"Prediction":"## Decision\n\nThe OpenFIDO project will utilize an API Gateway to manage access to the workflow service. The React application will authenticate users and, upon successful authentication, receive a JWT token containing their organization information.  This token will be included in all requests to the API Gateway.\n\nThe API Gateway will be configured to:\n\n1. **Validate the JWT token:**  Verify the token's signature, expiration, and other relevant claims.\n2. **Enforce authorization based on organization:**  The API Gateway will inspect the organization claim within the JWT and only allow access to resources associated with that organization.\n3. **Forward requests to the workflow service:** If the token is valid and the user has permission, the request will be forwarded to the appropriate workflow service endpoint.\n\nThis approach provides a centralized and secure mechanism to control access to the workflow service based on organization affiliation.\n","GenTime":"2024-08-28 11:50:59"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"text":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n\n\n##Decision\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","MatchedIds":[1740,2221,1742,4859,2520],"Prediction":"## Decision\nUse AWS RDS Aurora MySQL implementation for our database persistence mechanism. The data will be structured in tables representing locations, bookables, and bookings, leveraging the relational capabilities of MySQL. \n\nProduction environments will run with 1 primary and 1 replica for high availability, provisioned via Riglet CloudFormation stacks. Staging & Integration environments will run with only 1 Primary to minimize costs. \n","GenTime":"2024-08-28 11:51:02"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"text":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n\n##Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","MatchedIds":[2932,1193,3863,994,2498],"Prediction":"## Decision\n\nWe will opt for **solution 2: Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses.**\n\nHere's the rationale:\n\n* **Maintainability:** Introducing an additional property to store class names adds complexity and redundancy to the codebase. This approach would be harder to maintain and understand in the long run.\n* **Cleanliness:** Renaming classes to be consistent with factory methods promotes a cleaner and more intuitive API. Developers using the library can expect a clear and predictable naming convention.\n* **Backwards Compatibility:** Introducing deprecated subclasses ensures that existing code that relies on the old class names continues to function. This minimizes breaking changes and allows users to migrate to the new naming convention at their own pace.\n\n**Implementation Details:**\n\n1. **Rename** existing op classes to match the naming convention of their corresponding factory methods (e.g., `BernoulliDistribution` becomes `Bernoulli`).\n2. **Create** deprecated subclasses with the old class names that extend the newly renamed classes.\n3. **Add** deprecation notices to the old subclasses, clearly indicating the preferred class name and providing guidance on migrating to the new naming convention.\n\nThis approach achieves both consistency and backwards compatibility, resulting in a more user-friendly and maintainable codebase. \n","GenTime":"2024-08-28 11:51:11"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","MatchedIds":[152,3456,3328,4225,4179],"Prediction":"## Decision\n\nThe decision is to adopt GitHub Actions for CI in this repository. Given its impending general availability, long-term future, and potential to become the standard for GitHub projects, early adoption in a simple use case offers valuable experience and positions the project well for the future. \n","GenTime":"2024-08-28 11:51:13"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"text":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n\n\n##Decision\nThe mocking will be done as much as possible by using the the `msw` package.\\n","MatchedIds":[452,4223,3323,1387,2273],"Prediction":"## Decision\nWe will adopt `msw` for mocking fetch calls in our tests. This approach, as championed by the Testing Library creator, promises a more streamlined and less brittle testing experience compared to `jest-fetch-mock`, by intercepting fetch calls at a higher level and promoting a centralized configuration approach. \n","GenTime":"2024-08-28 11:51:15"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"text":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n\n##Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","MatchedIds":[560,3529,4843,3853,2410],"Prediction":"## Decision\n\n**Chosen option:** Remove the reading of `PLAY_LANG` in tracking consent and instead accept a `data-language` attribute.\n\n### Reasons\n\nThis option best addresses the decision drivers:\n\n* **Support for classic services:**  Using a `data-language` attribute provides a consistent mechanism for both Play and classic Java services to communicate the language preference to the tracking consent banner.\n* **Time sensitivity:** This solution requires minimal changes to the tracking consent implementation, enabling faster deployment. \n* **Minimizing changes to tracking consent:** This is a less invasive change compared to other alternatives like introducing new cookies or communication mechanisms.\n* **Avoid ZAP test exemptions:** By not relying on `PLAY_LANG` being accessible from Javascript, we eliminate the need for teams to add exemptions for this cookie in their ZAP security tests.\n* **Consistency in language settings:** Aligning with the approach used by `govuk-frontend` and `hmrc-frontend` components promotes consistency across frontend components.\n\n### Consequences:\n\n* **Positive:**\n    * Provides a unified solution across different service types (Play, classic Java).\n    * Enables faster rollout of the fix.\n    * Minimizes disruption for service teams.\n    * Improves consistency in handling language settings across frontend components.\n* **Negative:**\n    * Requires a small change to the tracking consent implementation.\n    * Service teams integrating tracking consent will need to adopt the `data-language` attribute. \n\nThis decision prioritizes a swift and minimally disruptive solution while addressing the technical limitations and promoting a consistent approach across different service types and frontend components. While it requires a small adjustment to the integration process for service teams, the benefits in terms of consistency, maintainability, and security outweigh the drawbacks. \n","GenTime":"2024-08-28 11:51:24"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"text":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n\n\n##Decision\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","MatchedIds":[3488,1575,1924,3469,583],"Prediction":"## Decision\n**Alerting should be configured within the monitoring system itself, specifically at the notification channel level.**\n\n### Rationale\n\n* **Centralized Control:** Modern monitoring systems (e.g., Prometheus with Alertmanager, Datadog, NewRelic) offer robust notification routing and silencing capabilities within their configuration. This centralizes alert management and provides a single source of truth for all alert-related settings.\n* **Flexibility and Granularity:** These systems allow for fine-grained control over alert routing, including defining notification recipients (individuals or teams), notification methods (SMS, email, chat platforms), time-based silencing, and escalation policies.  This addresses the need for arbitrary flexibility.\n* **Integration with Existing Tooling:** By configuring alerting within the monitoring system, we leverage the existing investment in these tools and their capabilities. This reduces the need for external scripting or manual intervention, simplifying maintenance and knowledge transfer.\n* **Quick Iteration and Silencing:** Most monitoring systems provide easy-to-use interfaces or APIs for adjusting notification rules and silencing alerts. This allows for rapid response to changing conditions or during incident mitigation, directly addressing the need for quick action.\n\n### Positive Consequences\n* **Reduced Alert Fatigue:** By implementing intelligent routing and silencing, on-call teams receive only the most relevant alerts, improving their focus and response time for critical issues.\n* **Improved SLA Compliance:** Prompt and accurate notifications for high-priority alerts minimize the time to resolution, contributing to meeting or exceeding agreed-upon service levels.\n* **Simplified Alert Management:** Centralized configuration and control make it easier to understand, troubleshoot, and audit the alerting setup, fostering better collaboration and knowledge sharing.\n\n### Negative Consequences\n* **Potential Vendor Lock-in:** Choosing a specific monitoring system with advanced alerting features may lead to some level of vendor lock-in. However, the benefits often outweigh this concern, especially given the maturity and feature set of popular solutions. \n","GenTime":"2024-08-28 11:51:35"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"text":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n\n##Decision\n### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","MatchedIds":[1067,2914,3637,1614,4766],"Prediction":"## Decision\n\nChosen option: Simple and consistent naming conventions, aligned with common practices:\n\n- **Folder for Assets:** We will use the folder name **`assets`** for storing assets. This name is widely used and easily understandable. \n- **Filename Pattern for ADRs:**  We will use the pattern **`XXXX-short-descriptive-title.md`**, where `XXXX` represents the ADR number padded with leading zeros (e.g., `0001-introduce-adr.md`). This pattern keeps ADRs organized and allows easy identification and sorting. \n- **Titles in ADR Documents:** We will use **title case for ADR titles**, capitalizing the first letter of each significant word (e.g., `## Introduce Architectural Decision Records`). This approach maintains consistency and readability. \n\n### Rationale\n\n* **Clarity and Consistency:** These choices prioritize clear, consistent naming conventions across the project.\n* **Common Practices:** These conventions align with commonly used patterns, making it easier for team members to adapt and for newcomers to understand the project structure. \n* **Simplicity:** These options avoid unnecessary complexity and promote ease of use.\n","GenTime":"2024-08-28 11:51:41"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"text":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n\n\n##Decision\nWe have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","MatchedIds":[3104,620,729,3905,50],"Prediction":"## Decision\n\nWe will migrate from the Kong proxy to the Nginx Ingress Controller for the following reasons:\n\n* **Kubernetes Native:** Using the Nginx Ingress Controller provides a more integrated and native experience within our Kubernetes environment. This aligns with the principle of using the platform's capabilities whenever possible.\n* **Security Enhancements:** The Nginx Ingress Controller allows us to secure third-party web applications that lack built-in security mechanisms. This addresses our primary concern and mitigates potential risks associated with exposing unsecured applications. \n* **Open Source and Community Support:** Nginx Ingress Controller is a well-established open-source project with strong community support. This ensures access to a wider range of resources and expertise for troubleshooting and future development. \n\n### Migration Strategy\n\nThe migration will be carefully planned and executed in phases:\n\n1. **Pilot Testing:** We will conduct thorough testing of the Nginx Ingress Controller in a staging environment. This will allow us to validate its functionality, performance, and compatibility with our existing infrastructure and applications. \n2. **Gradual Rollout:**  We will migrate applications incrementally, starting with a small subset and gradually expanding to the remaining workloads. This approach minimizes disruption to our production services and allows us to identify and address any issues early on.\n3. **Monitoring and Validation:** Throughout the migration process, we will closely monitor the performance and stability of the Nginx Ingress Controller. We will establish robust monitoring and alerting mechanisms to detect and respond to any potential issues promptly.\n\n### Positive Consequences:\n\n* **Enhanced Security Posture:** Securing third-party applications significantly improves our overall security posture and reduces the attack surface.\n* **Simplified Architecture:** Migrating to a Kubernetes-native solution simplifies our infrastructure and reduces reliance on external dependencies.\n* **Improved Maintainability:** Leveraging a widely adopted open-source solution enhances maintainability and reduces the risk of vendor lock-in. \n\n### Negative Consequences: \n\n* **Migration Effort:** Migrating to a new ingress controller will require dedicated effort from our engineering team. \n* **Potential Compatibility Issues:** There might be unforeseen compatibility issues with some existing applications or services. Thorough testing during the pilot phase will mitigate this risk.\n\nThis migration aligns with our strategic goal of leveraging Kubernetes-native solutions whenever feasible, enhances our security posture, and simplifies our infrastructure in the long run. \n","GenTime":"2024-08-28 11:51:54"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"text":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n\n##Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","MatchedIds":[4556,3566,4539,5142,2270],"Prediction":"## Decision\n\nInstead of using an ordinal parameter, define a set of named constants for each mode. This will improve code readability and make the API more self-documenting. \n\nFor example, instead of:\n\n```\n# mode = 0: FIFO, mode = 1: LIFO\ndef process_data(data, mode):\n  if mode == 0:\n    # ...\n  elif mode == 1:\n    # ...\n```\n\nUse:\n\n```\nMODE_FIFO = 0\nMODE_LIFO = 1\n\ndef process_data(data, mode):\n  if mode == MODE_FIFO:\n    # ...\n  elif mode == MODE_LIFO:\n    # ...\n```\n\nThis approach:\n\n* **Enhances readability:** The code clearly expresses the intent of using specific modes.\n* **Improves maintainability:** Changing or adding modes becomes easier and less error-prone.\n* **Provides better documentation:**  The constants act as self-explanatory labels for the different modes.\n* **Reduces cognitive load:** Developers don't need to memorize or look up the meaning of numeric values. \n","GenTime":"2024-08-28 11:52:01"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"text":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n\n\n##Decision\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","MatchedIds":[2687,4557,1652,263,2386],"Prediction":"## Decision\nWe will no longer use jQuery for new features and will migrate existing functionality to vanilla JavaScript as part of our normal development process. This decision aligns with our commitment to using modern, lightweight, and secure solutions that adhere to accessibility guidelines and support our chosen browsers. \n\nWe acknowledge that a full migration will take time. We will prioritize migrating jQuery functionality based on the risk it poses and the areas of the codebase we are actively developing.\n","GenTime":"2024-08-28 11:52:05"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"text":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n\n\n##Decision\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","MatchedIds":[4642,2516,469,4056,1173],"Prediction":"## Decision\n\nWe will use **Nginx** as a reverse proxy in front of **Gunicorn** to serve static assets for the `service-stac` admin pages and enhance security. \n\n### Detailed Configuration:\n\n1. **Static Files:**  Nginx will be configured to serve static files (CSS, images, icons) directly. This offloads the task from Gunicorn\/Django, improving performance. A specific directory within `service-stac` will be designated for static files and Nginx will be pointed to this location.\n2. **Reverse Proxy:** Nginx will act as a reverse proxy, receiving client requests and forwarding them to Gunicorn. This setup provides several benefits:\n    * **Security:**  Nginx acts as a buffer, shielding Gunicorn from direct exposure and mitigating certain types of attacks.\n    * **Performance:** Nginx can handle static file serving more efficiently and can also cache content, further enhancing performance.\n    * **Load Balancing (Future):**  If scaling becomes necessary, multiple Gunicorn workers can be added, and Nginx can distribute traffic between them.\n3. **Gunicorn Configuration:** Gunicorn will continue to serve the Django application, but it will listen only to requests from Nginx (configured for internal communication).\n\n### Diagram:\n\n```\nClient Request  -->  Nginx (Reverse Proxy)  -->  Gunicorn  -->  Django Application\n                        ^                                     |\n                        | Static Files Served Directly      |\n                        +-------------------------------------+ \n```\n\n### Advantages:\n\n* **Improved Performance:**  Offloading static file serving to Nginx optimizes resource usage and speeds up page load times.\n* **Enhanced Security:** The reverse proxy setup adds a layer of protection, making the application more resilient to attacks.\n* **Scalability:** The architecture allows for easy scaling by adding Gunicorn workers behind Nginx. \n* **Flexibility:** Nginx provides a robust platform for further optimizations, such as caching and SSL termination.\n\n### Potential Considerations:\n\n* **Nginx Configuration:** Correctly configuring Nginx for static file serving and reverse proxy functionality is crucial for proper operation.\n* **Monitoring:** Monitoring Nginx and Gunicorn health and performance is essential to ensure the system runs smoothly. \n","GenTime":"2024-08-28 11:52:16"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"text":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n\n##Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","MatchedIds":[583,699,4370,1376,718],"Prediction":"## Decision \nChosen option: **Kubernetes**, because it provides a robust and scalable way to manage and update containerized applications. \n\n### Positive Consequences\n* **Easy Updates:** Kubernetes allows for rolling updates with zero downtime, ensuring that the application is always available. \n* **Scalability:**  Kubernetes can easily scale the application up or down based on demand. \n* **Self-Healing:**  Kubernetes monitors the health of containers and automatically restarts or replaces unhealthy ones.\n* **Version Control:**  Kubernetes deployments can be version controlled, allowing for easy rollbacks if necessary.\n\n### Negative Consequences\n* **Complexity:**  Kubernetes can be complex to set up and manage, especially for smaller teams.\n* **Infrastructure Costs:** Running a Kubernetes cluster requires additional infrastructure, which can increase costs.\n\n### Alternatives Considered\n* **Docker Swarm:** Simpler to set up than Kubernetes, but less feature-rich.\n* **Directly on VMs:**  Possible but would require manual management of updates and scaling.\n\n### Justification\nWhile Kubernetes introduces some complexity, its ability to simplify updates, enhance scalability, and ensure application availability outweighs the drawbacks, making it the ideal choice for deploying and managing Docker containers at scale.\n","GenTime":"2024-08-28 11:52:23"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"text":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n\n##Decision\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","MatchedIds":[2462,5061,1193,3159,2466],"Prediction":"## Decision\n\nOur C++ libraries will be exception-neutral:\n\n* **No Throwing on Errors:** Our APIs will **not** throw exceptions to signal errors. This ensures compatibility with users who cannot use exceptions in their projects. \n* **`Status` and `StatusOr<T>`:** Errors will be reported using either `Status` for operations returning `void`, or `StatusOr<T>` for operations returning a value of type `T`. These classes provide a consistent way to check for success\/failure and examine error details.\n* **Leveraging Existing Non-Throwing Mechanisms:** Where applicable, we will utilize existing non-throwing error reporting mechanisms offered by the underlying libraries we depend on (e.g., `std::ios_base::iostate` for I\/O operations).\n\n## Consequences\n\n* **Broad Compatibility:** Exception neutrality makes our libraries usable in a wider range of C++ projects, regardless of their stance on exceptions. \n* **Unified Error Handling:** The use of `Status` and `StatusOr<T>` provides a consistent and predictable error handling model across all our C++ APIs.\n* **Not Idiomatic for All:** This decision deviates from the exception-handling preferences of a significant portion of the C++ community, potentially making our APIs feel less \"natural\" to some users.\n* **Breaking Change:** Transitioning existing APIs to this exception-neutral approach constitutes a breaking change. We acknowledge this impact and will handle it as follows:\n    *  **Google Cloud Storage:** Being at Alpha stage, the API will be immediately changed to be exception-neutral.\n    *  **Google Cloud Bigtable:** Due to its Beta status, we will communicate a separate migration plan and timeline for transitioning the Bigtable API to an exception-neutral design. \n","GenTime":"2024-08-28 11:52:32"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"text":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n\n##Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","MatchedIds":[1376,4096,2410,4370,861],"Prediction":"## Decision\nWe will use Travis CI for continuous integration. This leverages existing knowledge and tooling within the BEIS GitHub account while fulfilling the requirement for Docker support in our deployment pipeline. \n","GenTime":"2024-08-28 11:52:34"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"text":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n\n\n##Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","MatchedIds":[2243,2240,3469,699,2249],"Prediction":"## Decision\n\nWe will implement a lightweight health check for the Elasticsearch sidecar container. \n\n**Rationale:**\n\n* **Error Distinction:** The health check will specifically target the keystore-updater's functionality (successful call to `\/_nodes\/reload_secure_settings` API endpoint), allowing clear distinction between sidecar and Elasticsearch errors.\n* **Minimized Side Effects:** Given the low probability of sidecar failures, we will not directly tie the sidecar health to the pod's readiness probe. This prevents transient sidecar issues from impacting Elasticsearch availability. However, the sidecar health status will be reported separately for monitoring and alerting.\n* **Reliability and Simplicity:** We will implement a simple, independent health check mechanism for the sidecar. This could be a script within the sidecar container periodically testing the API call and logging the result to stdout. This log can be then scraped by a monitoring system like Prometheus.\n* **Startup Delay:**  We will implement a configurable startup delay for the sidecar health check, during which errors will not be reported. This accommodates the Elasticsearch startup time and prevents false positives.\n\n**Implementation Details:**\n\n1. **Health Check Script:** A simple script within the sidecar container will:\n    * Wait for the configured startup delay.\n    * Attempt the `\/_nodes\/reload_secure_settings` API call with appropriate credentials.\n    * Log success or failure with timestamps to stdout. \n2. **Monitoring and Alerting:**  \n    * Configure Prometheus to scrape the sidecar logs.\n    * Set up alerts based on consecutive sidecar failures outside the startup delay window. This indicates a persistent issue requiring attention. \n3. **Documentation:**  Document the health check mechanism, configuration options (startup delay), and expected behavior.\n\nThis approach ensures that we can monitor the sidecar health and get alerted on potential issues without unnecessarily impacting Elasticsearch availability. It also maintains a clear separation between the sidecar and the main application health, supporting the decision drivers. \n","GenTime":"2024-08-28 11:52:44"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"text":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n\n\n##Decision\nFeature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","MatchedIds":[1372,3144,2539,621,2793],"Prediction":"## Decision\n\nThis module will use feature toggles based on the pattern described in the Capital One article, \"Building Feature Toggles into Terraform\". \n\nSpecifically:\n\n* **DynamoDB Table Provisioning:**  A boolean variable (`create_dynamodb_table`) will control whether a DynamoDB table is provisioned by the module. This allows flexibility for use cases within the larger Data Storage Module where a DynamoDB table is not needed.\n* **DynamoDB Autoscaling:** A boolean variable (`enable_dynamodb_autoscaling`) will dictate whether DynamoDB autoscaling is enabled for the provisioned table. This provides granular control over scaling behavior.\n\nThis approach provides clear and maintainable feature toggles, improving the module's flexibility and adaptability for different use cases within the larger Data Storage Module. \n","GenTime":"2024-08-28 11:52:49"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"text":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n\n\n##Decision\nGiven the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","MatchedIds":[3416,1173,2884,399,4358],"Prediction":"## Decision\n\nStore the Elasticsearch mappings and data transformation logic as a **separate configuration file**, alongside the main data file. This configuration file will be loaded by the application and used during the import process.\n\n**Benefits:**\n\n* **Clean separation:** Keeps complex mappings and transformation logic separate from the raw data and application code, improving maintainability and readability.\n* **Flexibility:** Allows for easy modification of mappings and transformations without modifying the application code or environment variables.\n* **Version control:**  Configuration files can be version-controlled alongside the codebase, ensuring consistent behavior across environments and deployments.\n\n**Implementation Details:**\n\n* The configuration file can be in JSON or YAML format for easy parsing.\n* The application can use a command-line argument or configuration option to specify the path to this configuration file.\n* Consider using a schema validation library to ensure the configuration file adheres to the expected structure. \n","GenTime":"2024-08-28 11:52:54"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"text":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","MatchedIds":[4896,4890,2756,4899,1792],"Prediction":"## Decision\n\n**We will use a strategic project structure for organizing modules, where modules are grouped into a hierarchy of functional areas.**\n\nThis structure, referred to as **strategic structure**, will group modules into high-level functional areas that reflect the domain and system architecture. This approach will address the drawbacks of the flat structure:\n\n**Benefits of Strategic Structure:**\n\n* **Improved Scalability:** The hierarchical organization allows for better management and navigation as the project grows.\n* **Enhanced Clarity and Navigation:** Grouping modules by functional area reduces confusion and makes it easier to locate specific components.\n* **Dependency Visualization:**  The structure can inherently suggest dependency directions, with higher-level areas potentially depending on lower-level ones.\n* **Abstraction Levels:** Different levels in the hierarchy can represent different levels of abstraction within the system.\n* **Clear Entry Points:**  The structure can highlight entry points by placing them in prominent positions within the hierarchy.\n* **Meaningful Organization:**  Beyond module names, the strategic structure provides a clear overview of module relationships and organization.\n* **Reduced Repository Splitting:** A well-organized structure within a single repository can often delay the need to extract modules into separate repositories.\n* **Microservice Flexibility:** While not directly addressing microservices, a strategic structure can be adapted to group modules related to a specific microservice within a dedicated functional area.\n\n**Example:**\n\nInstead of a flat structure with numerous modules, a strategic structure might organize them like this:\n\n```\nklokwrk-project\n\u251c\u2500\u2500 domain\n\u2502   \u251c\u2500\u2500 customer\n\u2502   \u2502   \u251c\u2500\u2500 api\n\u2502   \u2502   \u251c\u2500\u2500 service\n\u2502   \u2502   \u2514\u2500\u2500 repository\n\u2502   \u2514\u2500\u2500 order\n\u2502       \u251c\u2500\u2500 api\n\u2502       \u251c\u2500\u2500 service\n\u2502       \u2514\u2500\u2500 repository\n\u251c\u2500\u2500 infrastructure\n\u2502   \u251c\u2500\u2500 persistence\n\u2502   \u2514\u2500\u2500 messaging\n\u2514\u2500\u2500 application\n    \u2514\u2500\u2500 web\n```\n\nThis example groups modules into functional areas like `domain`, `infrastructure`, and `application`, with further sub-grouping within each area. This structure immediately provides more context and clarity compared to a flat list of modules. \n","GenTime":"2024-08-28 11:53:06"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"text":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n\n##Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","MatchedIds":[2896,2145,2122,2515,2892],"Prediction":"## Decision\n\nWe will refactor the Object Storage blob-store implementation to use the AWS SDK for Java directly, targeting the S3 API. \n\nHere's the justification:\n\n- **Simplified Codebase:**  Directly using the AWS SDK for Java will significantly reduce the complexity of our codebase by removing the abstraction layer introduced by Apache jclouds. This will lead to more maintainable and understandable code.\n- **Improved Performance:** Removing the jclouds layer will streamline the interaction with the S3 API, potentially leading to performance gains in both execution time and resource utilization.\n- **Reduced Build Times:** Removing the jclouds dependency will have a positive impact on the project's build time.\n- **Leverage Swift S3 Compatibility:** Given Swift's compatibility layer for S3, we can use the AWS SDK to interact with both Swift and S3 without needing separate implementations. This eliminates the need for dedicated Swift code and further simplifies the codebase.\n- **Reactive Alignment:** The AWS SDK for Java v2 offers a non-blocking, asynchronous API that aligns well with our Reactive programming style.\n\n**Migration Plan:**\n\n1. **Introduce AWS SDK Dependency:** Add the AWS SDK for Java as a dependency to the project.\n2. **Refactor Existing Implementation:**  Gradually replace jclouds-based code with the equivalent AWS SDK calls.\n3. **Testing and Validation:**  Thoroughly test the new implementation against both S3 and Swift environments to ensure compatibility and functionality.\n4. **Remove jclouds Dependency:** Once the migration is complete and validated, remove the Apache jclouds library from the project.\n\nThis decision simplifies our blob-store implementation, improves performance, and aligns with our goal of using a more modern and reactive programming style. \n","GenTime":"2024-08-28 11:53:15"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"text":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n\n##Decision\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","MatchedIds":[5255,1408,4226,3321,400],"Prediction":"## Decision\n\nWe will enforce consistent code style across all our JavaScript code by using the Prettier code formatter. \n\nThis decision will be implemented as follows:\n\n* **Prettier will be adopted as the single source of truth for code style.** All JavaScript files will be automatically reformatted to adhere to the default Prettier style guide. \n* **We will integrate Prettier into our development workflow:**\n    *  **Pre-commit hooks:** Prettier will be run automatically when a developer tries to commit code.  This ensures that only code adhering to the style guide gets committed.\n    *  **IDE integration:**  We will establish a common Prettier configuration for popular IDEs used within the team and encourage developers to enable format-on-save functionality. \n    *  **CI\/CD pipeline:** Our continuous integration process will be updated to run Prettier and fail builds if the code style is not met.  \n\nThis approach addresses the following:\n\n* **Reduces manual formatting:** Developers can rely on Prettier to handle code style, freeing up their time. \n* **Consistent formatting:**  The same style will be applied across all files, regardless of who wrote the code or which IDE they used.\n* **Improved code review experience:**  By eliminating style inconsistencies, code reviews can focus on the actual logic and functionality. \n* **Eliminate style debates:** Prettier acts as the objective standard, preventing subjective discussions about code style preferences. \n","GenTime":"2024-08-28 11:53:23"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"text":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n\n\n##Decision\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","MatchedIds":[1012,1480,3253,445,5213],"Prediction":"## Decision\n\nWe will implement a process for ingesting data from Google Sheets into the data platform. This will involve:\n\n1. **Automated Data Extraction:** We will utilize a library like **`pandas`** in Python, which has built-in functionality to read data directly from Google Sheets given appropriate credentials. This will enable us to schedule regular data pulls from the spreadsheets.\n\n2. **Data Validation and Cleaning:**  Upon ingestion, we will implement data quality checks using a framework like **Great Expectations** to ensure the incoming data conforms to our expected schema and data types. Any discrepancies or missing values will be flagged for further investigation and potential cleaning before being loaded into the platform.\n\n3. **Transformation and Loading:** Once validated, we will transform the data into a format suitable for our data platform. This might involve restructuring, renaming columns, or converting data types. After transformation, the data will be loaded into the appropriate tables or storage systems within the platform.\n\n4. **Orchestration and Scheduling:** We will use an orchestration tool like **Apache Airflow** to create a directed acyclic graph (DAG) that defines the entire data pipeline. This DAG will manage the scheduling of the data extraction, validation, transformation, and loading processes.\n\n5. **Monitoring and Alerting:**  We will implement monitoring and alerting mechanisms to track the success and performance of the data pipeline. This might involve logging key metrics, setting up alerts for failed tasks, or visualizing data quality over time.\n","GenTime":"2024-08-28 11:53:32"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"text":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n\n\n##Decision\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","MatchedIds":[1614,1563,138,1067,2706],"Prediction":"## Decision\n\nWe have decided to adopt the proposal of adding `steps.<id>.outcome` to the steps context. \n\n### Outcome\n\nThe `outcome` field will represent a binary status of the step's execution:\n\n* `\"success\"`:  The step completed successfully.\n* `\"failure\"`:  The step failed to complete.\n\nThis provides a clear and concise way for downstream steps to determine the execution status of previous steps.\n\n### Conclusion\n\nThe use of `steps.<id>.conclusion` is rejected for the following reasons:\n\n* **Redundancy:** The information conveyed by a \"conclusion\" can often be derived from the `outcome` and existing `steps.<id>.outputs`. \n* **Complexity:** Introducing a separate \"conclusion\" field adds unnecessary complexity to the steps context.  \n\nBy relying on `outcome` and `outputs`, we maintain a simpler and more consistent approach for defining step dependencies and workflows. \n","GenTime":"2024-08-28 11:53:37"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"text":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n\n##Decision\n```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","MatchedIds":[1545,847,2067,577,4443],"Prediction":"## Decision\nWe will create a lightweight analytics client library that will be usable by both the iOS and Android platforms. \n\n**Implementation Details:**\n\n*   **Language:** The library will be developed using [Kotlin Multiplatform](https:\/\/kotlinlang.org\/docs\/multiplatform.html), allowing us to write the core logic once and compile it to compatible binaries for both iOS and Android.\n*   **Interface:** The client library will expose a simple API for tracking screen views and events.  We will define clear methods like `trackScreenView(screenName: String)` and `trackEvent(eventName: String, parameters: Map<String, Any>)`.\n*   **Network Communication:** The library will handle batching events and sending them to the Ophan backend using HTTPS requests.  Retry mechanisms will be implemented to ensure data reliability. \n\n**Alternatives Considered:**\n\n*   **Platform-Specific Libraries:** Developing separate analytics libraries for iOS (Swift) and Android (Kotlin\/Java) would lead to code duplication and increase maintenance efforts.\n*   **Third-Party Analytics Tools:**  While tools like Firebase Analytics or Amplitude offer rich features, they may introduce overhead and potential data privacy concerns. Using Ophan allows for better control and understanding of our analytics data.\n\n**Considerations:**\n\n*   This approach prioritizes building a minimal and maintainable solution for the MVP.\n*   We will need to invest in defining a robust data schema for events and screen views to ensure consistency across platforms. \n*   As requirements evolve, we may consider integrating additional features like offline data persistence or user identification. \n","GenTime":"2024-08-28 11:53:45"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"text":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n\n##Decision\nNot yet decided.\\n- [Global Validation](global_validation.md)\\n","MatchedIds":[1271,1310,1309,1307,1274],"Prediction":"## Decision\nIntroduce an internal cache that stores a copy of the KeySet after a successful `kdbGet()`. This cache will be consulted on subsequent `kdbGet()` calls:\n- If the requested keys are a subset of the cached KeySet, return a **non-deep** copy of the relevant keys from the cache. \n- If the requested keys are not entirely present in the cache, perform a `kdbGet()` on the missing keys, update the cache, and return the requested keys. \n- `kdbSet()` will invalidate the cache, requiring a fresh `kdbGet()` from the backend on the next access. \n\nThis approach offers several advantages:\n- Reduces the number of backend accesses, improving performance, especially for repeated reads.\n- Avoids returning a large KeySet when only a small subset is required.\n- Simplifies `kdbSet()` by eliminating the need for deep duplication.\n\nHowever, it also introduces:\n- Increased memory consumption due to the cached KeySet. \n- Cache management complexity, including invalidation logic. \n\nFurther considerations:\n- Investigate cache eviction strategies to manage memory consumption, potentially using LRU (Least Recently Used).\n- Explore configurable cache size limits to prevent excessive memory usage.\n- Evaluate the performance impact of non-deep vs. deep copying for the cached KeySet. \n","GenTime":"2024-08-28 11:54:04"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"text":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n\n##Decision\nUse `GitProcess.exec` even in tests that need to lean on the git process.\\n","MatchedIds":[2080,3841,3497,1741,4600],"Prediction":"## Decision\n\nUpdate the CircleCI configuration to use a specific version of Git that is compatible with our project. This can be achieved by:\n\n* **Using a Docker image with the desired Git version pre-installed.**  This provides better control over the build environment and ensures consistency. \n* **Explicitly installing the required Git version during the CircleCI build process.** This can be done using a package manager like apt or yum, depending on the underlying CircleCI image. \n\nWe should also update our documentation to specify the required Git version for local development and testing to prevent similar issues in the future.\n","GenTime":"2024-08-28 11:54:07"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"text":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n\n##Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","MatchedIds":[2397,5211,263,2041,847],"Prediction":"## Decision\n\nWe will use Auth0 for user authentication, leveraging its OAuth2 capabilities for potential future migration to a BEIS-preferred service. \n\nGiven the limitations of Auth0's provided views (\"Classic\" lacking progressive enhancement and \"New\" offering a subpar user experience), we will develop **custom authentication views**.  This approach allows us to:\n\n1. **Prioritize user experience:**  We can design and build views specifically tailored to our service and user needs, ensuring accessibility and a positive interaction. \n2. **Maintain control over design and implementation:** Custom views grant greater flexibility and avoid dependence on Auth0's potentially evolving view offerings.\n3. **Align with progressive enhancement principles:**  We can ensure our authentication flow is accessible and functional for all users, regardless of JavaScript support.\n\nWhile developing custom views requires additional effort, it outweighs the drawbacks of using Auth0's suboptimal solutions and aligns with best practices for user experience and accessibility. \n","GenTime":"2024-08-28 11:54:14"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"text":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n\n##Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n","MatchedIds":[2405,2397,5213,1480,3217],"Prediction":"## Decision\n\nGiven the time constraints and the lack of a clear single sign-on solution from Hackney, we will use Auth0 as a temporary solution for authentication and authorization. This will allow us to:\n\n* **Meet user needs:** Provide individual user accounts and track agent names associated with actions in the system.\n* **Improve security:**  Move away from Basic Authentication to a more secure OAuth2-based approach.\n* **Maintain flexibility:**  Auth0's OAuth2 implementation will allow for relatively straightforward migration to a Hackney-supported single sign-on provider when one becomes available. \n\nThis approach allows us to deliver the required functionality and security improvements in the short term while minimizing rework when Hackney finalizes their single sign-on strategy. \n","GenTime":"2024-08-28 11:54:18"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"text":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n\n\n##Decision\nUse a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","MatchedIds":[3123,84,1612,1193,2499],"Prediction":"## Decision\n\nWe will introduce a dedicated type name for each event, distinct from its .NET CLR type name, to be used within SqlStreamStore. This will be achieved by:\n\n1. **Defining a convention for event type names:** This could be a simple string concatenation (e.g., `EventName` becomes `appname.context.EventName`) or a more structured approach like using UUIDs.  The chosen convention should be clearly documented and enforced.\n2. **Storing the event type name within the event itself:**  This ensures that events remain portable even if their underlying .NET types change or are moved between namespaces.\n3. **Using the event type name for all operations within SqlStreamStore:**  This includes writing events, reading events, and setting up subscriptions.\n\nThis approach decouples the event store from the intricacies of .NET type management, ensuring long-term flexibility and resilience to refactoring.  \n\n**Additionally:**\n\n* We will evaluate existing libraries or tools that might simplify the management and serialization of event types independent of .NET types. \n* We will document the rationale for this decision and the chosen naming convention to guide future development. \n","GenTime":"2024-08-28 11:54:24"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"text":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n\n\n##Decision\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","MatchedIds":[194,50,3104,729,745],"Prediction":"## Decision\n\nThis ADR has been **superseded**. \n\nWhile the decision to use IPSec for router-to-cell traffic was made with the best intentions at the time, new information and evolving security best practices require us to re-evaluate this approach. \n\nSpecifically:\n\n* **IPSec Maintenance:** As noted in a previous ADR, the chosen IPSec implementation might be deprecated and lack ongoing maintenance, raising concerns about long-term viability and security.\n* **Complexity and Performance:**  IPSec introduces additional complexity to the networking stack. This complexity can lead to increased configuration overhead, troubleshooting difficulties, and potential performance bottlenecks.\n* **Modern Encryption Alternatives:**  More modern and efficient encryption alternatives like mutual TLS (mTLS) have gained significant traction and offer robust security with potentially better performance characteristics.\n\n## Updated Approach\n\nWe will investigate and implement a more modern and maintainable approach to encrypt traffic between the router and application instances.  This will likely involve:\n\n* **Evaluating mTLS:** We will prioritize exploring the feasibility and benefits of using mTLS between the router and application instances. \n* **Phasing out IPSec:** Based on the evaluation of mTLS, we will plan and execute a phased migration to sunset the existing IPSec implementation. \n\nThis approach prioritizes:\n\n* **Security:**  Ensuring the confidentiality of tenant data remains paramount.\n* **Maintainability:**  Adopting technologies with active maintenance and support.\n* **Performance and Scalability:**  Minimizing performance overhead and ensuring the platform scales efficiently. \n\nThis decision will be revisited and potentially revised based on the findings of the mTLS evaluation. \n","GenTime":"2024-08-28 11:54:33"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"text":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n\n\n##Decision\nWe have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","MatchedIds":[729,211,3595,3890,2256],"Prediction":"## Decision\n\nWe have decided to **disallow password authentication** and require **SSH key pair authentication** for all access to servers in the cluster.\n\n### Reasons:\n\n* **Security:** SSH key pairs offer significantly improved security compared to password authentication.  \n    * Key pairs are significantly stronger than passwords and less susceptible to brute force attacks.\n    * Disabling password authentication eliminates risks associated with weak or reused passwords.\n* **Auditability:** Key-based authentication allows for better auditing and tracking of who accessed a server and when.\n* **Automation:** SSH keys simplify automation tasks, allowing scripts and tools to interact with servers without requiring interactive password prompts.\n\n### Implementation Details:\n\n1. **Key Generation and Distribution:**\n    * A secure mechanism will be established for generating unique SSH key pairs for authorized users and systems. \n    * Secure distribution channels will be used to provide authorized users with their private keys (e.g., secure key management system or offline distribution).\n2. **Server Configuration:**\n    * Servers will be configured to disable password-based authentication. \n    * The authorized_keys file will be managed to ensure only authorized public keys are present.\n3. **Monitoring and Rotation:**\n    *  Regular security audits will be conducted to review access logs and ensure key hygiene.\n    *  A key rotation policy will be implemented to define the frequency for changing SSH key pairs.\n\n### Negative Consequences: \n\n* **Slightly Increased Onboarding Overhead:**  Initial setup for key-based authentication requires a small amount of additional effort compared to passwords.\n* **Key Management Complexity:** Securely managing and rotating SSH keys requires careful processes. \n\n### Mitigation of Negative Consequences:\n\n* **Documentation and Tooling:**  Clear documentation and, if possible, automated tooling will be provided to simplify key generation, distribution, and usage for users.\n* **Centralized Key Management:**  Consider using a centralized key management system to streamline key rotation, revocation, and auditing.\n\nThis approach prioritizes security and auditability for our server infrastructure while minimizing risks associated with password compromise. \n","GenTime":"2024-08-28 11:54:43"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"text":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n\n\n##Decision\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","MatchedIds":[4096,699,2393,1376,771],"Prediction":"## Decision\nChosen option: Implement a `devcontainers` based workflow for local development, because:\n\n* **Simplified onboarding:** This will streamline the setup process for new developers joining the project, and for existing developers returning to the project after a period of time. \n* **Reduced maintenance overhead:** Devcontainers will eliminate the reliance on manually updated setup documentation, reducing the potential for errors and inconsistencies.\n* **Improved consistency and reproducibility:** This ensures a consistent development environment across different machines and operating systems, reducing \"works on my machine\" issues.\n* **Future-proofing:**  Adopting `devcontainers` aligns with the wider industry trend and enables future integration with cloud development environments like Github Codespaces. This will further enhance accessibility for contributors and potentially streamline development workflows.\n\n### Positive Consequences \n* Reduced time spent on setting up development environments, freeing up developers to focus on delivering features and improvements. \n* Lower barrier to entry for non-developers (e.g., designers, product owners) who want to contribute to the application. \n* Improved developer experience and satisfaction due to a more efficient and consistent workflow. \n","GenTime":"2024-08-28 11:54:49"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"text":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n\n\n##Decision\n#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","MatchedIds":[216,3104,1173,4056,2249],"Prediction":"## Decision\n\n**Option 1:  Dynamic Listener with SNI Routing**\n\n* Configure Envoy to use a single listener that terminates TLS. \n* Implement Server Name Indication (SNI) routing within Envoy. This will route traffic based on the domain name provided in the TLS handshake to the appropriate backend service.\n* Define a mapping between hostnames, domains, and backend services within the gateway configuration.\n* Leverage Envoy's dynamic configuration capabilities to update the listener and routing rules when the gateway configuration changes. \n\n**Option 2:  Dedicated Listener per Host+Domain**\n\n* For each host+domain requiring TLS, create a dedicated listener in Envoy.\n* Each listener will be configured with a specific certificate and private key corresponding to the associated domain.\n* The gateway configuration will specify the mapping between host+domains and their respective listeners. \n\n**Decision:**\n\nWe will go with **Option 1: Dynamic Listener with SNI Routing**. This approach offers the following advantages:\n\n* **Reduced Resource Consumption:** Using a single listener for multiple domains reduces the overhead on Envoy compared to creating a dedicated listener for each host+domain.\n* **Simplified Configuration:** Managing a single listener and leveraging SNI routing simplifies the configuration process, especially as the number of host+domains increases.\n* **Flexibility and Scalability:** This approach provides greater flexibility and scalability by dynamically updating the routing based on the gateway configuration, accommodating new host+domains easily.\n\n**Status:** Accepted\n\n**Consequences:**\n\n* **Increased Complexity:** Implementing SNI routing introduces additional complexity in the Envoy configuration.\n* **Potential for Misconfiguration:** If not implemented correctly, SNI routing could lead to traffic being misdirected.\n\n**Mitigation:**\n\n* Thoroughly test the SNI routing configuration to ensure correct functionality and prevent misdirection of traffic.\n* Implement robust monitoring and alerting systems to quickly detect and address any issues with TLS termination or routing.\n\nThis decision prioritizes resource efficiency, simplified configuration, and scalability, accepting a slight increase in complexity.  \n","GenTime":"2024-08-28 11:55:01"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"text":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n\n##Decision\n- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","MatchedIds":[1310,1289,1292,1307,1318],"Prediction":"## Decision\n\n- Fix the locking issue by ensuring that locks are released when the reference count reaches 0, even if the Key is temporarily added and removed from a KeySet.\n- Harmonize the C++ API for KeySet and Key by introducing reference counting for KeySets. This change will ensure consistency across the API and eliminate unexpected behavior. \n- Update the affected documentation and examples to reflect the API changes. \n","GenTime":"2024-08-28 11:55:04"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"text":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n\n##Decision\n[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","MatchedIds":[1171,4359,3147,3144,4056],"Prediction":"## Decision\n\nThe Modernisation Platform will adopt a multi-layered approach to network monitoring and testing, combining both real-time monitoring tools and proactive testing frameworks:\n\n**1. Real-time Network Monitoring:**\n\n* **AWS CloudWatch:** Leverage CloudWatch for continuous monitoring of core networking metrics such as VPC Flow Logs, VPN connections, and load balancer health. This provides a baseline understanding of network activity and potential issues.\n* **Security Groups and Network ACL Monitoring:** Implement automated monitoring and alerting for any changes made to Security Groups and Network ACLs, ensuring continuous adherence to security policies. This can be achieved using AWS CloudTrail and custom scripts\/tools.\n\n**2. Proactive Network Testing:**\n\n* **AWS Network Configuration Validator:** Integrate Network Configuration Validator into our CI\/CD pipeline to proactively identify potential network misconfigurations and security vulnerabilities before deployment. \n* **Infrastructure Testing with Terratest:** Utilize Terratest, our chosen infrastructure testing framework, to write automated tests that validate the network configuration deployed by Terraform. This includes testing security group rules, routing configurations, and connectivity between resources.\n* **Periodic Penetration Testing:** Conduct regular penetration testing exercises, potentially using tools like AWS Security Hub and third-party vulnerability scanners, to simulate real-world attack scenarios and identify potential weaknesses in our network perimeter.\n\n**3. Tooling Integration:**\n\n* Integrate the chosen monitoring and testing tools with our existing alerting and incident management systems (e.g., PagerDuty) to ensure timely notification and response to any identified issues.\n\nThis multi-faceted approach will allow us to gain confidence in the Modernisation Platform's network topology and configuration by combining real-time visibility with proactive testing and validation measures.\n","GenTime":"2024-08-28 11:55:12"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"text":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n\n##Decision\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","MatchedIds":[4642,4096,2516,4833,1089],"Prediction":"## Decision\n\nChosen option: **Platform as a Service (PaaS)**, using a service like **Google App Engine** or **AWS Elastic Beanstalk**.\n\n### Justification:\n\n* **Easy to update:** PaaS solutions excel at simplifying deployments and updates. We can use CI\/CD pipelines to automate the entire process, minimizing downtime and manual intervention. \n* **High availability:**  PaaS providers offer built-in scaling and redundancy features, ensuring our application remains available even during traffic spikes or infrastructure issues.\n* **Easy to configure HTTPS:**  Most PaaS solutions provide streamlined mechanisms for configuring SSL certificates and enabling HTTPS, crucial for security and user trust.\n* **Observability:**  Leading PaaS providers offer integrated monitoring, logging, and tracing tools, making it easier to monitor application health, identify bottlenecks, and troubleshoot issues.\n\n### Considered Alternatives:\n\n* **Virtual Private Server (VPS):** While offering greater control, VPS requires more hands-on management for server configuration, security patching, and scaling. This increases the complexity and contradicts our need for an easy-to-manage solution.\n* **Infrastructure as a Service (IaaS):**  Similar to VPS, IaaS demands significant operational overhead, making it less suitable for our requirement of ease of management.\n\n### Potential Drawbacks:\n\n* **Vendor lock-in:** Choosing a PaaS solution might lead to vendor lock-in, making it potentially challenging to migrate to a different provider in the future.\n* **Cost:** PaaS solutions can be more expensive than managing infrastructure ourselves, especially at scale. However, the benefits of reduced management overhead and increased development velocity often outweigh the cost difference. \n\n### Conclusion:\n\nA PaaS solution aligns best with our requirements for an easy-to-manage, highly available, and observable hosting solution. While potential drawbacks exist, the benefits significantly outweigh the concerns, making it the optimal choice for this situation. \n","GenTime":"2024-08-28 11:55:22"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"text":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n\n\n##Decision\nAdd kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","MatchedIds":[4983,3237,3682,3572,241],"Prediction":"## Decision\n\nWe should introduce a dedicated `PackageOptions` class (or similar, depending on the language) to encapsulate package-specific configurations. \n\nThis class would be passed to the package during the initial adding process and stored internally within the package. On each subsequent call to `start`, the package would access its configuration from the `PackageOptions` object, eliminating the need to repeatedly pass kwargs.\n\nThis approach provides several benefits:\n\n- **Improved code readability:**  Passing a single, well-defined object is clearer than a long list of kwargs. \n- **Enhanced maintainability:** Changes to package configurations can be made within the `PackageOptions` class, minimizing impact on other parts of the codebase.\n- **Flexibility:** This approach allows for easier extension with additional package-specific configurations in the future.\n\n**Example:**\n\nInstead of:\n\n```python\n# Adding the package\nadd_package(\"my_package\", arg1=\"value1\", arg2=\"value2\") \n\n# Starting the package repeatedly with the same kwargs\nstart(package_name=\"my_package\", arg1=\"value1\", arg2=\"value2\")\nstart(package_name=\"my_package\", arg1=\"value1\", arg2=\"value2\")\n```\n\nWe would have:\n\n```python\n# Define package options\noptions = PackageOptions(arg1=\"value1\", arg2=\"value2\")\n\n# Add package with options\nadd_package(\"my_package\", options=options) \n\n# Start the package without needing to repeat kwargs\nstart(package_name=\"my_package\") \nstart(package_name=\"my_package\") \n```\n\nThis approach ensures consistency and avoids redundancy in passing kwargs while maintaining flexibility and clarity in package configuration management. \n","GenTime":"2024-08-28 11:55:31"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"text":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n\n\n##Decision\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","MatchedIds":[532,1193,2859,528,1883],"Prediction":"## Decision\n\nWe will implement the **multiFrame Viewer** as a **generic component** capable of displaying and interacting with a sequence of frames. \n\nHere's a breakdown:\n\n**Phase 1: Core Functionality for Transcription Projects:**\n\n1. **Image Display:**  The component will primarily focus on displaying a series of images, supporting both horizontal and vertical filmstrip orientations. \n2. **Navigation:**  Basic navigation controls will allow users to move sequentially between frames (previous\/next) or jump directly to a specific frame. \n3. **Zoom & Pan (Optional):**  While not initially prioritized, the architecture should allow for the potential addition of zoom and pan functionality for individual images in later iterations. \n4. **Integration with lib-classifier:** The multiFrame Viewer will be designed to integrate seamlessly with the existing lib-classifier architecture, ensuring that frame data is correctly fetched and displayed.\n\n**Phase 2:  Extensibility for Future Use Cases:**\n\n1. **Abstract Data Layer:** The multiFrame Viewer will not be tightly coupled to image data. It will use an abstract data layer that can handle various data types (e.g., videos, audio) in the future.\n2. **Plugin System (Consideration):** To support diverse project requirements (e.g., ecology projects), we will investigate the potential of a plugin system. This would allow developers to extend the multiFrame Viewer with custom functionality (e.g., annotation tools, data overlays) without modifying the core component.\n\n**Implementation Details:**\n\n- We will leverage modern JavaScript frameworks\/libraries (e.g., React, Vue.js) to ensure a performant and maintainable codebase. \n- Accessibility will be a priority, ensuring the multiFrame Viewer is usable by individuals with disabilities.\n- Unit tests will be written alongside the component to guarantee functionality and ease of future development.\n\n**Benefits of this Approach:**\n\n- **Flexibility:** The generic nature of the multiFrame Viewer allows it to be easily adapted for various project types and data formats in the future.\n- **Maintainability:**  A clear separation of concerns and a well-defined plugin system (if implemented) will make the component easier to maintain and extend.\n- **Reusability:**  The multiFrame Viewer can become a valuable shared component within the Zooniverse ecosystem, benefiting multiple projects. \n","GenTime":"2024-08-28 11:55:44"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"text":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n\n\n##Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","MatchedIds":[4047,1669,4096,714,2968],"Prediction":"## Decision\n\nTo simplify future upgrades and avoid data compatibility issues, GOV.UK Docker will adopt a strategy of **not persisting data in Docker volumes for database services**. Instead, we will:\n\n1. **Use ephemeral storage for database services in GOV.UK Docker**: This means that data within the database containers will not be persisted across container restarts. This is acceptable for development environments where data loss is not critical. \n2. **Emphasize external database solutions for production environments**:  Developers will be encouraged to use external database instances (either managed services like AWS RDS or self-hosted solutions) for production deployments. This provides data durability, scalability, and better control over upgrades.\n3. **Provide clear documentation and examples**:  Documentation for GOV.UK Docker will be updated to clearly explain the rationale behind this decision. Examples will be provided showing how to configure applications to connect to external database instances.\n\nThis approach brings several benefits:\n\n* **Simpler upgrade path**:  Upgrading database services in GOV.UK Docker will no longer be hindered by data compatibility issues, as each container starts with a clean slate. \n* **Clear separation of concerns**: Using external databases for production reinforces good practices by separating the application layer from the data layer.\n* **Improved data handling**: External databases offer more robust solutions for data persistence, backups, and disaster recovery compared to relying on Docker volumes. \n\nThis decision prioritizes a smoother developer experience for GOV.UK Docker while promoting best practices for managing data in production environments. \n","GenTime":"2024-08-28 11:55:51"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n\n##Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n","MatchedIds":[2121,2127,1306,279,4636],"Prediction":"## Decision\nImplement a cluster-wide `Task` registry.\n\n* Each node will register its running `Task`s with the registry.\n* The `list` operation will query all registered nodes and aggregate the results. \n\nThis registry can be implemented using a distributed key-value store or a dedicated service. \n","GenTime":"2024-08-28 11:55:54"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"text":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n\n\n##Decision\nWe create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","MatchedIds":[816,21,842,810,1442],"Prediction":"## Decision\n\n**Implement a generic NFT module within the Cosmos SDK, drawing heavily from the existing implementations in `irismod\/nft` and `modules\/incubator\/nft`.**\n\nThis module will focus solely on the core functionality of NFTs:\n\n* **Minting:** Defining the process to create new NFTs, attaching basic metadata like ID, denomination, and owner.\n* **Transfer:** Enabling the transfer of NFT ownership between addresses. \n* **Burning:** Allowing the destruction of NFTs, removing them from circulation.\n* **Querying:** Providing methods to query NFT data, like ownership, metadata, and denominations.\n\nThis generic module **will not** include functionality for:\n\n* **Application-specific logic:** Features like NFT auctions, fractionalization, or complex royalties will be left to other modules or external applications built on top of the core NFT module.\n* **Specific NFT standards:**  While the module can store generic metadata, it won't enforce compliance with external standards like ERC-721. This flexibility allows for future integrations and experimentation.\n\n**Rationale:**\n\n* **Composability:** By focusing on core functionality, the module becomes a building block for a variety of NFT use cases, fostering innovation within the ecosystem.\n* **Interchain Compatibility:** A streamlined NFT module with basic operations is more likely to be adopted across different Cosmos Zones and integrated with interchain protocols.\n* **Future-Proofing:** By avoiding tying the module to specific standards or use-cases, it remains flexible to adapt to the evolving NFT landscape.\n\n**Next Steps:**\n\n1. **Design the core NFT module:** Define the concrete data structures, message types, and state transitions for the functionalities outlined above.\n2. **Implement the module:**  Develop the code for the NFT module, ensuring high code quality, security, and testing coverage.\n3. **Documentation and Examples:** Provide thorough documentation explaining the module's design and usage, including practical examples for developers. \n4. **Community Review:** Open the proposed NFT module design and implementation for community feedback and contributions.\n\nBy taking these steps, Cosmos Hub can provide a powerful and versatile foundation for NFTs, attracting developers and users while fostering a thriving NFT ecosystem. \n","GenTime":"2024-08-28 11:56:06"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"text":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n\n\n##Decision\nThe system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","MatchedIds":[2045,2050,5009,915,3845],"Prediction":"## Decision\n\nWe will implement the submission state machine using a finite state machine pattern within the Data Submission Service codebase. This approach provides a clear and maintainable way to manage the lifecycle of a submission. \n\nThis approach offers several advantages: \n\n* **Explicit State Transitions:**  Clearly defines the possible states and transitions, reducing ambiguity and improving code readability.\n* **Centralized Logic:**  Business logic related to state transitions and actions is consolidated, making the system easier to understand and modify. \n* **Extensibility:** The design accommodates the addition of new states and transitions in the future, supporting evolving requirements like approval processes.\n\nWhile we may explore external state machine libraries in the future, directly implementing the pattern in code is suitable for the current scope and allows for tight integration with the existing service logic. \n","GenTime":"2024-08-28 11:56:10"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"text":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n\n\n##Decision\nWe make our code available as open source.\\n","MatchedIds":[398,1416,1477,3853,2502],"Prediction":"## Decision\n\nWe will strive to leverage open-source software whenever possible. When building new components, we will follow open-source principles, aiming for transparency in design and implementation, enabling collaboration both within the team and with the wider community. We will embrace an iterative release cycle, encouraging early and frequent contributions. Furthermore, we will promote an inclusive environment where the best ideas, regardless of their origin, are valued and implemented. \n","GenTime":"2024-08-28 11:56:15"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"text":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n\n\n##Decision\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","MatchedIds":[3794,187,2471,4358,3793],"Prediction":"## Decision\n\nWe will be updating our Cloud.gov deployment strategy to better reflect our current workflow and address the development team's needs.\n\n1. **Consolidate Staging Environments:**  We will decommission the redundant staging environment in Cloud.gov. With approvals consolidated under the raft-tech repository, a single staging environment in the `tanf-staging` space is sufficient for pre-production deployments. \n\n2. **Create Multiple Development Environments:**  We will establish multiple deployment environments within the `tanf-dev` space on Cloud.gov.  This will provide the vendor development team with isolated spaces to test their code without interfering with each other's work, alleviating the \"crowding\" issue. The exact number of environments will be determined based on team size and concurrent development needs. \n\n3. **Maintain Dedicated Release Environments:**  As per ADR 018, we will maintain dedicated deployment environments within the `tanf-staging` space for release-specific features. This ensures that release candidates can be thoroughly tested and validated before being promoted to production.\n\nThis new structure offers a more efficient and streamlined deployment pipeline while addressing the collaboration and testing needs of the development team. \n","GenTime":"2024-08-28 11:56:22"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"text":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n\n\n##Decision\nWe will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","MatchedIds":[3488,2471,4358,108,4025],"Prediction":"## Decision\n\nWe will leverage a containerized approach for both Jenkins master and agents using Docker and AWS ECS (Elastic Container Service). \n\n**Jenkins Master:**\n\n* **Deployment:** The Jenkins master will be deployed as a Docker container on an ECS cluster running on AWS Fargate. This offers serverless management, autoscaling, and eliminates the need for managing underlying EC2 instances.\n* **Configuration:**  \n    * Configuration as Code (CasC) will be used to manage the Jenkins master configuration using a YAML file stored in a version control system (e.g., Git). This ensures configuration consistency and simplifies disaster recovery.\n    * Important plugins and configurations will be baked into the master Docker image for faster startup and consistency.\n    * A persistent EFS (Elastic File System) volume will be mounted to the Jenkins master container to store Jenkins data, including job configurations, build history, and plugins.\n* **Management:** \n    * Updates to the Jenkins master image or configuration will trigger a new deployment through CI\/CD pipelines. \n    * CloudWatch will be used for monitoring the health and performance of the Jenkins master.\n\n**Jenkins Agents:**\n\n* **Deployment:** Agents will be deployed dynamically as ephemeral ECS tasks on the same Fargate cluster as the Jenkins master. This allows for efficient resource utilization as agents are provisioned on demand based on job requirements. \n* **Configuration:**\n    * A base agent Docker image will be created with common tools and dependencies. \n    * Teams can extend this base image to create custom agent images with specific tools or configurations as needed.\n    * The Jenkins master will use the ECS plugin to dynamically provision and connect to agents based on job labels. \n* **Management:**\n    * Agent images will be versioned and stored in ECR (Elastic Container Registry).\n    * Agent lifecycle will be entirely managed by Jenkins and ECS, eliminating the need for manual intervention.\n\n**Advantages of this approach:**\n\n* **Scalability and Elasticity:** ECS Fargate provides seamless autoscaling for both the master (in case of high load) and agents (based on job demands).\n* **Self-Service and Isolation:** Teams can manage their Jenkins configurations and agent images independently without affecting others.\n* **Infrastructure as Code:** Terraform will be used to define the infrastructure, ensuring consistency and reproducibility.\n* **Cost-Effective:** Fargate's serverless nature optimizes resource consumption and reduces costs.\n* **Simplified Management:** By containerizing both master and agents, deployment, updates, and maintenance become more efficient.\n\n**Next Steps:**\n\n* Define the CI\/CD pipeline for automated deployment of the Jenkins infrastructure and application.\n* Establish security best practices for accessing and managing the Jenkins platform.\n* Document the process for teams to create their custom agent images and integrate them with the Jenkins master.\n* Implement monitoring and alerting for the overall health and performance of the Jenkins platform. \n","GenTime":"2024-08-28 11:56:36"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"text":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n\n\n##Decision\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","MatchedIds":[1310,3863,1031,1797,2526],"Prediction":"## Decision\nBased on the JMH benchmark results comparing `java.lang.reflect.Array.getLength(obj)` to directly accessing the `length` property of an array:\n\n* **If** the performance difference is negligible (within an acceptable margin of error):\n    * We will use the generic approach with `java.lang.reflect.Array.getLength(obj)` to determine the array length.\n    * **Rationale:** This approach adheres to Decision Driver 2 by reducing code duplication and complexity by using a single code path for all array types. \n\n* **Else If** the performance difference is significant:\n    * We will implement a hybrid approach:\n        1. Use direct `length` property access for commonly used array types (e.g., `int[]`, `String[]`, etc.).\n        2. Fallback to `java.lang.reflect.Array.getLength(obj)` for other array types.\n    * **Rationale:** This balances performance (Decision Driver 1) for common cases while retaining flexibility for less frequent scenarios. \n\nWe will create a dedicated class to encapsulate this array length retrieval logic to maintain code cleanliness and potential future optimizations.\n","GenTime":"2024-08-28 11:56:43"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"text":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n\n\n##Decision\n### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","MatchedIds":[1708,2824,2221,1545,847],"Prediction":"## Decision\n\nThe Simple app will prioritize offline functionality to ensure usability in areas with limited or unreliable network connectivity. \n\n### Offline Capabilities:\n\n- **Data Storage:** All patient data, blood pressure readings, and other relevant information will be stored locally on the user's device using a suitable database (e.g., SQLite).\n- **Core Functionality:**  Essential app features, including patient registration, recording blood pressure readings, viewing patient history, and managing appointments, will be available offline.\n- **Data Synchronization:** The app will employ a robust synchronization mechanism to exchange data with the server whenever an internet connection is available.\n    - **Queueing System:**  Any actions performed offline will be queued and synchronized with the server when connectivity is restored.\n    - **Conflict Resolution:** A conflict resolution strategy will be implemented to handle potential data conflicts that may arise from concurrent offline edits.\n- **Data Visibility:**  \n    - Users will primarily have access to data synced to their device. \n    -  To accommodate patient and nurse movement, the app will provide options to:\n        -  **Manually trigger syncs:** Users can manually initiate data synchronization when they anticipate needing access to updated information.\n        -  **Cache recent data:**  The app will cache a limited history of recently accessed or modified records from the server for offline availability, even if those records fall outside the usual sync parameters.\n        -  **(Optional) Expanded Sync Settings:** Explore options to allow users to temporarily expand their sync radius to encompass nearby facilities, if frequent movement patterns are detected or anticipated.  This would need to be balanced with performance considerations on lower-end devices. \n\n### Implementation Considerations:\n\n- **Database Design:** Design the local database to efficiently store and retrieve data offline.\n- **Synchronization Logic:** Develop robust synchronization logic to handle different network conditions and data conflicts.\n- **User Interface:** Design the user interface to clearly indicate online\/offline status and provide feedback on synchronization progress. \n- **Testing:**  Thoroughly test the app's offline functionality under various network conditions to ensure reliability and data integrity.\n","GenTime":"2024-08-28 11:57:00"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"text":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n\n\n##Decision\nAfter a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","MatchedIds":[2516,1574,5257,3831,2520],"Prediction":"## Decision\nTo meet the specified requirements, we will use Amazon S3 for storing the raw images\/gifs and their thumbnails combined with a managed search service like Elasticsearch.\n\n### Rationale\n\n* **Scalability and Cost-effectiveness:**  Amazon S3 provides scalable and cost-effective storage for large amounts of data, making it suitable for storing images and gifs.\n* **Search Capabilities:** While S3 itself doesn't offer advanced search features, integrating it with Elasticsearch allows for efficient searching by title and tags. Elasticsearch is a powerful search and analytics engine that can easily handle the expected load.\n* **Microservice Integration:**  This approach aligns with the existing microservices architecture. A separate microservice can handle image uploads and interact with S3 and Elasticsearch. \n\n### Implementation Details\n\n1. **Image Upload Service:** An \"Image Upload\" microservice will be responsible for:\n   * Receiving images\/gifs from users.\n   * Generating thumbnails of various sizes.\n   * Storing both raw images\/gifs and thumbnails in designated S3 buckets.\n   * Extracting title and tag information and indexing them in Elasticsearch along with references to the S3 locations.\n\n2. **Image Search Service:** An \"Image Search\" microservice will be responsible for:\n    * Receiving search queries from users (by title or tag).\n    * Querying Elasticsearch to retrieve matching image IDs.\n    * Fetching image URLs from S3 using retrieved IDs.\n    * Returning search results to the user.\n\n### Positive Consequences\n\n* **Scalability:** The solution can easily handle a growing number of users and images.\n* **Performance:** Elasticsearch ensures fast and efficient search results.\n* **Cost-effective:** Utilizing managed services like S3 and Elasticsearch optimizes costs.\n\n### Negative Consequences\n\n* **Increased Complexity:**  Integrating two different services (S3 and Elasticsearch) introduces some complexity compared to a single-solution approach. However, this is mitigated by using managed services.\n* **Potential Latency:** Introducing a separate search service might add a small delay to search queries compared to a database-integrated solution. However, Elasticsearch's performance should minimize this. \n","GenTime":"2024-08-28 11:57:12"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"text":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n\n\n##Decision\nImplement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","MatchedIds":[2772,3253,3815,5118,994],"Prediction":"## Decision\n\nCreate two implementations of the `Comparator` interface: `compareBySize` and `compareByDate` in the `Observation` class. \n\n* `compareBySize` compares two Observation objects based on the size of the whale sighting returning an integer to reflect their order. \n* `compareByDate` compares two Observation objects based on the `sightingTime` (Date) returning an integer to reflect their order. \n\nThese comparators can be used with `Collections.sort()` or `Stream.sorted()` to enable flexible sorting of whale observations based on different criteria. \n","GenTime":"2024-08-28 11:57:17"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n\n##Decision\nWe will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","MatchedIds":[1404,4224,5255,1408,4226],"Prediction":"## Decision\nWe will check CSS code style using Stylelint.\n\nWe will use a shareable configuration such as stylelint-config-standard to ensure consistency across projects where possible, and deviate where necessary.\n\nWe will run Stylelint as part of the test suite. \n","GenTime":"2024-08-28 11:57:23"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"text":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n\n\n##Decision\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","MatchedIds":[3521,2975,3682,1067,4695],"Prediction":"## Decision\n\nChosen option: **Global installation of `@log4brains\/cli` with optional local installation of `@log4brains\/web`**.\n\n### Reasoning\n\nThis approach balances simplicity and flexibility:\n\n- **Simplicity:** Users can install the command-line interface (CLI) globally with a single command like `npm install -g @log4brains\/cli`. This eliminates the need for project-specific configurations for the CLI.\n- **Language Agnostic:** The globally installed CLI can be used in any project, regardless of the programming language.\n- **Project Customization:**  The web interface (`@log4brains\/web`) can be installed locally as a development dependency for projects that require its features.  This maintains flexibility for users who prefer a web-based interface for previewing and managing ADRs. \n- **Faster \"Getting Started\":**  By separating the CLI and web interface installation, users can get started with creating and managing ADRs immediately after installing the CLI.\n\n### Implementation Details\n\n1. **Global CLI Installation:** Users will install the CLI globally using: `npm install -g @log4brains\/cli` or `yarn global add @log4brains\/cli`\n2. **Local Web Interface (optional):**  Users can install the web interface as a dev dependency within their project: `npm install -D @log4brains\/web` or `yarn add -D @log4brains\/web`\n3. **Updated `init-log4brains` Script:** \n    - The `npx init-log4brains` script will be updated to detect if the CLI is installed globally.\n    - If the CLI is found globally, the script will prompt users if they want to install `@log4brains\/web` locally for the web interface features.\n    - The script will no longer modify `package.json`.\n\n### Positive Consequences:\n\n- **Simpler installation** for all users.\n- **Language-agnostic** approach broadens Log4brains's appeal.\n- **Reduced project clutter** by making the web interface installation optional.\n- **Faster onboarding** and a smoother initial experience. \n\n### Negative Consequences:\n\n- Users who prefer a web interface will need to install `@log4brains\/web` separately.  However, this is mitigated by the `init-log4brains` script guiding users through the process. \n","GenTime":"2024-08-28 11:57:35"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"text":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n\n\n##Decision\n- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","MatchedIds":[586,1279,3827,3428,1631],"Prediction":"## Decision\n\nWe will refactor `markmap-lib` to separate transforming and rendering into distinct packages: `@markmap\/transformer` and `@markmap\/viewer`. This will allow consumers to install only the dependencies they need for their specific use case, significantly reducing the overall footprint for most users. Additionally, we will implement a build process that produces a single minimized bundle for each package, eliminating the code duplication from extraneous bundles and further reducing package size. \n","GenTime":"2024-08-28 11:57:39"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"text":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n\n\n##Decision\nWe will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","MatchedIds":[3488,4937,187,856,2701],"Prediction":"## Decision\nWe will adopt a middle ground approach. While full automation would be ideal for minimizing long-term maintenance overhead, the upfront investment is too costly and might slow down the migration process itself. \n\nInstead, we will:\n\n1. **Prioritize Observability:** Implement robust monitoring and alerting for migrated services. This will help quickly identify issues and prioritize remediation efforts.\n2. **Document and Share Knowledge:** Encourage engineers to document common issues and their solutions encountered during the migration and early operational phase. This shared knowledge base can help reduce repeated effort and accelerate troubleshooting.\n3. **Identify Automation Opportunities:**  As patterns emerge in the maintenance tasks, we will prioritize automating these common pain points. This iterative approach allows us to target automation efforts for maximum impact without unnecessary upfront investment.\n\nThis approach balances the need for rapid migration with the long-term goal of reducing maintenance overhead for newly migrated services.\n","GenTime":"2024-08-28 11:57:45"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"text":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n\n\n##Decision\nThe set of functions implemented in one GCP project belongs to a single domain.\\n","MatchedIds":[4889,2183,4896,3203,2696],"Prediction":"## Decision\n\n**We will enforce strict boundaries between projects through well-defined APIs and dedicated build processes.**\n\n### Decision Details\n\n1. **API-Driven Communication**:  Projects will interact exclusively through well-documented and versioned APIs. This promotes decoupling and allows for independent evolution of project functionalities without impacting other parts of the platform.\n\n2. **Dedicated Build Processes**: Each project will have its own independent build process and artifact repository. This enforces isolation, prevents unintended dependencies, and simplifies testing and deployment.  \n\n3. **Access Control**: Access to project codebases and artifacts will be restricted to authorized personnel within the corresponding domain. This reinforces security and ownership boundaries.\n\n4. **Documentation and Governance**:  Clear guidelines will be established for creating, maintaining, and evolving projects within the platform. These guidelines will address aspects such as naming conventions, API design principles, and dependency management. \n\n### Benefits\n\n* **Improved Modularity and Reusability**: Projects can be developed, tested, and deployed as independent units, fostering code reuse and reducing development time.\n* **Increased Fault Isolation**: Problems within one project are less likely to impact other parts of the platform.\n* **Simplified Maintenance and Evolution**: Changes within a project can be made with reduced risk of unintended consequences for other projects.\n* **Clear Ownership and Responsibility**: Teams can take ownership of their respective projects, improving accountability and fostering a sense of responsibility.\n\n### Potential Drawbacks\n\n* **Increased Initial Complexity**: Establishing clear boundaries and communication channels between projects might require additional upfront effort. \n* **Risk of Duplication**: Without careful coordination, similar functionalities might be redundantly implemented across different projects. \n\n### Conclusion\n\nBy enforcing strict boundaries between projects, we aim to create a robust, scalable, and maintainable platform that allows for independent evolution and innovation within each domain. While this approach might introduce some initial complexity, the long-term benefits in terms of maintainability, scalability, and security outweigh the potential drawbacks. \n","GenTime":"2024-08-28 11:57:57"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"text":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n\n##Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","MatchedIds":[1291,1303,1319,1308,1317],"Prediction":"## Decision\nDefine a new type `ElektraBoolean` as an enumeration with values `ELEKTRA_BOOLEAN_TRUE`, `ELEKTRA_BOOLEAN_FALSE` and `ELEKTRA_BOOLEAN_UNSET`.\nUse this type consistently throughout Elektra's API and codebase for representing boolean values. \n\n**Rationale:**\n\n- Addresses inconsistencies stemming from using different representations of boolean values (e.g., `int`, `bool`).\n- Introduces a dedicated type for Elektra booleans, improving code clarity and reducing ambiguity.\n- The `ELEKTRA_BOOLEAN_UNSET` value allows explicit differentiation between set and unset boolean flags.\n\n**Implementation Details:**\n\n- Define the `ElektraBoolean` enumeration in a central header file.\n- Update existing functions and data structures using booleans to use `ElektraBoolean`.\n- Provide helper functions for converting between `ElektraBoolean` and other common boolean representations (`bool`, `int`, string literals \"true\" and \"false\").\n\n**Impact:**\n\n- API changes: Functions using booleans will now use `ElektraBoolean`.\n- Code changes: Existing code needs to be adapted to use the new type and helper functions.\n\n**Benefits:**\n\n- Improved code readability and maintainability through consistent boolean handling.\n- Reduced risk of errors due to implicit conversions and assumptions about boolean representations.\n- Enhanced flexibility for future extensions, e.g., adding support for tri-state booleans. \n","GenTime":"2024-08-28 11:58:05"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"text":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n\n##Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n","MatchedIds":[3203,1477,1035,3483,1416],"Prediction":"## Decision\n\nWe will not develop the software in one big (Gradle) project. While acknowledging the ease of integration a single project offers, the drawbacks outweigh the benefits. \n\nDeveloping smaller, independent modules, each with its own Gradle project, will be the chosen approach. This allows for:\n\n* **Improved code quality:** Defined API boundaries between modules enforce better code structure and design, leading to higher quality and maintainability.\n* **Increased reusability:** Independent modules can be easily reused in other projects, increasing development efficiency and reducing code duplication.\n* **Simplified testing:** Smaller modules are easier to test in isolation, leading to more robust and reliable software.\n\nWhile this approach may require additional effort for integration, we believe the long-term benefits of modularity, reusability, and code quality are crucial for the success and maintainability of the software.\n","GenTime":"2024-08-28 11:58:10"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"text":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n\n##Decision\nIt will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","MatchedIds":[3853,1416,3203,4868,2687],"Prediction":"## Decision\nTo ensure good code quality with ease of change, integration, and error correction, we will adopt a multi-faceted approach that includes:\n\n**1. Code Style and Linting:** Enforce a consistent code style across the project using linters. This improves readability and helps catch potential errors early on. We will choose a linter specific to the language used in the project (e.g., ESLint for JavaScript, Pylint for Python).\n\n**2. Unit Testing:** Implement a robust suite of unit tests with high code coverage. This ensures individual components function as expected and provides a safety net for future changes, making refactoring and bug fixes less error-prone. We will aim for a minimum of 80% code coverage.\n\n**3. Continuous Integration and Continuous Deployment (CI\/CD):** Implement a CI\/CD pipeline that automatically builds, tests, and deploys code changes. This ensures that new code integrates seamlessly with the existing codebase and helps identify potential issues early in the development cycle.\n\n**4. Code Review:** Enforce mandatory code reviews by at least one other developer for all code changes before merging. This helps catch potential issues related to design, functionality, and code style, fostering knowledge sharing and ensuring code quality.\n\n**5. Modular Design:** Design the application with modularity in mind. This means breaking down the codebase into smaller, independent modules with well-defined interfaces. This improves maintainability, testability, and reusability, making it easier to change or replace individual components without affecting the rest of the system. \n\nBy combining these practices, we can establish a robust development process that promotes high-quality code, facilitates smooth integration, and simplifies error correction, ultimately leading to a more maintainable and scalable application. \n","GenTime":"2024-08-28 11:58:19"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"text":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n\n\n##Decision\nThe recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","MatchedIds":[1089,1094,631,1239,1091],"Prediction":"## Decision\n**Azure Functions** will be used to host the ingestion sub-system.\n\n## Justification\nGiven the requirement for a managed service that supports scheduling, background execution, .NET Core, and Cosmos DB connectivity, Azure Functions emerges as the optimal choice for the following reasons:\n\n* **Serverless and Managed:** Azure Functions aligns with the preference for a managed service, abstracting infrastructure management and enabling a focus on code.\n* **Scheduling Capabilities:** The service's once-a-day execution requirement is readily addressed by Azure Functions' robust timer trigger functionality.\n* **Integration with Cosmos DB:** Azure Functions seamlessly integrates with Cosmos DB, simplifying data access and manipulation.\n* **Cost-Effectiveness:** Given the infrequent execution schedule, Azure Functions' consumption-based pricing model ensures cost optimization.\n\nWhile other options like App Services or Container Instances could potentially fulfill the requirements, they might introduce unnecessary overhead for a simple, scheduled task. Azure Functions provides a streamlined, efficient solution specifically designed for this purpose. \n","GenTime":"2024-08-28 11:58:24"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"text":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n\n##Decision\nChosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","MatchedIds":[4637,4741,4644,2513,2859],"Prediction":"## Decision\nChosen option: **A custom DSL (Domain Specific Language) based on a subset of SQL with extensions for library-specific features**, because it offers the best balance between familiarity, flexibility, and specificity for our use case. \n\n### Positive Consequences:\n\n* **Familiarity & Usability:**  Leveraging a SQL-like syntax provides a familiar structure for users already acquainted with database queries, easing the learning curve.\n* **Flexibility:** A custom DSL allows us to define exactly the features and operators necessary for our abstract search system, avoiding unnecessary complexity from a full-fledged query language.\n* **Extensibility:** The DSL can be easily extended with custom functions and operators tailored to support specific search features offered by different libraries. \n* **AST Generation:** Parsing a well-defined DSL into an AST is relatively straightforward, enabling robust query analysis and transformation.\n\n### Negative Consequences:\n\n* **Development Effort:** Designing, implementing, and maintaining a custom DSL (including a parser and potentially a validator) requires dedicated development effort.\n* **Learning Curve (Minor):**  While SQL-like syntax provides a head start, users will still need to learn the specific nuances and limitations of our DSL.\n\n\n### Syntax Features:\n\n**Core SQL Subset:**\n\n* **SELECT:** For specifying target fields (potentially optional if the context implies it).\n* **FROM:**  For indicating the search scope (e.g., specific library or collection).\n* **WHERE:**  For defining search criteria.\n* **Logical Operators:**  AND, OR, NOT for combining conditions.\n* **Comparison Operators:**  =, !=, >, <, >=, <= for value comparisons.\n* **Parentheses:** For grouping expressions. \n\n**Custom Extensions:**\n\n* **Full-text Search:** Operators like `CONTAINS`, `STARTS_WITH`, and `ENDS_WITH` for textual searches.\n* **Range Queries:** Support for specifying ranges (e.g., date ranges) using operators like `BETWEEN`.\n* **Wildcard Support:**  `*` and `?` wildcards for partial matching.\n* **Library-Specific Functions:** Custom functions tailored to individual library features (e.g.,  `GEO_DISTANCE(location, radius)` for location-based searches).\n\n### Implementation Notes:\n\n* **Parser Generator:** Utilize a parser generator tool (like ANTLR or Bison) to simplify the creation of a robust parser for the DSL.\n* **AST Design:** Carefully design the structure of the AST to facilitate easy traversal and transformation into library-specific queries.\n* **Validation:** Implement validation rules within the parser or as a separate step to ensure that queries adhere to the DSL grammar and constraints.\n\n### Future Considerations:\n\n* Explore the possibility of using an existing, lightweight query language as the foundation for the DSL, potentially reducing development effort. However, carefully evaluate whether such a language offers sufficient customization and alignment with our specific needs.\n* As the system evolves, monitor the DSL's complexity and consider refactoring or introducing new abstractions if necessary to maintain maintainability and usability. \n","GenTime":"2024-08-28 11:58:39"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"text":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n\n\n##Decision\nWe will use Sentry for application exception monitoring.\\n","MatchedIds":[3388,2387,3295,4852,1545],"Prediction":"## Decision \n\nUse Sentry for real-time production error discovery and reporting. This will consolidate error reporting tooling and leverage existing experience and integrations with DLS applications. \n","GenTime":"2024-08-28 11:58:41"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"text":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n\n\n##Decision\nTesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","MatchedIds":[282,284,287,5059,4308],"Prediction":"## Decision\n**img2pdf**\n\n**Reasoning:**\n\n* **Simplicity and Focus:** `img2pdf` is specifically designed for converting images to PDF, making it a more focused and potentially reliable solution for this specific task. Other options like ImageMagick offer broader image manipulation capabilities, which might not be necessary in this context and could introduce unnecessary complexity.\n* **File Size Efficiency:** Initial tests indicate that `img2pdf` produces reasonably sized PDF files without compromising image quality.\n* **Dependency Management:**  Opting for an external command like `img2pdf` can be advantageous in terms of dependency management, particularly if we aim to minimize the footprint of the application. We avoid incorporating a large library like ImageMagick into the project.\n\n**Additional Considerations:**\n\n* While `tesseract` is primarily an OCR engine, it can generate PDF output. However, OCR is not a requirement in this context, and using `tesseract` for image-to-PDF conversion might be overkill.\n* PDFBox provides Java library functionalities, but using an external command can potentially simplify the architecture and reduce code complexity for this specific task.\n\n**Further Exploration:**\n\n* A more detailed comparison of the output PDF file sizes generated by each tool would be beneficial.\n* Evaluate if `img2pdf` can adequately handle the expected range of image formats and sizes in our use case. \n","GenTime":"2024-08-28 11:58:49"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"text":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n\n\n##Decision\nSpring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","MatchedIds":[4096,1576,856,3470,3328],"Prediction":"## Decision\n\nA standalone microservice written in **[Go](https:\/\/go.dev\/)** will be created to provide the HTML Diff Service. \n\n**Reasons:**\n\n* **Performance:** Go is a compiled language known for its speed and efficiency, making it ideal for a performance-critical task like HTML diffing. \n* **Concurrency:** Go's goroutines and channels provide a lightweight and efficient way to handle concurrent requests for diffs.\n* **Small footprint:** Go binaries are statically linked and have a small memory footprint, minimizing the overhead of running the service. \n* **Simplified Deployment:**  A standalone Go service can be easily containerized (e.g., using Docker) and deployed independently from the main Rails application. \n* **Java Dependency Removal:** Aligns with the goal of removing the Java dependency and simplifying the Rails application.\n\nThis approach offers a robust, performant, and maintainable solution for providing HTML diff functionality as a standalone service, improving the overall architecture of the system. \n","GenTime":"2024-08-28 11:58:56"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"text":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n\n\n##Decision\nModules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","MatchedIds":[621,2539,2536,3147,2793],"Prediction":"## Decision\n\nWe will adopt a two-tiered approach to using Terraform modules within the Modernisation Platform:\n\n**1. Platform-Level Modules:**\n\n* These modules encapsulate core infrastructure components that are standardized across the platform. This includes components like networking infrastructure (VPCs, subnets, etc.), security groups, and common services (e.g., logging, monitoring).\n* Platform-level modules are versioned and maintained centrally, ensuring consistency and reducing duplication of effort across different applications.\n* These modules will have well-defined inputs and outputs, allowing for flexibility while maintaining a consistent structure.\n\n**2. Application-Specific Modules:**\n\n* Teams are free to create application-specific modules to manage infrastructure unique to their applications. These modules can build upon the foundation provided by the platform-level modules.\n*  Application-specific modules reside within the application's code repository and are managed by the respective development teams.\n* While encouraged to follow best practices and guidelines, application-specific modules have more flexibility in terms of structure and implementation to cater to specific application needs. \n\n**Additional Considerations:**\n\n* We will create and maintain clear documentation for all platform-level modules, including usage instructions, input\/output specifications, and versioning information.\n*  A process for proposing, reviewing, and incorporating new platform-level modules will be established to ensure their relevance and quality.\n*  We will explore tooling and automation to simplify the discovery, usage, and management of both platform-level and application-specific modules within the Modernisation Platform.\n\nThis two-tiered approach balances the need for standardization and reusability with the flexibility required by individual applications. It promotes consistency and efficiency while empowering teams to manage their specific infrastructure needs. \n","GenTime":"2024-08-28 11:59:05"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"text":"## Context\\nWe need to choose webserver.\\n\n\n##Decision\nWe will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","MatchedIds":[1243,4833,3657,1240,583],"Prediction":"## Decision\n\nWe will use **Nginx**. \n\nWhile Apache is a valid option and we have experience with it, Nginx offers several advantages:\n\n* **Performance:** Nginx is known for its high performance and low resource consumption, especially under high load. \n* **Reverse Proxy Capabilities:** Nginx excels as a reverse proxy and load balancer, which will be beneficial as our application grows.\n* **Simplicity and Flexibility:** Nginx has a simpler configuration and a more lightweight footprint compared to Apache. \n\nThis decision prioritizes performance and scalability for our web application. \n","GenTime":"2024-08-28 11:59:09"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"text":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n\n##Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","MatchedIds":[2050,5213,1767,1786,1784],"Prediction":"## Decision\n\nWe will use GOV.UK Notify to send notifications for all new public-facing digital services. \n\nThis decision is based on the following:\n\n* **Reduced development effort:** Using GOV.UK Notify eliminates the need to build and maintain custom notification infrastructure. \n* **Cost-effectiveness:** GOV.UK Notify provides a free tier for sending emails, which is sufficient for many use cases. \n* **Reliability and Monitoring:** GOV.UK Notify is a robust service operated by GDS with built-in monitoring and delivery feedback mechanisms.\n* **Security and Data Protection:** GOV.UK Notify is a secure platform that helps prevent accidental data breaches compared to ad-hoc solutions like using BCC in emails.\n* **Government Standard:** Using GOV.UK Notify aligns with government best practices and promotes consistency across digital services.\n\nFor initial implementations or specific use cases, the CSV upload functionality can be utilized. However, for long-term sustainability and scalability, integration with the GOV.UK Notify API should be prioritized.\n","GenTime":"2024-08-28 11:59:15"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"text":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n\n\n##Decision\nThe application will be hosted on Microsoft Azure Cloud datacenter.\\n","MatchedIds":[1094,3657,1239,1095,302],"Prediction":"## Decision\nWe will host the application on a **managed cloud datacenter** such as **Azure** or **AWS**.\n\nThis decision is based on the following factors:\n\n* **Scalability:** Cloud providers offer a wide range of services and tools that simplify scaling applications on demand, allowing for greater flexibility and cost-effectiveness compared to managing physical infrastructure.\n* **Cost:** Utilizing a managed cloud datacenter eliminates the upfront investment and overhead associated with procuring, maintaining, and upgrading hardware in a private data center.\n* **Technology Choices:** Cloud providers offer a rich ecosystem of managed services like databases, message queues, and monitoring tools, allowing for faster development and deployment cycles. \n\nWhile a private datacenter might offer more direct control, the scalability, cost-efficiency, and breadth of services offered by managed cloud datacenters make them the more suitable choice for modern applications. \n","GenTime":"2024-08-28 11:59:20"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"text":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n\n\n##Decision\n* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","MatchedIds":[915,920,1016,1863,4841],"Prediction":"## Decision\n\nThe Children's Social Care Alpha Service will implement a **centralised event stream processing platform** to ingest, process, and expose the data related to Looked After Children and Foster Families. \n\n**Reasoning:**\n\n* **Scalability and Real-time Insights:**  The event-driven approach allows the platform to scale effectively as data from numerous LAs, IFAs, and SSDA903 flows in. This facilitates near real-time analysis and insights into individual pathways and trends within the Care System.\n* **Data Integration and Consistency:** The platform will act as a central repository, consolidating data from disparate sources like LA CMS, IFA systems, and SSDA903, ensuring data consistency and reducing reliance on complex point-to-point integrations.\n* **Flexibility and Future-Proofing:**  Adopting an event-driven architecture provides flexibility to incorporate future data sources and adapt to evolving requirements within the Children's Social Care domain.\n\n**Implementation Details:**\n\n* **Technology:** We will employ a cloud-based managed service such as Apache Kafka or AWS Kinesis for robust and scalable event streaming.\n* **Data Transformation:**  Implement data transformation pipelines to cleanse, enrich, and standardize the data received from different sources into a unified format.\n* **Data Access and APIs:**  Expose the processed data through well-defined APIs to enable easy consumption by the Alpha Service and other authorized applications.\n\nThis decision prioritises creating a flexible and scalable foundation for the Children's Social Care Alpha Service to ingest, process and utilize critical data for analysis and insights.\n","GenTime":"2024-08-28 11:59:28"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"text":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n\n\n##Decision\nWe will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","MatchedIds":[78,3657,3653,3715,1243],"Prediction":"## Decision\n\nWe will use the latest stable release of Blacklight 7. This provides the most up-to-date feature set and security while still allowing for a good upgrade path from older versions. We will re-evaluate this decision if compatibility issues arise with other project dependencies. \n","GenTime":"2024-08-28 11:59:31"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"text":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n\n\n##Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","MatchedIds":[856,4707,4096,631,1376],"Prediction":"## Decision\n\n**Adopt a hybrid approach, leveraging infrastructure-as-code (IaC) tools alongside carefully crafted wrapper scripts.**\n\nThis approach balances the benefits of both worlds:\n\n**IaC for Core Infrastructure:**\n\n* Use IaC tools like Terraform or Crossplane to manage the core infrastructure components (e.g., Kubernetes cluster creation, network configuration, storage provisioning). This brings:\n    * **Predictability and Repeatability:** Environments are provisioned consistently.\n    * **Scalability:** IaC simplifies managing multiple environments and scaling resources.\n    * **Documentation:** IaC code serves as documentation of the infrastructure setup.\n\n**Wrapper Scripts for Operational Tasks and Domain-Specific Logic:**\n\n* Develop focused wrapper scripts for tasks and workflows that benefit from automation and abstraction:\n    * **Common Operational Tasks:** Adding nodes, rotating credentials, performing backups.\n    * **Enforcing Best Practices:** Scripts can enforce conventions for deployments, security hardening, and logging.\n    * **Domain-Specific Logic:**  Tasks specific to your application or organization's workflows.\n\n**Key Considerations:**\n\n* **Script Design:**\n    * Keep scripts concise and focused on specific tasks.\n    * Favor composability; allow scripts to be used independently or combined for complex workflows.\n    * Provide clear documentation and usage examples.\n* **Transparency:**\n    * Design scripts to be auditable. Make it easy to understand how they interact with underlying tools.\n    * Provide options for users to bypass or customize script behavior when needed.\n* **Version Control and Testing:**\n    * Treat scripts as production code. Version control them and implement automated tests to ensure reliability.\n\n**Positive Consequences:**\n\n* **Improved Developer Experience:**  Simplify common tasks for developers while still allowing flexibility.\n* **Reduced Error Rates:**  Automation reduces the potential for human error.\n* **Increased Agility:** IaC enables faster infrastructure provisioning and updates, while focused scripts streamline operations.\n\n**Negative Consequences:**\n\n* **Potential for Complexity:**  Maintaining a balance between IaC and scripting requires careful planning and organization.\n* **Learning Curve:** Teams need familiarity with IaC tools and scripting best practices. \n\n**Mitigation:**\n\n* **Standardization:** Establish clear standards and guidelines for both IaC and script development.\n* **Training and Documentation:** Provide comprehensive documentation and training resources for your chosen tools and workflows.\n* **Community of Practice:** Encourage knowledge sharing and collaboration within your engineering team to build expertise. \n","GenTime":"2024-08-28 11:59:45"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"text":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n\n\n##Decision\nWe will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","MatchedIds":[2181,1863,2039,5009,4021],"Prediction":"## Decision\n\n### Interim Solution: Versioned JSON Schema in GitHub\n\nFor the initial phase, we will implement a lightweight solution using JSON Schema to define the data structure and validation rules for each framework's MI return.\n\n* **JSON Schema:**  We will use JSON Schema to define the required fields, data types, and validation rules for each framework's submission. This provides a structured, human-readable, and easily parsable format.\n* **GitHub Repository:** A dedicated GitHub repository will store the JSON Schema files. \n    * Each framework will have a dedicated directory within the repository.\n    * Schema versions will be managed through Git, allowing us to track changes and revert if needed.\n    * We will leverage Git tags to mark specific versions of the schema, aligning them with framework versions or submission periods. \n* **Data Validation and Template Generation:**  Services will directly access and utilize the JSON Schemas from the GitHub repository for:\n    * **Validation:** Validating submitted data against the corresponding framework's schema.\n    * **Template Generation:** Generating templates in various formats (Excel, ODS, CSV) based on the schema definition.\n\n### Long-Term Approach:  Centralized Schema Registry with API\n\nThe longer-term solution involves establishing a dedicated Schema Registry service with a well-defined API. \n\n* **Centralized Schema Management:**  The Schema Registry will serve as the single source of truth for all framework data submission rules. It will provide a centralized platform for managing, storing, and accessing these schemas. \n* **Version Control:** The Schema Registry will handle versioning of schemas, ensuring that historical versions are retained and accessible. Services can request specific versions of a schema. \n* **API Access:** A robust API will be developed to enable:\n    * Schema retrieval by framework and version.\n    * Potentially, schema validation requests.\n    * Access control and authorization mechanisms to manage schema visibility and modification rights.\n\n### Transition Plan\n\n1. **Implement Interim Solution:** Start with the JSON Schema and GitHub approach for immediate needs.\n2. **Design and Develop Schema Registry:** In parallel, initiate the design and development of the Schema Registry service and API. \n3. **Migration:**  Once the Schema Registry is operational, migrate the schemas from the GitHub repository to the registry.  Update services to interact with the Schema Registry API.\n\n### Justification\n\n* **Pragmatic Approach:** The interim solution offers a practical way to manage schemas using readily available tools while the long-term solution is under development.\n* **Scalability and Maintainability:** The Schema Registry provides a scalable and maintainable solution as the number of frameworks and complexity of validation rules increases. \n* **Single Source of Truth:**  A centralized Schema Registry with API access promotes consistency, reduces redundancy, and streamlines data management across multiple services. \n","GenTime":"2024-08-28 11:59:59"}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"text":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n\n\n##Decision\nWe've decided to implement very simple feature flags that can be toggled with environment variables.\\n","MatchedIds":[3653,187,1173,5102,4508],"Prediction":"## Decision\nWe will implement a feature flag management system to control feature visibility. This system will offer:\n\n* **Centralized Management:** A dashboard or configuration file to easily toggle features on\/off for different environments (e.g., local, staging, production). \n* **Environment-Specific Flags:**  Ability to enable features in specific environments (like staging) for testing while keeping them hidden in production.\n* **Code-Level Integration:**  A simple API for developers to check flag status within the application codebase. This could be as simple as:\n    ```python\n    if feature_flags.is_enabled('new_checkout_flow'):\n       # Execute code for the new checkout flow \n    else:\n       # Use the existing checkout flow\n    ```\n* **Persistence:** Feature flag states should be persisted so that changes are reflected across application restarts.\n\n**Implementation Considerations:**\n\n* **Open-Source Solutions:** Consider using pre-built feature flag libraries or services (e.g., LaunchDarkly, Unleash, Flagsmith) to accelerate implementation and minimize maintenance.\n* **Custom Solution:** If building in-house, prioritize a robust and scalable solution to handle potential growth in the number of features and environments.\n* **Gradual Rollout:** For critical features, explore the possibility of percentage-based rollouts using the feature flag system to gradually expose the feature to users and monitor for issues.\n\nThis approach allows us to develop and deploy features continuously, even before they are ready for public release, without the need for long-lived feature branches. It provides greater control and flexibility in managing feature lifecycles and reduces the risk of merge conflicts and production issues. \n","GenTime":"2024-08-28 12:00:07"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"text":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n\n\n##Decision\nThe decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","MatchedIds":[2499,5024,3295,1545,578],"Prediction":"## Decision\n\nWe will leverage the industry-standard TLS protocol to establish a secure connection tunnel between the Wormhole Connector, residing in the customer's datacenter, and the Kyma platform. The connection will be initiated from the customer's environment (outbound) to avoid firewall complications. \n\n**Further considerations:**\n\n* **Authentication:**  Implement mutual TLS authentication to verify both the Wormhole Connector and the Kyma platform.\n* **Encryption:** Employ strong encryption ciphers to protect data in transit. \n* **Key Management:**  Define a secure mechanism for managing and rotating TLS certificates and keys.\n* **Scalability & High Availability:**  Design the solution to handle a high volume of connections and ensure high availability. This might involve load balancing and failover mechanisms. \n\n**TODO:**\n\n* Evaluate and select a suitable TLS library for implementation.\n* Define the certificate issuance process, potentially leveraging a public Certificate Authority or a private PKI infrastructure. \n* Implement monitoring and logging for security-related events.\n","GenTime":"2024-08-28 12:00:13"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"text":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","MatchedIds":[4062,4701,4804,3820,1145],"Prediction":"## Context and Problem Statement \nOur application needs to interact with several external APIs to provide real-time data insights. However, these APIs have varying levels of reliability and latency. How can we ensure our application remains responsive and resilient even when facing external API disruptions? \n## Decision Drivers \n* **Reliability:** The application must remain functional even if one or more external APIs experience downtime.\n* **Performance:** The application must maintain acceptable response times, minimizing the impact of slow or unresponsive APIs.\n* **Maintainability:** The chosen solution should be easy to implement, monitor, and maintain without adding significant complexity. \n","GenTime":"2024-08-28 12:00:22"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"text":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n\n\n##Decision\nThe decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","MatchedIds":[2204,4378,1255,2513,485],"Prediction":"## Decision\n\nDue to the unavailability of returning geo-distance within Azure Search results, we will perform the distance calculation on the application side. \n\nHere's a breakdown of the decision:\n\n**Option 1: Wait for Azure Search Feature Implementation**\n\n* **Pros:** No immediate development effort required. \n* **Cons:**  Relies on Microsoft's roadmap and prioritization, which is uncertain. Delays the application's requirement for accurate distance calculations. \n\n**Option 2: Calculate Distance on the Application Side**\n\n* **Pros:**  Provides a solution immediately. Allows for control over the distance calculation logic (e.g., using the Haversine formula for more accurate distance on a sphere). \n* **Cons:**  Adds complexity to the application layer. May require additional processing time, potentially impacting performance if dealing with a large number of results.\n\n**Chosen Option:** Option 2 - Calculate Distance on the Application Side\n\n**Rationale:** \n\nWhile waiting for Azure Search to implement the feature seems ideal, the uncertainty of the timeline poses a significant risk. Calculating the distance on the application side provides a pragmatic and immediate solution. The added complexity is manageable, and the potential performance impact can be mitigated through optimization techniques if necessary.\n\n**Implementation Considerations:**\n\n* Use an efficient distance calculation algorithm like the Haversine formula.\n* Consider caching frequently accessed distances to reduce computation overhead.\n* If performance becomes a concern, explore asynchronous processing of distance calculations or alternative architectural patterns for optimization. \n","GenTime":"2024-08-28 12:00:32"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"text":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n\n\n##Decision\n**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","MatchedIds":[2499,885,84,775,4385],"Prediction":"## Decision\n\nWe will adopt a **hybrid approach** to event design, prioritizing **thin events** as the default choice but allowing for **fat events** where necessary.\n\n**Rationale:**\n\n* **Flexibility and Scalability**: Thin events offer significant advantages in terms of maintainability, payload size, and data consistency. By minimizing the data included in the event, we reduce coupling between services and simplify future updates. \n* **Performance Optimization**: Thin events minimize network traffic and processing overhead for the majority of consumers that may not require the complete data set.\n* **Audit Logging and Specific Use Cases**: We acknowledge that certain scenarios, like audit logging, benefit from the comprehensive information provided by fat events. In these specific situations, we will deviate from the thin event approach and include the necessary data within the event payload. \n\n**Implementation Details:**\n\n* **Standard Event Structure:** We will define a standard event structure that includes:\n    * **Event Metadata:**  Event type, timestamp, source, and unique identifier.\n    * **Resource Identifier:** A unique identifier of the resource affected by the event.\n    * **Optional Payload:** A flexible field to accommodate additional data for specific event types, used sparingly for fat events.\n* **API Contracts:**  Well-defined APIs will be provided for consumers needing to retrieve additional data related to the event, using the resource identifier provided.\n* **Event Versioning:**  We will implement a versioning strategy for events, allowing consumers to handle different versions gracefully.  \n\nThis hybrid approach provides a pragmatic balance, leveraging the strengths of both thin and fat events to build a robust and scalable event-driven architecture. \n","GenTime":"2024-08-28 12:00:41"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"text":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n\n\n##Decision\nIntroduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","MatchedIds":[2280,779,2221,1883,1255],"Prediction":"## Decision\nIntroduce a new function called `application.bookmarks.addById(id)` which encapsulates the lookup and construction of the bookmark object. This function will:\n\n1. **Retrieve the necessary information** for the bookmark based on the provided `id` (e.g., 'gbj1jf'). This might involve fetching data from a cache or making a request to the backend if the data is not readily available.\n2. **Construct a `Bookmark` object** using the retrieved information.\n3. **Delegate the actual bookmark addition** to the existing `application.bookmarks.add(bookmark)` function.\n\nThis approach addresses the concerns outlined in the context:\n\n* **No need for additional lookups later:** The necessary information is retrieved and stored with the bookmark.\n* **Avoids storing every article:** Only bookmarked articles require additional data storage.\n* **Keeps the UI element clean:** The link only needs to provide the article ID.\n* **Maintains the existing `application.bookmarks.add` API:**  The new function acts as a convenient wrapper.\n\n**Example:**\n\n```javascript\n\/\/ In the UI:\n<a href=\"javascript:application.bookmarks.addById('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\n\n\/\/ In the application code:\napplication.bookmarks.addById = async function(id) {\n  \/\/ 1. Retrieve information for the given ID\n  const articleData = await getArticleData(id); \/\/ Fetch data from cache\/backend\n\n  \/\/ 2. Construct Bookmark object\n  const bookmark = new Bookmark(id, articleData.url, articleData.source); \n\n  \/\/ 3. Delegate to the existing add function\n  await application.bookmarks.add(bookmark); \n}\n``` \n","GenTime":"2024-08-28 12:00:51"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"text":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n\n\n##Decision\nSwitch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","MatchedIds":[1320,3504,2396,4557,1048],"Prediction":"## Decision\n\nRevert to the previous stable version of Yarn.  Investigate the issues encountered with the latest upgrade and document potential solutions or workarounds. Once the issues are resolved, and the upgrade is deemed stable, we can revisit upgrading again. \n","GenTime":"2024-08-28 12:00:53"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"text":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n\n##Decision\nProvide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","MatchedIds":[1319,1279,1308,2975,1303],"Prediction":"## Decision \n\n**Introduce a guideline for plugin developers defining when to emit errors and warnings.**\n\nThis guideline should cover, at a minimum:\n\n* **The difference between errors and warnings:** Errors should be used for situations where the plugin cannot complete its operation successfully. Warnings should be used for situations where the plugin can continue, but there may be unexpected behavior or data loss.\n* **Common scenarios and their appropriate error\/warning levels:** This could include situations like:\n    * Invalid user input\n    * Resource exhaustion \n    * Network errors\n    * File system errors\n    * Internal plugin errors\n* **Best practices for error messages:**  This should cover aspects like:\n    * Providing specific and actionable information to the user\n    * Avoiding technical jargon\n    * Using consistent language and formatting\n\n**In addition to the guideline:**\n\n* **Update the existing Error Codes Guideline:** Provide examples for each error\/warning category illustrating when its usage is appropriate.\n* **Develop tools for static analysis:**  These tools can help identify potential issues with error and warning usage in plugins during development. \n\nThis approach ensures a consistent experience for Elektra users and makes it easier for plugin developers to make the right decisions regarding error and warning handling.\n\n**Relevant Documents:**\n\n* [Error Codes Guideline](..\/dev\/error-categorization.md) (This link should point to the updated document incorporating the new guidelines.) \n","GenTime":"2024-08-28 12:01:01"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"text":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n\n##Decision\nSolution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","MatchedIds":[1758,2498,2183,647,3203],"Prediction":"## Decision\n\nWe will use **Constructor-based Dependency Injection** to resolve object dependencies within our modules. This approach offers several advantages:\n\n* **Improved Testability:** Constructor injection makes it easier to write unit tests because dependencies are explicitly provided, allowing for easy mocking and isolation of individual components.\n* **Increased Code Clarity:** By explicitly stating dependencies in the constructor, the code becomes more self-documenting and easier to understand. It's clear at a glance what collaborators each class requires.\n* **Reduced Boilerplate:** Compared to other DI approaches like method injection or service locators, constructor injection typically leads to less boilerplate code, especially when combined with a DI framework.\n* **Enforces Immutability:** Constructor injection encourages designing classes with immutable dependencies, leading to more robust and predictable code. \n\nTo simplify the management of dependencies and leverage the benefits of constructor injection, we will employ a **Dependency Injection framework**. This framework will be responsible for:\n\n* **Object instantiation and lifecycle management.**\n* **Resolving and injecting dependencies.**\n* **Configuration and wiring of the application's object graph.**\n\nWhile the specific DI framework (e.g., Spring, Guice) will be decided based on project needs and existing tech stack, the core principle of constructor-based injection will be adhered to across all modules.\n","GenTime":"2024-08-28 12:01:09"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"text":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n\n\n##Decision\nWe are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","MatchedIds":[2410,4047,4056,1376,1182],"Prediction":"## Decision\n\n1. **Target Platform:** The strategic platform for hosting all GOV.UK infrastructure is **GOV.UK PaaS**. All new services and infrastructure should be designed and deployed with this as the primary target. \n\n2. **Interim Strategy:** Recognizing that full migration to GOV.UK PaaS will take time, we will adopt a phased approach:\n\n    * **New Services:** All new GOV.UK services and applications **must** be deployed on GOV.UK PaaS. \n    * **Existing Services:** Existing services will be migrated to GOV.UK PaaS based on a prioritized roadmap, considering factors like:\n        * Business criticality\n        * Technical feasibility\n        * Dependencies on other systems \n    * **AWS as Interim Solution:**  For services where immediate migration to GOV.UK PaaS is not feasible, **AWS will be used as the interim hosting platform**.  This will ensure alignment with GDS policy and allow us to leverage AWS services for modernization during the transition period. \n\n3. **Modernization during Transition:** When migrating existing services to AWS as an interim step, efforts should be made to modernize the application architecture and leverage AWS managed services wherever possible. This approach will ease the eventual migration to GOV.UK PaaS.\n","GenTime":"2024-08-28 12:01:16"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"text":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n\n##Decision\nRejected: keep default plugins as-is\\n","MatchedIds":[1278,1302,1596,1318,3860],"Prediction":"## Decision\n\nIntroduce the concept of **plugin variants** and **plugin families**:\n\n- **Plugin variants:** Represent different compilation options for a single plugin, like *debug* vs. *release*, or variants for different architectures. These are internal to the plugin and not exposed directly during mounting.\n- **Plugin families:**  A plugin family groups together multiple plugin variants with the same base functionality. For example, the `dump` plugin family might have variants for different compilation options (debug\/release) and different storage backends (default\/custom).\n\nUse a naming scheme to clearly identify plugin families and select the appropriate variant:\n\n- Plugin families are referred to by their base name, e.g., `dump`.\n- When mounting, users can specify constraints to select a specific variant, e.g., `kdb mount <path> dump --variant=release,custom-storage`.\n- If no variant is specified, the default variant (e.g., `release,default-storage`) is used.\n\n**Benefits:**\n\n- Reduces the number of visible plugins, making it easier for users to browse and mount plugins.\n- Provides a clear mechanism to handle different compilation options and default configurations.\n- Enables users to select specific plugin variants if needed, while still providing sensible defaults.\n\n**Additional considerations:**\n\n- Define a clear mechanism for specifying and resolving variant constraints.\n- Document the available variants and their differences for each plugin family.\n- Investigate using plugin metadata to store variant information and simplify selection.\n\n- [Plugin Variants](plugin_variants.md)\n- [Plugin Families](plugin_families.md) \n","GenTime":"2024-08-28 12:01:26"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"text":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n\n\n##Decision\nWe will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","MatchedIds":[2181,1180,1924,4644,693],"Prediction":"## Decision\n\nWe will implement a standardized error response format based on the principles of JSON:API for consistency and clarity. This format will be used for all API endpoints to communicate client-side errors (4xx status codes).\n\n### Error Response Format\n\nThe error response will be a JSON object with the following structure:\n\n```json\n{\n  \"errors\": [\n    {\n      \"status\": \"400\",\n      \"code\": \"INVALID_PARAMETER\",\n      \"title\": \"Invalid Parameter\",\n      \"detail\": \"The 'email' parameter is not a valid email address.\",\n      \"source\": {\n        \"pointer\": \"\/data\/attributes\/email\"\n      }\n    }\n  ]\n}\n```\n\n**Fields:**\n\n* **errors:** An array of error objects, allowing for multiple errors to be reported simultaneously.\n* **status (required):**  The HTTP status code (e.g., \"400\", \"401\", \"422\") as a string. This should align with the standard HTTP status code for the error type.\n* **code (optional):** A unique machine-readable error code for programmatic handling (e.g., \"INVALID_PARAMETER,\" \"MISSING_FIELD,\" \"RATE_LIMIT_EXCEEDED\"). \n* **title (optional):** A short, human-readable summary of the error (e.g., \"Invalid Parameter,\" \"Authentication Required\").\n* **detail (optional):** A more detailed, human-readable explanation specific to the error instance (e.g., \"The 'email' parameter is not a valid email address.\").\n* **source (optional):** An object containing references to the specific part of the request that caused the error, using JSON Pointer syntax (RFC 6901).\n\n**Example:**\n\n```json\n{\n  \"errors\": [\n    {\n      \"status\": \"422\",\n      \"code\": \"MISSING_FIELD\",\n      \"title\": \"Missing Field\",\n      \"detail\": \"The 'password' field is required.\",\n      \"source\": {\n        \"pointer\": \"\/data\/attributes\/password\"\n      }\n    }\n  ]\n}\n```\n\n### Advantages\n\n* **Consistency:** Provides a predictable and easily parsable error response format across the entire API.\n* **Informative:** Offers detailed information to help developers understand and debug client-side errors.\n* **Standard-Based:** Aligns with the widely adopted JSON:API error object specification for broader compatibility.\n\n### Considerations\n\n* **Error Code Design:** A well-defined and documented set of error codes is crucial for effective programmatic error handling by clients.\n* **Internationalization (i18n):** For applications supporting multiple languages, consider providing translations for the `title` and `detail` fields. \n","GenTime":"2024-08-28 12:01:40"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"text":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n\n\n##Decision\nWe want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","MatchedIds":[4099,33,4354,1558,4623],"Prediction":"## Decision\n\n### Genesis Transactions\n\nInstead of assigning the initial supply as an unspendable coinbase output, we will introduce the concept of **genesis transactions** within the genesis block. \n\nThese transactions will be defined within the `genesis.json` file and will be executed during the blockchain's initialization. They will have the following properties:\n\n* **No Signature Verification:** Genesis transactions are inherently trusted and therefore will not require signatures.\n* **Special Address:**  A dedicated address (e.g., \"genesis_treasury\") will be used as the sender of these transactions. This address will not be usable for regular transactions.\n* **Pre-defined Recipients:** The genesis transactions will explicitly specify recipient addresses and the amounts to be allocated, effectively distributing the initial supply.\n* **Included in TxDB:**  Unlike regular coinbase transactions, these genesis transactions will be indexed and added to the TxDB, enabling the recipients to spend their allocated funds from the outset. \n\n\n### Advantages\n\n* **Flexibility:**  Allows for a more granular and customizable initial token distribution beyond a single coinbase output.\n* **Spendability:** Initial supply is immediately accessible and spendable by the designated recipients.\n* **Transparency:** The allocation of initial tokens is clearly defined and auditable within the genesis file.\n\n\n### Implementation Notes\n\n* The codebase will need to be modified to distinguish between genesis transactions and regular transactions, particularly during block processing and TxDB handling.\n* The format for defining genesis transactions within `genesis.json` should be straightforward and human-readable. \n* Documentation and examples will be provided to clarify the usage and implications of genesis transactions. \n","GenTime":"2024-08-28 12:01:49"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"text":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n\n\n##Decision\nWe will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","MatchedIds":[3126,84,1854,523,2696],"Prediction":"## Decision\nTo ensure consistency and improve maintainability, we will adopt English as the standard language for all code identifiers, including mobx-state-tree model properties and React component props. \n\nWhile using Finnish terms might offer a sense of familiarity for some developers, opting for English provides several benefits:\n\n- **Unified codebase:**  A single language for identifiers enhances code readability and reduces confusion, especially for new developers or when collaborating with international teams.\n- **Easier debugging and error tracing:** Standardized naming makes it simpler to follow data flow and identify potential issues.\n- **Reduced cognitive load:** Developers don't have to switch between languages, streamlining the development process.\n\nTo mitigate any potential drawbacks, we will provide comprehensive documentation that clearly maps the English identifiers back to their corresponding Finnish domain concepts. \n","GenTime":"2024-08-28 12:01:54"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"text":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n\n\n##Decision\nMomenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","MatchedIds":[1855,2963,4323,779,1122],"Prediction":"## Decision\n\nWe introduceren een nieuw attribuut \"displayLabel\" in lagen.json. Dit attribuut beschrijft hoe de URL weergegeven moet worden in de tabel. \n\n**Mogelijke waarden voor \"displayLabel\":**\n\n- **\"field:[veldnaam]\"**: Toont de waarde van het gespecificeerde veld als label. Bijvoorbeeld: `{\"displayLabel\": \"field:dossiernummer\"}` toont de waarde van het veld \"dossiernummer\".\n- **\"[vaste string]\"**: Toont een vaste string als label. Bijvoorbeeld: `{\"displayLabel\": \"Bekijk detailpagina\"}`.\n- **\"[veldnaam1] - [veldnaam2]\"**: Toont een combinatie van twee velden als label. Bijvoorbeeld: `{\"displayLabel\": \"nummer - status\"}` toont de waarden van de velden \"nummer\" en \"status\" gescheiden door een streepje.\n\n**Standaardgedrag:**\n\nAls \"displayLabel\" niet gedefinieerd is in lagen.json, dan wordt de URL zelf getoond als label.\n\n**Implementatie:**\n\n1. Voeg het attribuut \"displayLabel\" toe aan de lagen.json configuratie.\n2. Pas de tabel component aan om de waarde van \"displayLabel\" te gebruiken voor het renderen van URL's. De component zal de logica bevatten om de verschillende types \"displayLabel\" te interpreteren en de juiste label te tonen.\n\n**Voordelen:**\n\n- Flexibiliteit in het tonen van URL's in de tabel.\n- Verbeterde gebruikerservaring door duidelijke en informatieve labels.\n- Eenvoudig te configureren via lagen.json. \n","GenTime":"2024-08-28 12:02:03"}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"text":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n\n\n##Decision\nBased on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","MatchedIds":[282,475,2342,102,2513],"Prediction":"## Decision\n\n**Option Chosen:** Introduce a new microservice that sits between Solr and the transformation service. The content is offloaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\n\n**Rationale:**\n\n* **Scalability and Performance:** This approach addresses the core issue of scaling. By decoupling content extraction from the main Search Service flow, we can leverage the asynchronous nature of the transformation service and handle large volumes of documents without impacting search performance. \n* **Maintainability:** Introducing a dedicated microservice promotes modularity and separation of concerns. It simplifies both Search Service and the transformation service while providing a clear boundary for future development and maintenance.\n* **Backward Compatibility:**  Maintaining the existing synchronous API for Search Services ensures minimal disruption to existing integrations. \n\n**Implementation Details:**\n\n1. **New Microservice:** Create a new microservice (\"Content Extraction Service\") responsible for:\n    * Receiving content extraction requests from the Search Service.\n    * Submitting transformation jobs to the Transformation Service asynchronously.\n    *  Caching and retrieving transformed content. \n    *  Providing the extracted text back to the Search Service.\n2. **Asynchronous Communication:**  Leverage message queues or a similar mechanism to handle asynchronous communication between the Content Extraction Service and the Transformation Service.\n3. **Caching:** Implement a robust caching strategy within the Content Extraction Service to minimize redundant transformations and improve response times. \n4. **Monitoring and Error Handling:** Incorporate robust monitoring and error handling mechanisms to ensure the reliability and resilience of the content extraction pipeline. \n\n**Alternatives Considered:**\n\n* **Refactoring V0 API:** While seemingly straightforward, refactoring a core API can introduce breaking changes and complexities. \n* **Search Service using V1 Public API:**  Directly integrating with the V1 API could create tight coupling and limit future flexibility. \n* **New Content Tracker:** Introducing a new component adds complexity, and it's unclear if it offers significant advantages over the chosen approach. \n","GenTime":"2024-08-28 12:02:14"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"text":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n\n\n##Decision\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","MatchedIds":[160,2261,4699,4428,4408],"Prediction":"## Decision\n\nWe will use a centralized event bus pattern for communication between internal components of the Transcript Editor. \n\n### Positive Consequences:\n* **Simplicity and Ease of Reasoning:** An event bus provides a straightforward publish-subscribe mechanism. Components don't need to be aware of each other directly, simplifying data flow and reasoning about component interactions.\n* **Extensibility:** Adding new components or modifying existing ones is easier with an event bus. New components can subscribe to relevant events, and existing components can publish new events without requiring changes in other parts of the system.\n* **Flexibility for Future Use Cases:** Exposing internal events on the bus allows for greater flexibility when integrating the editor into larger systems or when unforeseen use cases arise. External components can listen to and react to these internal events as needed.\n\n### Negative Consequences:\n* **Potential for Event Overload:**  Without proper management, a large number of events on the bus can become difficult to maintain and debug.\n* **Loose Coupling:** While beneficial in many ways, loose coupling can sometimes make it harder to track down dependencies and understand the flow of data through the system.\n\n### Mitigation:\n* Implement a well-defined event naming convention and schema to maintain clarity and prevent confusion.\n* Consider using tools for visualizing and monitoring events on the bus to aid in debugging and understanding data flow.\n* If performance becomes a concern, explore optimizations such as targeted event subscriptions and efficient event handling mechanisms. \n\nThis approach balances simplicity, extensibility, and the ability to expose internal events for future use cases, aligning with the desired decision drivers. \n","GenTime":"2024-08-28 12:02:23"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"text":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n\n##Decision\n### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","MatchedIds":[4119,4346,4097,4099,1721],"Prediction":"## Decision\n\n### Standardize on Protobuf\nWe will standardize on using Protobuf for encoding public keys and signatures on-chain. \n\n### Justification\n\n* **Simplicity and Performance:** Protobuf offers a simple and efficient binary encoding format. This is more space-efficient than Amino and generally faster to parse, particularly for languages with mature Protobuf libraries.\n* **Wide Language Support:** Protobuf has extensive language support, ensuring interoperability with a wide range of tools and applications. This directly addresses the need for ease of parsing in various languages.\n* **No Versioning Requirements:** As stated in the context, cryptographic agility is achieved by considering updated algorithms as entirely new cryptosystems. Therefore, the versioning capabilities of Amino are unnecessary in this context.\n\n### Consequences\n\n* **Migration Effort:** Migrating from the existing Amino-based encoding to Protobuf will require code changes and potentially data migration. This effort is mitigated by the well-defined nature of both encoding schemes.\n* **Tooling and Library Support:** While Protobuf is widely supported, teams working with less common languages may need to rely on less mature libraries. \n* **Reduced Code Size:** Using Protobuf will likely result in smaller code size compared to Amino, due to the more compact binary format.\n\n\nThis decision prioritizes simplicity, performance, and a wider range of language support by leveraging Protobuf for encoding public keys and signatures. It acknowledges the associated migration effort while emphasizing the long-term benefits in terms of efficiency and interoperability. \n","GenTime":"2024-08-28 12:02:31"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"text":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n\n\n##Decision\nWe will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","MatchedIds":[3890,620,631,1376,856],"Prediction":"## Decision\n\nWhile the read-only access successfully mitigated the initial threat, the resulting deployment bottlenecks necessitate a revised approach. We will implement a multi-tiered approach:\n\n1. **Elevated \"Emergency\" Role:** Create a new, more permissive role in the cluster with permissions to **delete** pods and other resources within specific namespaces. This role will be highly time-limited (e.g., expiring after 1 hour) and require multi-factor authentication for activation. This allows for swift remediation of deployment issues without granting permanent elevated access.\n2. **Improved Deployment Procedures:**  The root cause of the conflicting deployments needs to be addressed. This might involve:\n    * **Resource Labeling and Selectors:** Enforce stricter labeling conventions within deployments to prevent accidental resource overlaps. \n    * **Deployment Pipelines:** Implement better error handling and resource management within the Concourse pipelines. This could include pre-deployment checks for resource conflicts and automatic rollbacks on deployment failure.\n    * **Canary Deployments or Blue\/Green:**  Investigate deployment strategies like canary deployments or blue\/green deployments to further reduce the risk of deployment issues impacting production. \n\n3. **Auditing and Monitoring:** Enhance auditing and monitoring around deployment activities and \"Emergency\" role usage. This data will help identify recurring issues in the deployment process and improve future mitigation strategies.\n\nBy implementing these changes, we aim to strike a balance between maintaining a secure environment and enabling efficient deployment processes. This approach allows for timely remediation of issues while minimizing the need for broad administrative access and minimizing service disruptions. \n","GenTime":"2024-08-28 12:02:39"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"text":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n\n\n##Decision\nUse Node.js to complete this problem.\\n","MatchedIds":[577,4443,472,3715,578],"Prediction":"## Decision\n\nGiven that the problem instructions state \"C++ or node.js are preferred\" and I am more familiar with node.js, I will use **node.js** to implement the solution. My familiarity with node.js will allow for faster development, which is beneficial given the limited time available. \n","GenTime":"2024-08-28 12:02:42"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","MatchedIds":[1563,2261,4699,4428,4408],"Prediction":"## Decision\nWe will use Architecture Decision Records (ADRs) to document all significant architectural decisions made on this project, using the lightweight Markdown format described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions. \n","GenTime":"2024-08-28 12:02:45"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"text":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n\n\n##Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","MatchedIds":[3127,138,2791,279,1246],"Prediction":"## Decision\n\nWe will implement a generic mechanism for scheduling the removal of projection data based on time elapsed since its creation. This mechanism will consist of the following:\n\n1. **TTL (Time-to-Live) Attribute:** For each projection entry that requires time-based cleanup, we will include a `createdAt` timestamp attribute. \n\n2. **Cleanup Service:** A dedicated background service will periodically query for projection entries where the difference between the current time and `createdAt` exceeds the defined \"cool down\" period. This service could be implemented as a cron job or using a scheduling mechanism within our infrastructure.\n\n3. **Configurable Retention Period:** The \"cool down\" or retention period will be configurable per projection or even per entry if needed. This allows for flexibility in managing different data lifecycles within the system.\n\n4. **Optional Event Triggered Removal:**  While the primary mechanism is time-based, we will include the option to remove entries based on specific events. This can be achieved by publishing an event that triggers a delete operation within the projection, allowing for more fine-grained control when necessary.\n\n**Benefits:**\n\n* **Automated Data Cleanup:** This approach ensures automated removal of stale projection data, preventing unnecessary data accumulation.\n* **Flexibility:** The configurable retention period allows for adapting to different projection requirements.\n* **Generic Solution:**  This mechanism provides a reusable solution for managing time-based data retention across multiple projections.\n* **Improved Performance:** By removing unnecessary data, we can improve query performance and reduce storage costs.\n\n**Example Implementation (SQL):**\n\n```sql\n-- Table structure for a sample projection\n\nCREATE TABLE user_action_log (\n    id UUID PRIMARY KEY,\n    user_id UUID NOT NULL,\n    action_type VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Cleanup query executed by the background service\nDELETE FROM user_action_log \nWHERE created_at < NOW() - INTERVAL '1 hour'; -- Replace '1 hour' with the configurable cool down period\n```\n\nThis solution provides a balance between simplicity, efficiency, and flexibility, addressing the need for time-based data cleanup in projections without relying on business logic events. \n","GenTime":"2024-08-28 12:02:56"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"text":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n\n##Decision\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","MatchedIds":[4359,452,3070,2273,4179],"Prediction":"## Decision\nWe will use Continuous Integration and Continuous Deployment (CI\/CD) pipelines for our testing and integration. This will allow us to automate the process of building, testing, and deploying our code, ensuring that new changes are integrated and tested quickly and reliably. We will utilize a platform such as GitHub Actions, GitLab CI\/CD or Jenkins to achieve this. \n","GenTime":"2024-08-28 12:02:59"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"text":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n\n##Decision\nAccess controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","MatchedIds":[3214,4292,2215,3890,672],"Prediction":"## Decision\n\nImplement a two-tiered permission system that checks both author-based and ACL-based permissions when determining access to a resource:\n\n1. **Author-Based Permissions:** This layer will first check if the user requesting access is the depositor or proxy depositor of the work or collection. If so, they are granted full access, regardless of any ACL settings. \n\n2. **ACL-Based Permissions:** If the user is not the author, the system will consult the resource's ACLs. This will involve:\n\n    * **Identifying User & Group Memberships:** Determine the user's individual identity and any groups they belong to.\n    * **Matching Permissions:**  Compare the user and their group memberships against the resource's ACL to identify granted permissions (read, write, etc.).\n    * **Caching for Performance:**  Implement a caching mechanism to optimize ACL lookups and minimize performance impact.\n\nBy combining these two approaches, we ensure a flexible and secure permission model. Authors retain full control over their creations, while ACLs offer granular control for wider access management based on user and group identities. \n","GenTime":"2024-08-28 12:03:10"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"text":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n\n##Decision\nImplement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","MatchedIds":[545,2470,2397,3950,1326],"Prediction":"## Decision\n\nWe will leverage Armadillo's existing OpenID application structure and role-based access control to manage user permissions. Here's how:\n\n1. **Cohort as an OpenID Resource:** Each cohort will be represented as an OpenID application, functioning as a resource server. \n2. **Role-Based Access:** We will define a custom scope, `read:<study>`, for each study within the cohort. This scope will grant access to read data in the shared folder `<study>`.\n3. **Delegated User Management:**\n    *  **Consortium Representatives:** Will have administrative access to all cohort applications. They can assign the `manage-roles` scope for a specific cohort application to designated cohort representatives.\n    * **Cohort Representatives:** Upon receiving the `manage-roles` scope, cohort representatives can manage study researchers' access. They can grant or revoke the `read:<study>` scope to users within their cohort.\n4. **Federated Identity:** Users will authenticate with their institutional identity provider. Armadillo will receive user information, including group memberships, from the identity provider.\n5. **Auditing:** Armadillo will log all role assignments and changes, providing a clear audit trail for compliance and accountability.\n\nThis approach allows for a decentralized management structure where cohort representatives control access to their data while consortium representatives retain oversight and administrative control. It leverages existing OpenID capabilities and ensures a secure and auditable process for managing access to sensitive research data. \n","GenTime":"2024-08-28 12:03:19"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"text":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n\n\n##Decision\nA decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","MatchedIds":[4859,3657,2221,3221,3776],"Prediction":"## Decision\n\nTo facilitate a smoother transition to production for users of the Origin SDK, we will adopt **PostgreSQL** as the primary database solution moving forward. \n\n### Justification\n\n1. **Production-Ready:** PostgreSQL is a robust, open-source object-relational database system known for its reliability and scalability. This makes it a suitable choice for handling production workloads.\n\n2. **Ease of Transition:** Migrating from SQLite to PostgreSQL is relatively straightforward, with readily available tools and documentation to guide the process.\n\n3. **Familiarity:**  PostgreSQL is widely used and familiar to many developers, potentially easing the learning curve for new Origin SDK adopters.\n\n4. **Feature Richness:** Compared to SQLite, PostgreSQL offers a broader range of features including advanced data types, concurrency control, and extensions, potentially beneficial for future growth and scalability of the Origin platform.\n\n5. **Community Support:** PostgreSQL boasts a large and active community, translating to readily available resources and support for developers encountering challenges. \n\nThis change will streamline the development-to-production pipeline for Origin SDK users, encouraging wider adoption and contributing to a more robust and scalable platform.\n","GenTime":"2024-08-28 12:03:25"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"text":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n\n##Decision\nAll \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","MatchedIds":[1308,1302,1303,1293,1291],"Prediction":"## Decision\nTo address the shortcomings of the existing error concept in Elektra, a new hierarchical and categorized error system will be implemented. \n\n**1. Hierarchical Structure:**\n- Errors will be categorized into a hierarchical structure based on their root cause, e.g., validation errors, resource errors, I\/O errors. \n- This hierarchy will allow for more general error handling at higher levels while enabling specific error handling for individual error codes at lower levels.\n\n**2. Standardized Error Codes:**\n- A new, well-defined specification for error codes will be introduced. \n- Each error code will be assigned a unique identifier within its category in the hierarchy.\n- This will reduce redundancy by promoting the reuse of existing error codes for similar error situations.\n\n**3. Centralized Error Management:**\n- A centralized specification file (or database) will be established to manage the error codes and their descriptions.\n- This central repository will:\n    - Provide a single source of truth for error codes.\n    - Simplify error code lookup and management.\n    - Facilitate consistency and avoid duplication.\n\n**4. Improved Developer Experience:**\n- Tooling will be provided to aid in navigating and understanding the error hierarchy and descriptions.\n- This will simplify the process of finding appropriate error codes for new plugins and identifying the root cause of existing errors.\n\n**5. Enhanced Application Integration:**\n- The hierarchical nature will allow application developers to handle errors at different levels of granularity.\n- By checking for error categories, applications can react to broader error conditions without needing to handle each specific error code individually.\n\n**Implementation Notes:**\n- The specific format of the new error codes and the structure of the hierarchy will be defined in a separate ADR.\n- Migration to the new system will require updating existing plugin error codes, but the benefits in maintainability and developer experience justify this effort. \n","GenTime":"2024-08-28 12:03:41"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"text":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n\n\n##Decision\n* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","MatchedIds":[1463,2626,472,4695,3715],"Prediction":"## Decision\nWe will adopt Kotlin as the primary language for all new Android projects. \n\n### Positive Consequences\n* **Full Google support:**  Leveraging a language officially supported by Google ensures access to the latest tools, libraries, and best practices within the Android ecosystem.\n* **Modern Language Features:** Kotlin offers numerous modern language features like null safety, coroutines, and data classes which can lead to more concise, readable, and less error-prone code.\n* **Growing Community and Resources:**  Kotlin has a rapidly expanding community, providing ample learning resources, libraries, and support for developers.\n* **Interoperability with Java:**  Kotlin's seamless interoperability with Java allows for a gradual migration path if needed and the continued use of existing Java libraries.\n\n### Negative Consequences\n* **Learning Curve:** There might be a slight learning curve for developers familiar with Java, but Kotlin's similarity in syntax and structure makes the transition relatively smooth. \n* **Potential Impact on Build Times:** While Kotlin's compilation speed has significantly improved, there might be a minor impact on build times compared to pure Java projects, particularly in larger codebases. \n","GenTime":"2024-08-28 12:03:47"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"text":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n\n\n##Decision\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","MatchedIds":[3853,5255,1408,4226,1418],"Prediction":"## Decision\n\nWhile allowing for personal preferences initially fostered a consistent codebase, introducing a tool like SwiftLint is a good step towards ensuring long-term maintainability and consistency, especially as the team grows. \n\nHowever, simply enforcing \"most\" code style conventions leaves room for ambiguity and potential future disagreements. \n\nTherefore, we will take the following actions:\n\n1. **Document the rationale:** Clearly document the reasoning behind any deviations from standard SwiftLint rules in the `.swiftlint.yml` configuration file. This will help new developers understand the project's specific choices.\n2. **Strive for full compliance:**  Actively work towards full compliance with a well-established Swift style guide (e.g., the official Swift guidelines or a widely adopted community standard like Raywenderlich's).  \n3. **Automate with Git hooks:** Integrate SwiftLint into the development workflow using Git pre-commit hooks. This will provide immediate feedback and ensure code style adherence before commits are made.\n4. **CI integration:**  Include SwiftLint in the continuous integration pipeline to prevent merging code that violates the defined style guidelines.\n\nBy taking these steps, we can leverage the benefits of SwiftLint while acknowledging the existing codebase and minimizing potential friction as the project evolves. \n","GenTime":"2024-08-28 12:03:54"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"text":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n\n\n##Decision\n4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","MatchedIds":[2261,920,4804,3820,1145],"Prediction":"## Decision\n\n**Adjust project timelines and deadlines:**\n\nTo accommodate Thabang's absence and the anticipated productivity dip during the submission period, we will adjust the project timelines and deadlines as follows:\n\n1. **Identify critical tasks dependent on Thabang between April 16-25th.** For these tasks, we will explore the following options:\n    * **Delegation:** If possible, delegate tasks to another team member with the necessary expertise.\n    * **Postponement:** If delegation is not feasible, postpone the tasks to a later date after Thabang returns.  \n    * **Remote Collaboration:**  If the task is time-sensitive, explore options for remote collaboration with Thabang, considering potential limitations. \n\n2. **Reassess sprint goals for sprints spanning April 29th - May 10th.**  Given the anticipated productivity impact, we will adjust sprint goals to ensure realistic and achievable targets. This might involve:\n    * **Reducing sprint scope:** Prioritize essential tasks and deferring less critical features to a later sprint.\n    * **Extending sprint duration:** Consider extending the sprint duration to accommodate the anticipated slowdown.\n\n3. **Communicate adjusted timelines clearly to all stakeholders.** This includes the team, product owner, and any other relevant parties, ensuring everyone understands the rationale behind the adjustments and the updated schedule.\n\nThis approach emphasizes flexibility and proactively addresses potential bottlenecks caused by overlapping events. By adjusting timelines and expectations, we can mitigate the impact of Thabang's absence and the submission period on the project's overall progress and maintain a sustainable pace for the team. \n","GenTime":"2024-08-28 12:04:03"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"text":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n\n\n##Decision\nIf a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","MatchedIds":[1320,3497,5102,3853,3682],"Prediction":"## Decision\n\n**Refuse to build the project if the `package-lock.json` file is missing.** \n\n**Reasoning:**\n\n* **Enforces Best Practices:**  This strictly enforces the recommended practice of committing the `package-lock.json` file, leading to more reliable and deterministic builds.\n* **Transparency:** Developers are explicitly informed about the missing lock file and understand its importance in the build process.\n* **Security:**  By enforcing the presence of the lock file, we ensure the `npm audit` stage runs, maintaining a higher level of security against known vulnerabilities.\n\n**Implementation:**\n\nThe build pipeline should be configured to:\n\n1. **Check for `package-lock.json`:**  At the beginning of the build process, verify the existence of the lock file in the project root.\n2. **Fail Build:** If the file is missing, halt the build process and log a clear error message indicating the missing file and explaining the rationale behind this requirement.\n3. **Proceed with Build:**  If the file exists, proceed with the build as usual, including running `npm ci` and `npm audit` stages.\n\n**Long-Term Benefits:**\n\n* Increased build reliability and consistency across different environments.\n* Reduced likelihood of unexpected issues arising from inconsistent dependency versions.\n* Improved security posture by ensuring vulnerability scanning is always performed. \n","GenTime":"2024-08-28 12:04:10"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"text":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n\n\n##Decision\n1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","MatchedIds":[2410,4841,1799,1480,1180],"Prediction":"## Decision\n\n**Treat `court-case-service` as a general purpose API, consumable by multiple clients, including but not limited to `prepare-a-case`.**\n\n**Reasoning:**\n\n* **Flexibility & Reusability:** Designing `court-case-service` as a general purpose API promotes reusability and extensibility. This approach avoids tight coupling to `prepare-a-case`, allowing other applications or services to leverage the API in the future. \n* **Decoupling & Maintainability:** Decoupling the services simplifies maintenance and evolution. Changes in `prepare-a-case` logic or data requirements won't necessitate modifications to `court-case-service`, and vice-versa. This reduces development effort and minimizes the risk of regressions.\n* **Resiliency:** While `prepare-a-case` needs to handle partial failures gracefully, this is best achieved through robust error handling within `prepare-a-case` itself, rather than by tightly coupling it to a dedicated gateway. This promotes a more resilient architecture where each service takes responsibility for its own failure modes. \n\n**Implementation Considerations:**\n\n* **Clearly define API contracts:** A well-defined and documented API contract (e.g., using OpenAPI\/Swagger) ensures consistency and facilitates independent evolution of both consumer and provider. \n* **Versioning strategy:** Implement a robust API versioning strategy (e.g., semantic versioning) to manage future changes and ensure backward compatibility for existing consumers.\n* **Error handling & documentation:** Provide comprehensive error responses with clear semantics, enabling `prepare-a-case` and other consumers to implement appropriate error handling and graceful degradation strategies. \n","GenTime":"2024-08-28 12:04:19"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"text":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n\n\n##Decision\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","MatchedIds":[4556,3203,4539,536,1193],"Prediction":"## Decision\n\nWe will use a **decorator** to implement the `Model` functionality. \n\nThis decision prioritizes **ease of use** while maintaining **clarity of implementation**.\n\n### Justification\n\n1. **Ease of use:** Decorators provide a concise and Pythonic way to add functionality to functions without modifying their core logic. This reduces boilerplate code compared to a class-based approach.  The example function, decorated, could look something like:\n\n```python\nfrom skypy.model import model\n\n@model(outputs='redshift')\ndef redshift_gamma(shape, scale, size):\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\nSample `size` redshifts from a gamma distribution with the\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\n\"\"\"\n# redshift distribution\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\nreturn redshift\n```\n\n2. **Clarity of implementation:**  While decorators can introduce complexity if overused, a well-structured decorator for the `Model` can remain readable and understandable. The core logic of the decorator will involve inspecting the function's input arguments and output, determining its dependencies in the DAG, and managing the creation\/population of the relevant table columns. This can be encapsulated within the decorator, keeping the user-facing function clean and focused on the scientific computation.\n\n### Additional Considerations:\n\n* **Flexibility:** Decorators can be easily extended using arguments to accommodate variations in model behavior (e.g., specifying input columns, output table names).\n* **Testability:** Decorators can be unit tested independently to ensure they correctly handle DAG construction and data management, while the decorated functions can be tested for their core scientific logic.\n\nBy striking a balance between ease of use and clarity, the decorator-based approach empowers users to define their simulation models concisely while providing a robust framework for DAG construction and execution. \n","GenTime":"2024-08-28 12:04:29"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"text":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n\n\n##Decision\nDelivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","MatchedIds":[2526,1523,2678,2791,307],"Prediction":"## Decision \n\nTo handle \"Ready to Pay\" orders and ensure \"at least once delivery\" while preventing duplicate payments, we will implement a system using **idempotency keys** and a **queue-based processing system**. \n\n### Approach\n\n1. **Idempotent Orders:** Each \"Ready to Pay\" order will be assigned a unique, immutable idempotency key. This key can be generated using a universally unique identifier (UUID) and will be used for all subsequent payment processing attempts for that order.\n\n2. **Queueing System:** Instead of directly processing payments, \"Ready to Pay\" orders will be placed in a dedicated queue. This queue ensures that orders are processed sequentially and reliably, even in the face of failures.\n\n3. **Idempotency Check:** Before processing a payment, the payment processor will check if an order with the same idempotency key has already been successfully processed. \n    * If an order with the same key is found, the processor will recognize it as a duplicate attempt and will not attempt to process the payment again. Instead, it will return the successful response from the previous attempt. \n    * If no previous payment attempt is found, the processor will proceed with processing the payment.\n\n4. **Guaranteed Delivery:** The queueing system will guarantee that the order is delivered to the payment processor at least once. If the initial attempt fails, the message will be retried automatically.\n\n### Positive Consequences\n\n* **Prevents Double Payments:**  Idempotency keys provide a robust mechanism for preventing double payments, even with retries and failures.\n* **Guaranteed Processing:**  The queue-based system ensures that every \"Ready to Pay\" order will be processed at least once.\n* **Fault Tolerance:**  The system is resilient to failures. If a payment attempt fails, the order will be retried, and the idempotency key will prevent duplicate payments.\n\n### Negative Consequences\n\n* **Increased Complexity:**  Implementing idempotency keys and a queue-based system adds complexity to the architecture.\n* **Potential for Delays:**  Queue-based processing may introduce slight delays in payment processing, especially during periods of high traffic.\n\n### Overall\n\nThis approach prioritizes the critical business need of reliable payment processing while guaranteeing protection against double payments. The added complexity is a necessary trade-off for achieving these essential requirements. \n","GenTime":"2024-08-28 12:04:41"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"text":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n\n##Decision\n* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","MatchedIds":[1279,1463,3572,3691,2626],"Prediction":"## Decision\n\nWe will investigate techniques to optimize build times, including:\n\n* **Plugin optimization:**\n    * Identify and disable unused plugins in `settings.gradle.kts` and individual module `build.gradle.kts` files.\n    * Explore plugin alternatives with improved performance for our use cases.\n* **Modularization review:**\n    * Analyze the module structure to identify potential areas for consolidation or restructuring to reduce inter-module dependencies and improve build parallelism. \n* **Configuration caching:** \n    * Enable and configure Gradle's configuration caching to avoid unnecessary configuration phases for unchanged projects.\n* **Build performance profiling:**\n    * Utilize Gradle Profiler and other profiling tools to pinpoint performance bottlenecks and target optimization efforts effectively. \n* **Evaluate build farm:**\n    * Investigate implementing a build farm to parallelize builds and potentially leverage more powerful hardware for faster build times. \n\nWe will prioritize these actions based on their potential impact and feasibility within our project.\n","GenTime":"2024-08-28 12:04:47"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"text":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n\n\n##Decision\nWhile implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","MatchedIds":[4843,4848,4454,581,4436],"Prediction":"## Decision\n\nWe will refactor the `verify-front-journey-hint` cookie to separate its original purpose (non-repudiation) from the sign-in hint functionality. This is important for the following reasons:\n\n* **Clearer Purpose and Usage:** Having a dedicated cookie for each feature enhances clarity and makes it easier to understand their specific roles and data. This separation simplifies maintenance and debugging.\n* **Improved Security:** Combining different functionalities in a single cookie can inadvertently create security vulnerabilities. Separation minimizes potential attack vectors and limits the impact if one cookie is compromised.\n* **Data Minimization:** Each cookie should only store information necessary for its specific purpose. This approach aligns with data privacy best practices and reduces the overall amount of sensitive data stored.\n\nWe will implement the following changes:\n\n1. **Create a New Cookie:** Introduce a dedicated cookie for the sign-in hint functionality. This cookie will store a list of recently used IDPs (entity IDs) and their associated timestamps.\n2. **Refactor Existing Cookie:** The `verify-front-journey-hint` cookie will revert to its original purpose and only store the `entity_id` for non-repudiation. \n3. **Adjust Logic:** Update the application logic to utilize the new sign-in hint cookie for displaying user hints and the original cookie for non-repudiation flows.\n4. **Review Expiry Times:**  Re-evaluate the expiry times for both cookies to ensure they are appropriate for their specific functionalities and adhere to data retention policies.\n\nThis separation ensures a cleaner, more secure, and maintainable solution for both the non-repudiation journey and sign-in hint features. \n","GenTime":"2024-08-28 12:04:55"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"text":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n\n\n##Decision\n* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","MatchedIds":[78,4802,1048,70,3374],"Prediction":"## Decision\n\nThe Dapr Java SDK will target **Java 8** as the minimum supported version. \n\nWhile Java 11 offers advantages as the latest LTS release, prioritizing compatibility with the broader Java ecosystem using Java 8 is essential for adoption.\n\nThis decision considers:\n\n* **Java 8 prevalence:**  A significant portion of Java projects still rely on Java 8 in 2019. Supporting Java 8 ensures wider accessibility and reduces friction for developers integrating Dapr.\n* **Future migration:** Dapr can re-evaluate Java version support as the ecosystem shifts, potentially making Java 11 the baseline in the future.\n* **Balancing innovation and adoption:**  Supporting Java 8 allows the Dapr Java SDK to be readily usable by a larger audience while still taking advantage of newer Java features when appropriate. \n\nWe will actively monitor Java community trends and re-evaluate this decision as needed to ensure the Dapr Java SDK remains relevant and performant. \n","GenTime":"2024-08-28 12:05:02"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"text":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n\n\n##Decision\nWe will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","MatchedIds":[4868,1758,4896,1575,1545],"Prediction":"## Decision\n**We will use a lightweight message broker like RabbitMQ combined with a simple framework like Spring Boot.** \n\nThis offers several advantages:\n\n* **Easy to Learn & Use**: Both RabbitMQ and Spring Boot are known for their ease of use and have excellent documentation. \n* **Fast Local Setup**: Docker images for RabbitMQ and simple Spring Boot project setup make getting started locally extremely fast. \n* **Clearly Demonstrates Distributed Concepts**: Using a message broker exposes the realities of distributed systems, like eventual consistency, in a more apparent way than traditional architectures. \n* **Illustrates Data Flexibility**:  By decoupling systems with messages, we can show how data can be consumed and transformed by multiple services for different purposes.\n\n**Trade-offs:**\n\n* This setup might be overly simplistic for complex production scenarios. \n* Additional tooling might be needed for message schema evolution and monitoring. \n\n**However, for the purpose of providing a low barrier to entry and demonstrating the core concepts of an alternative architecture, this approach provides a good balance.** \n","GenTime":"2024-08-28 12:05:09"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"text":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n\n##Decision\n[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","MatchedIds":[2397,4857,2041,61,2539],"Prediction":"## Decision\n\nWe will use Auth0 for authentication and authorization of Link Platform Administrators. \n\n### Reasoning\n\n* **Rapid Implementation:** Auth0 offers a quick and easy integration path, enabling us to get the administrator authentication and authorization up and running quickly. \n* **Robust Security:**  Auth0 provides industry-standard security features, including multi-factor authentication (MFA), to protect administrator accounts and, by extension, the Link Instances themselves.\n* **Flexibility and Scalability:**  Auth0 can adapt to our future needs, allowing us to easily integrate with other identity providers or implement more fine-grained authorization policies as the Link Platform grows.\n\n### Implementation Details\n\n* We will leverage Auth0's user management features to create and manage administrator accounts.\n*  Each Link Instance will be configured to use Auth0 for authentication.\n* We will define roles and permissions within Auth0 to control administrator access to specific data and configuration options within a Link Instance.\n\n### Alternatives Considered\n\n* **Custom Solution:** Building a custom authentication and authorization system would offer maximum control but would also require significant development time and ongoing maintenance. \n* **Alternative Identity Provider:**  While other identity providers exist, Auth0 provides a good balance of ease of use, features, and security.\n\n### Potential Concerns\n\n* **Vendor Lock-in:**  Relying on a third-party provider like Auth0 introduces a degree of vendor lock-in.\n* **Cost:**  While Auth0 has a free tier, utilizing more advanced features or scaling to a larger number of administrators may require a paid subscription.\n\n### Mitigation Strategies\n\n* **Abstraction Layer:** We will implement an abstraction layer between the Link Platform and Auth0 to minimize vendor lock-in and allow for easier migration to a different solution in the future.\n* **Cost Monitoring:**  We will closely monitor Auth0 usage and cost to ensure it remains within acceptable limits. \n","GenTime":"2024-08-28 12:05:19"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"text":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n\n\n##Decision\nWe will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","MatchedIds":[2236,2249,856,2243,3915],"Prediction":"## Decision\n\nGiven the constraints around Istio upgrades and the cf-for-k8s operator experience, we will leverage Kubernetes' built-in mechanisms for managing deployments and updates. Specifically:\n\n1. **Utilize `kpack` for application deployments:** `kpack` allows building container images directly within the Kubernetes cluster. This enables us to automatically rebuild and redeploy applications with the latest compatible Istio sidecar whenever a new cf-for-k8s version, including an Istio upgrade, is deployed.\n2. **Implement rolling updates with `Deployment` objects:** By utilizing Kubernetes `Deployment` objects and defining update strategies (e.g., rolling update), we can ensure a controlled rollout of applications with new sidecars. This minimizes downtime and allows for easy rollback if issues arise.\n3. **Leverage PodDisruptionBudgets (PDBs):** PDBs can be used to ensure that a minimum number of application instances are always available during the rolling update process, maintaining service availability.\n\nThis approach removes the need for manual post-install actions or scripting while adhering to best practices for Istio upgrades. Users will experience a single `kapp deploy` step for upgrading cf-for-k8s, while the system handles the complexities of sidecar injection and application restarts seamlessly in the background. \n\nThis decision prioritizes automation and a streamlined operator experience while ensuring compatibility with updated Istio versions. It leverages existing Kubernetes features to provide a robust and maintainable solution for managing Istio sidecar upgrades in a cf-for-k8s environment. \n","GenTime":"2024-08-28 12:05:28"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"text":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n\n\n##Decision\nWe will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","MatchedIds":[4179,1258,4223,3323,4359],"Prediction":"## Decision\n\nWe will use `react-testing-library` for our integration tests, focusing on testing user interactions and component behavior. Instead of asserting the exact DOM structure, we will primarily verify the expected state changes or model updates in response to user actions. This approach minimizes testing implementation details and ensures our tests remain resilient to UI changes, as long as the underlying functionality remains consistent. \n","GenTime":"2024-08-28 12:05:31"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"text":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n\n\n##Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","MatchedIds":[875,4539,8,3653,3657],"Prediction":"## Decision\n\nGiven the user need for features present in the HEAD of the ODH main branch, we will update the relevant components accordingly. \n\nTo mitigate the operational complexity increase from diverging from the v0.8.0 release:\n\n* We will document the delta between ODH v0.8.0 and the deployed commit hashes for all affected components. \n* We will prioritize testing and issue reporting for the updated components in the Thoth Station environment. \n* We will evaluate the feasibility of re-synchronizing with a future ODH release once it includes the required Elyra features. \n\nThis decision prioritizes user needs while acknowledging and mitigating the potential risks associated with running a partially non-release version of ODH. \n","GenTime":"2024-08-28 12:05:35"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"text":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n\n\n##Decision\nReading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","MatchedIds":[2466,3096,3127,1927,1558],"Prediction":"## Decision\n\nThe `loadStream()` method will return a `TransactionAwareStream` object instead of a standard stream object. This specialized stream will have the following properties:\n\n1. **Bound to the Transaction:** The `TransactionAwareStream` will be internally associated with the transaction from which it was loaded. Any attempt to perform operations on the stream outside the boundaries of this transaction will result in an error.\n\n2. **Automatic Closure:** The `TransactionAwareStream` will be automatically closed when its associated transaction commits or rolls back. This eliminates the risk of resource leaks, especially in scenarios with retries where multiple streams might be opened.\n\n3. **Transactional Reads:**  Reading data from the `TransactionAwareStream` will be considered part of the transaction itself. This ensures that reads are consistent with the transactional view and any failures during the read process will cause the entire transaction to fail.\n\n4. **Optional: Lazy Loading:** To optimize for performance, we could consider implementing the actual loading and reading of the stream data lazily. This means data would only be fetched from the underlying storage when it's explicitly read from the `TransactionAwareStream`, potentially reducing overhead if only parts of the stream are consumed within the transaction.\n\n**Consequences:**\n\n* **Improved Resource Management:**  Automatic closure eliminates the possibility of leaking streams in the case of retries or errors.\n* **Enhanced Transactional Consistency:**  Reading streams within the transaction guarantees data consistency and atomicity with other transactional operations.\n* **Potential Performance Overhead:**  Depending on the implementation of `TransactionAwareStream`, there might be a slight performance impact due to the added transactional logic. However, this can be mitigated with optimizations like lazy loading.\n\nThis approach addresses the issues of resource leaks and transactional consistency by tightly coupling the stream lifecycle to the transaction. \n","GenTime":"2024-08-28 12:05:44"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"text":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n\n##Decision\nUse TypeScript for static typing.\\n","MatchedIds":[2290,536,1410,3827,2687],"Prediction":"## Decision\nWe will use TypeScript for static type checking. While Flow offers a potentially easier initial integration with our existing JavaScript codebase, TypeScript provides a more robust, developer-friendly, and mature ecosystem. \n\nThe benefits we gain from TypeScript, such as improved type accuracy, better error messaging, and a richer ecosystem, outweigh the initial effort required for migrating our codebase.\n","GenTime":"2024-08-28 12:05:47"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"text":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n\n\n##Decision\nWhilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","MatchedIds":[923,1464,3147,3914,2183],"Prediction":"## Decision\n\nWe will not pursue making Cloud Security Watch cloud agnostic. The tool's value proposition is deeply rooted in identifying and addressing AWS misconfigurations. Expanding its scope to other cloud providers would significantly increase complexity and potentially dilute its effectiveness in its primary function. We will instead focus on making Cloud Security Watch the most comprehensive and insightful AWS security monitoring tool available. \n","GenTime":"2024-08-28 12:05:50"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"text":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[4223,3323,2273,4359,4785],"Prediction":"## Decision\n\nWe will use Jest as our testing framework. It has excellent support for TypeScript and Node, and its mocking capabilities will allow us to write efficient and reliable tests.\n","GenTime":"2024-08-28 12:05:52"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"text":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n\n\n##Decision\nThis ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","MatchedIds":[138,1216,4167,5244,2261],"Prediction":"## Decision\n\nWe will refactor Content Publisher to use a new domain model based around the concept of an **Edition**. \n\n### Edition-based Domain Model\n\nThe key principles of this approach are:\n\n* **Editions are immutable:** Once an edition is created, its content (title, body, images, etc.) cannot be changed. This ensures that historical versions are always preserved and accurately reflect what was published or worked on.\n* **Documents have multiple editions:** A document can have many editions, representing its evolution over time. Each edition can be uniquely identified and retrieved. \n* **TimelineEntries track actions on editions:** TimelineEntries will be refocused to accurately capture user actions performed on specific editions, rather than storing state.\n\n###  Benefits of Edition-Based Model\n\n* **Solves Pain Points:**\n    * Users can discard drafts without affecting published editions.\n    * Accurate links and statuses can be maintained for live editions.\n    * Images can be managed independently for each edition.\n    * TimelineEntries become more focused and flexible.\n* **Enables Intended Features:**\n    * Edition comparison becomes possible.\n    * Republishing previous editions becomes straightforward.\n    * Showing changes between editions becomes possible.\n\n### Implementation Details\n\n* The current `documents` table will be replaced with an `editions` table.\n* The `images` table will be refactored to associate images with specific editions.\n* Relationships between documents and other entities (like timelines) will be reviewed and potentially refactored to work with the edition-based model. \n* Papertrail can still be used for tracking low-level changes, but the application logic will primarily rely on the edition history. \n\n### Positive Consequences\n\n* **Improved data integrity and versioning:** Provides a robust and reliable way to manage document history.\n* **Enhanced user experience:** Enables features that improve user workflows and understanding of document history. \n* **Future-proof architecture:** This model is more scalable and adaptable to new features and requirements. \n\n### Negative Consequences\n\n* **Increased complexity:** The new model introduces additional complexity that needs to be managed. \n* **Migration effort:** Migrating existing data to the new model will require careful planning and execution.\n\n### Follow-up Decisions Required\n\n* **Migration strategy:**  A detailed plan for migrating existing data to the new model needs to be defined. \n* **Impact on other parts of the system:** The impact of this change on other parts of the system that interact with Content Publisher needs to be assessed. \n* **UI\/UX considerations:** The user interface and user experience will need to be updated to accommodate the new model and features. \n","GenTime":"2024-08-28 12:06:05"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"text":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n\n##Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","MatchedIds":[729,771,216,3657,2093],"Prediction":"## Decision\n\nWe have decided to make the Datalabs network accessible via a VPN connection. This will allow authorised users to connect to the network securely and access the servers, without exposing any services publicly. We will use JASMIN's existing VPN infrastructure to provide this access. \n\n## Status\n\nAccepted\n\n## Consequences\n\n* Only users with valid VPN credentials will be able to access the Datalabs network. \n* This approach does not require any additional public IP addresses or firewall rules. \n* Users will need to connect to the VPN before they can access Datalabs resources. \n","GenTime":"2024-08-28 12:06:10"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"text":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n\n\n##Decision\n### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","MatchedIds":[2471,187,4025,3147,4358],"Prediction":"## Decision\n\nWe will implement ephemeral environments for pull requests (PRs) in the Development AWS account. These environments will be created on demand for each PR and automatically destroyed after evaluation is complete or after a predetermined period.\n\nHere\u2019s how we will implement it:\n\n1. **Infrastructure as Code:** We will use Terraform to define all the infrastructure needed for a PR environment (e.g., ECS tasks, load balancers, S3 buckets).\n2. **Automated Provisioning:** A CI\/CD pipeline will be triggered on every PR creation. This pipeline will execute the Terraform code to provision the environment dynamically. The environment will be configured with a unique identifier (e.g., PR number).\n3. **Environment Variables:** Environment-specific configurations like database connection strings will be injected as environment variables during deployment.\n4. **Automated Testing:** The CI\/CD pipeline will run automated tests (unit, integration, and end-to-end) against the PR environment.\n5. **User Acceptance Testing (UAT):** A dedicated subdomain or URL path can be used to access the PR environment, facilitating user acceptance testing.\n6. **Log Aggregation and Artifact Retention:**  We will configure centralized logging to collect logs from all PR environments. Crucial logs and artifacts will be persisted in a central location (e.g., S3 bucket) with appropriate tagging for traceability and debugging even after environment teardown.\n7. **Automated Teardown:** Once the evaluation is complete (e.g., PR is merged or closed), the CI\/CD pipeline will automatically trigger the Terraform code to destroy the environment, freeing up resources. Alternatively, environments can be automatically destroyed after a specific time to manage costs.\n\nThis approach offers several benefits:\n\n* **Faster Feedback Loops:**  Provides developers with quicker feedback on their changes.\n* **Improved Collaboration:** Facilitates collaboration during code reviews by offering a live environment to test and review features.\n* **Reduced Risk:** Isolates changes in separate environments, reducing the risk of impacting other development work.\n* **Cost Optimization:**  Ephemeral environments ensure resources are only used when necessary.\n\nThis decision requires us to define a clear process for managing PR environments, including access control, naming conventions, and cost optimization strategies.\n","GenTime":"2024-08-28 12:06:22"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"text":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n\n##Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n","MatchedIds":[2140,2121,2133,1306,2897],"Prediction":"## Decision\n\n* Introduce a distributed `TaskManager` using a shared database (e.g., Cassandra) to coordinate tasks across the cluster.\n\n* **Task Persistence:**\n    * Store `Task` objects in the database, including their status, details, and the node they are assigned to.\n\n* **Task Scheduling and Distribution:**\n    * Design a mechanism (e.g., a scheduler service) that assigns new tasks to nodes based on load and availability. This service would interact with the shared database to track task assignments and node status.\n\n* **Task Execution and Monitoring:**\n    * Update the `TaskManager` implementation on each node to retrieve assigned tasks from the database.\n    * Implement a heartbeat mechanism between nodes and the database to track node availability and task progress.\n\n* **API Changes:**\n    * Extend the existing Task Management API to allow querying tasks across the cluster.\n    * Provide new endpoints to retrieve task details and status from any node in the cluster.\n\n* **Considerations:**\n    * Implement proper error handling and retry logic to handle node failures and network partitions.\n    * Consider using a message queue (e.g., Kafka, RabbitMQ) to handle task distribution and status updates asynchronously.\n    * Design the system to be scalable, ensuring it can handle an increasing number of tasks and nodes. \n","GenTime":"2024-08-28 12:06:29"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"text":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n\n##Decision\nUpdate the flist with the up to date one\\n","MatchedIds":[187,351,1739,4868,3328],"Prediction":"## Decision\n\nWe will leverage the pre-built flist images from CI to optimize the deployment process for hosted 3Bot instances. \n\nInstead of cloning and building the entire project during deployment, we will:\n\n1. **Configure the CI pipeline to generate and publish a new flist image upon successful builds.** This image will contain all the necessary dependencies and configurations, ensuring it remains up-to-date.\n\n2. **Modify the deployment process to directly utilize the latest pre-built flist image.** This eliminates the cloning and installation steps, significantly reducing the overall deployment time.\n\n3. **Implement versioning for the flist images.** This allows for rollback capabilities and ensures deployments use the desired version of the 3Bot. \n\nBy adopting this approach, we leverage the CI's capabilities to streamline deployments, reduce overhead, and maintain consistency across hosted 3Bot instances. \n","GenTime":"2024-08-28 12:06:38"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"text":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n\n##Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","MatchedIds":[2122,2892,2896,2145,2888],"Prediction":"## Decision\nImplement a background task that periodically cleans up **enqueuedMailsV3** and **deletedMailsV2** as well as the corresponding blobs in Object Storage.\n\nHere's a breakdown of the solution:\n\n* **Schedule**: The task will run periodically, configurable via James configuration. The frequency will be a trade-off between storage cost optimization and potential performance impact.\n* **Cleanup Logic**:\n    * **Identify eligible entries**: The task will leverage the `browseStart` value to identify slices in **enqueuedMailsV3** older than a configurable retention period. All entries within these slices will be considered for deletion.\n    * **Check deletion status**: For each eligible entry in **enqueuedMailsV3**, the task will check **deletedMailsV2** to confirm if the corresponding mail has been deleted.\n    * **Delete entries and blobs**:  If a mail is marked as deleted in **deletedMailsV2**, the corresponding entries in both **enqueuedMailsV3** and **deletedMailsV2** will be deleted. The associated blob in Object Storage will also be deleted using the information stored in the deleted **enqueuedMailsV3** entry. \n* **Error Handling**: The task will implement robust error handling and logging. Failed deletions will be retried at a later time.\n* **Monitoring**: The task will expose metrics about its execution and impact, such as the number of entries and blobs deleted, execution time, and any encountered errors.\n\n**Benefits**:\n\n* **Reduced storage costs**: By deleting obsolete mail data, we reduce the storage footprint in both Cassandra and Object Storage.\n* **Improved privacy**: Deleting old mail data aligns with privacy regulations and best practices. \n* **Optimized performance**: Removing obsolete data can improve the performance of other mail queue operations, such as browsing and searching. \n\n**Considerations**:\n\n* **Retention period**: The configurable retention period should be chosen carefully, balancing legal requirements, user expectations, and storage costs. \n* **Impact on Cassandra compaction**: Deleting large amounts of data can trigger more frequent compactions in Cassandra, potentially impacting performance. Monitoring compaction activity after implementing this solution is crucial. \n* **Object Storage deletion costs**:  Some Object Storage providers might charge for deleting large amounts of data. This cost should be factored into the overall cost-benefit analysis. \n","GenTime":"2024-08-28 12:06:51"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"text":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n\n\n##Decision\n[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","MatchedIds":[5087,241,350,3827,1576],"Prediction":"## Decision\nMigrate from Vundle to [vim-plug](https:\/\/github.com\/junegunn\/vim-plug) for managing Vim plugins. This decision is based on vim-plug's active development, positive community feedback, and its feature set.\n","GenTime":"2024-08-28 12:06:53"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"text":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n\n##Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","MatchedIds":[3144,328,2536,3147,2793],"Prediction":"## Decision\n\nTo mitigate the risk of outdated modules and simplify future upgrades, we will implement a policy of **strict versioning and timely upgrades for all shared Terraform modules**. This includes:\n\n1. **Versioning:** All shared Terraform modules will be versioned using [semantic versioning](https:\/\/semver.org\/). We will establish a clear and consistent versioning policy across all modules.\n2. **Centralized Repository:** All shared modules will be stored in a centralized repository for easier access, maintenance, and discoverability.\n3. **Automated Testing:**  Each new module version will undergo automated testing using tools like [Terratest](https:\/\/terratest.gruntwork.io\/) to ensure backward compatibility and functionality before release.\n4. **Upgrade Cadence:** We will define a clear upgrade cadence (e.g., quarterly) for all namespaces to ensure that they remain up-to-date with the latest module versions. \n5. **Upgrade Communication:**  Clear communication channels will be established to notify teams of upcoming module releases, changes, and upgrade deadlines. This could include release notes, dedicated communication channels, and\/or automated notifications.\n6. **Upgrade Support:**  Offer dedicated support and resources to assist teams during the upgrade process, minimizing disruption and ensuring a smooth transition.\n\nThis strategy promotes consistency, reduces technical debt, and simplifies the management of shared Terraform modules within the organization. \n","GenTime":"2024-08-28 12:07:01"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"text":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n\n\n##Decision\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","MatchedIds":[1792,4699,4428,4408,3119],"Prediction":"## Decision\n\n**Project file structure will follow a modified MVVM approach.**\n\n**Rationale:**\n\nWhile MVVM is a widely accepted pattern in the .NET\/WPF community, strictly adhering to the \"one view, one view model, one model\" rule can lead to unnecessary complexity, especially in UI-heavy applications. \n\n**Structure:**\n\n* **Model:** Contains core business logic, data entities, and data access layer components. \n* **View:** Houses XAML files for UI elements and basic code-behind for view-specific logic. \n* **ViewModel:** Holds presentation logic, commands, and data binding logic for the Views. ViewModels can be granular or consolidated depending on the complexity and shared logic between views.\n* **Shared:** A dedicated folder for common elements like Converters, Custom Controls, and Services used across the application.\n* **Assets:** Contains non-code resources like images, fonts, and resource dictionaries. \n* **Lib:** Contains external libraries and dependencies. \n\n**Guidelines:**\n\n* Aim for a pragmatic application of MVVM, prioritizing maintainability and clarity over strict adherence.\n* Favor consolidated ViewModels for related Views to avoid redundancy and promote code reuse.\n* Clearly document any deviations from the standard MVVM structure and the rationale behind them.\n\n**Consequences:**\n\n* **Positive:** Improved code organization, maintainability, and testability. Leverages community knowledge and resources for MVVM.\n* **Potential Challenges:** Requires understanding the balance between MVVM principles and project-specific needs. Proper documentation is crucial to explain structural choices. \n\nThis modified approach aims to harness the benefits of MVVM while providing flexibility and addressing concerns about potential over-engineering in specific scenarios. \n","GenTime":"2024-08-28 12:07:10"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"text":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n\n\n##Decision\n- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","MatchedIds":[389,4644,3776,3657,3053],"Prediction":"## Decision\n\nChosen Option: **Consolidate database access to backend services and deprecate the separate DB API.**\n\n### Reasoning\n\nThe original rationale for separating the database API into a distinct layer with restricted permissions is no longer relevant or important.  The current setup introduces unnecessary complexity with a dedicated service that provides no significant security or performance benefits.  \n\nThis decision aligns with the following drivers:\n\n* **Reduce complexity:** Removing the separate DB API simplifies the architecture, streamlines development, and reduces operational overhead. \n* **Improve performance:** While not a primary driver, direct database access from backend services can potentially improve performance by removing an extra network hop.\n* **Share code:**  Utilizing shared libraries like `knex` in `fxa-shared` promotes consistency and reduces code duplication.\n\n### Implementation\n\n1. **Migrate auth-server to use direct database access via knex in fxa-shared.** \n2. **Deprecate the existing DB API service.** This includes updating documentation and routing traffic away.\n3. **Monitor performance and security** after the migration to ensure no regressions. \n\n### Potential Concerns \n\n* **Security:** Ensure appropriate database access controls are in place for each backend service. This might require auditing and adjusting database user roles and permissions.\n* **Future Scalability:** While not a concern currently, consider if future requirements might necessitate a separate database layer for performance or security reasons. \n\n### Conclusion \n\nBy consolidating database access to backend services, we can simplify the FxA architecture, reduce code duplication, and potentially improve performance without compromising security. This decision aligns with the principle of keeping the architecture as simple as possible while meeting current needs. \n","GenTime":"2024-08-28 12:07:19"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"text":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n\n\n##Decision\n1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","MatchedIds":[1652,3657,1758,4859,1462],"Prediction":"## Decision\n\nGiven our move to a Python\/React stack and the need to modernize our database interaction, we will adopt the following approach:\n\n**1. Migrate from Phinx to Alembic:** \n\nWe will transition from Phinx to [Alembic](https:\/\/alembic.sqlalchemy.org\/) for database migrations. Alembic is a robust, Python-native migration tool that integrates well with SQLAlchemy, our chosen ORM. This shift aligns with our broader technology direction and leverages the strengths of the Python ecosystem.\n\n**2. Leverage SQLAlchemy as the ORM:**\n\nFor interacting with the database, we will utilize [SQLAlchemy](https:\/\/www.sqlalchemy.org\/). SQLAlchemy offers a powerful and flexible ORM, allowing us to write Pythonic code that interacts with the database without relying on raw SQL strings. This abstraction improves code readability, maintainability, and security.\n\n**3. Implement a Data Access Layer (DAL):**\n\nTo further enhance modularity and maintainability, we will implement a Data Access Layer (DAL) between the application logic and the ORM.  The DAL will encapsulate database operations, providing a clean separation of concerns and making it easier to refactor or swap out database technologies in the future if needed.\n\n**4. Integrate with GraphQL (if applicable):**\n\nIf a GraphQL API is being used, we will integrate SQLAlchemy models into the GraphQL schema. This can be achieved using libraries such as [Graphene](https:\/\/graphene-python.org\/) or [Strawberry](https:\/\/strawberry.graphql.io\/). This approach provides a consistent and type-safe way to expose data from the database to the frontend application.\n\n**5. Prioritize Parameterized Queries:**\n\nTo prevent SQL injection vulnerabilities, all database queries will utilize parameterized queries offered by SQLAlchemy. This approach ensures that user-supplied data is handled safely and does not compromise the integrity of the database.\n\nBy adopting these measures, we gain the following benefits:\n\n* **Improved Developer Experience:** SQLAlchemy offers a more intuitive and Pythonic way to interact with the database compared to raw SQL strings.\n* **Enhanced Maintainability:**  A well-defined DAL promotes code organization and separation of concerns, making the codebase easier to understand and maintain.\n* **Increased Security:**  Utilizing parameterized queries significantly reduces the risk of SQL injection attacks.\n* **Future Flexibility:**  Choosing well-established and widely supported tools like SQLAlchemy and Alembic provides flexibility for future development and potential database migrations. \n","GenTime":"2024-08-28 12:07:32"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"text":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n\n\n##Decision\nModel the links as `<button>` elements to make the interaction more normal.\\n","MatchedIds":[4323,1855,2359,2274,2074],"Prediction":"## Decision\n\nUse `<button>` elements for bookmarks, styled to appear as links. \n\nWhile it's true that anchors (`<a>`) are the semantically correct element for navigating to web pages, the requirement for tab navigation and Enter key activation conflicts with standard anchor behavior.  \n\nThis approach prioritizes accessibility and user experience. We retain the visual indication of a link while enabling expected keyboard navigation.  \n","GenTime":"2024-08-28 12:07:35"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"text":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n\n\n##Decision\nComplex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","MatchedIds":[2634,1317,2631,2637,3571],"Prediction":"## Decision\nWe will write helper functions for complex scratch\/ftduino functions that take complex arguments or have complex implementations. \n\nThis approach favors maintainability and code reuse over potential (minor) performance gains of direct code generation. \n","GenTime":"2024-08-28 12:07:43"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"text":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n\n\n##Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","MatchedIds":[1279,1631,5259,3827,2290],"Prediction":"## Decision\nUse `module.exports` for all plugin exports to maintain consistency and compatibility with HapiJS's CommonJS module system. \n","GenTime":"2024-08-28 12:07:45"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"text":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n\n\n##Decision\nSince every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","MatchedIds":[2181,2502,1106,1180,2963],"Prediction":"## Decision\n\nWe will use [Apache Avro](https:\/\/avro.apache.org\/) for defining data schemas for our topics. \n\n### Justification\n\n* **Schema evolution and compatibility:** Avro provides robust support for schema evolution, allowing us to make changes to our schemas over time while maintaining backward and forward compatibility. This is essential for evolving data streams without disrupting existing consumers.\n* **Data serialization:** Avro offers a compact binary serialization format, which reduces data size and improves performance for both producers and consumers.\n* **Confluent Schema Registry integration:** Avro is tightly integrated with Confluent Schema Registry, providing centralized schema management, validation, and versioning.\n* **Widely adopted and supported:** Avro is a mature and widely adopted schema definition language, with support in various programming languages and data processing frameworks.\n\n### Implementation Details\n\n* **Schema definition:** Schemas will be defined using Avro's JSON schema definition language.\n* **Schema storage and retrieval:** We will utilize Confluent Schema Registry for storing, retrieving, and managing the Avro schemas associated with each topic.\n* **Client libraries:** Producers and consumers will use Avro client libraries to serialize and deserialize messages according to the defined schemas.\n* **Schema evolution strategy:** We will follow Avro's schema evolution rules to ensure compatibility between different versions of our schemas.\n* **Validation:**  All messages published to a topic will be validated against the corresponding Avro schema to ensure data integrity.\n\nThis approach ensures data consistency, promotes interoperability, and simplifies data management within our messaging infrastructure.\n","GenTime":"2024-08-28 12:07:54"}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"text":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n\n\n##Decision\nMoving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","MatchedIds":[2703,4179,1258,3521,4338],"Prediction":"## Decision\n\nCo-locate the test files alongside the source files they are testing. \n\n### Reasoning\n\n* **Improved Developer Experience:**  Having tests adjacent to the code they verify reduces friction during development. Switching between files becomes trivial, speeding up both the comprehension and modification of code and tests.\n* **Clearer Relationship:**  Placing tests next to source code reinforces their connection, making it immediately obvious which tests correspond to which modules or functions.\n* **Simplified Organization:** This approach can lead to a flatter, more intuitive project structure, particularly as the codebase grows.  \n\n### Implementation Details\n\n* **Naming Convention:** Adopt a consistent naming scheme for test files that clearly links them to their counterparts (e.g., `myComponent.ts` and `myComponent.test.ts`).\n* **IDE Support:** Most modern IDEs and code editors offer features to facilitate navigation between source and test files, further streamlining the workflow.\n\n### Potential Concerns\n\n* **Project Structure:**  In very large projects, a purely co-located approach could lead to a cluttered directory structure. Evaluate whether subdirectories within a feature or component folder would be beneficial. \n* **Build Tools:** Ensure your build process and testing framework support this structure and can correctly identify and execute tests. \n","GenTime":"2024-08-28 12:08:00"}
{"File Name":"insight\/0001-python.md","Context":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"text":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n\n##Decision\nWe choose Python as our base programming language.\\n","MatchedIds":[472,4271,3715,1000,1472],"Prediction":"## Decision\n\nWe will use **Python** as the base programming language for this project. \n\nThis decision is based on the following factors:\n\n* **Popularity and Community Support:** Python boasts a large and active community, providing access to extensive learning resources, libraries, and frameworks. This widespread adoption lowers the barrier to entry for new developers and fosters a collaborative development environment.\n* **Readability and Maintainability:** Python prioritizes code readability, making it easier to understand and maintain, especially in collaborative settings and for long-term projects. This reduces the potential for errors and simplifies future development efforts.\n* **Versatility and Rich Ecosystem:** Python's extensive ecosystem offers numerous libraries and frameworks well-suited for diverse tasks, including web development, data science, machine learning, scripting, and automation. This versatility allows the language to adapt to various project needs.\n* **Strong Industry Adoption:** Python has seen significant adoption across various industries, solidifying its place as a reliable and future-proof choice for software development.\n\nWhile Python will serve as the primary language, we recognize that specific functionalities may benefit from the unique strengths of other languages.  Therefore, the use of alternative languages will be evaluated on a case-by-case basis, requiring clear justification and documentation through Architectural Decision Records (ADRs) to ensure informed decision-making and maintainability. \n","GenTime":"2024-08-28 12:08:07"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"text":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n\n\n##Decision\nWe will use the system adr-tools to manage the ADR documents\\n","MatchedIds":[2914,3568,1614,2261,2091],"Prediction":"## Decision\n\nUse `adr-tools` to manage the Architectural Decision Records for the project. The tool, installed via Homebrew, will help standardize and automate the process of creating, updating, and maintaining ADRs. Using a dedicated tool encourages consistency in format and simplifies record keeping, which will improve the project's maintainability and knowledge sharing. \n","GenTime":"2024-08-28 12:08:10"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"text":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n\n\n##Decision\n* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","MatchedIds":[1035,4285,317,1462,699],"Prediction":"## Decision\n\nWe will migrate away from using `schema.sql` and `data.sql` for database initialization in favor of a database migration tool. This decision is driven by the following factors:\n\n* **Unsustainable in Production:** Dropping and recreating the database on each deployment is not feasible for production environments. It leads to data loss and downtime, which are unacceptable for live systems.\n* **Difficult to Maintain:** As the application grows, managing database changes solely through `schema.sql` becomes increasingly complex and error-prone.\n* **Lack of Version Control:** The current approach doesn't provide a clear history of database changes, making it difficult to track or rollback modifications.\n\nWe will evaluate and choose a suitable database migration tool like **Flyway** or **Liquibase**. This will allow us to:\n\n* **Version Control Database Changes:**  Track each database change as a separate migration script, providing a clear history of modifications.\n* **Enable Incremental Updates:** Apply only the necessary changes to the database schema, avoiding data loss and minimizing downtime.\n* **Improve Collaboration:**  Facilitate collaboration among developers by providing a structured approach to managing database changes.\n\nWe will design a migration strategy to transition from the current setup to the chosen migration tool with minimal impact on the existing application.\n","GenTime":"2024-08-28 12:08:17"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"text":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n\n\n##Decision\nAdd a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","MatchedIds":[2549,3337,3657,2121,1745],"Prediction":"## Decision\n\nModify the osquery enrollment process to include a set of basic host details during the initial `Enroll` call.  This data should be collected on the client-side before enrollment and submitted along with the standard enrollment data. This will ensure the server always has immediate context about the host, even in cases where the host crashes immediately after enrollment or distributed queries are delayed\/fail. \n","GenTime":"2024-08-28 12:08:20"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"text":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n\n##Decision\nUse a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","MatchedIds":[5070,400,1416,1418,3203],"Prediction":"## Decision\n\nWe will implement a CI\/CD pipeline that enforces code quality checks at every stage. This pipeline will include:\n\n* **Automated Tests:** Running our suite of unit, integration, and end-to-end tests on every code change to ensure functionality and catch regressions. \n* **Static Code Analysis:** Integrating tools like SonarQube or a similar platform to automatically analyze code for style violations, security vulnerabilities, and adherence to our defined best practices.\n* **Code Review Process:**  Mandating code reviews for all changes before merging into the main branch.  Reviewers will be responsible for ensuring code readability, maintainability, and alignment with our best practices.\n* **Documentation:**  Requiring clear and concise documentation for all new features and significant code changes. This includes in-code comments, API documentation, and architectural decision records.\n\nThis multifaceted approach will ensure that code quality is a continuous focus throughout the development lifecycle and helps us maintain a high standard for our codebase.\n","GenTime":"2024-08-28 12:08:26"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"text":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n\n\n##Decision\nWe will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","MatchedIds":[3915,3950,2397,631,731],"Prediction":"## Decision\n\nWe will define a standard set of roles to manage access to the Kubernetes cluster, mapping to common responsibilities and access levels. These roles will be applied to users and groups using RoleBindings within the cluster. This approach provides flexibility and clarity in managing permissions, minimizing security risks and ensuring proper segregation of duties. \n\nHere\u2019s a proposed structure:\n\n* **Cluster Roles:** Define cluster-wide permissions for activities like managing namespaces, nodes, and cluster-level resources.\n    * **cluster-admin:** Full access to the cluster. Reserved for highly privileged users and emergency situations.\n    * **cluster-viewer:** Read-only access to all namespaces and resources. Useful for monitoring and auditing.\n* **Namespaced Roles:** Define permissions within specific namespaces.\n    * **namespace-admin:** Full access within the assigned namespace. Suitable for development teams managing their own applications.\n    * **namespace-editor:** Can create, modify, and delete most resources within the namespace but not namespaces themselves. Suitable for developers deploying and managing their applications.\n    * **namespace-viewer:** Read-only access within the assigned namespace. Useful for monitoring and debugging.\n\n**Implementation Details:**\n\n* Roles will be defined using YAML files and stored in a dedicated Git repository for version control and auditing. \n* RoleBindings will be used to grant specific users and groups access to these predefined roles.\n* A process for requesting and approving access to specific roles will be implemented to ensure security and accountability. \n\nThis approach provides a clear and maintainable way to manage access control within the Kubernetes cluster, aligning with the principle of least privilege and supporting secure operations. \n","GenTime":"2024-08-28 12:08:35"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"text":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n\n##Decision\nChosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","MatchedIds":[4738,1193,2498,1883,3187],"Prediction":"## Decision\nChosen option: **The study should remain a pure DTO object.**\n\n### Justification\nKeeping the study as a DTO (Data Transfer Object) promotes better separation of concerns and simplifies data management in the long run. \n\nHere's why:\n\n* **Improved maintainability:** DTOs are simple data containers. Separating data (the study) from the logic of queries and fetchers makes the code cleaner, easier to understand, and less prone to errors.\n* **Increased testability:**  Testing DTOs is straightforward as they primarily involve verifying data integrity. Testing complex logic within the study object would be more challenging.\n* **Flexibility and extensibility:**  If the internal workings of queries or fetchers change, it won't directly impact the study object. You can modify these components independently, improving the system's adaptability to future requirements.\n* **Potential for reuse:** DTOs are easily serializable and transferable across different parts of the application or even different systems. A study DTO could be reused in other contexts without carrying unnecessary dependencies.\n\n### Implementation Details\n\n1. **Maintain the study object as a DTO** containing only data fields related to the study itself (e.g., study ID, title, description, creation date, etc.).\n2. **Create separate classes or modules for Query and Fetcher logic.** These components should be responsible for handling the complexities of constructing queries and interacting with external resources.\n3. **Pass the study DTO as an argument** to the Query and Fetcher instances when needed. This approach clearly defines data flow and dependencies.\n\n### Positive Consequences\n* Cleaner codebase with better separation of concerns.\n* Improved testability and maintainability.\n* Greater flexibility for future changes and enhancements.\n* Potential for code reuse and better system architecture.\n","GenTime":"2024-08-28 12:08:44"}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"text":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n\n\n##Decision\nUnique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","MatchedIds":[1307,856,2793,1372,2256],"Prediction":"## Decision\n\nAdopt a structured approach for organizing Ansible variables to enhance maintainability and reduce merge conflicts:\n\n1. **Group-Specific Variable Files:** Instead of a single, monolithic `vault.yml` file, create separate `vault.yml` files for each logical group of hosts or applications. This allows for better separation of concerns and reduces the likelihood of merge conflicts.\n\n2. **Directory Structure:** Organize variable files within a dedicated directory structure within the Ansible project. This could follow a pattern like `group_vars\/<group_name>\/vault.yml`.\n\n3. **Variable Precedence:** Leverage Ansible's variable precedence rules to define default values in common variable files and override them as needed in group-specific or host-specific files. This promotes reusability and consistency.\n\n4. **Documentation:** Clearly document the purpose and usage of variables within each `vault.yml` file. This improves maintainability and understanding.\n\n5. **Templating (Optional):** For highly complex scenarios, consider using Ansible's templating features (e.g., Jinja2) to dynamically generate variable values based on environment or other factors.\n\nBy implementing these practices, we can achieve:\n\n- **Reduced Merge Conflicts:** Smaller, group-specific variable files decrease the likelihood of overlapping changes.\n- **Improved Organization:** A clear directory structure and consistent naming conventions make it easier to locate and manage variables.\n- **Enhanced Maintainability:** Documentation and a well-defined structure simplify ongoing maintenance and updates. \n- **Increased Reusability:** Default values and a structured approach encourage variable reuse across different groups and environments.\n","GenTime":"2024-08-28 12:08:56"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"text":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n\n\n##Decision\nMake all relevant services source and type dispatching.\\n","MatchedIds":[1341,84,1339,2802,2183],"Prediction":"## Decision\n\nTo accommodate both local and Federated Properties in Wikibase v2, we will extend the existing dispatching mechanism to consider both the **entity type** and the **entity source**.\n\nInstead of solely relying on the entity type, dispatching services will now receive both the **entity ID** and its **source** (local or remote). This will allow the dispatcher to:\n\n1. **Identify the entity type:**  The entity ID will still be used to determine if the entity is a Property or another entity type.\n2. **Determine the source:** The additional source information will indicate whether the Property is local (database-backed) or remote (Federated\/API-backed).\n3. **Delegate to the correct service:** Based on the combination of type and source, the dispatcher can now delegate to the appropriate service implementation for local or remote Properties.\n\nThis approach ensures backward compatibility with existing entity type dispatching while introducing the necessary differentiation between local and Federated Properties. The implementation will involve updating existing dispatching services and potentially introducing a new abstraction layer to handle the combined type and source information. \n","GenTime":"2024-08-28 12:09:02"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"text":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n\n\n##Decision\nUpdate Rummager configuration so it uses backend-redis, and remove api-redis.\\n","MatchedIds":[2471,3147,3657,1464,4061],"Prediction":"## Decision\n\nWe will consolidate the three existing Redis instances (backend-redis, logs-redis, api-redis) into a single, larger Redis instance in Elasticache. \n\nThis decision is based on the following:\n\n* **Reduced Overhead:** Managing a single instance is simpler than managing three separate instances. This reduces administrative overhead and simplifies monitoring.\n* **Cost Optimization:** AWS charges based on instance size and resources. Consolidating instances may offer cost savings compared to running multiple smaller instances.\n* **Improved Resource Utilization:** Combining workloads allows for better resource utilization. \n* **Simplified Architecture:** A single Redis instance promotes a cleaner and easier-to-understand architecture.\n\n**Implementation Details:**\n\n1. **Capacity Planning:**  Assess the resource requirements (memory, CPU, network) of all three existing instances to determine the appropriate size for the consolidated Redis instance.\n2. **Migration Strategy:** Develop a plan to migrate data from the existing instances to the new consolidated instance. This may involve a phased approach or a temporary maintenance window.\n3. **Application Updates:** Update application configurations to point to the new consolidated Redis instance. \n4. **Monitoring and Testing:** Monitor the performance of the consolidated instance closely after migration to ensure it meets the needs of all applications.\n\n**Considerations:**\n\n* **Security:**  Ensure appropriate security groups and access control lists are in place for the consolidated instance.\n* **Data Persistence:** Evaluate the need for data persistence and backups for the consolidated instance.\n* **Performance Testing:**  Conduct thorough performance testing to validate that the consolidated instance can handle the combined workload of all applications without impacting performance. \n","GenTime":"2024-08-28 12:09:11"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"text":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n\n##Decision\nUse autowire for now.\\n","MatchedIds":[1035,2317,2184,4271,866],"Prediction":"## Decision\n\nWe will use `autowire` for communication between our Single Page App and the server. \n\nWhile this approach represents a tightly coupled design and doesn't align with the REST maturity model, the benefits in terms of rapid development and ease of change are significant for our current single-client context.  This decision doesn't restrict us from adopting a more RESTful approach in the future. As our needs evolve and we identify use cases requiring decoupling or multiple client applications, we can incrementally introduce RESTful routes and migrate functionality as needed. \n","GenTime":"2024-08-28 12:09:15"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"text":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","MatchedIds":[3572,1243,4868,1254,3389],"Prediction":"## Decision\n\nWe will migrate Twig to the [**{New Riglet Flavor Name}**] riglet flavor. This decision is driven by the following factors:\n\n* **Modernization:** The new riglet flavor offers [**{Specific benefits of new flavor e.g., improved performance, simpler configuration, better resource utilization}**]. \n* **Future-Proofing:** Adopting the newer flavor aligns Twig with the company's infrastructure roadmap and ensures better support and compatibility in the future.\n* **Testing Opportunities:**  This migration presents a valuable opportunity to test and evaluate newer approaches in a production-like environment, potentially leading to further optimizations and improvements.\n\nWe will undertake a phased migration approach to mitigate risks and ensure a smooth transition.\n","GenTime":"2024-08-28 12:09:19"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"text":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n\n\n##Decision\n* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","MatchedIds":[4852,3488,3469,311,4909],"Prediction":"## Decision\n\nWe will use OpenTelemetry as the distributed tracing framework across all MilMove services. \n\n### Rationale\n\nOpenTelemetry satisfies the stated goals:\n\n* **Wide language support:** OpenTelemetry provides client libraries for a vast array of programming languages and frameworks commonly used in the MilMove codebase.\n* **Backend Agnostic:** A primary benefit of OpenTelemetry is its vendor-neutral approach. It does not lock us into a specific backend system for trace data, allowing flexibility to choose and switch between various analysis tools and platforms in the future.  \n* **Industry Standard:** OpenTelemetry is quickly becoming the industry standard for distributed tracing, backed by major cloud providers and the CNCF (Cloud Native Computing Foundation). Adopting OpenTelemetry ensures future compatibility and reduces reliance on vendor-specific solutions.\n\n### Implementation Details\n\n* **Instrumentation:**  Integrate OpenTelemetry libraries into existing MilMove services, instrumenting key operations like HTTP requests, database calls, and message queue interactions.\n* **Context Propagation:** Ensure proper propagation of tracing context across service boundaries using OpenTelemetry's context propagation mechanisms.\n* **Configuration:**  Establish a consistent configuration approach for OpenTelemetry across services, allowing for centralized control over sampling rates, service names, and other relevant settings.\n\n### Consequences\n\n**Positive:**\n* **Improved Observability:** OpenTelemetry empowers developers and operators to understand the complex interactions within MilMove, enabling quicker troubleshooting and performance analysis.\n* **Future Flexibility:** The backend-agnostic nature of OpenTelemetry offers flexibility to evaluate and adopt different tracing backends as needs evolve.\n* **Industry Alignment:** Adhering to an industry standard like OpenTelemetry ensures interoperability with other systems and tools and simplifies knowledge sharing within the engineering community. \n\n**Negative:**\n* **Integration Effort:**  Integrating OpenTelemetry into existing services requires development time and effort. \n* **Learning Curve:**  Teams need to invest in learning the OpenTelemetry APIs and best practices for instrumentation and context propagation.\n\n**Mitigation:**\n\n* **Incremental Rollout:**  Start by instrumenting critical services or components and gradually expand coverage to minimize initial integration overhead.\n* **Training and Documentation:** Provide developers with adequate training resources and documentation on OpenTelemetry to streamline the learning process.  \n","GenTime":"2024-08-28 12:09:30"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"text":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n\n\n##Decision\nUnoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","MatchedIds":[287,282,273,5059,4382],"Prediction":"## Decision\n**Unoconv**\n\nWhile a Java-only solution using Apache POI is appealing, testing shows that Unoconv provides the most visually accurate PDF conversions for both `.docx` and `.odt` files, closely matching the native view. \n\nHere's why:\n\n- **Visual Accuracy:** Unoconv leverages LibreOffice internally, resulting in high-fidelity conversions that preserve the layout, fonts, and formatting of the original documents. \n- **Format Support:** Unoconv handles both `.docx` and `.odt` effectively, meeting the requirement for the most commonly used formats.\n- **Free and Open Source:**  Unoconv aligns with the requirement for free software tools.\n\nAlthough Unoconv introduces an external dependency, the superior output quality for the crucial `.docx` and `.odt` formats makes it the best choice. \n","GenTime":"2024-08-28 12:09:35"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"text":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n\n\n##Decision\nUse one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","MatchedIds":[4644,2140,1291,3488,729],"Prediction":"## Decision\n\nUtilize a central configuration storage within Elektra itself to manage instance and cluster information. \n\n### Positive Consequences\n\n- **Leverages Elektra's Strengths:** This approach leverages Elektra's existing capabilities for hierarchical data management and access control. \n- **Centralized Management:** Provides a single source of truth for instance and cluster configurations.\n- **Unified Configuration:**  Allows managing Elektra Web alongside other Elektra configurations, promoting consistency and reducing tool sprawl.\n\n### Negative Consequences\n\n- **Potential Complexity:** Designing the schema and access patterns within Elektra for this new information might introduce complexity.\n- **Bootstrapping:**  A small bootstrapping mechanism might be required to initially locate and connect to the central configuration.\n\n### Implementation Details:\n\n- **Schema:** Define a clear and well-documented schema within a dedicated Elektra namespace to store:\n    - Instance details (IP\/hostname, port, authentication information)\n    - Cluster membership information \n- **API:** Expose API endpoints within Elektra Web to interact with this configuration, allowing authorized users to:\n    - List, add, modify, and remove instances and clusters. \n    - Fetch configuration for a specific instance or all instances within a cluster. \n- **Security:** Leverage Elektra's access control mechanisms (e.g., ACLs) to restrict modification of these configurations to authorized users. \n","GenTime":"2024-08-28 12:09:47"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"text":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\n\n##Decision\nWe will use progressive enhancement for all our user-facing applications.\\n","MatchedIds":[2410,3806,2405,2687,4841],"Prediction":"## Decision\nOur service will be progressively enhanced. Whilst we acknowledge that some existing services do not follow this approach, the service standard and service manual are clear that this is a requirement for all new services. Building a progressively enhanced service ensures we meet the needs of all our users, including those with accessibility needs, and aligns with the government's digital inclusion strategy. \n","GenTime":"2024-08-28 12:09:52"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"text":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n\n##Decision\nThere shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","MatchedIds":[4508,4498,2523,4493,4507],"Prediction":"## Decision\n\nTo address the problem of generating storefront URLs in mail templates for headless setups and various trigger points, we introduce a new concept: **Mail template variables resolved via a dedicated event.**\n\n**1. Placeholder Variables in Mail Templates:**\n\nInstead of directly embedding storefront URLs in the mail templates, we will use placeholder variables. These variables will follow a specific naming convention, for example:\n\n* `{{ storefrontUrl.resetPassword }}`\n* `{{ storefrontUrl.newsletterConfirm }}`\n* `{{ storefrontUrl.orderDetails }}`\n\n**2. New Event: `MailActionUrlGeneratedEvent`:**\n\nA new event will be dispatched whenever a mail template with placeholder variables needs to be rendered. This event, `MailActionUrlGeneratedEvent`, will provide the following information:\n\n* **Context:** The context of the mail being sent (e.g., newsletter subscription, order confirmation).\n* **Sales Channel ID:** The ID of the sales channel associated with the action.\n* **Variables:** An array of placeholder variable names found in the template.\n\n**3. Subscribers to Resolve Variables:**\n\nVarious parts of the system (e.g., storefront bundle, plugins, custom implementations) can subscribe to the `MailActionUrlGeneratedEvent`. \n\n* **Storefront Bundle:** If the storefront bundle is installed, it will listen to this event and resolve variables like `storefrontUrl.resetPassword` using its routing mechanisms. \n* **Plugins and Custom Code:** Extensions can listen for specific contexts and\/or variable names. For instance, a custom order management system could resolve  `storefrontUrl.orderDetails` with its own logic.\n* **Fallback Mechanism:**  A fallback mechanism will be implemented to handle unresolved variables. This could involve using a configurable base URL or outputting a warning message.\n\n**4. Rendering the Mail Template:**\n\nAfter the event is dispatched and processed, the resolved URLs will replace the placeholder variables in the mail template. The final rendered content will then be used to send the email.\n\n**Advantages of this approach:**\n\n* **Flexible and Extensible:**  Allows different system parts to contribute to URL generation based on the context.\n* **Headless-Friendly:**  The storefront bundle is not required to generate URLs.\n* **Centralized Logic:** Provides a single point for managing and customizing mail template URLs.\n* **Future-Proof:**  Compatible with various triggers for sending emails, including Business Events and FlowBuilder.\n\n**Implementation Notes:**\n\n* The new event and variable resolution logic will be integrated into the Shopware core mail system.\n* Documentation and examples will be provided to guide developers on implementing subscribers for custom URL resolution. \n","GenTime":"2024-08-28 12:10:05"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"text":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n\n\n##Decision\nWebpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","MatchedIds":[1576,1680,3827,5259,3521],"Prediction":"## Decision\n\nWe will use **UglifyJS** to minify JavaScript code. \n\nThis decision is based on the following:\n\n* **Reduced bundle size:** Our analysis shows UglifyJS consistently produces significantly smaller bundles (40% smaller, 25% smaller with gzip) compared to JSMin. This directly addresses our goal of shipping less code to clients.\n* **Existing pre-compilation step:** Using UglifyJS integrates seamlessly into our existing workflow as it is a node-based tool and we already have a pre-compilation step in our Popups pipeline. \n\nThis change will improve the user experience by decreasing page load times due to the significantly smaller JavaScript bundles. \n","GenTime":"2024-08-28 12:10:10"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"text":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n\n\n##Decision\nWe will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","MatchedIds":[1202,458,1125,425,1196],"Prediction":"## Decision\n\nWe will create a central repository for NHSUK styles. This repository will be version controlled and use SASS for managing styles.  New sites and updates to existing sites will be required to utilize the style repository to ensure consistency across NHSUK. A review process for proposed changes to the central style repository will be established to ensure quality and gather feedback before wider deployment.\n","GenTime":"2024-08-28 12:10:14"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"text":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n\n##Decision\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","MatchedIds":[4047,2520,4859,3657,1574],"Prediction":"There is already a decision made in the provided context. \n\n## Decision\nThe decision is to use **MongoDB** as the persistent data store for the Performance Platform.\n\n## Reasoning\nThe decision is based on the following factors:\n\n* **Familiarity:** GDS has prior experience running MongoDB in production.\n* **Flexibility:** MongoDB allows for storing unstructured data (JSON documents) and applying structure at query time.\n* **Current Scale:** The initial data volume is expected to be small, making MongoDB a suitable choice.\n\n## Consequences\n* Use MongoDB with an appropriate replica-set configuration to ensure data redundancy and availability. \n\nThe decision acknowledges that they might revisit this choice in the future as they learn more about the user needs and the data volume grows. \n","GenTime":"2024-08-28 12:10:23"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"text":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n\n##Decision\nWe decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","MatchedIds":[3692,2215,4623,3746,3094],"Prediction":"## Decision\n\nImplement a two-phase commit protocol for data that needs to be written both off-chain and on-chain. This involves:\n\n1. **Prepare Phase:** Write the data off-chain and obtain a durable reference (e.g., hash, unique ID). Then, initiate an on-chain transaction that stores this reference but doesn't finalize the data availability.\n2. **Commit\/Rollback Phase:**\n    * **Success:**  If the on-chain transaction confirms, update the off-chain storage to mark the data as committed and accessible. \n    * **Failure:** If the on-chain transaction fails or times out, rollback the off-chain data, ensuring consistency. \n\nThis approach ensures that the off-chain data is only considered valid and accessible once the on-chain reference is successfully committed. This eliminates the inconsistency issue and prevents data corruption. \n","GenTime":"2024-08-28 12:10:28"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"text":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n\n##Decision\nArchitecture tests with using Archunit should be implemented.\\n","MatchedIds":[429,4896,2181,138,428],"Prediction":"## Decision\n\nWe will implement **Architecture Fitness Functions** as part of our CI\/CD pipeline to continuously monitor and validate adherence to defined architectural principles.\n\nThis will involve:\n\n* **Formalizing Architectural Principles:** Clearly document the agreed-upon architectural principles for the application. These principles should be specific, measurable, achievable, relevant, and time-bound (SMART). \n* **Defining Fitness Functions:** Translate each principle into testable fitness functions. These can be automated tests or scripts that evaluate specific aspects of the codebase or deployment artifacts (e.g., static code analysis, dependency checks, infrastructure-as-code validation).\n* **Integrating with CI\/CD:** Include the execution of these fitness functions as a mandatory stage in the CI\/CD pipeline. This ensures that any violations of architectural principles are detected early and prevent deployment.\n* **Establishing Feedback Loops:** Report the results of the fitness function evaluations back to the development team. This could be in the form of build reports, notifications, or dashboards, allowing for prompt identification and remediation of violations.\n\nThis approach provides several benefits:\n\n* **Proactive Enforcement:** Prevents architectural drift by actively validating principles throughout the development lifecycle.\n* **Improved Code Quality:** Encourages adherence to best practices and promotes a consistent architecture.\n* **Increased Agility:** Provides early feedback and reduces the risk of costly rework later in the development process.\n* **Facilitates Knowledge Sharing:** Formalizes architectural principles and makes them transparent to the entire team. \n","GenTime":"2024-08-28 12:10:36"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"text":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n\n\n##Decision\nWe leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","MatchedIds":[2221,5024,1326,2488,1173],"Prediction":"## Decision\n\nWe will implement a dedicated \"Settings\" table within the Molgenis database to centrally manage application settings. \n\n### Settings Table Structure\n\nThe table will have the following structure:\n\n| Column Name  | Data Type | Description                                      |\n|--------------|-----------|--------------------------------------------------|\n| app_id       | VARCHAR   | Unique identifier for the application.          |\n| setting_key  | VARCHAR   | Unique key for the setting within the application.|\n| setting_value| TEXT      | Value of the setting. Can be a string, JSON, etc.|\n\n### Linking Apps with Settings\n\nEach application will query the \"Settings\" table using its unique `app_id` during initialization. This allows settings to be retrieved dynamically at runtime.\n\n### Access Control\n\nAccess control to the settings will be managed through the existing Molgenis permission system. Specific roles will be granted permissions to view and modify settings for each application. \n\n### Benefits\n\nThis approach provides several benefits:\n\n* **Centralized Management:** All application settings are stored and managed in a single location.\n* **Dynamic Configuration:** Applications can retrieve settings at runtime, enabling flexibility.\n* **Secure Access Control:**  Leveraging the Molgenis permission system ensures secure access to settings. \n* **Simplified Deployment:**  Changes to settings can be deployed independently of application code updates.\n\n### Future Considerations\n\n* Implement a caching mechanism to improve performance for frequently accessed settings. \n* Develop a user interface within Molgenis for managing application settings. \n","GenTime":"2024-08-28 12:10:44"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"text":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n\n\n##Decision\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","MatchedIds":[211,3595,3915,4397,3497],"Prediction":"## Decision\n\nWe will use the **semver resource** for controlling the flow of Concourse pipelines instead of the pool resource. \n\n### Reasoning\n\n* **Security**: Minimizing the number of credentials passed to Concourse pipelines and instances is a priority. Using the semver resource allows us to leverage existing AWS credentials, reducing the risk associated with managing additional GitHub access keys.\n* **Simplicity**: The semver resource integrates seamlessly with our existing AWS infrastructure. Implementing the pool resource would require a more complex setup and potentially introduce new points of failure. \n\n### Negative Consequences\n\n* We will need to ensure appropriate IAM permissions are in place to prevent unintended manipulation of the S3 bucket used by the semver resource.\n* The semver resource might not offer the same level of granularity in controlling job execution order compared to the pool resource. We will need to investigate alternative solutions within Concourse if finer control is needed in the future. \n\n### Future Considerations\n\n* We should investigate future Concourse features or alternative solutions that address the limitations of both the pool and semver resource for managing job dependencies and triggering jobs without resource changes. \n","GenTime":"2024-08-28 12:10:51"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"text":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n\n##Decision\nRemove files on empty KeySet.\\n","MatchedIds":[1289,1318,1310,3746,1274],"Prediction":"## Decision\nStore an empty file.\n\nThis approach offers several advantages:\n\n* **Simplicity:** It aligns with the intuitive understanding of an empty KeySet translating to an empty configuration file.\n* **Consistency:**  It mirrors the behavior of `kdbGet()` on an empty file, which would yield an empty KeySet. \n* **Efficiency:** It avoids storing extraneous data or metadata representing \"emptiness.\"\n\nHowever, it necessitates the following considerations:\n\n* **Error Handling:**  Storage plugins should be equipped to handle empty files gracefully during `kdbGet()`.\n* **Distinction from Errors:** An empty file resulting from `kdbSet()` should be distinguishable from potential errors (like file system issues) that could also result in no data being read. This distinction might involve error codes or metadata associated with the storage operation. \n","GenTime":"2024-08-28 12:10:56"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"text":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n\n\n##Decision\nDivide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","MatchedIds":[3635,4619,539,2914,1067],"Prediction":"## Decision\n\nThe ADR Util module will be broken down into smaller, more focused modules. \n\nInstead of relying on a shared context passed through the utility functions, each module will be responsible for managing its own data and dependencies. This means:\n\n* Functions that require access to the ADR directory or files will receive them as explicit arguments instead of relying on the shared context. \n* Caching of the ADR directory and files will be handled locally within the functions that require it, if necessary.\n\nThis approach will:\n\n* **Reduce coupling:** Modules will be less interdependent, making it easier to understand, modify, and test individual components.\n* **Improve maintainability:** Smaller, more focused modules are easier to reason about and debug. \n* **Prevent circular dependencies:** By removing the shared context, we eliminate the possibility of circular dependencies arising from modules indirectly depending on each other through the shared state. \n\nWe will refactor the existing code to adhere to this new structure.  This may involve creating new modules for specific concerns like link handling or file system interactions.\n\nThis decision supersedes the previous decision to lazily initialize the shared context ([ADR 5](5-Utility_Context_Lazily_Initialized.md)), as it eliminates the need for a shared context altogether.  It also aligns with the decision to centralize the definition of the filename ([ADR 6](6-Centralize_Definition_of_Filename.md)) by ensuring that functions requiring the filename receive it explicitly. \n","GenTime":"2024-08-28 12:11:05"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"text":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n\n\n##Decision\nAlthough we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","MatchedIds":[3853,2386,1767,1866,1416],"Prediction":"## Decision\nWe will adopt Rails Internationalization to manage all static text within the application. This will involve:\n\n* **Extracting all existing inline text** in Ruby, JavaScript, and HTML files into dedicated YAML files within `config\/locales\/en`. \n* **Organising these translations** in a logical hierarchy that reflects the structure of the application, for better maintainability and discoverability. \n* **Using the `I18n.t()` method** to reference these translations within our codebase.\n* **Implementing a process for adding and reviewing new translations** to ensure consistency and accuracy.\n\nThis approach will improve the maintainability, consistency and accessibility of our application's content, as well as make future internationalization efforts easier.\n","GenTime":"2024-08-28 12:11:10"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"text":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n\n##Decision\nWe will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","MatchedIds":[3682,4642,3470,3070,469],"Prediction":"## Decision\n\nWe will use **Express.js** as our server framework. \n\nThis decision is driven by the following factors:\n\n* **Simplicity and Ease of Use:** Express.js is renowned for its minimalist approach, making it easy to understand and use, even for developers with less experience. \n* **Flexibility:**  Express.js allows for easy implementation of custom routing logic, enabling us to effortlessly match template files based on URL patterns, thereby simplifying the prototyping process.\n* **Node.js Ecosystem:**  Being a part of the Node.js ecosystem, Express.js benefits from a vast pool of readily available packages and modules, streamlining development and reducing the need to reinvent the wheel for common functionalities. \n* **Wide Adoption and Community Support:** The popularity of Express.js ensures access to a large and active community, providing valuable resources, documentation, and support for troubleshooting any potential issues that might arise.\n\nThis choice aligns with the goal of making the prototype kit accessible to users with varying coding skill levels while enabling the emulation of a live service environment. \n","GenTime":"2024-08-28 12:11:17"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"text":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","MatchedIds":[4890,4889,4600,3722,2261],"Prediction":"## Decision\n**We will use a simple git workflow with linear history on the main development branch and feature branches rebased against the main development branch.**\n### Decision details\nThe proposed workflow is almost the same as the highly popular [Gitflow](https:\/\/nvie.com\/posts\/a-successful-git-branching-model\/), but it promotes linear history on the main development branch. Here is a short description:\n* Linear commit history is kept on the `master` branch.\n* The `develop` branch is used as the main development branch.\n* When a new release should be created, a release branch is created from the `develop` branch.\n* Bugfixes are done on bugfix branches, and they are merged (`--no-ff`) to both `develop` and `master` branches.\n* All other branches are considered to be feature branches. \n* Feature branches branch off from the `develop` branch and are named according to the [semantic branch naming](.\/0006-branch-naming-and-commit-messages.md) convention.\n* When a feature is finished, the corresponding feature branch is rebased against `develop` and merged (`--ff`) into it.\n* Before merging the feature branch, local `develop` should be updated with the remote `develop`.\n* Release branches are merged (`--no-ff`) into both `master` and `develop` branches.\n* The `master` branch should always contain production-ready code.\n#### Advantages of the linear commit history on the main development branch\n* Provides a clean and straightforward history, making it easy to understand the project's evolution.\n* Simplifies the process of finding bugs and reverting changes, as the history is linear and easy to follow.\n* Improves the efficiency of Continuous Integration and Continuous Delivery (CI\/CD) pipelines, as the history is clean and easy to work with.\n#### Resolving broken CI builds\nBroken CI builds are resolved by not allowing `push --force` to the `develop` branch. The only way to introduce changes to the `develop` branch is through merge requests (pull requests). In addition, the chosen CI system should not run the build on the merge request itself. Instead, it should use the `merge --no-ff` strategy to check if the build is successful when merging to the target branch. If the build (or tests or anything else) fails, the merge is not allowed. After the local merge, the developer pushes changes to the remote, triggering CI build on the updated remote branch. \n#### Release versioning\nWith this approach, release versioning is straightforward, and we can use different strategies to express it. Usually, release branches will be named with the next release version number. It is also very convenient to tag (`git tag`) merge commits on the `master` branch with corresponding release versions. Tags can be used later to recreate any historical version from the `master` branch.\n#### Additional notes\nChosen git workflow promotes \"recipe\" organization of commits in the feature branches. They can be easily reviewed in the merge request as a whole or as individual commits. After the feature branch is merged with the `--ff` option, linear history is preserved.\nThe choice of linear history comes with a price: resolving merge conflicts is more difficult and sometimes requires more work. It is the tradeoff that we are willing to accept to get all the benefits of linear history.","GenTime":"2024-08-28 12:11:36"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"text":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n\n\n##Decision\nWe will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","MatchedIds":[2240,729,979,2236,3104],"Prediction":"## Decision\n\nWe will use **Istio** as our service mesh. \n\n### Justification\n\n* **Meets Requirements:** Istio directly addresses Verify's needs for data exfiltration control, strict authentication (including mTLS), and provides a comprehensive service mesh solution.\n* **Maturity and Support:** Istio is considered an emerging standard with robust features, a large community, and active development, making it a reliable choice.\n* **Ease of Deployment:**  Istio's sidecar injection mechanism simplifies deployment, minimizing the configuration burden on development teams.\n\n### Reasons for Rejecting Alternatives:\n\n* **AWS App Mesh:**  While AWS App Mesh is a viable service mesh, its unavailability in the London region and lack of automatic sidecar injection at the time of evaluation made it unsuitable for Verify's immediate needs. \n* **Linkerd:**  While Linkerd (especially version 2.x) offers a lightweight approach, its smaller feature set compared to Istio and the ongoing major rewrite during the evaluation period presented potential risks and limitations for Verify's security and functionality requirements.\n\n### Additional Considerations\n\n*  We will investigate and evaluate future versions of AWS App Mesh and Linkerd as they become available. The service mesh landscape is constantly evolving, and reevaluation will ensure we're leveraging the most suitable technology for Verify's needs. \n","GenTime":"2024-08-28 12:11:44"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"text":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\n\n##Decision\nWe've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","MatchedIds":[1992,157,160,1855,3171],"Prediction":"## Decision\n\nWe will implement a **global z-index management system** to bring order and predictability to how z-index values are used throughout our application. \n\n### Chosen Option: \n\n**Centralized Z-Index Management with a Utility-First Approach:**\n\n- **Create a dedicated z-index scale:**  Define a limited set of semantic z-index values within a shared stylesheet or design system. These values should represent common UI layers (e.g., base, overlays, modals, tooltips, popovers, etc.). \n- **Utilize CSS utility classes:**  Instead of directly applying z-index values to elements, create utility classes that correspond to the defined z-index scale (e.g., `.z-overlay`, `.z-modal`, `.z-tooltip`).\n- **Enforce usage of utility classes:** Encourage (or strictly enforce through code reviews) the use of these utility classes for all z-index related styling.\n\n### Positive Consequences:\n\n- **Eliminates z-index guesswork:** Developers no longer need to invent arbitrary z-index values, leading to a more maintainable codebase.\n- **Predictable stacking order:** A well-defined z-index scale ensures consistent and predictable stacking of elements.\n- **Reduced risk of z-index conflicts:**  By limiting the available z-index values, conflicts between components are minimized.\n- **Scalability:** The system easily scales as new UI patterns emerge; simply add new z-index values to the scale.\n\n### Negative Consequences:\n\n- **Initial Implementation Effort:**  Requires upfront work to define the z-index scale, create utility classes, and update existing code to use the new system.\n- **Potential for Over-Abstraction:** A very granular z-index scale can become cumbersome if not carefully designed. \n\n### Example:\n\n```css\n\/* Shared Stylesheet *\/\n:root {\n  --z-base: 0;\n  --z-dropdown: 10;\n  --z-modal: 50;\n  --z-tooltip: 100;\n}\n\n.z-dropdown { z-index: var(--z-dropdown); }\n.z-modal { z-index: var(--z-modal); }\n.z-tooltip { z-index: var(--z-tooltip); }\n```\n\n```html\n<!-- Using the z-index utility classes -->\n<div class=\"modal z-modal\"> <\/div>\n<div class=\"tooltip z-tooltip\"> <\/div> \n```\n\n### Additional Considerations:\n\n- **Tooling:** Consider using a CSS preprocessor like Sass or a linter to enforce z-index scale adherence.\n- **Documentation:** Document the z-index scale clearly for all developers to understand and follow. \n","GenTime":"2024-08-28 12:11:58"}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"text":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n\n\n##Decision\nWe decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","MatchedIds":[2127,2150,2134,2191,4644],"Prediction":"## Decision\n\nWe will replace the existing JMX-based command-line interface with a new Webadmin command-line interface. This new CLI will interact with the James server through the existing Webadmin HTTP APIs. \n\nHere's a breakdown:\n\n**1. Deprecate and Remove JMX CLI:**\n\n- The existing JMX-based CLI will be marked as deprecated and eventually removed in a future James release. \n- Proper deprecation notices will be added to the documentation and release notes, guiding users to the new Webadmin CLI.\n\n**2. Webadmin CLI Development:**\n\n- A new command-line interface will be developed leveraging a framework that supports HTTP communication (e.g., using libraries like OkHttp or Apache HttpClient). \n- This CLI will interact with the Webadmin REST endpoints for all functionalities.\n\n**3. Enhanced User Experience:**\n\n- The new CLI will prioritize an intuitive user experience by adopting an \"entity-first\" syntax. \n-  For example, instead of `GET USER james`, the command structure would be `USER james GET`. \n- This approach will make commands easier to remember and use.\n\n**4. Security:**\n\n- By relying on HTTP and the Webadmin authentication mechanisms, the new CLI inherently addresses the security vulnerabilities associated with JMX.\n\n**5. Documentation and Examples:**\n\n- Comprehensive documentation and clear examples will be provided to facilitate a smooth transition for users from the old JMX CLI to the new Webadmin CLI.\n","GenTime":"2024-08-28 12:12:07"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"text":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n\n\n##Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records (ADRs), as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), to maintain a centralized log of architectural choices throughout the project's lifecycle. \n","GenTime":"2024-08-28 12:12:10"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"text":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n\n\n##Decision\nUse Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","MatchedIds":[3031,36,3721,3321,2655],"Prediction":"## Decision\n\nWe will use **React Styleguidist** for our component library documentation. \n\n### Justification\n\nBoth React Storybook and React Styleguidist meet many of the criteria. However, Styleguidist aligns more closely with our core requirement of **easy documentation editing for both designers and engineers.** \n\nHere's a breakdown:\n\n**React Styleguidist Advantages:**\n\n* **Markdown-focused Documentation:** Styleguidist heavily emphasizes Markdown documentation alongside components. This makes it incredibly straightforward for designers familiar with Markdown to contribute to component descriptions, usage examples, and additional notes without deep diving into React code.\n* **Live Editing & Playground:** Styleguidist's live reload and built-in playground make iterating on components and their documentation seamless. This tight feedback loop is beneficial for both engineers and designers.\n* **Themeability:** Styleguidist offers flexibility in styling the style guide itself to match our existing documentation's look and feel.\n\n**React Storybook Considerations:**\n\n* **Focus on Isolated Component Examples (Stories):** Storybook excels at showcasing components in various states and with different inputs. While it supports Markdown, it often requires more setup and tooling to achieve a documentation flow as smooth as Styleguidist's. \n* **Steeper Learning Curve for Non-Developers:** While powerful, Storybook might present a slightly steeper learning curve for designers, particularly regarding the concept of \"stories\" and their potential configuration options.\n\n### Conclusion\n\nChoosing React Styleguidist prioritizes a collaborative documentation workflow, especially for designers. Its emphasis on Markdown and the straightforward live editing environment make it a compelling choice for our component library. \n","GenTime":"2024-08-28 12:12:18"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"text":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n\n\n##Decision\nThe interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","MatchedIds":[3981,3187,4538,311,1545],"Prediction":"## Decision\n\nWe will simplify the communication between the Processor and Interpreter by **using direct function calls and return values**, eliminating the intermediate Registers component for this purpose.\n\n**Justification:**\n\nThe current use of Registers as a communication intermediary adds unnecessary complexity for the following reasons:\n\n* **Straightforward interaction:** The communication flow is inherently simple. The Processor instructs the Interpreter to execute an instruction, and the Interpreter responds with execution status and potential IP updates. This does not warrant the overhead of a dedicated mediator.\n* **Unnecessary decoupling:**  Forcing the Processor and Interpreter to be unaware of each other through a Registers mediator would be artificial. The domain model clearly defines their direct interaction, and introducing such decoupling would obscure the core logic.\n* **Overkill for notification:** Implementing a notification mechanism within Registers to signal changes to the Processor and Interpreter is excessive. The direct communication flow allows for immediate responses without the need for asynchronous notifications.\n\n**Consequences:**\n\n* **Positive:**\n    * **Reduced complexity:** Simplifying the communication significantly reduces the overall complexity of the system.\n    * **Improved readability and maintainability:** The code becomes easier to understand and maintain with the clear flow provided by direct function calls.\n    * **Improved performance:** Eliminating the overhead of the Registers intermediary can lead to slight performance improvements.\n\n* **Negative:**\n    * **Reduced flexibility:** Direct coupling slightly reduces flexibility for future changes, such as introducing asynchronous communication. However, given the current domain model and the straightforward interaction, this trade-off is acceptable.\n\n**Future Considerations:**\n\nIf, in the future, the communication needs evolve to become more complex or require asynchronous behavior, we can reassess the need for a mediator. However, based on the current requirements, direct function calls provide a more efficient and manageable solution.\n","GenTime":"2024-08-28 12:12:27"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"text":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n\n##Decision\nDeveloper scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","MatchedIds":[398,2703,1792,5177,3653],"Prediction":"## Decision\n\nThe demo\/visualisation scripts will be stored in a dedicated `scripts` directory within the `pace-developers` repository. \n\n### Justification\n\nWhile storing scripts within individual project repositories seems convenient at first, it poses several long-term challenges:\n\n* **Versioning issues:** Scripts developed for a specific project version might become incompatible with future updates, leading to confusion and potential errors.  \n* **Ambiguity for multi-project scripts:**  The increasing modularity of PACE means that scripts might leverage functionality from multiple projects.  A centralised location avoids ambiguity about where such scripts should reside.\n* **Maintainability and discoverability:** Scattering scripts across multiple repositories hinders discoverability and maintainability. A centralised repository provides a structured approach, especially when combined with clear documentation practices.\n\nCentralising the scripts in `pace-developers` offers significant advantages:\n\n* **Clear expectations:**  Placing scripts outside of the main project repositories manages expectations \u2013 it's clear they are not part of the core, tested codebase.\n* **Explicit dependency management:**  Dependencies on specific versions of PACE projects can be explicitly defined, ensuring reproducibility. \n* **Proximity to decision documentation:**  Storing scripts alongside developer documentation within `pace-developers` allows for better context and facilitates understanding design decisions. \n\n### Implementation Considerations\n\n* **Documentation:** Clear guidelines on script contribution, including required metadata (e.g., purpose, dependencies, example usage), will be provided within `pace-developers`. \n* **Organisation:** The `scripts` directory can be further structured by project or functionality to improve organisation and discoverability. \n* **Version Control:** Consider using git tags or branches within `pace-developers` to associate scripts with specific releases of PACE projects, further enhancing reproducibility.  \n","GenTime":"2024-08-28 12:12:36"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"text":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n\n\n##Decision\nWe will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","MatchedIds":[317,3221,241,3657,398],"Prediction":"## Decision\n\nWe will use Flyway for database migration management. \n\nFlyway provides the following advantages:\n\n* **Version control for database changes:** Flyway uses SQL (or Java-based) migration scripts that can be version-controlled alongside our application code, ensuring consistency between the application and the database schema.\n* **Reliable and repeatable migrations:** Flyway tracks which migrations have been applied and ensures they are only applied once, preventing accidental re-runs and data corruption.\n* **Rollback capability:** In case of errors or the need to revert changes, Flyway allows for easy rollback to a previous database version. \n* **Integration with our development workflow:**  Flyway can be easily integrated into our CI\/CD pipeline to automate database migrations during deployments. \n\n**Alternatives Considered:**\n\n* **Liquibase:** Offers similar functionality to Flyway. We chose Flyway due to its simplicity and ease of integration with Spring Boot, which we are already using.\n\n**Further Considerations:**\n\n* We will establish clear naming conventions and best practices for writing migration scripts to ensure maintainability and readability.\n* Developers will be responsible for writing and testing their own migration scripts, but code reviews will be mandatory to ensure quality and prevent potential issues.\n* We will implement automated testing for our migrations using a separate test database to catch any problems early in the development cycle. \n","GenTime":"2024-08-28 12:12:43"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"text":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n\n\n##Decision\nThe decision is to introduce another queue for Order Management\\n","MatchedIds":[307,4804,3820,1145,4616],"Prediction":"## Decision\n\nGiven the higher importance of messages from Order Management, we will implement a **prioritized message queue system**. This will ensure timely processing of order-related messages, while allowing flexibility in handling messages from Rating and Recommendation Managers. \n\nThis approach strikes a balance between reliable order processing and potential performance optimizations achievable through asynchronous handling of less critical messages. \n","GenTime":"2024-08-28 12:12:46"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"text":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n\n##Decision\nCreate a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","MatchedIds":[1740,4415,2471,1464,4358],"Prediction":"## Decision\n\nTo address the challenges of maintainability, discoverability, and versioning across different AWS Bare Metal Rig instances, we will adopt the following approach:\n\n* **Modularity with Terraform Modules:** Refactor the AWS Bare Metal Rig into a collection of reusable Terraform modules. Each module will represent a specific component or service (e.g., compute, database, networking). This allows for greater flexibility and customization when composing rigs for different projects. \n* **Centralized Module Registry:** Establish a central repository (e.g., a dedicated Git repository or a Terraform module registry) to host the official, versioned Terraform modules for the AWS Bare Metal Rig.\n* **Versioning and Documentation:** Utilize semantic versioning for all published modules. Maintain comprehensive documentation for each module, including its purpose, usage instructions, and supported configurations.\n* **Reference Architecture and Examples:**  Provide well-defined reference architectures and example implementations of the AWS Bare Metal Rig using the modular approach. These examples will serve as starting points for new projects and showcase best practices for composing and configuring rigs. \n* **Continuous Integration and Testing:** Implement CI\/CD pipelines to automate the testing and validation of the Terraform modules. This ensures the stability and reliability of the shared modules.\n\nThis approach promotes:\n\n* **Reusability and Consistency:**  Modules allow for consistent infrastructure deployments across different projects and teams.\n* **Discoverability:** A central registry makes it easy to find and consume available modules.\n* **Maintainability:**  Updates and bug fixes can be implemented in a centralized manner and easily propagated to dependent projects through version updates. \n* **Flexibility and Customization:**  Projects can choose the specific modules and configurations that best suit their needs, enabling variations on the AWS Bare Metal Rig concept. \n","GenTime":"2024-08-28 12:12:56"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"text":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n\n\n##Decision\nOur API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","MatchedIds":[1180,674,2181,3159,1907],"Prediction":"## Decision\n\nWe will implement HATEOAS using the following approach:\n\n**1. Standard Links:**\n\n* **JSON-API `links` Object:**  We will utilize the `links` object within our JSON responses as defined by the [JSON-API specification](https:\/\/jsonapi.org\/format\/#document-links). This provides a consistent structure for representing links related to resources and navigation.\n* **Minimum Required Links:**  Every resource representation will include a `self` link pointing to its canonical URL. \n* **Pagination:** For collections, we'll use `next`, `prev`, `first`, and `last` links to support pagination.\n* **Related Resources:**  Links to related resources will be included when appropriate, allowing clients to discover and navigate relationships between data entities.\n\n**2. Affordances for Actions:**\n\n* **HTTP Methods:**  We will leverage standard HTTP methods (GET, POST, PUT, PATCH, DELETE) to convey available actions on resources.\n* **Link Relation Types (Optional):** We may consider using standardized link relation types (defined in IANA's Link Relation Registry) to provide semantic meaning to links, enhancing discoverability of specific actions.\n\n**3. Avoiding Hypermedia Controls (for now):**\n\n* We will not initially implement more complex hypermedia controls like forms or custom media types (e.g., Siren, Collection+JSON). \n* This decision prioritizes simplicity and broader compatibility with existing tools.  We can revisit this if our API requires more dynamic interactions in the future.\n\n**Example:**\n\n```json\n{\n  \"data\": {\n    \"type\": \"orders\",\n    \"id\": \"123\",\n    \"attributes\": {\n      \"status\": \"pending\"\n    },\n    \"links\": {\n      \"self\": \"\/api\/orders\/123\",\n      \"customer\": \"\/api\/customers\/456\", \n      \"cancel\": \"\/api\/orders\/123\/cancel\" \n    }\n  }\n}\n```\n\n**Benefits:**\n\n* **Improved Discoverability:** Clients can dynamically understand available actions and resources by following links.\n* **Reduced Coupling:**  Clients are less reliant on out-of-band knowledge about API structure, making the API more evolvable.\n* **Enhanced Flexibility:**  New actions and resources can be introduced without breaking existing client integrations.\n\n**Considerations:**\n\n* **Complexity:** Implementing HATEOAS adds complexity to both server and client development.\n* **Tooling:**  Support for HATEOAS in client libraries and frameworks can vary.\n* **Learning Curve:** Developers unfamiliar with HATEOAS may require additional training.\n\nThis approach strikes a balance between achieving RESTful principles, maintainability, and ease of adoption. We will closely monitor its effectiveness and adapt as needed based on feedback and evolving API requirements. \n","GenTime":"2024-08-28 12:13:12"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"text":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n\n##Decision\nWe will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","MatchedIds":[4179,1258,1307,1173,350],"Prediction":"## Decision\n\nIntroduce a mechanism in `konfetti` to load configurations partially, based on namespaces or tags. This allows tests to load only the required subset of configurations, preventing unintended side effects from unrelated settings. \n","GenTime":"2024-08-28 12:13:15"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"text":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n\n\n##Decision\nThe syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","MatchedIds":[1180,2181,2823,1016,1767],"Prediction":"## Decision\n\nA dedicated service will be created to act as an aggregator and proxy for GP data from the NHS Syndication API. This service will:\n\n1. **Retrieve and cache data** from the NHS Syndication API on a regular schedule, ensuring freshness of data while minimising load on the source.\n2. **Transform the XML data** into a more readily consumable format like JSON.\n3. **Aggregate data** from different subpages (overview, services, facilities) into a single unified resource per GP surgery.\n4. **Expose a RESTful API** with clear and consistent data structures for consumption by Connecting to Services applications.\n\nThis approach offers several benefits:\n\n* **Decouples applications from the limitations** of the NHS Syndication API (direct access restrictions, XML format, data fragmentation).\n* **Improves performance and scalability** by caching data and reducing the number of requests to the source API.\n* **Provides a consistent and maintainable solution** for accessing and utilising GP data across multiple applications.\n\nThis new service will be responsible for all interactions with the NHS Syndication API, shielding other applications from its complexities and limitations. \n","GenTime":"2024-08-28 12:13:22"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"text":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n\n##Decision\nWe will use environment variables to configure applications.\\n","MatchedIds":[2221,1745,429,2471,3171],"Prediction":"## Decision\n\nWe will use configuration files to manage environment-specific settings. \n\n### Approach:\n\n1. **Configuration Files:** Define separate configuration files for each environment (e.g., `config.local.json`, `config.development.json`, `config.production.json`). These files will contain key-value pairs for environment-specific variables like API endpoints, database connection strings, logging levels, etc. \n\n2. **Environment Variables:** Use an environment variable (e.g., `APP_ENV`) to indicate the current environment. The application will load the appropriate configuration file based on this variable. \n\n3. **Configuration Loader:** Implement a configuration loader module that reads the environment variable and loads the corresponding configuration file. This module will provide a consistent interface for accessing configuration values throughout the application.\n\n### Example:\n\n**config.local.json:**\n```json\n{\n  \"apiUrl\": \"http:\/\/localhost:3000\",\n  \"databaseUrl\": \"mongodb:\/\/localhost:27017\/mydatabase\" \n}\n```\n\n**config.production.json:**\n```json\n{\n  \"apiUrl\": \"https:\/\/api.mydomain.com\",\n  \"databaseUrl\": \"mongodb:\/\/<user>:<password>@<host>:<port>\/<database>\" \n}\n```\n\n**Application Code:**\n\n```\nconst config = require('.\/configLoader').loadConfig();\n\nconsole.log(config.apiUrl); \/\/ Will output the correct API URL based on the environment\n```\n\n### Benefits:\n\n* **Clean Separation:** Configuration is decoupled from code, making it easier to manage and modify.\n* **Environment Consistency:** Ensures that applications use the correct settings for each environment.\n* **Simplified Deployment:** Deployments are smoother as environment-specific configurations are predefined. \n* **Security:** Sensitive information like database credentials can be stored securely outside of the codebase. \n","GenTime":"2024-08-28 12:13:34"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"text":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n\n##Decision\nWe will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","MatchedIds":[4436,2409,1745,351,1739],"Prediction":"## Decision\n\nWe will **standardize on using environment variables for configuring Verify Service Provider** and discontinue the use of a separate YAML configuration file.\n\nThis decision is driven by the following factors:\n\n* **Simplification:** Eliminating the YAML configuration method removes complexity and provides a single, consistent approach.\n* **Alignment with 12-Factor App Methodology:**  Configuring via environment variables aligns with the principles of 12-factor applications, promoting portability and reducing inconsistencies across environments.\n* **Improved Security:** Storing configuration within the application code (even within a YAML file) can present security risks. Environment variables offer a more secure mechanism for managing sensitive information. \n\nWhile previous limitations with the Java buildpack and concerns about Cloud Foundry's static file management necessitated a dual approach, moving forward, we will prioritize:\n\n* **Exploring alternative solutions** within the Java buildpack for specifying configuration file paths, if needed.\n* **Re-evaluating Cloud Foundry's static file management** capabilities to determine if they now meet our requirements for reliability.\n\nThis decision promotes a cleaner, more secure, and easier-to-maintain configuration approach for Verify Service Provider. \n","GenTime":"2024-08-28 12:13:40"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"text":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n\n\n##Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","MatchedIds":[4761,472,3715,4747,282],"Prediction":"## Decision\n\nWe will use **Scala** for developing LaTiS. \n\n**Reasoning:** Scala's hybrid nature, blending object-oriented and functional paradigms, directly addresses many of our decision drivers:\n\n* **Merits of Functional Programming:**  Scala's functional capabilities enable code that is concise, expressive, and promotes immutability, aligning with our need for code correctness and reasoned data transformations. \n* **Merits of Strongly Typed Languages:** Scala's strong typing system enhances code safety, reduces runtime errors, and improves overall code quality, crucial for a scientific computing project.\n* **Suitability for Processing Large Data Sets:** Scala's integration with the JVM and its support for concurrency primitives make it well-suited for handling the large datasets inherent to LaTiS.\n* **Support for DSLs:** Scala's concise syntax and powerful features empower us to develop expressive DSLs tailored for specific scientific domains, improving code readability and maintainability.\n\nWhile the availability of Scala developers might be less than for more established languages, the benefits outlined above, coupled with the growing community of scientific programmers adopting Scala, make it the most suitable choice for LaTiS. \n","GenTime":"2024-08-28 12:13:48"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"text":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n\n\n##Decision\nWe are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","MatchedIds":[2703,1193,1614,2236,2725],"Prediction":"## Decision\n\nWe will rename our default branch from `master` to `main` across all repositories\nin the cloudfoundry GitHub organisation. This change will be enacted\nrepository-by-repository. For repositories where this change could be\ndisruptive to downstream users (e.g. cf-deployment), we will continue using the\nname `master` for the default branch. \n","GenTime":"2024-08-28 12:13:52"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"text":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n\n\n##Decision\nWe keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","MatchedIds":[4931,1122,138,3203,1879],"Prediction":"## Decision\n\nWe will introduce a new data source that contains the municipal boundaries as they are reflected in the road segment data. This data source will be maintained and updated independently from the municipality registry to reflect the actual boundaries used for segmenting roads. This approach allows us to accurately determine the municipality of road segments even when they don't have associated streets and decouples the road segment data from potential inconsistencies or changes in the municipality registry. \n","GenTime":"2024-08-28 12:13:55"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"text":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[631,2539,1173,729,2968],"Prediction":"## Decision\n\n1. **Do not store ports as strings, even if the Kubernetes API allows it.**  Storing them as integers more accurately reflects their nature as network ports and avoids potential confusion or errors in processing. \n2. **Document this decision and the reasoning behind it.**  Clearly explain that while the Kubernetes API might offer flexibility, using strings for backend ports is inappropriate in this context because the External Service Operator relies on numerical port values. This clarity will guide future development and prevent regressions. \n","GenTime":"2024-08-28 12:13:59"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"text":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n\n\n##Decision\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","MatchedIds":[577,4443,362,4166,2470],"Prediction":"## Decision\n\nTo simplify the authorization flow and improve user experience, we will adopt the following strategy:\n\n**1. Eliminate unnecessary password prompts:**\n\n- For RPs that **do not require encryption keys**, like Monitor and AMO, we will **no longer require users to re-enter their password** if they have an active session. \n- The existing session token will be sufficient to request new OAuth tokens for these services.\n\n**2. Clearly communicate password requirements:**\n\n- For RPs that **require encryption keys**, like Sync, Lockwise, and Send, we will **clearly communicate to the user why their password is needed**. \n- This communication should happen **before** the password prompt appears to minimize user confusion and frustration.\n\n**3. Address the state management bug:**\n\n- We will prioritize fixing the bug in the state management logic that causes unnecessary password prompts. \n- This fix will ensure a consistent and predictable experience for users, minimizing password prompts to only when absolutely necessary. \n\n**This approach prioritizes user happiness and improved sign-in rates by reducing friction in the authorization flow. By clearly communicating password requirements and eliminating unnecessary prompts, we aim to create a more user-friendly and efficient experience.** \n","GenTime":"2024-08-28 12:14:06"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","MatchedIds":[4547,4910,4918,4544,4550],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nThis section is intentionally left blank. It is awaiting a context to be filled with a justified decision, its status, consequences, and other relevant information. \n","GenTime":"2024-08-28 12:14:10"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"text":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n\n##Decision\nWith this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","MatchedIds":[317,1462,929,3841,2403],"Prediction":"## Decision\n\nWe will move away from using the `DataMigrate` gem for running data migrations. Instead, we will implement a new approach with the following characteristics:\n\n* **Explicit Execution:** Data migrations will be separated from deployments and run as explicit steps, either manually or through a controlled automated process. This ensures visibility and control over data changes.\n\n* **Versioning and Tracking:** A robust versioning system for data migrations will be implemented (potentially using migration files like other frameworks). This allows for tracking which migrations have been applied and ensures they are run in the correct order.\n\n* **Rollback Capability:**  Migrations will be designed with rollback mechanisms whenever possible. This enables reverting data changes in case of errors or unintended consequences.\n\n* **Testing and Validation:** Data migrations will be thoroughly tested, including unit tests for individual migration steps and integration tests to validate their impact on the overall system.\n\nThis new approach will enhance the reliability, control, and maintainability of our data migration process, reducing the risk of unexpected issues during deployments. \n","GenTime":"2024-08-28 12:14:17"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"text":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n\n\n##Decision\nAs of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","MatchedIds":[1035,4644,428,311,2499],"Prediction":"## Decision\n\nWe will implement a REST API within the Spring Boot WebApp to enable communication with the simulator. \n\n### Positive Consequences\n* **Established communication:**  This creates a clear and standardized way for the simulator to interact with the WebApp.\n* **Flexibility:** REST APIs are language-agnostic, allowing future components written in different languages to interact with the WebApp.\n* **Leverages Spring Framework:**  Spring Boot provides excellent support for building RESTful services, simplifying development.\n\n### Negative Consequences\n* **Increased complexity:** Building and maintaining an API adds some overhead to the project.\n* **Potential for latency:**  Communication over HTTP introduces some latency compared to direct inter-process communication. \n\n### Additional Considerations\n* **API design:**  Careful consideration should be given to designing a clean, well-documented, and secure API.\n* **Authentication\/Authorization:** Security measures should be implemented to control access to the API.\n* **Performance testing:**  The API should be thoroughly tested under load to ensure it meets performance requirements.\n\nThis approach provides a practical way to bridge the gap between the simulator and the WebApp, leveraging existing technologies and offering flexibility for future development. \n","GenTime":"2024-08-28 12:14:26"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"text":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n\n\n##Decision\nI decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","MatchedIds":[1576,1240,1931,334,2290],"Prediction":"## Decision\nThis project will be built using React, Redux, and TypeScript.  Node, NPM, and Webpack will be used for the development and build toolchain. This combination of technologies represents a popular and modern web development stack, offering a good opportunity to gain practical experience. \n","GenTime":"2024-08-28 12:14:29"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"text":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n\n\n##Decision\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","MatchedIds":[801,5147,1747,4804,3820],"Prediction":"## Decision\n\nWhile using Mpc\/h offers flexibility in handling the uncertain value of h, we prioritize consistency and minimizing user surprise. \n\nTherefore, **we will adopt the Mpc convention (i.e., without the \/h factor) as the standard unit for distances**. This aligns with the default behavior of astropy, a widely used astronomy package.\n\nUsing a consistent unit convention will:\n\n- **Improve code readability** by avoiding potential confusion arising from mixing different unit conventions.\n- **Simplify integration with astropy** and potentially other libraries or datasets that adhere to the same standard. \n\nTo address the need for flexibility in propagating results across different h values, we will implement clear guidelines and helper functions for:\n\n- **Explicitly stating the assumed h value** within specific analysis contexts.\n- **Converting between Mpc and Mpc\/h units** as needed.\n\nThis approach balances the benefits of both conventions while promoting clarity and interoperability within the project.\n","GenTime":"2024-08-28 12:14:34"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"text":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n\n##Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","MatchedIds":[2397,2261,1480,3890,4021],"Prediction":"## Decision\n\nWe will implement RBAC using a combination of JSON Web Tokens (JWT) for storing role information and a policy-based authorization framework within the application. \n\n**Authentication:**\n\n* Users will authenticate using a secure authentication system (e.g., OAuth 2.0, SAML).\n* Upon successful authentication, the system will generate a JWT containing the user's ID and assigned roles.\n* The JWT will be signed and transmitted to the client application.\n\n**Authorization:**\n\n* The client application will include the JWT in every request to the server.\n* A middleware layer on the server will validate the JWT and extract the user's roles.\n* A policy-based authorization framework will be implemented, defining policies that map roles to specific actions and resources.\n* Before executing any request, the framework will verify if the user's roles grant access according to the defined policies.\n\n**Role Storage and Management:**\n\n* User roles will be stored and managed within the application's database.\n* An administrative interface will be provided for managing roles, assigning permissions, and associating users with roles.\n\n**Team Structure:**\n\n* A dedicated team, consisting of developers and security specialists, will be responsible for:\n    * Defining roles and permissions based on the principle of least privilege.\n    * Implementing and maintaining the authorization framework.\n    * Managing user roles and ensuring the security of the RBAC system. \n\nThis approach provides a flexible and secure solution for implementing RBAC in the application. It leverages industry standards like JWT and decouples authorization logic from the application code, making it easier to manage and maintain. \n","GenTime":"2024-08-28 12:14:43"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"text":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n\n\n##Decision\nThe Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","MatchedIds":[2397,5024,1173,469,4358],"Prediction":"## Decision\n\nWe will create a custom Django management command to create a superuser. This command can be executed either once in a deployment script or conditionally based on the lack of an existing superuser.\n\nThis approach offers several benefits:\n\n- **Production-Safe:** Avoids reliance on the Django CLI in production environments.\n- **Flexible:**  Allows for customization such as setting specific default values for the superuser.\n- **Auditable:**  Changes made through a management command are logged, improving security and accountability.\n\nFor added security, consider:\n\n- **Environment Variables:**  Pass sensitive information like the superuser's password through environment variables rather than hardcoding them in the command.\n- **Conditional Execution:**  Ensure the command only creates a superuser if one doesn't already exist to prevent accidental overwrites. \n\nThis approach ensures a robust and secure way to create the initial superuser in a production-ready manner. \n","GenTime":"2024-08-28 12:14:49"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"text":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n\n\n##Decision\nWe decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","MatchedIds":[4358,3416,3831,4847,2471],"Prediction":"## Decision\n\n### Promotion-based Deployment with Unique Identification\n\nWe will use a promotion-based deployment strategy, utilizing S3 object tags to manage artifact versions and environments. \n\n1. **Unique Artifact Identification:**  Upon successful creation of a binary artifact in the **Deploy_App** Jenkins job, we will tag the artifact in the S3 bucket (e.g., `govuk-integration-artefact`) with:\n    *  `artifact-version`: A unique identifier, like a timestamp or build number (e.g., `build-1678892231`). \n    *  `environment`: The environment this artifact is associated with (e.g., `integration`).\n\n2. **Environment-Specific Promotion:**\n   *  When deploying to Staging, the deployment job will look for artifacts in the `govuk-integration-artefact` bucket with the tag `environment=integration`. It will select the artifact with the latest `artifact-version` for deployment.\n   *  Upon successful deployment to Staging, the artifact's tags in S3 will be updated to include `environment=staging`.\n\n3. **Automated Cleanup (Optional):**  Implement a lifecycle policy on the S3 bucket to automatically delete older artifacts based on their `artifact-version` tag, preventing unnecessary storage costs. \n\n### Benefits:\n\n* **Clear Versioning:**  Provides a clear audit trail of which artifact version is deployed to each environment.\n* **Rollback Capability:**  Facilitates easy rollbacks by re-deploying a previously successful artifact identified by its `artifact-version` tag. \n* **Environment Isolation:**  Ensures that artifacts are explicitly promoted between environments, preventing accidental deployments of untested versions. \n\n### Implementation Notes:\n\n*  Jenkins pipelines should be updated to manage S3 tagging during build and deployment processes.\n*  Consider using infrastructure-as-code tools (like Terraform) to manage the S3 bucket lifecycle policy. \n","GenTime":"2024-08-28 12:14:59"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"text":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n\n\n##Decision\nThese specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","MatchedIds":[1614,4600,5102,2703,697],"Prediction":"## Decision\n\nTo address the dissatisfaction with our current PR flow and improve the size, speed, and quality of PRs, we will adopt the following changes:\n\n**1. Enforce Smaller PRs:**\n\n* We will **require** PRs to be focused on a single, well-defined change. Large PRs will be rejected and split into smaller, manageable units.\n* To facilitate this, we will encourage the use of feature flags and branch-based development for larger features, allowing incremental merging of smaller, functional parts.\n* Developers are encouraged to actively break down large tasks into smaller, independent subtasks and open separate PRs for each.\n\n**2. Improve PR Description Template:**\n\n* We will revise the existing PR template or introduce a new one to explicitly prompt for the following:\n    * A clear and concise description of the change being made.\n    * Steps to reproduce the issue (if applicable).\n    * Screenshots\/videos of the change (for UI changes).\n    * Testing instructions for reviewers.\n* The template should emphasize the importance of providing all necessary information for reviewers to understand and assess the change effectively.\n\n**3. Streamline Review Process:**\n\n* We will establish clear guidelines for reviewers to ensure timely and consistent feedback.\n* This includes setting expectations for review turnaround time and defining the level of detail expected in feedback.\n* We will investigate and implement tools or integrations to automate parts of the review process, such as linting, formatting checks, and basic automated testing.\n\n**4. Enhance Communication and Collaboration:**\n\n* We will encourage early and frequent communication between developers, QA, and design throughout the development process, not just during PR reviews.\n* This can be achieved through regular meetings, dedicated communication channels, or collaborative tools for discussing features and designs before and during development.\n* We will promote a culture of constructive feedback and encourage open discussions about technical decisions and potential improvements to the PR process.\n\n**Implementation:**\n\n* These changes will be rolled out incrementally, starting with the most impactful ones based on team feedback.\n* The effectiveness of these changes will be continuously evaluated and adjusted as needed based on data and feedback gathered from the team.\n* We will provide clear documentation and training to support the team in adapting to the new process.\n\n**This decision aims to create a more efficient, transparent, and collaborative PR process that leads to higher quality code and faster integration times.**\n","GenTime":"2024-08-28 12:15:11"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"text":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n\n##Decision\nThe option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","MatchedIds":[334,4618,802,4609,1193],"Prediction":"## Decision\n\nWe have decided to proceed with updating the UI repository to React 17 and `react-semantic-ui` to its latest version, accepting the removal of the `Responsive` component.  \n\n### Reasons\n\n* **Maintainability:** Keeping up-to-date with major libraries like React is crucial for long-term maintainability and security. It also grants access to the latest features and performance improvements.\n* **Consistency:**  Having a single, consistent React version across all apps and libraries simplifies development, reduces potential conflicts, and streamlines the integration process.\n* **Alternatives:** While the removal of `Responsive` is a breaking change, `react-semantic-ui` offers hooks for media queries, providing a more modern and versatile solution. We will adopt these hooks as the recommended replacement.\n\n### Mitigation Plan for Removing `Responsive`\n\n1. **Documentation:**  Clearly document the deprecation of the `Responsive` component and provide detailed migration instructions using the new `react-semantic-ui` hooks for media queries.\n2. **Codemods:** Investigate creating codemods to automate the replacement of the `Responsive` component where possible, simplifying the transition for developers.\n3. **Gradual Rollout:** Encourage teams to update their dependencies and migrate away from the deprecated component. A reasonable timeline will be provided for this transition period.\n\n### Justification for Choosing `react-semantic-ui` Hooks over `@artsy\/fresnel`\n\nWhile `@artsy\/fresnel` offers a solution, `react-semantic-ui`'s built-in hooks for media queries are a better fit for our existing ecosystem:\n\n* **Integration:**  As we are already using `react-semantic-ui`, leveraging its built-in hooks reduces the need for additional dependencies and potential conflicts.\n* **Flexibility:** The hooks offer a more granular control over rendering logic compared to a global context provider. \n* **Familiarity:** Developers already familiar with `react-semantic-ui` will find it easier to adapt to the new hooks.\n\nThe SSR limitations of `react-semantic-ui` will be addressed if they pose significant challenges during implementation. \n","GenTime":"2024-08-28 12:15:22"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"text":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n\n\n##Decision\nWe will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","MatchedIds":[2696,3972,4761,2756,2183],"Prediction":"## Decision\n\nWhile the core functionality of fetching and executing instructions remains consistent across these use cases, the variations in control flow and user interaction suggest a need for specialization. We will model a **BaseProcessor** class to encapsulate the core instruction execution logic.  Different application modes (classic VM, REPL, Debug) will then be implemented as specialized subclasses, inheriting from **BaseProcessor**:\n\n* **ClassicVMProcessor:**  Handles loading an entire program and executing it until termination. \n* **REPLProcessor:**  Manages the interactive loop, fetching and executing single instructions while providing input and output mechanisms.\n* **DebugProcessor:** Extends the execution cycle to support stepping, breakpoints, and inspection of program state.\n\nThis approach adheres to the Domain-Driven Design principle of reflecting the business domain in the code structure. The variations in program execution are treated as specific implementations of the more general \"running programs\" business rule. \n","GenTime":"2024-08-28 12:15:27"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"text":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n\n\n##Decision\nFor the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","MatchedIds":[1652,4271,3062,3203,4761],"Prediction":"## Decision\n\nWe will support the latest two major versions of PHP. \n\nThis approach balances accessibility with maintainability by:\n\n* **Accessibility:** Supporting the latest two versions ensures compatibility for a large portion of PHP users, as developers are generally encouraged to stay up-to-date with security and performance improvements. \n* **Maintainability:**  Limiting support to two versions reduces the testing burden and prevents the need for complex workarounds for outdated features or deprecated functionality in older PHP versions.\n\nWe will clearly document the minimum required PHP version and any dependencies on specific features. This allows developers to make informed decisions about using the library and avoids potential compatibility issues.\n\nWe will periodically re-evaluate the supported PHP versions based on usage statistics and the release of new PHP versions. This ensures that we continue to support a relevant range of PHP versions while keeping maintenance costs manageable. \n","GenTime":"2024-08-28 12:15:33"}
{"File Name":"PIMS\/database.md","Context":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"text":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n\n##Decision\nOriginally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","MatchedIds":[2520,699,2393,4096,2968],"Prediction":"## Decision\nChosen option: **PostgreSQL**, because:\n\n* **Relational Database:** PostgreSQL is a robust, open-source relational database management system (RDBMS) that excels at handling relational data and enforcing constraints, meeting the core requirements.\n* **Docker Support:** PostgreSQL has excellent official Docker images readily available, simplifying deployment within a Linux docker container on OpenShift.\n* **Entity Framework Core Compatibility:**  PostgreSQL is fully supported by Entity Framework Core 3.1, ensuring seamless integration with the application's data access layer.\n* **OpenShift Compatibility:** PostgreSQL runs smoothly on OpenShift, aligning with the existing infrastructure.\n\n### Positive Consequences\n* **Data Integrity:** PostgreSQL's constraint enforcement ensures the accuracy and reliability of property information.\n* **Scalability and Performance:** PostgreSQL is known for its ability to handle large datasets and deliver good performance, essential for a system like PIMS. \n* **Open Source and Cost-Effective:**  Leveraging PostgreSQL's open-source nature can potentially reduce licensing costs compared to some commercial database options.\n\n### Negative Consequences\n* **Operational Expertise:**  Depending on the team's existing database skills, adopting PostgreSQL might require some additional training or expertise. However, the widespread use and excellent documentation of PostgreSQL mitigate this to a large extent. \n","GenTime":"2024-08-28 12:15:40"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"text":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n\n\n##Decision\n- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","MatchedIds":[1712,2824,4859,3253,1701],"Prediction":"## Decision\n\n### Change in Sync Scope\n\nTo improve performance and address the issue of large databases slowing down the app, we will change the scope of data synchronization from the entire district to the individual block level. \n\n**Here\u2019s how it will work:**\n\n1. **User Selection:** Upon login, the user will be prompted to select the block they are primarily operating in. \n2. **Data Sync:** The app will only synchronize patient data belonging to the selected block. \n3. **Block Switching:** Users can switch to a different block, prompting a new data sync for that block.\n\n### Handling Inter-Block Patient Visits:\n\nTo accommodate the small percentage of patients who may visit facilities across blocks, we will implement the following:\n\n1. **Search Functionality Enhancement:** Implement a district-wide search functionality that can fetch patient information from the server in real-time. This will allow healthcare workers to access data for patients outside their selected block if needed.\n2. **Data Caching:** Cache search results locally to reduce the need for repeated server requests for the same patient.\n3. **Offline Mode Limitation:**  Clearly communicate to users that the offline mode will only contain data for their selected block. \n\n### Exceptions:\n\nFor the districts of Sindhudurg, Hoshiarpur, and Bathinda, where inter-block patient movement is higher, we will explore alternative solutions:\n\n* **Option 1:** Allow users to select multiple blocks during setup, with a limit to prevent excessive data syncing. \n* **Option 2:** Implement a more intelligent, dynamic syncing system that identifies and prioritizes data based on user activity and patient movement patterns.\n\n###  Benefits of this approach:\n\n* **Improved App Performance:**  Significantly reduce the database size on user devices, resulting in faster app loading, smoother navigation, and overall improved responsiveness. \n* **Reduced Data Usage:**  Syncing only the necessary data will minimize data consumption for users, especially in areas with limited connectivity.\n* **Scalability:**  This approach allows the app to scale effectively even with increasing numbers of patients and larger districts.\n\n### Next Steps:\n\n1. **Prototype & Testing:** Develop a prototype of the block-level syncing system and conduct thorough testing with users in both large and small districts. \n2. **Rollout & Monitoring:**  Release the feature in phases, closely monitoring its impact on app performance and user experience. Gather user feedback to identify and address any issues. \n3. **Exception Handling:** Implement and test the chosen solution for districts with higher inter-block patient movement. \n","GenTime":"2024-08-28 12:15:55"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"text":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n\n\n##Decision\nDelivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","MatchedIds":[3372,276,3831,4741,4738],"Prediction":"## Decision\n\n**Chosen option:** Implement a dedicated \"Composite Asset Processor\" microservice within the DLCS ecosystem.\n\n**Reasoning:**\n\n* **Separation of Concerns:** Handling composite assets introduces unique complexities best managed in isolation. A dedicated microservice allows for specialized logic and dependencies without impacting the core DLCS functionalities.\n* **Scalability and Resilience:** Processing composite assets may be resource-intensive. A separate microservice can be independently scaled and managed for optimal performance and fault tolerance.\n* **Maintainability and Extensibility:**  Future adaptations for new composite formats or processing requirements are simplified within a focused codebase, minimizing regression risks in other DLCS components. \n\n**Implementation Details:**\n\n1. **Ingestion:** The Composite Asset Processor will receive the composite artifact and metadata from the DLCS ingestion workflow.\n2. **Extraction:** The service will identify and extract individual embedded resources based on the composite format (e.g., PDF pages, TIFF directories).\n3. **Rasterization and Processing:** Each extracted resource will be rasterized if necessary and processed according to standard DLCS workflows for individual assets (e.g., thumbnail generation, metadata extraction).\n4. **Metadata Association:** The service will maintain the relationship between the original composite asset and the derived individual resources through appropriate metadata linking.\n5. **Storage and Delivery:** Individual resources will be stored within the DLCS repository, and the Composite Asset Processor will provide endpoints to access both the original composite and its derived assets.\n\n**Further Considerations:**\n\n* **Error Handling:** Robust mechanisms for error reporting and handling during extraction, rasterization, and processing are crucial.\n* **Format Support:** Prioritize support for commonly encountered composite formats initially, with plans for extensibility based on future needs.\n* **Performance Optimization:** Explore techniques for efficient extraction, parallel processing, and caching to minimize processing time for composite assets.\n","GenTime":"2024-08-28 12:16:05"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"text":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n\n\n##Decision\nWe will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","MatchedIds":[2997,4358,3440,631,4937],"Prediction":"## Decision\nFor customers who need a production-ready Kubernetes cluster on AWS and prefer a less technically demanding approach, we recommend **AWS Elastic Kubernetes Service (EKS)**. \n\nHere's why:\n\n* **Managed Service:** AWS manages the control plane (API server, etcd, etc.), reducing operational overhead and complexity.\n* **User Friendly:** EKS integrates seamlessly with the AWS console and CLI, providing an approachable interface for cluster creation and management.\n* **Scalability & Reliability:** EKS leverages AWS infrastructure for high availability and scalability, crucial for production environments.\n* **Integration:**  Being a native AWS service, EKS offers smooth integration with other AWS services, simplifying overall infrastructure management.\n\nWhile other options exist, EKS strikes a balance between ease of use and production readiness, making it suitable for a wider range of users. \n\n**Note:** We can provide guidance and documentation to further streamline the EKS adoption process for our customers. \n","GenTime":"2024-08-28 12:16:10"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"text":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n\n\n##Decision\nAfter playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","MatchedIds":[2273,452,4359,4223,3323],"Prediction":"## Decision\n\nWe will use **Cypress** as our primary integration test framework for this project. \n\nHere's why:\n\n* **Developer-friendly:** Cypress is known for its ease of use, excellent developer experience with convenient debugging tools, and strong documentation. This will lead to faster test creation and more maintainable tests in the long run.\n* **Modern Architecture:** Cypress is built specifically for modern JavaScript frameworks like Angular, Vue.js, and React.js, overcoming the challenges Selenium faces with SPAs.\n* **Fast Feedback Loop:** Cypress's architecture allows for fast test execution and direct access to the application's variables, leading to a quicker development feedback loop.\n\nWhile Selenium and TestCafe are valid options for cross-browser compatibility tests, the focus on developer productivity and Cypress's suitability for modern JavaScript applications make it a better fit for our primary integration testing needs. \n","GenTime":"2024-08-28 12:16:15"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"text":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n\n##Decision\n* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","MatchedIds":[3187,4096,1631,160,697],"Prediction":"## Decision\n\n**Chosen option:** Store each Jupyter Notebook with a dedicated dependency management file (e.g., `requirements.txt` for pip, `environment.yml` for conda) and a Dockerfile.\n\n### Justification\n\nThis approach addresses all decision drivers:\n\n* **User perspective:** Users can easily recreate the environment needed to run the notebook, regardless of their local setup. This reduces friction and improves the onboarding experience. \n* **Reproducibility:** By capturing both direct and transitive dependencies, we ensure consistent execution of the notebook across different machines and over time.\n* **Traceability:**  Dependency files and Dockerfiles provide a clear audit trail of the software used, aiding in troubleshooting and security analysis.\n\n### Positive Consequences \n\n* Improved collaboration and sharing of notebooks.\n* Increased confidence in results due to reproducible environments.\n* Streamlined deployment of notebooks as standalone components or services using Docker.\n\n### Negative consequences\n\n* Slightly increased overhead in maintaining dependency files and Dockerfiles.\n* Potential for dependency conflicts if not managed carefully.\n\n### Implementation Notes\n\n* Encourage the use of virtual environments within the Docker container to further isolate dependencies.\n* Investigate tools and workflows to automate the process of generating and updating dependency files and Dockerfiles (e.g., using `pip freeze`, `conda export`, or dedicated tools like `repo2docker`). \n* Provide clear documentation and examples to guide users on best practices for managing dependencies and building reproducible notebooks. \n","GenTime":"2024-08-28 12:16:23"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"text":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n\n\n##Decision\n`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","MatchedIds":[4281,3372,2183,2802,779],"Prediction":"## Decision\nWe will allow each `ml-aoi` Item to link to multiple label items using the STAC Item Links object. \n\nWhile limiting to a single label link offers the convenience of directly copying metadata and simplifying processing, it introduces limitations:\n\n* **Reduced Flexibility:**  Single label links restrict users from directly leveraging multi-class datasets and necessitate pre-processing steps to combine labels.\n* **Data Duplication:** Encouraging the creation of combined label items for multi-class scenarios can lead to redundant data, especially when referencing external label sources.\n\nAllowing multiple label links provides several advantages:\n\n* **Multi-class Support:** Directly supports training multi-class classifiers without intermediate processing.\n* **Flexibility:** Users can reference diverse label sources, including external ones, without modification.\n* **Reduced Duplication:** Minimizes data duplication by referencing existing labels.\n\nTo address the challenges of consuming multi-label `ml-aoi` items, we will introduce a **meta-label structure** within the `ml-aoi` Item. This structure will aggregate and expose essential metadata from all linked label items, making it easy for consumers to understand the label structure without crawling the entire catalog. This approach balances flexibility with usability, ensuring that `ml-aoi` items can effectively support complex use cases. \n","GenTime":"2024-08-28 12:16:31"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"text":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n\n\n##Decision\n* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","MatchedIds":[4626,4618,4609,311,1495],"Prediction":"## Decision\n\nWe will implement a hybrid approach combining the advantages of the current pull-based system with a new optional push-based mechanism:\n\n**1. Enhanced Pull Mechanism:**\n\n* **Server categorization:** Maintain the distinction between listed (public) and unlisted (private\/staging) servers in the DAO. \n* **Tiered Synchronization:** Introduce server tiers based on configuration capabilities (e.g., handling large scenes). Listed servers will prioritize pulling from higher tiers for scenes exceeding their limitations. \n* **Optional Propagation:**  Land owners can mark scenes as \"public\" or \"private\" during deployment. Public scenes will propagate across listed servers based on the tiered synchronization. Private scenes remain on the designated server (listed or unlisted) and can be selectively shared.\n\n**2. Optional Push Mechanism:**\n\n* **Targeted Broadcasting:** Introduce an optional push mechanism for specific use cases like local development or controlled testing environments. \n* **Authentication & Authorization:** Implement strict authentication and authorization mechanisms for initiating push operations to prevent unauthorized changes on listed servers.\n* **Selective Syncing:** Allow for granular control over which data to push, enabling efficient updates of specific scene elements or configurations.\n\n**Reasoning:**\n\n* **Backward Compatibility:** Preserves the existing infrastructure and functionality for listed servers, minimizing disruption.\n* **Flexibility:** Offers a tiered approach for scalability, accommodating different server capabilities and scene complexities.\n* **Controlled Exposure:** Empowers content creators with control over the reach and visibility of their work.\n* **Enhanced Development Workflow:** Enables efficient local development and staging environments through targeted push updates.\n\n**Future Considerations:**\n\n* Implement a monitoring system to track server load and scene propagation across tiers.\n* Explore decentralized synchronization mechanisms for improved redundancy and fault tolerance.\n* Develop tools and documentation for easy configuration and management of the hybrid synchronization system. \n","GenTime":"2024-08-28 12:16:41"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"text":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\n\n##Decision\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","MatchedIds":[2755,1610,3126,3715,2198],"Prediction":"## Decision\n\nWe will use **Unix timestamps (number of milliseconds since 1970-01-01 UTC)** for all date\/time values returned by the REST API.\n\n## Reasons\n\n* **Simplified client-side calculations:** The Elm frontend, which heavily relies on date\/time manipulation for rendering waiting times and other time-sensitive information, will benefit from receiving timestamps as readily calculable numbers. This eliminates the need for parsing ISO8601 strings into date\/time objects within the Elm application.\n\n* **Alignment with Elm ecosystem:** Utilizing Unix timestamps promotes consistency and aligns with the preferred approach for handling date and time in the Elm ecosystem.\n\n* **Conversion feasibility:** While ISO8601 offers human-readable formatting, converting between Unix timestamps and ISO8601 strings is straightforward and can be achieved with readily available libraries in both frontend and backend environments if needed.\n\n* **Reduced payload size:** Representing timestamps as numbers can potentially lead to smaller payload sizes compared to verbose ISO8601 strings, although the impact may be minimal in most cases.\n\n## Considerations\n\n* **Readability:** While less human-readable than ISO8601, Unix timestamps can be easily converted for logging and debugging purposes on the backend. \n\n* **Potential for confusion:**  Clear documentation and communication within the development team are crucial to ensure everyone understands the chosen representation for date\/time values and uses appropriate conversion methods when necessary.\n","GenTime":"2024-08-28 12:16:57"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"text":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n\n\n##Decision\nWe will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","MatchedIds":[152,118,1741,187,2393],"Prediction":"## Decision\n\nThe decision is to migrate CI\/CD pipelines for Libero products from Travis CI to GitHub Actions. \n\n**Reasons:**\n\n* **Future-Proofing:** Travis CI's acquisition and uncertain future present potential risks of service disruptions, pricing changes, or feature stagnation. Migrating to GitHub Actions, a mature and well-supported GitHub-native solution, mitigates these risks.\n* **Performance & Scalability:**  GitHub Actions' per-repository concurrency limits offer improved performance compared to the limitations encountered with the free Travis CI plan. This ensures faster build times and smoother development workflows.\n* **Integration & Ease of Use:** As a GitHub-native tool, GitHub Actions offers seamless integration with existing repositories, simplifying setup and management. This tight integration streamlines the development process.\n\n**Migration Plan:**\n\n*  A phased approach will be adopted, starting with a pilot project to understand and address any migration challenges. \n*  Documentation and guidelines will be provided to assist developers in migrating their projects to GitHub Actions.\n*  We will evaluate the need for any custom Actions and develop them accordingly.\n\nThis migration will ensure a more robust, performant, and future-proof CI\/CD process for Libero products.\n","GenTime":"2024-08-28 12:17:03"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"text":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n\n##Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","MatchedIds":[1462,3221,929,3657,2396],"Prediction":"## Decision\n\nTo ensure zero downtime deployments while changing the database schema, we will adopt the following strategies:\n\n* **Backward compatible migrations:** All database changes will be implemented in a backward-compatible manner. This means that the application should continue to function correctly with both the old and new schema versions present. \n* **Dual deployments with feature toggles:** New features requiring database changes will be deployed behind feature toggles. This allows deploying the application with the updated schema, but with the new features deactivated. Once the deployment is successful, the feature toggle can be enabled, activating the new functionality.\n* **Rolling updates for schema changes:** For larger schema alterations, we will employ rolling updates. This involves gradually updating the database schema across the production environment while minimizing downtime.  This might involve techniques like:\n    * Updating a subset of application instances to use the new schema.\n    * Routing traffic only to instances with the updated schema.\n    * Gradually updating remaining instances. \n* **Thorough testing:** All database migrations will undergo rigorous testing in staging environments that closely mirror production.  This will involve:\n    * Testing backward compatibility with previous application versions.\n    * Validating data integrity after the migration is applied.\n    * Performance testing under load to identify potential bottlenecks. \n\nBy combining backward-compatible migrations, feature toggles, rolling updates, and thorough testing, we can minimize risks associated with database changes and achieve zero downtime deployments.\n","GenTime":"2024-08-28 12:17:15"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"text":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","MatchedIds":[13,1180,2968,578,3295],"Prediction":"## Decision\n\nBased on the provided context, which outlines existing implementations, secret types, and required functionalities, we propose the following decisions for a unified approach to secret management:\n\n**1. Only Exclusive Secret Stores:**\n\nWe will **only** support service-exclusive secret stores. This simplifies the design and avoids potential conflicts or security vulnerabilities stemming from shared access. \n\n**2. Abstraction Interface:**\n\nA new abstraction interface (`SecretProvider`) will be introduced to provide a consistent way to interact with different secret stores. This interface will offer methods for:\n\n* `GetSecret(string) ([]byte, error)`: Retrieves a secret by its name.\n* `SetSecret(string, []byte) error`: Stores a secret by its name.\n* `DeleteSecret(string) error`: Removes a secret by its name.\n* `ListSecretNames(string) ([]string, error)`: Lists all secret names (optional).\n\n**3. Implementation:**\n\n* **Factory Method and Bootstrap Handler:**  \n  A factory method (`NewSecretProvider(ctx context.Context, config Config) (SecretProvider, error)`) will be responsible for creating concrete implementations of the `SecretProvider` interface based on the provided configuration. This factory method will be invoked during the bootstrap process of each service.\n\n* **Caching of Secrets:** \n  To reduce the load on the underlying secret stores and improve performance, a caching layer will be implemented within the `SecretProvider` abstraction. This cache will store secrets retrieved during the application's lifecycle, adhering to appropriate security measures for sensitive data in memory. \n\n* **Insecure Secrets:** \n  For development and testing purposes, a special `InsecureSecrets` configuration section will be introduced. This section will allow developers to specify secrets directly within the service configuration. These secrets will be loaded into a dedicated in-memory store that adheres to the `SecretProvider` interface.\n\n**4. Handling on-the-fly changes to `InsecureSecrets`:**\n\nFor development environments, it's beneficial to allow dynamic reloading of `InsecureSecrets` without service restarts. We will implement a mechanism (e.g., using a file watcher or an environment variable trigger) to reload these secrets upon modification.\n\n**5. Mocks:**\n\nTo facilitate testing, we will provide mocking utilities for the `SecretProvider` interface. This will allow developers to test their code without relying on actual secret stores.\n\n**6. Where will `SecretProvider` reside?**\n\nThe `SecretProvider` interface and its concrete implementations will reside in a dedicated package (e.g., `pkg\/secrets`) within the codebase. This promotes reusability and maintainability across different services.\n\n**7. Go Services:**\n\nGo services will interact with secrets exclusively through the `SecretProvider` interface. They will retrieve the concrete instance of the provider during the bootstrap process.\n\n**8. C Device Service:**\n\nThe existing mechanism for handling secrets in the C Device Service will be adapted to utilize the `SecretProvider` interface. A lightweight wrapper will be created to bridge the C code with the Go-based interface.\n\n## Consequences:\n\n* **Increased Security:** By using service-exclusive secret stores and a secure caching mechanism, we enhance the overall security posture of the system.\n* **Improved Developer Experience:** The abstraction layer simplifies secret management for developers and provides a consistent experience across different services and environments.\n* **Enhanced Testability:** Mocking capabilities streamline unit testing and ensure code quality.\n* **Flexibility:** The factory method allows easy integration with various secret stores, providing flexibility for future requirements.\n* **Centralized Secret Management:** This approach paves the way for a potential future transition to a centralized secret management solution.\n\nThis unified approach to secret management improves security, maintainability, and developer experience by providing a consistent and adaptable solution across different services and environments. \n","GenTime":"2024-08-28 12:17:34"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"text":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n\n\n##Decision\nWe will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","MatchedIds":[2207,3802,3470,2763,1823],"Prediction":"## Decision\n\nGiven that SEO is not a primary concern and the application is already developed, we will **not refactor the existing URLs** at this time. Consistency is desirable, but the effort involved in changing the existing URLs and potentially breaking existing links outweighs the benefits. \n\nHowever, we will **choose one style for all future URLs** to maintain consistency going forward. We will use the **\"action-sentence\" style**, as it is generally considered more user-friendly and self-explanatory.  \n\nThis decision balances the need for consistency with the pragmatic consideration of development effort and potential disruption. \n","GenTime":"2024-08-28 12:17:39"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"text":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n\n\n##Decision\n[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","MatchedIds":[1680,4940,3827,5259,3428],"Prediction":"## Decision\n\nGiven the increasing complexity of our JavaScript codebase after migrating to AngularJS, we decided to implement a build process using **Grunt**. This decision was made because:\n\n* **Consolidation**: Grunt, as a task runner, allows us to bundle and minify our growing number of JavaScript files, simplifying the inclusion of these files in our `index.html` and improving page load times. \n* **Popularity**: At the time of development, Grunt was a widely adopted and well-documented task runner with a robust ecosystem of plugins, making it easy to find solutions for our specific bundling needs. \n\nWhile Gulp was also considered as a viable option, the familiarity and widespread use of Grunt within the development community led us to choose it for this project. \n","GenTime":"2024-08-28 12:17:44"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"text":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n\n\n##Decision\nThe approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","MatchedIds":[1416,3853,539,2626,5070],"Prediction":"## Decision\n\nWe will adopt a modular architecture based on well-defined interfaces. Each package will expose its functionality through a public API, abstracting its internal implementation details. This approach ensures:\n\n* **Extendability:** New functionality can be added as separate packages implementing the defined interfaces without modifying existing code. \n* **Interchangeability:** Different implementations of the same functionality can be easily swapped by adhering to the interface contract.\n* **Minimal Interference:**  Changes within a package will be isolated to its own implementation and won't impact other packages as long as the public API remains consistent.\n\nThis guide should include:\n\n* **Interface-oriented design principles:**  Emphasize defining clear and concise interfaces for each package.\n* **Best practices for API design:**  Document guidelines on versioning, error handling, and documentation to ensure a consistent and user-friendly API surface.\n* **Package organization and naming conventions:** Establish clear rules for naming, structuring, and documenting packages to ensure consistency and maintainability.\n\nBy following these principles, we can create a flexible and maintainable codebase that allows for future growth and adaptation.\n","GenTime":"2024-08-28 12:18:00"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"text":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n\n\n##Decision\nWe decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","MatchedIds":[2240,49,2409,856,631],"Prediction":"## Decision\n\nWe will migrate the ownership of the Istio installation from cf-for-k8s to cf-k8s-networking. This will involve:\n\n* **Creating a new Helm chart:** This chart will be maintained by the cf-k8s-networking team and will be responsible for deploying and configuring Istio in a way that meets the needs of Cloud Foundry. \n* **Updating deployment processes:** The cf-for-k8s deployment process will be updated to remove the Istio installation and instead use the new Helm chart managed by cf-k8s-networking. \n* **Documentation:** We will update the documentation to reflect the new ownership of Istio and provide guidance on how to configure Istio for use with Cloud Foundry. \n\nThis approach will give the cf-k8s-networking team more control over Istio and allow them to easily make changes to the Istio configuration without having to go through the cf-for-k8s release process. This will enable faster iteration and development of networking features for Cloud Foundry. \n","GenTime":"2024-08-28 12:18:07"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"text":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n\n\n##Decision\nCode ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","MatchedIds":[1417,1430,2725,1614,3554],"Prediction":"## Decision\n\nTo ensure the long-term health and maintainability of Home Assistant integrations, we will adopt the following policies:\n\n**1. Encourage Code Ownership:**\n\n* **Contributor Responsibility:** Contributors are strongly encouraged to add themselves as code owners to integrations they contribute to, signifying their willingness to share responsibility for their code's maintenance. \n* **Reviewer Guidance:** Pull Request reviewers should actively remind contributors to add themselves as code owners, especially for substantial contributions or new integrations. \n\n**2. Foster Shared Ownership:**\n\n* **Multiple Code Owners:** Integrations should strive to have at least two code owners, ideally with diverse areas of expertise, to prevent bottlenecks and knowledge silos. \n* **Active Recruitment:**  For integrations with a single code owner or none, the Home Assistant team will proactively reach out to active contributors and encourage them to share ownership responsibilities.\n\n**3. Clarify Code Ownership Definition:**\n\n* **Documentation:** Update contributor documentation to clearly define \"code owner\" as someone who actively takes responsibility for an integration's maintenance, including reviewing code, addressing issues, and ensuring its continued functionality. This clarifies the role beyond simply acknowledging contribution.\n* **Communication:** Regularly emphasize the shared responsibility model of code ownership through various communication channels (e.g., developer meetings, forum posts) to foster a collaborative maintenance culture.\n\n**4. Address Existing Gaps:**\n\n* **Code Owner Audit:** Conduct an audit of existing integrations to identify those lacking sufficient code owners. Proactively reach out to potential candidates to ensure these integrations have adequate support.\n\nBy adopting these measures, we aim to build a more robust and sustainable integration ecosystem within Home Assistant, ensuring timely maintenance, fostering a collaborative environment, and ultimately providing a better user experience. \n","GenTime":"2024-08-28 12:18:17"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"text":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n\n\n##Decision\nA new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","MatchedIds":[1393,4119,3637,2963,3635],"Prediction":"## Decision\n\nTo enable the AMF validator to display the file location of errors, we need a mechanism to associate nodes in the AMF model with their source file location. We can achieve this by:\n\n1. **Introducing Location Annotations:** During the parsing stage, we will annotate each node in the AMF model with its corresponding location in the source file. This annotation will include:\n    * **File Name:** The name of the file where the node originates.\n    * **Line Number:** The specific line number where the node's definition starts.\n    * **Column Number:**  The column number in the specified line where the node's definition starts.\n\n2. **Preserving Annotations:**  The annotations will be preserved throughout the AMF processing pipeline, including transformations and manipulations of the model.\n\n3. **Utilizing Annotations in Validation:** The custom AMF validator will be updated to access and utilize these annotations.  When an error is detected, the validator will retrieve and include the associated location information (file, line, and column) in the error report. \n\n4. **Optional Annotation Removal:** In situations where these annotations are not desirable in the final output (e.g., for a minimized JSON-LD representation), we can introduce a dedicated processing step to remove them after validation.\n\nThis approach allows us to maintain the source file location context throughout the AMF processing lifecycle, enabling more informative and user-friendly error reporting in the custom validator. \n","GenTime":"2024-08-28 12:18:27"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"text":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n\n\n##Decision\nWe will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","MatchedIds":[3853,68,4179,1258,3572],"Prediction":"## Decision\n\nGiven the issues with JUnit thread reuse and the dangers of blindly calling `Thread.currentThread().interrupt()`, we will address SonarQube warnings related to RSPEC-2142 on a case-by-case basis.\n\n1. **Analyze the interrupted code:** For each instance of catching `InterruptedException`, carefully analyze the code to determine if the interruption truly needs to be propagated. \n    - If the interruption is relevant to the tested functionality, implement proper handling (e.g., cleaning up resources, setting flags) and potentially re-throw the exception or a relevant custom exception.\n    - If the interruption is not relevant and the test can safely continue or terminate, log the interruption for debugging purposes but avoid calling `Thread.currentThread().interrupt()`.\n2. **Suppress SonarQube warning (with justification):**  In cases where the interruption is deemed irrelevant and `Thread.currentThread().interrupt()` is not called, suppress the specific SonarQube warning with a clear justification comment explaining the reasoning and the potential consequences of propagating the interrupt. This ensures transparency and maintainability.\n\n### Status\n\nAccepted. \n\n**Note:** This approach prioritizes the stability and predictability of the test suite over strictly adhering to the SonarQube rule. It acknowledges the limitations of the rule in the context of JUnit's thread management. It also emphasizes the importance of code analysis and documentation to ensure the reasoning behind each decision is clear. \n","GenTime":"2024-08-28 12:18:35"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"text":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n\n##Decision\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","MatchedIds":[1575,786,3337,4909,861],"Prediction":"## Decision\n\nWe will use **MQTT** as the protocol and **EMQX** as the message broker. \n\n### Justification\n\n* **MQTT**: \n    * **IoT Focused**: MQTT is lightweight and designed for low-bandwidth, high-latency networks, making it ideal for IoT devices. \n    * **Low Resource Friendly**:  It has a small message header size, minimizing overhead. \n    * **Publish\/Subscribe**:  The publish\/subscribe model is well-suited for device-to-backend communication.\n* **EMQX**:\n    * **Scalability and Performance**: EMQX is a highly scalable and performant MQTT broker built for large-scale IoT deployments.\n    * **Kubernetes Native**: It offers seamless deployment on Kubernetes, simplifying management and scaling.\n    * **Stream Handling**: EMQX supports various plugins and extensions for handling large data streams, suitable for camera images.\n    * **Open Source and Enterprise Options**: Provides flexibility for future growth.\n\nWhile the team might not have direct experience with EMQX, its popularity in the IoT space and comprehensive documentation make it a manageable learning curve.\n\n### Positive Consequences\n\n* **Optimized for IoT**: The combination is tailored for IoT communication, ensuring efficient resource utilization on devices.\n* **Scalability**: EMQX allows the system to scale as the number of devices and data volume grow.\n* **Streamlined Development**: Utilizing a Kubernetes-native broker simplifies deployment and integration with existing infrastructure.\n\n### Negative Consequences\n\n* **Learning Curve**: The team will need to invest time in learning EMQX. However, the benefits outweigh this initial investment.\n\n### Alternatives Considered\n\n* **RabbitMQ with MQTT Plugin**: A viable option, but EMQX offers more specialized features for IoT and stream processing.\n* **Apache Kafka**: While powerful, Kafka might be overkill for this use case, especially considering the focus on low-resource devices.\n\nThis decision prioritizes a future-proof architecture that can handle the demands of an expanding IoT ecosystem. \n","GenTime":"2024-08-28 12:18:46"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"text":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n\n\n##Decision\nChosen option: Modify AdaptrisMessageListener to have callbacks.\\n","MatchedIds":[2791,2880,1799,2309,3295],"Prediction":"## Decision\n\nGiven the constraints of SQS requiring messages to remain on the queue for Dead Letter Queue functionality and the inability to rely on JMS exception handling for asynchronous feedback, we propose the following solution:\n\n**Implement a two-phase message processing with an interim status queue:**\n\n1. **Initial Processing and Status Change:**\n   - The `sqs-polling-consumer`, upon successfully reading a message from the source SQS queue, immediately sends a status update message to a dedicated \"status queue\". \n   - This status message contains the unique identifier of the original message and marks it as \"IN_PROGRESS\".\n   - The original message is then submitted to the workflow. \n   - Crucially, the original message is **not** deleted from the source queue at this point.\n\n2. **Asynchronous Callback and Finalization:**\n   - The workflow, upon successful completion, sends a message to the \"status queue\" marking the original message as \"PROCESSED\".\n   - A dedicated \"status listener\" monitors the \"status queue\". \n   - If the \"status listener\" receives a \"PROCESSED\" status:\n      -  The original message is deleted from the source queue.\n   - If the \"status listener\" doesn't receive a \"PROCESSED\" status within a defined timeout period (indicating workflow failure):\n      - No action is needed. SQS redrive policy will move the message to the Dead Letter Queue after the maximum retry attempts are exhausted.\n\n**Benefits:**\n\n- **Leverages SQS Redrive Policy:** This solution ensures messages are moved to the Dead Letter Queue when the workflow fails, relying on the built-in SQS mechanism.\n- **Avoids Message Loss:**  By not immediately deleting messages from the source queue, we prevent accidental message loss if the workflow fails before processing. \n- **Asynchronous and Decoupled:** The two-phase approach maintains the asynchronous nature of the system and avoids tight coupling between the consumer and the workflow.\n\n**Considerations:**\n\n- **Implementation Complexity:**  This approach introduces an additional \"status queue\" and requires careful handling of status messages and timeouts.\n- **Duplicate Message Handling:**  The \"status listener\" should be idempotent to handle potential duplicate status messages gracefully.\n\n**Overall, this solution provides a robust and reliable mechanism for handling workflow failures and leveraging the Dead Letter Queue functionality in SQS while respecting the constraints of the system.** \n","GenTime":"2024-08-28 12:18:59"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"text":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n\n\n##Decision\nReplace Standard with Prettier\\n","MatchedIds":[5255,1408,4226,3321,400],"Prediction":"## Decision\n\nGiven the conflict between Prettier and Standard, and considering Prettier's robust VS Code integration and widespread adoption, we will replace Standard with Prettier for code formatting. This decision promotes consistency and tooling simplicity across our projects. \n","GenTime":"2024-08-28 12:19:02"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"text":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n\n##Decision\nWe will build our own DHT implementation in Rust from scratch.\\n","MatchedIds":[1495,1500,1962,1982,3692],"Prediction":"## Decision\n\nHolochain will implement a custom distributed hash table (DHT) from the ground up in Rust. This decision is driven by the following factors:\n\n* **Language barrier:** The existing Go-based Kademlia implementation is not directly portable to the new Rust codebase.\n* **Holochain-specific requirements:** The existing Kademlia DHT has accumulated numerous Holochain-specific modifications, indicating a divergence from a standard implementation.\n* **Flexibility for future development:** A custom DHT will provide greater control and flexibility for implementing features such as the World Model, CRDT handling, graph properties via linking, and exploring alternative entry resilience and routing mechanisms.\n\nThis approach will allow Holochain to develop a DHT specifically tailored to its unique needs, paving the way for a more robust, efficient, and adaptable system in the long term. \n","GenTime":"2024-08-28 12:19:08"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n\n##Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","MatchedIds":[21,816,842,2181,1442],"Prediction":"## Decision\n\n### Queries\n\nThis ADR proposes a gRPC-first approach for queries in which every module exposes a query service. This design will dramatically improve the experience for gRPC clients and make it much easier to build flexible and performant clients without requiring code generation. Because we are using gRPC as the first-class mechanism to expose queries, protobuf naturally surfaces as a schema definition language for queries in addition to responses. \n\nThere are several advantages to defining a gRPC-first query interface for module developers:\n\n* **Schema Driven**: Defining messages for query requests and responses with protobuf allows us to have a well-defined and documented interface for client developers. Additionally, the strong typing offered by protobuf will make it much easier to write robust client applications.\n* **First-Class gRPC Gateway Support**: gRPC Gateway can be used to easily auto-generate REST routes based on gRPC services. Defining a gRPC-first interface will allow us to provide a consistent and well-defined REST API for every module.\n* **Streaming Support**: Unlike REST, gRPC allows for server-side streaming of responses. This could be particularly useful for queries that stream blocks or transactions, as it would allow us to easily implement these features in a performant and scalable way.\n\n#### Query Services\n\nEach module that requires an interface for reading data SHOULD define a `Query` service.  For example, the `x\/bank` module's query service definition could look as follows:\n\n```protobuf\n\/\/ bank.proto\npackage cosmos.bank.v1;\n\nservice Query {\n  \/\/ Returns the amount of coins held by an account at the given address.\n  rpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {}\n\n  \/\/ Returns all accounts that have a balance for any of the given denominations.\n  rpc AllBalances(QueryAllBalancesRequest) returns (QueryAllBalancesResponse) {}\n\n  \/\/ Returns the total amount of coins in circulation for all denominations.\n  rpc TotalSupply(QueryTotalSupplyRequest) returns (QueryTotalSupplyResponse) {}\n}\n\nmessage QueryBalanceRequest {\n  string address = 1;\n}\n\nmessage QueryBalanceResponse {\n  cosmos.bank.v1.Balance balance = 1;\n}\n\nmessage QueryAllBalancesRequest {\n  string address = 1;\n}\n\nmessage QueryAllBalancesResponse {\n  repeated cosmos.bank.v1.Balance balances = 1;\n}\n\nmessage QueryTotalSupplyRequest {\n}\n\nmessage QueryTotalSupplyResponse {\n  repeated cosmos.bank.v1.Balance supply = 1;\n}\n```\n\nThe messages defined in this proto file can then be reused by other protobuf messages, such as the message for the REST endpoint for a given gRPC service.\n\n#### Query Path Convention\n\nModules SHOULD define the query path for a particular service using the following convention:\n\n```\n\/cosmos.{module name}.v1.{query service name}\/{method name}\n```\n\nFor example, to query the balance of an account using the `x\/bank` module, the query path would be:\n\n```\n\/cosmos.bank.v1.Query\/Balance\n```\n\n#### gRPC Gateway\n\nREST endpoints SHOULD be generated from query services using gRPC Gateway using the query path convention above. For example, to expose the `Query` service defined above as REST routes, we would add the following to the `bank.proto` file:\n\n```protobuf\nimport \"google\/api\/annotations.proto\";\n\nservice Query {\n  \/\/ Returns the amount of coins held by an account at the given address.\n  rpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\n    option (google.api.http).get = \"\/cosmos\/bank\/v1\/balance\/{address}\";\n  }\n\n  \/\/ Returns all accounts that have a balance for any of the given denominations.\n  rpc AllBalances(QueryAllBalancesRequest) returns (QueryAllBalancesResponse) {\n    option (google.api.http).get = \"\/cosmos\/bank\/v1\/balances\/{address}\";\n  }\n\n  \/\/ Returns the total amount of coins in circulation for all denominations.\n  rpc TotalSupply(QueryTotalSupplyRequest) returns (QueryTotalSupplyResponse) {\n    option (google.api.http).get = \"\/cosmos\/bank\/v1\/supply\";\n  }\n}\n```\n\nNote that the `google.api.http` annotations define the REST endpoint and use curly braces to denote path parameters that are extracted from the request message. This allows us to define a one-to-one mapping between gRPC methods and REST endpoints that is clear and easy to understand.\n\n### Legacy Amino Queries\n\nExisting query paths that are not associated with a gRPC service will continue to use Amino for the time being. These legacy queries will be deprecated and eventually removed in a future release, at which point all queries will use the gRPC-first approach described above.\n","GenTime":"2024-08-28 12:19:34"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"text":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n\n\n##Decision\n- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","MatchedIds":[1310,1309,1289,1318,1307],"Prediction":"## Decision\nIntroduce new error codes to distinguish the ambiguous error cases.\n- For name modifications, create separate error codes for invalid names and locked key names. \n- For getting values, create distinct error codes for issues related to binary and non-binary keys. \n\nUpdate the API documentation to clearly describe the meaning of each new error code and the situations in which they will be returned.\n","GenTime":"2024-08-28 12:19:43"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"text":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n\n\n##Decision\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","MatchedIds":[3827,1193,1854,2221,1631],"Prediction":"## Decision\n\nThis starter seed will **not** assert an opinion on specific state management libraries. \n\nDevelopers should be free to choose the best tool for their specific project needs. This promotes flexibility and avoids locking projects into potentially outdated or inappropriate choices. \n\nWe value developer experience and want to empower developers to leverage their existing knowledge and preferences. \n\nHowever, the starter seed **will** include:\n\n* Clear documentation and examples demonstrating how to integrate popular state management libraries.\n* Guidance on choosing a state management solution based on project complexity and requirements. \n","GenTime":"2024-08-28 12:19:51"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"text":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n\n\n##Decision\nNew services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","MatchedIds":[1652,4761,3715,3470,2410],"Prediction":"## Decision\n\nCCS will support a restricted number of programming languages for new services, aiming to balance flexibility with maintainability and support costs. We will use the following criteria when deciding which languages to support:\n\n* **Popularity and Community Support:** The language should be widely used with a large and active community. This ensures access to ample learning resources, libraries, and support. \n* **Relevance to CCS Use Cases:** The language should be well-suited for the types of applications and services CCS typically builds, such as web applications, data processing, and APIs.\n* **Availability of Skilled Developers:** There should be a healthy pool of developers proficient in the language, both within CCS and in the wider market, to ensure future recruitment and support. \n* **Tooling and Integration:** The language should have mature and robust tooling for development, testing, and deployment, and integrate well with existing CCS systems and infrastructure.\n* **Cost Considerations:** The language should ideally be open-source or have a cost-effective licensing model.\n\nBased on these criteria, CCS will initially support the following programming languages for new services:\n\n* **Python:**  A versatile language with a large community, extensive libraries, and strong support for data science and web development. Python aligns well with existing CCS skills and integrates with the GOV.UK Frontend Toolkit.\n* **Java:**  A mature and widely adopted language with a strong track record in enterprise applications. CCS has existing Java expertise and the language offers excellent tooling and a wide range of frameworks.\n* **Javascript (Node.js):** A popular language for web development, offering full-stack development possibilities with Node.js. It benefits from a large community and good tooling.\n\nThis list will be reviewed periodically to ensure it remains relevant to CCS's evolving needs and the technological landscape. New languages may be considered if they offer compelling advantages and meet the defined criteria. \n\nFor any new service, the choice of programming language must be justified based on the project's specific requirements, team expertise, and alignment with the criteria outlined above. This documented justification should be included in the service's architectural documentation. \n","GenTime":"2024-08-28 12:20:03"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"text":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n\n\n##Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","MatchedIds":[1614,1669,2091,4609,1563],"Prediction":"## Decision\n\nWe will **archive the `decentraland\/standards` repository**, leaving both `decentraland\/adr` and `decentraland\/proposals` repositories active. \n\n### Justification\n\nThis decision is based on the following rationale:\n\n* **Redundancy:**  Maintaining both ADRs and a separate standards repository introduces unnecessary complexity and potential for conflicting information. Most standards can be effectively documented and tracked as ADRs, streamlining the process.\n* **Inactivity and Maintenance:** The `decentraland\/standards` repository appears to be inactive and has a strict approval process that hinders agility.  \n* **Leveraging Existing Processes:** The `decentraland\/proposals` repository already provides a well-defined, public platform for community contributions and has a formal review process suitable for new ideas and standards proposals.\n* **Centralization and Clarity:**  Consolidating architectural decisions and standards within the `decentraland\/adr` repository improves discoverability and ensures a single source of truth for this type of documentation.\n\n**Action Items:**\n\n* **Archive `decentraland\/standards`:**  Properly archive the repository to preserve its history while indicating its inactive status.\n* **Review Existing Standards:** Evaluate existing content within `decentraland\/standards` and migrate any relevant information to new or existing ADRs within `decentraland\/adr`.\n* **Communication:** Clearly communicate this decision to stakeholders and the Decentraland community, emphasizing the continued importance of `decentraland\/adr` and `decentraland\/proposals` for contributing to architectural decisions and standards. \n","GenTime":"2024-08-28 12:20:13"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\\nContext for rate limiting pattern\\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"text":"## Context\\nContext for rate limiting pattern\\n\n\n##Decision\nProsa about the decision in the ADD\\n","MatchedIds":[2823,3159,4804,3820,1145],"Prediction":"## Decision\n\nWe will implement rate limiting using a token bucket algorithm at the API gateway level. This will allow us to:\n\n* **Control the rate of requests:** Define specific limits for requests per second, minute, or hour, preventing service abuse and overload.\n* **Apply limits at different granularities:**  Apply rate limits globally, per API endpoint, per user, or based on specific criteria like IP addresses or API keys.\n* **Customize responses:**  Provide clear error messages (e.g., HTTP status code 429 - Too Many Requests) with information about the rate limit and retry-after periods. \n\nThis approach offers a good balance of control, flexibility, and performance. \n","GenTime":"2024-08-28 12:20:20"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"text":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n\n\n##Decision\nStore shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","MatchedIds":[3831,2703,672,2510,1225],"Prediction":"## Decision\n\nShard files will be organized in a directory hierarchy, divided by year and month.\n\nFor example, a shard for an event on July 12, 2024, will be stored in:\n\n`$PROJECT_DIR\/2024\/07\/2024-07-12-shard-uuid.md`\n\nWhere:\n\n* `$PROJECT_DIR` is the root directory for the project.\n* `2024` is the year.\n* `07` is the month.\n* `2024-07-12-shard-uuid.md` is the filename, consisting of the date, a descriptive prefix (\"shard\"), a UUID and the \".md\" extension. \n\nThis scheme provides the following benefits:\n\n* **Improved organization:** Shards are grouped by date, making it easier to browse and find specific events.\n* **Enhanced performance:**  Distributing files across multiple directories can improve file system performance, especially on older systems. \n* **Flexibility:** The structure can easily accommodate projects with varying levels of activity. \n","GenTime":"2024-08-28 12:20:26"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"text":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n\n\n##Decision\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","MatchedIds":[2261,160,4507,138,3187],"Prediction":"## Decision\n### Chosen Option: Event-Driven Approach with Template Modification\n\nThis approach leverages an event-driven architecture to dynamically add the `ViewedByCustomer` component to result templates, ensuring consistency and maintainability.\n\n### Detailed Explanation:\n\n1. **Event Trigger:** A dedicated event, like `resultsRendered`, will be emitted after the results are rendered on the page. This provides a clear hook for modifying the templates.\n\n2. **Centralized Logic:** A dedicated module or service will listen for the `resultsRendered` event. Upon triggering:\n    - It will iterate through all rendered result templates.\n    - It will check if the `UserActions` component's option to display the `ViewedByCustomer` component is enabled.\n    - For each template, it will check if the `ViewedByCustomer` component is already present. If not, it will dynamically add the component to the template.\n\n3. **Template Modification:**  The actual mechanism for adding the component to the template will depend on the framework being used (e.g., direct DOM manipulation, template directives in Vue.js, etc.). However, the core logic remains the same - ensure the component is added only once.\n\n### Positive Consequences:\n\n- **Consistency:** Guarantees the `ViewedByCustomer` component is present on all result templates, preventing inconsistencies.\n- **Maintainability:**  Centralized logic for managing the component's presence reduces code duplication and simplifies updates.\n- **Extensibility:**  The event-driven approach allows for flexibility in case future requirements demand adding other components dynamically.\n\n### Negative Consequences:\n\n- **Slight increase in complexity** due to the introduction of events and centralized logic. However, this is outweighed by the benefits of maintainability and consistency. \n- **Potential performance impact** if the number of result templates is exceptionally large. This can be mitigated by optimizing the template modification logic. \n","GenTime":"2024-08-28 12:20:37"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"text":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n\n##Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","MatchedIds":[674,539,3127,2802,3831],"Prediction":"## Decision\n\nUtilize the HTTP PATCH method with a JSON Merge Patch (RFC 7396) request body. This approach offers several advantages:\n\n* **Standardized Approach:** JSON Merge Patch is a well-defined standard for expressing partial updates, ensuring interoperability and clarity.\n* **Explicit Merging:** It explicitly defines how to add, replace, or remove members from a JSON document, preventing unintended modifications.\n* **Reduced Payload Size:** Clients only need to send the specific fields requiring updates, minimizing bandwidth consumption.\n\n**Implementation Details:**\n\n* The API endpoint for updating resources should accept PATCH requests.\n* The request body must adhere to the JSON Merge Patch specification (RFC 7396).\n* The server-side logic should implement the JSON Merge Patch algorithm to apply the changes to the existing resource.\n* Proper error handling should be implemented for invalid patch requests or conflicts.\n\n**Example:**\n\n**Request:**\n\n```http\nPATCH \/api\/resources\/123\nContent-Type: application\/merge-patch+json\n\n{\n  \"name\": \"Updated Name\",\n  \"description\": null\n}\n```\n\nThis request would update the \"name\" property and remove the \"description\" property of the resource with ID 123. \n","GenTime":"2024-08-28 12:20:44"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"text":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n\n\n##Decision\nWe will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","MatchedIds":[2631,2637,2629,2687,3682],"Prediction":"## Decision\nThe scratch ftduino app will communicate with a local web-server that will act as a proxy to invoke the arduino-cli and scratch-to-c programs. This avoids the need for Electron, minimizing application size and dependency on specific frameworks while retaining flexibility and broad compatibility across browsers. \n","GenTime":"2024-08-28 12:20:56"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"text":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n\n\n##Decision\nWe outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","MatchedIds":[2270,4152,839,28,84],"Prediction":"## Decision\nA document named `ROLES.md` will be created in the root of the project repository. This document will be used to keep a glossary of terms for roles in the Esperanza protocol. All conflicting instances in the codebase and documentation will be updated to reflect the terminology defined in `ROLES.md`. From now on, diverging from this glossary will be considered a bug and should be addressed promptly. \n\nFurthermore, a linter rule will be implemented to check for adherence to the `ROLES.md` glossary. This will help catch any inconsistencies early in the development process. \n","GenTime":"2024-08-28 12:21:01"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"text":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n\n\n##Decision\nWe will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","MatchedIds":[1376,2619,2471,74,3145],"Prediction":"## Decision\n\nTo reduce the operational overhead of managing multiple operating systems, DXC will adopt a strategy of **\"minimal OS customization\"**.  We will prioritize the use of **AWS managed images**, such as Amazon Linux 2 or Ubuntu LTS, whenever possible. \n\nThis decision is based on the following rationale:\n\n* **Reduced maintenance overhead:** AWS managed images benefit from automatic security updates and patching, reducing the burden on DXC teams.\n* **Improved security posture:** AWS managed images adhere to AWS security best practices and are regularly scanned for vulnerabilities.\n* **Faster deployment times:** Using readily available images accelerates the deployment process for new applications and services.\n\nIn specific cases where an AWS managed image cannot meet the requirements, we will evaluate the use of **pre-configured images from the AWS Marketplace**. \n\nBuilding and maintaining custom images will only be considered as a last resort when neither AWS managed images nor Marketplace offerings can fulfill the specific needs of a project.  Any custom image creation must be accompanied by a clear justification and a plan for ongoing maintenance and security patching. \n","GenTime":"2024-08-28 12:21:08"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"text":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n\n##Decision\nWe accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","MatchedIds":[649,2701,1477,2500,1758],"Prediction":"## Decision\n\nWe will use Spring Boot\/Framework as the base framework for the project. While acknowledging the benefits of decoupling from 3rd party libraries, the practical advantages and built-in abstraction capabilities of Spring outweigh the potential need for future technology replacements.  This approach will allow us to leverage Spring's features for increased development speed and maintainability without introducing unnecessary abstraction layers. \n","GenTime":"2024-08-28 12:21:12"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"text":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n\n##Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","MatchedIds":[2687,3153,4179,1258,847],"Prediction":"## Decision\n\nTo address the non-deterministic nature of browser-based accessibility testing and improve the replicability of our results, we will adopt a hybrid approach. \n\nWe will develop a new accessibility conformance testing engine, codenamed \"Alfa\", which will consist of two main components:\n\n1. **A standalone analysis engine:** This engine will be responsible for parsing HTML, CSS, and eventually JavaScript, constructing a DOM and CSSOM, and performing accessibility rule checks. It will be implemented in a platform-independent language, such as Node.js, to ensure consistent execution across different environments. By moving most of the analysis logic outside the browser, we reduce our dependence on its inherent non-determinism.\n\n2. **A lightweight browser integration layer:**  This layer will be responsible for fetching website content, including handling network requests and executing JavaScript in a controlled environment. It will then provide the analysis engine with the resulting DOM, CSSOM, and potentially AOM, for analysis. While we will strive to minimize its scope, this layer will still leverage the browser's capabilities for tasks like rendering and JavaScript execution, which are difficult to replicate reliably outside a browser context.\n\nThis approach allows us to leverage the strengths of both worlds:\n\n* **Improved Replicability:** By performing the core analysis in a controlled, deterministic environment, we gain greater control over the testing process, making results easier to reproduce and reason about.\n* **Reduced Complexity:** Isolating the analysis logic from the browser simplifies debugging and maintenance, while still allowing us to leverage browser APIs for specific tasks. \n* **Future-Proofing:** This architecture allows us to integrate future accessibility APIs like AOM seamlessly, regardless of their browser-specific implementations. \n\nWe acknowledge that this approach requires a significant architectural shift. However, we believe that the long-term benefits in terms of reliability, maintainability, and future-proofing outweigh the initial investment. \n","GenTime":"2024-08-28 12:21:23"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"text":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n\n\n##Decision\nDon't start using Styled Components\\n","MatchedIds":[1125,2655,157,5087,1855],"Prediction":"### Decision\n\nHold off on adopting Styled Components for now. While they offer maintainability advantages, unresolved testing conflicts present a significant risk. \n\n**Reasoning:**\n\nA core principle of software development is \"Don't ship broken code.\"  Inability to reliably test is a dealbreaker. We can't sacrifice confidence in our codebase for potential maintainability gains. \n\n**Action Items:**\n\n1. **Investigate testing issues thoroughly:**  \n    * Isolate the specific problems encountered. \n    * Consult Styled Components documentation and community resources for solutions.\n    * Consider if our testing setup needs adjustment to better handle CSS-in-JS.\n2. **Explore alternatives\/workarounds:** \n    * Are there testing libraries specifically designed for Styled Components?\n    * Can we refactor our components or tests to minimize the conflict points?\n3. **Re-evaluate after investigation:** \n    * If testing issues are resolved, revisit the decision to adopt.\n    * If issues persist, document them clearly and consider alternative CSS approaches. \n","GenTime":"2024-08-28 12:21:34"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"text":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\n\n##Decision\nWe will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","MatchedIds":[28,812,3768,842,1944],"Prediction":"## Decision\n\n### Design\n\nThis ADR proposes the creation of an `x\/authz` module to the Cosmos SDK which will introduce **authz grants**. An authz grant enables one account to grant permission to another account to execute specific actions on its behalf. This addresses the needs for delegated voting rights and contributes to the \"sub-keys\" functionality.\n\n#### Authz Grants\n\nAn authz grant comprises:\n\n* **Grantor:** The account granting permission.\n* **Grantee:** The account receiving permission.\n* **Authorization:**  Defines the specific action allowed. This can be broadly categorized as:\n    * **Generic authorization:** Allows any type of message to be executed, potentially with restrictions on gas usage and other parameters.\n    * **Specific authorization:** Permits only a specific type of message, such as voting on a proposal.\n* **Expiration:** An optional timestamp after which the grant is no longer valid.\n\nThe `x\/authz` module will manage the lifecycle of authz grants:\n\n* **Grant creation:** The grantor submits a transaction to create a new grant.\n* **Grant execution:**  The grantee submits a transaction that includes the grant and the message to be executed.\n* **Grant revocation:** The grantor can revoke a grant at any time. \n\n#### Interaction with Other Modules\n\n* **`x\/gov` (Governance):**  Enabling users to delegate their voting power to other accounts without delegating their actual stake.\n* **`x\/feegrant` (Fee Grant):**  The grantor can combine authz grants with fee grants, allowing the grantee to execute actions on their behalf without paying fees.\n* **`x\/group` (Group):**  Integration with the group module will allow for more complex authorization scenarios, such as requiring a majority vote within a group to execute a grant. \n\n### Implementation\n\n* A new module `x\/authz` will be created to manage the authz grants. \n* This module will store grant information in the application state. \n* It will define messages for creating, revoking, and executing grants.\n*  The `x\/authz` module will be designed to be extensible, allowing other modules to define their own authorization types. \n\n### Security Considerations\n\n* **Grant specificity:** The module will be designed to allow for fine-grained control over the actions that can be performed with a grant.\n* **Grant revocation:** Grantors will have the ability to revoke grants at any time.\n* **Key management:** Users will be responsible for securely managing their private keys. The module will not introduce any new key management mechanisms.\n\n### Future Work\n\n* Explore more complex authorization scenarios, such as delegation with time-bound constraints or quorum-based approvals.\n* Consider integrating with other modules that could benefit from authz functionality, such as `x\/bank` for delegated spending.\n* Investigate alternative authorization mechanisms, such as capability-based security.\n\nThis design builds upon the work done by the Gaian's team at Hackatom Berlin 2019, adapting and extending it to fit within the broader context of the Cosmos SDK and its existing modules. By implementing authz grants, the Cosmos SDK will offer a powerful and flexible mechanism for delegated authorization, enabling a wide range of new use cases and fostering a more secure and user-friendly blockchain ecosystem. \n","GenTime":"2024-08-28 12:21:52"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"text":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n\n\n##Decision\nA new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","MatchedIds":[2499,3127,1133,2067,4415],"Prediction":"## Decision\n\nWe will use a wrapper class around LiveData called **SingleLiveEvent** to handle events that should be triggered only once.\n\n**Implementation Details:**\n\n* **SingleLiveEvent** will hold the event data and a boolean flag to indicate whether the event has been consumed.\n* When the event is posted to the SingleLiveEvent, the flag is set to `false`.\n* Observers of SingleLiveEvent will receive the event only if the flag is `false`.\n* Upon observing the event, the flag is set to `true` preventing subsequent observers from receiving the same event.\n* This ensures that events like errors or navigation commands are executed only once, even after lifecycle events like device rotation or fragment re-creation.\n\n**Example Usage:**\n\n```kotlin\nclass MyViewModel : ViewModel() {\n    private val _navigateToDetails = SingleLiveEvent<Int>()\n    val navigateToDetails: LiveData<Int> = _navigateToDetails\n\n    fun onItemClicked(itemId: Int) {\n        _navigateToDetails.value = itemId \n    }\n}\n\n\/\/ In Fragment:\nviewModel.navigateToDetails.observe(viewLifecycleOwner) { itemId ->\n    \/\/ Navigate to details fragment using itemId\n}\n```\n\n**Benefits:**\n\n* Prevents unintended side-effects caused by duplicate event consumption.\n* Simplifies the handling of one-time events in LiveData.\n* Improves the overall user experience by avoiding unexpected behavior after lifecycle changes.\n","GenTime":"2024-08-28 12:22:01"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"text":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n\n\n##Decision\nWe will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","MatchedIds":[284,287,282,273,2337],"Prediction":"## Decision\n\n**Use a two-step process with `nbconvert` and `Asciidoctor PDF`**.\n\n### Justification:\n\n* **Reduced container size:** Avoiding a full LaTeX installation significantly reduces the AutoFlow container size, making it more efficient to build and deploy.\n* **Leveraging existing tools:** Both `nbconvert` and `Asciidoctor PDF` are well-established tools with active communities. This ensures good documentation, support, and future maintenance.\n* **Flexibility:** This two-step approach allows for potential customization of the intermediate ASCIIDoc output before PDF generation if needed.\n\n### Potential drawbacks:\n\n* **Two-step complexity:**  The two-step process might be slightly more complex to implement than a single-tool solution. \n* **Potential formatting limitations:** While Asciidoctor PDF is powerful, it may not perfectly replicate all the formatting capabilities of a LaTeX-based solution. This trade-off is considered acceptable given the size constraints.\n\n### Implementation plan:\n\n1. Integrate `nbconvert` to convert Jupyter notebooks to ASCIIDoc format.\n2. Integrate `Asciidoctor PDF` to convert the generated ASCIIDoc files to PDF.\n3. Thoroughly test the conversion process with various Jupyter notebook features (code blocks, Markdown, images, etc.) to ensure fidelity and identify potential limitations. \n4. Document the chosen approach and any identified limitations for future reference and user guidance.\n","GenTime":"2024-08-28 12:22:09"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"text":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n\n\n##Decision\nReceivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","MatchedIds":[674,2499,779,1216,1193],"Prediction":"## Decision\n\nWhile the current flow components (explorers, emitters, receivers) loosely map to CRUD principles, we should not limit ourselves to this paradigm. \n\nHere's why and what we can do:\n\n1. **CRUD is not enough:** Many integrations require actions beyond CRUD. Sending commands, triggering events, or implementing custom logic are valid use cases. \n2. **Flexibility over strictness:** HEPTAconnect should allow for diverse integration scenarios. Enforcing a strict CRUD model might hinder flexibility and innovation.\n3. **Clear component roles:** While not bound to CRUD, each component should have a well-defined purpose:\n    - **Explorers:** Responsible for data discovery and retrieval.\n    - **Emitters:**  Responsible for initiating data transfer or actions.\n    - **Receivers:** Responsible for processing incoming data or executing commands. \n4. **Embrace commands and events:** Explicitly support commands as first-class citizens alongside entities. This allows for clear expression of intent beyond data transfer. \n5. **Consider a dedicated \"Deleter\" component:**  Given the need for deletion and the potential complexity around it, a dedicated component might be beneficial. This would mirror the existing component structure and offer a standardized approach.\n\n**Moving forward:**\n\n* **Documentation:** Clearly document the purpose of each flow component and how they can be used beyond basic CRUD operations. \n* **Explore command patterns:**  Investigate implementing command patterns for better expressing integration actions.\n* **Evaluate \"Deleter\" component:** Analyze the need and potential benefits of a dedicated component for handling deletions.\n\nBy embracing flexibility and moving beyond a strict CRUD approach, HEPTAconnect can cater to a wider range of integration scenarios and empower users to build more powerful and versatile data flows. \n","GenTime":"2024-08-28 12:22:21"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"text":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n\n\n##Decision\nLauncher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","MatchedIds":[2546,1924,1927,2975,3290],"Prediction":"## Decision\n\nAdd two new fields to each emitted log line within the launcher:\n\n1. **`uuid`**: A universally unique identifier (UUID) generated for each individual log line. This allows distinguishing between log lines with identical content.\n2. **`data_crc`**: A cyclic redundancy check (CRC) hash of the log line's data, calculated *before* the `uuid` is added. This enables efficient identification of duplicate log lines by comparing hashes instead of the entire content.\n\n**Implementation Details:**\n\n* The `uuid` should be generated using a robust UUID generation algorithm (e.g., UUIDv4) to ensure global uniqueness.\n* The `data_crc` should utilize a fast and reliable CRC algorithm (e.g., CRC32) for efficient hash calculation. \n* The order of operations is crucial: calculate the `data_crc` first, then append the `uuid` to the log line before emitting it.\n\n**Example Log Line Structure:**\n\n```\n{\n  \"timestamp\": \"...\",\n  \"severity\": \"...\", \n  \"message\": \"...\",\n  ... other log data fields ...,\n  \"data_crc\": \"8742a414\", \n  \"uuid\": \"550e8400-e29b-41d4-a716-446655440000\" \n} \n```\n\nThis approach provides a lightweight and efficient mechanism for ensuring per-log line uniqueness and facilitating downstream deduplication without requiring extensive data inspection. \n","GenTime":"2024-08-28 12:22:35"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"text":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n\n\n##Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","MatchedIds":[4644,3488,1575,276,3187],"Prediction":"## Decision\nChosen option: **Configuration service with REST API**.\n\n### Justification\nWe will use a configuration service with a REST API to store and serve the FaaS function details (gateway address, function name) required by the generic component. This approach satisfies all decision drivers:\n\n* **Supported by most languages**:  Virtually all languages used to build applications have libraries and frameworks to interact with REST APIs, making it compatible with the generic component regardless of its implementation. \n* **Easy integration with MICO**: Configuration services are standard components within microservice architectures, and MICO likely already has provisions for integrating with them. If not, it should be relatively straightforward to implement.\n* **Well-known and proven solution**:  Configuration services using REST APIs are widely used and well-documented, offering a mature and reliable approach for managing and distributing configuration data. \n\nAlternatives considered:\n\n* **Hardcoded configuration**: This approach violates the \"easy to integrate\" and potentially \"supported by language\" drivers as updating hardcoded values might be cumbersome and require recompilation or redeployment.\n* **Environment variables**: While simple, environment variables can be less maintainable for complex configurations and might not be suitable for sensitive data.\n\n### Positive Consequences\n\n* **Centralized configuration management**: Enables easy updates and rollbacks of FaaS function mappings without modifying the generic component.\n* **Improved maintainability**:  Provides a clear separation of concerns between application logic and configuration data.\n* **Flexibility and scalability**: Allows for dynamic routing and updates to FaaS functions as requirements evolve.\n\n### Negative Consequences\n\n* **Additional service dependency**: Introduces a dependency on the configuration service, requiring its availability and potentially increasing system complexity.\n* **Potential single point of failure**: If the configuration service is unavailable, it could impact the generic component's functionality. Implement redundancy mechanisms to mitigate this risk. \n","GenTime":"2024-08-28 12:22:45"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\\nSome rules regarding usage of health-module apis.\\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"text":"## Context\\nSome rules regarding usage of health-module apis.\\n\n\n##Decision\nInput for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","MatchedIds":[1180,3159,5142,3203,2181],"Prediction":"## Decision\n\nThe health module API endpoints MUST be considered internal. This means:\n\n* **No external dependencies**: These endpoints should NOT be used for integrations with third-party services or external monitoring tools. \n* **Dedicated monitoring**: A separate monitoring system, using appropriate internal credentials or mechanisms, should be responsible for checking the health endpoints and triggering alerts.\n* **Security**: Direct access to health endpoints from outside the infrastructure MUST be blocked.  This can be enforced via network configuration or API gateway rules.\n* **Minimal Exposure**: Health endpoints should expose only minimal information relevant to the service's health status.  Sensitive data or internal details MUST NOT be accessible through these endpoints.\n\n**Rationale:**\n\n* Health endpoints are primarily designed for internal system monitoring and troubleshooting. \n* Exposing them externally creates unnecessary security risks and potential attack vectors.\n* Internal monitoring tools can be better tailored to understand the nuances of the health checks and trigger appropriate alerts.\n","GenTime":"2024-08-28 12:22:57"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"text":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n\n\n##Decision\nWe In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","MatchedIds":[4896,429,2498,2183,2502],"Prediction":"## Decision\n\n**We will adopt Domain-Driven Design (DDD) as a core principle for software development within our projects.** This decision is driven by the numerous benefits DDD offers, aligning with our goals of:\n\n* **Improved communication:** By establishing a ubiquitous language, we bridge the gap between technical and domain experts, fostering clearer communication and shared understanding.\n* **Maintainable codebase:**  DDD's focus on modeling the business domain results in a codebase that directly reflects the business logic, making it easier to understand, maintain, and evolve over time.\n* **Reduced complexity:**  Breaking down large systems into smaller, well-defined bounded contexts allows us to manage complexity effectively and focus on specific problem domains.\n* **Enhanced flexibility:** DDD promotes loose coupling and modularity, enabling easier adaptation to changing business requirements and reducing the impact of future modifications. \n\n**Implementation Strategy:**\n\n* **Collaborative Modeling:** We will actively involve both domain experts and developers in collaborative modeling sessions to define the ubiquitous language, bounded contexts, and domain model. \n* **Strategic DDD Patterns:** We will leverage strategic DDD patterns like bounded contexts and context mapping to define clear boundaries and relationships between different parts of the system.\n* **Tactical DDD Patterns:** Within each bounded context, we will employ tactical DDD patterns such as aggregates, entities, value objects, and domain events to design a rich and expressive domain model.\n* **Continuous Refinement:**  We recognize that DDD is an iterative process. We will continuously refine our domain model based on feedback from stakeholders and the evolving understanding of the business domain. \n","GenTime":"2024-08-28 12:23:06"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"text":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n\n\n##Decision\nUse the Tailwind CSS framework for design and layout.\\n","MatchedIds":[425,1992,1240,157,400],"Prediction":"## Decision\n\n**Chosen option:** Adopt Tailwind CSS as our primary CSS framework.\n\n### Positive Consequences:\n\n- **Increased development speed:** Tailwind's utility-first approach allows for rapid prototyping and iteration directly in HTML, reducing the need for context switching between files and writing custom CSS.\n- **Improved maintainability:** By using a consistent set of utility classes, we minimize the risk of CSS conflicts and style bloat, making our codebase easier to maintain and scale. \n- **Design system consistency:**  We can leverage Tailwind's configuration options to enforce our design system's constraints, ensuring consistency across our projects. \n- **Reduced learning curve:**  Tailwind's approach is generally easier for developers to grasp, especially those familiar with inline styles or who are new to CSS frameworks.\n\n### Negative Consequences:\n\n- **HTML verbosity:**  Using multiple utility classes can make our HTML more verbose. However, this can be mitigated with tools like Prettier and Tailwind's  `@apply` directive for component-specific styles.\n- **Potential for class name conflicts:** While rare, it is possible to have naming conflicts with Tailwind's utility classes, requiring careful consideration when naming our own components. \n- **Learning curve for Tailwind configuration:** While applying utility classes is straightforward, customizing Tailwind's default settings and extending it with custom styles requires understanding its configuration options. \n\n### Mitigation Strategies:\n\n- Encourage the use of linters and formatters to manage HTML verbosity.\n- Define a clear naming convention for custom components to avoid conflicts with Tailwind classes.\n- Invest in developer education and documentation to ensure the team understands Tailwind's configuration and best practices.\n\n### Conclusion:\n\nWhile Tailwind CSS brings some trade-offs, its advantages in development speed, maintainability, and design consistency outweigh the potential drawbacks. By adopting best practices and mitigation strategies, we can leverage its strengths to improve our frontend development workflow.\n","GenTime":"2024-08-28 12:23:21"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"text":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n\n\n##Decision\n### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","MatchedIds":[21,1949,810,1982,4354],"Prediction":"## Decision\n\nThis ADR proposes implementing the simplest form of validator consensus key rotation primarily within the Cosmos SDK, leaving the Tendermint consensus logic untouched. \n\n### Implementation Details\n\n1. **New Cosmos SDK `Msg`:** Introduce a new message type, tentatively named `MsgRotateConsKey`, for rotating the validator's consensus key. This message will include:\n    * **Validator Operator Address:** The address of the validator operator initiating the rotation.\n    * **New Consensus Pubkey:** The new consensus public key to be used by the validator.\n    * **Self-Delegation Signature:** A signature generated by the validator's **operator key**, signing over the new consensus public key. This ensures that only the legitimate operator can initiate a rotation.\n\n2. **`MsgRotateConsKey` Handler:** Implement a handler for the `MsgRotateConsKey` that performs the following:\n    * **Verification:**\n        * Verify the sender is a valid validator operator.\n        * Verify the self-delegation signature against the operator's current pubkey.\n    * **Update Validator Information:**\n        * Update the validator's consensus pubkey in the staking module's store with the new pubkey.\n    * **Emit Event:** Emit an event signifying a successful consensus key rotation. \n\n3. **Tendermint Integration:** No changes are needed in the Tendermint core logic. From Tendermint's perspective, this is simply a validator update with a new consensus key. The existing validator update mechanism will handle the change seamlessly. \n\n### Considerations\n\n* **Synchronization:** The validator needs to ensure the new consensus key is properly configured in their node before the rotation takes effect. This might involve a brief downtime for the validator. \n* **Light Client Support:** Light clients will need to handle validator consensus key rotation events to ensure they can verify the validator signatures.\n* **Governance:**  Consider whether governance participation (voting, proposals) should be restricted for a period after a key rotation. This adds a layer of security in case the previous key was compromised.\n\n### Future Work\n\n* **Multiple Consensus Keys:**  Explore the concept of supporting multiple consensus keys per validator, allowing for more sophisticated key management schemes and potential uptime benefits during rotations.\n* **Key Rotation Schedule\/Policy:** Investigate adding mechanisms for enforcing key rotation schedules or policies, potentially through on-chain governance. \n\nThis simplified approach leverages the existing infrastructure of Cosmos SDK and Tendermint, minimizing complexity while offering a significant security improvement for validator key management. \n","GenTime":"2024-08-28 12:23:37"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"text":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n\n\n##Decision\n* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","MatchedIds":[650,693,3187,160,4695],"Prediction":"## Decision\n* Acceptance tests should be meaningful for end users\n\nChosen option: **Yes**, acceptance tests should be written one layer above (UI). This is because the UI layer is where end users directly interact with the game server. \n\n### Positive Consequences\n* **Improved clarity for end users:**  By framing tests from the UI perspective, they directly reflect how users experience the game. This makes the tests more understandable and relatable for non-technical stakeholders.\n* **More realistic testing:** Testing at the UI level ensures that the entire system, including the server and any client-side logic, is working as intended.\n* **Facilitates black-box testing:** Testing from the UI allows for black-box testing of the game server. This means that tests can be written without knowledge of the server's internal workings, which can help to uncover unexpected issues.\n\n### Negative consequences\n* **Increased complexity:** UI testing can be more complex and time-consuming to set up and maintain compared to lower-level testing.\n* **Potential for fragility:** UI tests can be more prone to breaking when changes are made to the UI.\n\n**Justification:** While there is increased complexity, writing acceptance tests from the UI layer provides the most meaningful feedback from an end-user perspective, aligning with the primary decision driver. This approach ultimately leads to a higher quality user experience. \n","GenTime":"2024-08-28 12:23:44"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"text":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n\n##Decision\nUse a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","MatchedIds":[2181,138,4841,2183,1545],"Prediction":"## Decision\n\nWe will create a 'Question Bank' microservice to store and manage common questions and their related data. \n\n### Functionality:\n\n1. **Storage:** The Question Bank will persist common questions, their potential variations, and associated metadata. This metadata could include:\n    - **Tags:**  Categorize questions (e.g., \"outcomes,\" \"supplier capabilities,\" \"security requirements\").\n    - **Relationships:** Link questions to relevant outcomes, supplier offerings, or other questions. \n    - **Weightings:** For future use in automatic matching, assign weights to specific answers or question combinations.\n2. **Retrieval:** The service will allow other components to retrieve questions and their related data via an API. This retrieval can be based on various criteria like tags, agreement types, or specific keywords. \n3. **Versioning:**  Implement version control for questions to accommodate changes in legal requirements, business processes, or market trends.\n4. **Analysis (Future):** Implement functionality to analyze buyer responses and match them to suitable offerings automatically. This will likely involve machine learning algorithms and might be developed in a future phase.\n\n### Data Flow:\n\n1. During agreement creation, the system will query the Question Bank for relevant questions based on the agreement type and other relevant factors.\n2. Buyer responses will be stored temporarily within the context of the buying journey.\n3. Upon agreement finalization (e.g., call-off), relevant response data will be transferred to the agreement records for permanent storage.\n\n### Benefits:\n\n- **Consistency:** Ensures standardized questioning across different commercial agreements.\n- **Efficiency:** Reduces duplication of effort and improves consistency in data capture.\n- **Future-Proofing:** Lays the foundation for automating the matching of buyer needs to CCS offerings.\n- **Data Analysis:**  Facilitates the analysis of common buyer needs and preferences over time. \n\n### Considerations:\n\n- The specific data structure for storing questions and their relationships needs to be carefully designed to support efficient retrieval and future analysis.\n- Access control mechanisms should be implemented to manage who can create, edit, or view questions and their associated data. \n- Integration with machine learning tools and techniques will be crucial for the future automation of need-matching. \n","GenTime":"2024-08-28 12:23:56"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"text":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n\n\n##Decision\nWe will use isort, black, and flake8.\\n","MatchedIds":[5255,1408,4226,3321,3451],"Prediction":"## Decision\n\nWe will adopt a combination of linters and automated code formatters to enforce coding standards and best practices:\n\n* **Code Formatting:** We will utilize [Prettier](https:\/\/prettier.io\/) for all supported languages. Prettier will be configured with a standardized ruleset to ensure consistent code style across all projects. We will enforce this by:\n    * Setting up Prettier as a pre-commit Git hook, automatically formatting code before every commit.\n    * Integrating Prettier checks within our continuous integration pipeline, rejecting any commits that are not formatted correctly. \n* **Linting:** We will utilize ESLint (for Javascript) or a similar language-specific linting tool for other languages. Linters will be configured with a standardized ruleset focusing on code quality, potential errors, and adherence to best practices. This will be enforced by:\n    * Integrating linting checks within our continuous integration pipeline, rejecting any commits with linting errors.\n    * Encouraging developers to incorporate linters as part of their IDE setup for real-time feedback.\n\nThis approach reduces the cognitive load on developers by automating code style and best practice enforcement, leading to cleaner, more maintainable codebases and a more efficient development process. \n","GenTime":"2024-08-28 12:24:03"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\\nWe need to choose a web framework for the app.\\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"text":"## Context\\nWe need to choose a web framework for the app.\\n\n\n##Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","MatchedIds":[1240,4271,3715,469,472],"Prediction":"## Decision\n\nWe will use **React** for building the web application's frontend. \n\nWhile the context doesn't specify frontend or backend, the term \"web framework\" generally implies frontend development. React is a popular and widely adopted JavaScript library for building user interfaces, offering:\n\n* **Component-based architecture:** This promotes reusability, maintainability, and a better development experience.\n* **Large and active community:**  Ensures ample resources, libraries, and support for developers.\n* **Strong ecosystem:** Tools like Create React App and Next.js simplify development and deployment. \n\nBy choosing React, we lay a strong foundation for building a modern, scalable, and maintainable web application. \n","GenTime":"2024-08-28 12:24:07"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"text":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n\n##Decision\nWe decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","MatchedIds":[5142,1866,2968,779,1924],"Prediction":"## Decision\n\nImplement **Frequency Capping** in the Snippet Service by leveraging the capabilities of the Firefox Messaging Center and the Firefox Telemetry Pipeline. \n\nThis will involve:\n\n* **Server-Side Enforcement:** Migrate the decision-making logic for content eligibility (based on frequency caps) to the server.\n* **Global Frequency Capping:**  Track the total number of impressions for each content piece server-side.\n* **User Frequency Capping:**  Utilize user identifiers and persistent tracking mechanisms (respecting user privacy) to enforce per-user impression limits.\n* **Content Selection:**  Modify the Snippet Service to request and respect the server's determination of content eligibility based on frequency capping rules.\n\nThis approach addresses the technical limitations of client-side content selection and enables the Snippet Service to provide Content Managers with essential frequency control mechanisms. \n","GenTime":"2024-08-28 12:24:14"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"text":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","MatchedIds":[3784,2261,4699,4428,4408],"Prediction":"## Decision\nWe will utilize Architecture Decision Records (ADRs) as described by Michael Nygard ([http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)). ADRs are lightweight and modular, making them ideal for documenting architectural decisions in an agile manner. Each record will capture the context, decision, and consequences in a concise format, enabling easy understanding and future reference for the team.  We will store and maintain these ADRs in a version-controlled repository easily accessible to all stakeholders. \n","GenTime":"2024-08-28 12:24:18"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"text":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n\n##Decision\n[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","MatchedIds":[3789,4388,4025,4358,3793],"Prediction":"## Decision\n\nTo address the need for automated provisioning and documentation of Cloud.gov-brokered services, we will implement the following:\n\n**1. Infrastructure as Code (IaC):**\n\n* Utilize Terraform to define and manage the S3 buckets and RDS PostgreSQL instance. \n* This allows for repeatable and consistent provisioning across all environments (dev, staging, prod).\n* Terraform scripts will include:\n    * Creation of S3 buckets with appropriate access controls (public for static assets, private for PII data).\n    *  Provisioning of RDS PostgreSQL instance with:\n        *  Security group configurations to restrict access.\n        * Database user creation and appropriate permissions.\n    *  Optionally leverage Terraform modules for reusable infrastructure components.\n\n**2. CI\/CD Integration:**\n\n* Integrate Terraform scripts into the existing CI pipeline.\n* Upon successful code changes and pipeline triggers:\n    * Terraform will provision or update the required Cloud.gov resources.\n    * This automates the deployment process and ensures consistency across environments.\n\n**3. Security Best Practices:**\n\n* Implement least privilege principle for S3 bucket policies and RDS security groups.\n* Encrypt sensitive data at rest in S3 using server-side encryption (SSE-S3).\n* Enable RDS encryption at rest if the database stores sensitive data.\n* Disable public access to the RDS instance and only allow connections from authorized sources (e.g., application servers within the Cloud.gov space).\n\n**4. Documentation:**\n\n* Create comprehensive documentation outlining:\n    * Terraform code structure and functionality.\n    * Security configurations and best practices implemented.\n    * Steps for verifying correct provisioning and configuration.\n    * Troubleshooting guide for common issues.\n\n**5. Automated Testing:**\n\n* Develop automated tests to verify:\n    * Successful resource provisioning by Terraform.\n    * Correct access control configurations for S3 buckets and RDS.\n    *  Integration between the application and the provisioned services.\n\nBy implementing these measures, we can ensure a secure, automated, and well-documented process for provisioning and managing Cloud.gov-brokered services for the TANF application. This approach will streamline deployments, enhance security, and provide a clear path for future maintenance and updates. \n","GenTime":"2024-08-28 12:24:33"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"text":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n\n\n##Decision\nI decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","MatchedIds":[2520,4832,4859,1661,3657],"Prediction":"## Decision\n\nChosen option: **JSON files**, because:\n\n* **Simplicity:** Currency data is relatively static (currency codes, symbols, etc.). JSON offers a simple, human-readable format that is easy to manage and parse without the overhead of a database.\n* **Lightweight:** This approach is extremely lightweight, requiring no database server or complex setup. It's ideal if the project is small and resource-constrained. \n* **Flexibility:** While simple, JSON offers enough flexibility to structure the currency data efficiently.\n\n### Positive Consequences \n\n* **Easy Implementation:**  Integrating JSON file reading into the project is straightforward in most programming languages. \n* **Fast Access:** Reading from local JSON files will likely be faster than querying a database, especially for such small and static data.\n\n### Negative Consequences\n\n* **Limited Scalability:** This solution might not scale well if the currency data becomes surprisingly large or requires frequent updates.\n* **Data Integrity:** JSON files lack the built-in data integrity checks of a database. You'll need to handle potential data inconsistencies in your code.\n\n### Alternatives Considered:\n\n* **Relational Database:** Overkill for such simple data. Introduces unnecessary complexity.\n* **NoSQL Database:**  Similar to a relational database, it's likely unnecessary for managing a small set of currency data.\n\n### Recommendation:\n\nStart with JSON files due to their simplicity and efficiency for this specific context. If the project evolves and currency data management becomes more complex (e.g., frequent updates, large datasets), reconsider a database solution at that time. \n","GenTime":"2024-08-28 12:24:43"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"text":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n\n\n##Decision\n- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","MatchedIds":[1430,138,4852,1924,2968],"Prediction":"## Decision\n\nBased on the context provided, it appears you are missing information typically present in an Architectural Decision Record (ADR).  To formulate a proper decision, I need more details about the existing behavior, problem statement, and potentially the history and references. \n\nPlease provide the missing sections:\n\n* **Context:** Describe the background and overall situation leading to this decision.\n* **Existing Behavior:** Explain how the system currently works related to the decision being made.\n* **Problem Statement:** Clearly define the issue or challenge that needs to be addressed.\n* **History (optional):** Outline any previous attempts to solve this problem and their outcomes.\n* **References (optional):** List any relevant documentation, ADRs, or external resources.\n\nOnce you provide this information, I can help you formulate a well-informed and appropriate architectural decision. \n","GenTime":"2024-08-28 12:24:49"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"text":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n\n\n##Decision\n* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","MatchedIds":[979,1575,1372,3488,775],"Prediction":"## Decision\n\nWe will implement a **store-and-forward mechanism with selective synchronization** between the fog and the cloud for device contextual representation and data. This approach aims to balance simplicity with efficiency, especially considering the constrained environment.\n\nHere's a breakdown of the decision:\n\n**Store-and-forward:** \n\n* The fog service will store all device data locally, ensuring data persistence even during intermittent cloud connectivity.\n* When the connection to the cloud is available, the fog service will forward the stored data.\n\n**Selective Synchronization:**\n\n* Instead of forwarding all data indiscriminately, we will implement a selective synchronization strategy based on:\n    * **Message Type:** Prioritize critical messages like device registration and schema updates over raw sensor data. \n    * **Data Relevance:**  Implement a data aggregation or filtering mechanism on the fog to reduce data redundancy and prioritize relevant information for the cloud. This could involve:\n        * Sending only data changes (delta updates) instead of full state updates.\n        * Applying time-based aggregation (e.g., sending hourly averages instead of raw readings).\n        * Using thresholds or event triggers for data forwarding (e.g., send data only when a specific value is exceeded).\n    * **Cloud Connectivity:**  Implement a queuing mechanism on the fog to prioritize critical messages during limited bandwidth or unreliable connectivity. Less critical data can be sent when the connection is more stable.\n\n**Rationale:**\n\n* **Reduced Network Load:** By selectively synchronizing data, we minimize the bandwidth consumption and network load, which is crucial for constrained environments and potentially unreliable connections.\n* **Improved Responsiveness:** Prioritizing critical messages ensures timely updates on device registration and schemas, even with limited bandwidth.\n* **Data Prioritization:**  Focusing on relevant data allows for efficient cloud processing and storage by reducing data redundancy.\n\n**Future Considerations:**\n\n* We can further enhance the synchronization strategy with more sophisticated data processing techniques at the fog layer, like machine learning for anomaly detection, data compression, or edge analytics.\n* Explore implementing a publish-subscribe model for data exchange between fog and cloud, allowing for more flexible and scalable data sharing.\n\nThis approach provides a good balance between simplicity and efficiency for the first stage of the new fog service integration. It lays the foundation for a more robust and scalable solution as the system evolves. \n","GenTime":"2024-08-28 12:25:01"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"text":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n\n\n##Decision\nThe `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","MatchedIds":[359,1180,2968,2471,2181],"Prediction":"## Decision\n\nA Marain instance is composed of the following:\n\n1. **Services:** The core functional units of Marain. Each service encapsulates a specific business capability and exposes a well-defined API for interaction. Examples of Marain services include:\n    * **Workflow Service:** Manages and executes business workflows.\n    * **Operations Service:** Provides monitoring, logging, and alerting functionality.\n    * **Tenancy Service:** Manages tenants, their hierarchy, and associated configurations. \n2. **Infrastructure:** The underlying infrastructure on which the Marain services are deployed and operated. This encompasses:\n    * **Compute Resources:** Virtual machines, containers, or serverless functions that execute the service code.\n    * **Storage:** Databases, object stores, and caches used by the services to persist and retrieve data.\n    * **Networking:** Load balancers, API gateways, and service meshes that route traffic between services and external clients.\n3. **Configuration:** A set of parameters that define the behavior and appearance of a Marain instance. This includes:\n    * **Service Configuration:** Specific settings for each service, such as connection strings, API keys, and feature toggles.\n    * **Tenant Configuration:** Customized configurations for individual tenants, such as storage preferences, regional settings, and service entitlements.\n    * **Instance-Level Configuration:** Global settings that apply to the entire Marain instance, such as logging levels, security policies, and resource quotas.\n4. **Data:** The information managed and processed by the Marain services. This data is owned by the tenants and can be categorized as:\n    * **Tenant Data:**  Business-specific data belonging to a particular tenant.\n    * **System Data:**  Operational data generated by the Marain platform itself, such as logs, metrics, and audit trails.\n\nEach Marain instance is logically isolated from other instances, ensuring data segregation and independent operation. This enables multiple deployments of Marain, such as a dedicated development instance, a shared staging environment, and isolated production instances for each customer or environment. \n","GenTime":"2024-08-28 12:25:12"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"text":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n\n##Decision\nWe will run a self hosted set of Docker tools\\n","MatchedIds":[3598,2997,3602,923,629],"Prediction":"## Decision\n\nWe will use **Option 3: Self-hosted Docker Tools in cluster**. \n\nWhile this option requires more upfront effort, it offers the best combination of control, integration, and cost-effectiveness in the long run. \n\nHere's a breakdown of our reasoning:\n\n* **Control & Security:**  Self-hosting gives us full control over the configuration and security of our image registry and scanning processes. We can tailor it to our specific needs and integrate it tightly with our existing infrastructure.\n* **Integration:** This option allows for seamless integration with our current AWS infrastructure, leveraging S3 for storage and potentially integrating with other AWS security and monitoring tools.\n* **Cost-Effectiveness:** Although there is an initial investment in setup and maintenance, self-hosting can be more cost-effective in the long term compared to a SaaS solution, especially as our usage scales. \n* **Future-Proofing:**  AWS is constantly evolving. By self-hosting, we can adopt new features and tools at our own pace and avoid vendor lock-in. \n\nWe acknowledge the initial setup and maintenance effort required for Option 3. However, we believe the benefits of tighter integration, enhanced security, potential cost savings, and future-proofing outweigh these drawbacks. \n","GenTime":"2024-08-28 12:25:28"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"text":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n\n##Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","MatchedIds":[2405,3950,866,578,3217],"Prediction":"## Decision\n\nWe will manage authorization separately from authentication. While Auth0 will handle user authentication, roles and permissions will be managed within our application. \n\nThis approach offers several advantages:\n\n* **Decoupling:**  Avoids tight integration with Auth0, maintaining flexibility to switch authentication providers in the future.\n* **Fine-grained control:** Allows for customized role and permission definitions specific to our application's needs.\n* **Enhanced security:** Separation of concerns limits the impact of potential vulnerabilities in a single provider.\n\nWe will investigate and select an appropriate authorization library or framework compatible with our technology stack to implement this.\n","GenTime":"2024-08-28 12:25:35"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"text":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n\n\n##Decision\nIt was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","MatchedIds":[4179,1258,4338,3203,1387],"Prediction":"## Decision\n\nWe will implement a hybrid approach that allows for incremental testing improvements while waiting for the dependency injection RFC:\n\n1. **Introduce a new `ServiceLocator` class:** This class will provide access to key dependencies like `WikibaseRepo`. Initially, it will return the actual singleton instances. \n2. **Update `RepoHooks` to use the `ServiceLocator`:** Instead of directly accessing `WikibaseRepo`, the `RepoHooks` methods will use the `ServiceLocator` to retrieve the dependency. \n3. **Create a `TestServiceLocator`:** This subclass of `ServiceLocator` will allow for injecting mock dependencies in test environments. \n4. **Gradually refactor `RepoHooks` methods:** For each method or group of related methods, refactor them to accept dependencies as arguments. This will make them individually testable. During testing, these dependencies can be provided via the `TestServiceLocator`. \n\nThis approach offers several benefits:\n\n* **Incremental refactoring:** We can gradually improve the testability of `RepoHooks` without a large upfront rewrite.\n* **Testability without extensive mocking:** By injecting dependencies at the method level, we can often test individual methods without mocking the entire `WikibaseRepo` object.\n* **Preparation for dependency injection:** Once the dependency injection RFC is implemented, we can easily switch from the `ServiceLocator` to the new mechanism.\n\nThis approach provides a pragmatic solution to increase test coverage in the interim before a larger architectural change with dependency injection is implemented. \n","GenTime":"2024-08-28 12:25:44"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"text":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n\n##Decision\nChosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","MatchedIds":[4096,718,4370,1376,771],"Prediction":"## Decision\nWe will use Docker Compose to define and manage the PostgreSQL service. This approach offers several advantages:\n\n* **Simplicity**: Docker Compose simplifies the process of defining and running multi-container applications. In this case, our service tests can easily spin up a PostgreSQL instance with a single command.\n* **Reproducibility**:  Docker Compose ensures a consistent environment for running service tests across different machines and developers. This consistency minimizes \"works on my machine\" issues.\n* **Automation**: We can integrate Docker Compose into our CI\/CD pipeline to automatically start the PostgreSQL container before the service tests and shut it down afterward. This automation reduces manual intervention and ensures tests run against a fresh database instance.\n\n### Additional Considerations:\n\n* We can leverage a pre-built PostgreSQL image from Docker Hub to further simplify the setup process.\n* For persistent data storage, we can configure Docker Compose to use a named volume, ensuring data persists across container restarts. \n","GenTime":"2024-08-28 12:25:50"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"text":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n\n\n##Decision\n![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","MatchedIds":[1376,4041,1767,4047,2410],"Prediction":"## Decision\n\nWe will implement a split DNS configuration utilizing:\n\n* **Internal DNS servers:** These servers will be responsible for resolving internal hostnames (e.g., Puppet, Graphite, Logstash) and will be accessible only within the internal network. This ensures security and separation of concerns.\n* **External DNS provider:** We will utilize a managed DNS provider (e.g., AWS Route53, Google Cloud DNS) for public-facing DNS records. This provides high availability, scalability, and easy integration with other cloud services.\n\n**Implementation details:**\n\n* **Internal DNS zones:** Create separate internal DNS zones for each environment (e.g., production, staging, integration) to facilitate parallel deployments and testing. \n* **DNS records for internal services:** Maintain A records within the internal DNS zones pointing to the private IPs of infrastructure services.\n* **Public DNS records:** Utilize CNAME records to map public subdomains (e.g., alerts.integration) to load balancers or specific instances based on the active stack.\n* **DNS-based traffic routing:** Implement weighted routing or failover mechanisms within the external DNS provider to control which stack receives production traffic and ensure seamless transitions between environments.\n* **Testing environments:** Utilize DNS records to direct traffic to specific environments during testing. This allows for validation of new infrastructure and applications without impacting the live site.\n* **Publishing domain mapping:** Configure the external DNS provider to delegate the \"publishing.service.gov.uk\" subdomain to the appropriate load balancer or IP address based on the active production stack.\n\nThis approach provides a flexible and robust DNS architecture that meets the requirements for resolving internal and external services, facilitating testing and deployments, and ensuring seamless user experience with the publishing domain. \n","GenTime":"2024-08-28 12:26:00"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"text":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n\n\n##Decision\nWe should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","MatchedIds":[3682,4868,1387,3156,2689],"Prediction":"## Decision\n\nWe will refactor the testing process to eliminate the need for manual modification of the `window` object and the implicit ordering dependency between `ElmContext` instantiation and Elm code evaluation. \n\nInstead of relying on side effects from `ElmContext` to modify the global environment, we will:\n\n1. **Change `ElmContext` to be a simple object that encapsulates all the fake objects.** This object will be passed as an argument to the compiled Elm code.\n2. **Modify the `Compiler` from elm-spec-core to generate code that accepts the `ElmContext` object as an argument.**  This eliminates the need for an IFFE and allows for explicit dependency injection.\n3. **Update elm-spec-runner, karma-elm-spec-framework, and the harness to use this new mechanism.** They will be responsible for creating the `ElmContext` object and passing it to the compiled Elm code.\n\nThis approach provides the following benefits:\n\n* **Simplified API:** Test writers no longer need to manage the order of operations or interact with the `window` object directly.\n* **Improved testability:** Decoupling `ElmContext` from global state makes it easier to test in isolation.\n* **Reduced risk of errors:** Explicit dependency injection makes the relationship between components clearer, leading to more robust code. \n","GenTime":"2024-08-28 12:26:07"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"text":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n\n\n##Decision\nTry first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","MatchedIds":[319,1741,3504,3456,3109],"Prediction":"## Decision\n\nWe will use **CircleCI** as our job manager for the following reasons:\n\n* **Existing Knowledge:** We have prior experience with CircleCI, which can accelerate the setup and development of our CI\/CD pipelines. Leveraging existing knowledge reduces the learning curve and potential for errors.\n* **Free Tier Availability:**  CircleCI's free plan provides a low-risk opportunity to prototype and test our pipelines before committing to a paid plan. This allows us to validate our approach and assess CircleCI's suitability for our needs.\n* **Kubernetes Integration:** CircleCI offers good integration with Kubernetes, including features for container building, image pushing, and deployments. This streamlined integration simplifies the orchestration of our application lifecycle within the Kubernetes cluster. \n* **Avoid Jenkins Complexity:** While Jenkins is a powerful tool, it can be complex to set up and manage, especially for a small team or project. Jenkins X also introduces additional complexity which may be unnecessary at this stage. CircleCI offers a more user-friendly and streamlined experience.\n\n### Next Steps:\n\n1. **Design CI\/CD Pipeline:** We will create a proof-of-concept CircleCI pipeline to build, test, and deploy a sample application to our Kubernetes cluster.\n2. **Evaluate CircleCI:**  Based on the proof-of-concept, we will evaluate CircleCI's performance, ease of use, and suitability for our specific application and deployment requirements.\n3. **Migration Plan:** If satisfied with the evaluation, we will develop a plan to migrate existing CI\/CD workflows from SolanoCI to CircleCI.\n\n### Future Considerations:\n\n* We will re-evaluate Jenkins and Jenkins X if CircleCI doesn't meet our long-term needs or if our project scales significantly.\n* We will explore other CI\/CD solutions like GitHub Actions if they offer compelling advantages for our use case.\n","GenTime":"2024-08-28 12:26:19"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"text":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n\n\n##Decision\nWe will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","MatchedIds":[3657,578,1180,429,3692],"Prediction":"## Decision\n\nWe will adopt a containerized approach for defining and isolating dApps within Titania. \n\nEach dApp will be packaged and run within its own Docker container. This provides several benefits:\n\n* **Isolation:** Each dApp operates within its own environment, preventing conflicts in dependencies and resources.\n* **Security:** Enhanced security through containerization limits the impact of a compromised dApp.\n* **Portability:** Containerized dApps can be easily deployed and managed across different environments.\n* **Scalability:**  Independent scaling of individual dApps based on their specific resource needs.\n\nTo facilitate communication and interoperability between dApps, we will utilize a lightweight message broker. This allows for asynchronous communication and decouples the dApps, enhancing modularity and scalability. \n\nThis approach provides a flexible and scalable architecture for running multiple dApps within Titania while maintaining isolation and security. \n","GenTime":"2024-08-28 12:26:24"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"text":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n\n\n##Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","MatchedIds":[4656,21,842,4650,810],"Prediction":"## Decision\n\n### Leveraging `baseapp.router` for Packet Routing\n\nThis ADR proposes to simplify the implementation of the ICS 26 Routing Module by leveraging the existing message routing capabilities of `baseapp.router` instead of creating a separate routing module. \n\nThis approach offers several benefits:\n\n* **Reduced Boilerplate:** Eliminates the need for a dedicated routing module, simplifying the codebase and reducing potential for errors.\n* **Improved Ergonomics:** Integrates smoothly with the SDK's existing message handling mechanism, making it easier for developers to work with IBC packets. \n* **Leveraging Existing Functionality:** Capitalizes on the robustness and maturity of `baseapp.router`, minimizing the need for new code and potential vulnerabilities.\n\nTo achieve this, we will use the packet's destination port and channel as routing identifiers within `baseapp.router`. This allows us to directly route incoming packets to the appropriate module's handler.\n\n### Handling Atomic Multi-message Transactions with `CodeTxBreak`\n\nTo address the challenge of atomic multi-message transactions involving IBC packets, this ADR proposes the introduction of a new `CodeType`, `CodeTxBreak`. \n\nHere's how it works:\n\n* **Partial Transaction Execution:** `CodeTxBreak` allows for the partial execution of a transaction. When triggered, it halts the execution of subsequent messages in the transaction.\n* **IBC State Preservation:** Crucially, while `CodeTxBreak` prevents further application-level message execution, it preserves any IBC-related state changes made by preceding messages. \n* **Example:** In a transaction containing an IBC token receive message followed by a stake delegation message, if the token receive encounters an error, `CodeTxBreak` can be used. This halts the transaction before the stake delegation, preventing unintended consequences while ensuring the IBC transfer's effects are preserved.\n\n**Implementation Details:**\n\n* A new error code `CodeTxBreak` will be added to the SDK's error handling mechanism.\n* The IBC module will be modified to return `CodeTxBreak` in scenarios where a transaction needs to be halted but IBC state changes should be preserved.\n* The `baseapp` will be updated to recognize `CodeTxBreak` and handle it appropriately, stopping further message execution while preserving relevant state.\n\n**Benefits:**\n\n* **Atomicity and Consistency:**  Maintains consistency within IBC operations while preventing inconsistent state in multi-message transactions.\n* **Improved Error Handling:** Provides a more nuanced approach to error handling in IBC-related transactions, allowing for safer and more predictable behavior. \n* **Enhanced Developer Experience:** Offers a clear mechanism for developers to manage complex transaction flows involving IBC. \n\nBy leveraging `baseapp.router` and introducing `CodeTxBreak`, this ADR presents a practical and efficient approach to implement ICS 26 Routing Module while addressing the complexities of atomic multi-message transactions involving IBC. This leads to a cleaner, more robust, and developer-friendly IBC implementation within the Cosmos SDK. \n","GenTime":"2024-08-28 12:26:47"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"text":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n\n\n##Decision\nFor a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","MatchedIds":[2888,2892,2134,1631,3853],"Prediction":"## Decision\n\nWe will define three artifact quality levels: **Production**, **Incubating**, and **Experimental**:\n\n* **Production**:  Artifacts designated as Production are designed for use in a wide range of environments, including mission-critical systems. \n    * **Requirements**:\n        * Thoroughly tested with a well-defined test suite boasting high code coverage.\n        * Stable API with clear documentation and backward compatibility guarantees.\n        * Performance optimized and regularly benchmarked.\n        * Security audited and vulnerabilities actively addressed.\n        * Maintained with a clear roadmap and long-term support plan.\n    * **Examples**: Core James Mail Server, established James libraries.\n* **Incubating**: Artifacts classified as Incubating represent promising functionality, but may not yet meet all the criteria for Production. \n    * **Requirements**:\n        * Actively developed with a growing test suite.\n        * API subject to change, with clear communication about potential breaking changes.\n        * Performance under evaluation and optimization ongoing.\n        * Security best practices followed, but formal audits might be pending.\n        * Supported with community engagement encouraged.\n    * **Examples**: New James extensions, evolving command-line tools.\n* **Experimental**: Experimental artifacts provide access to cutting-edge features or functionalities still under heavy development. \n    * **Requirements**:\n        * Basic testing and documentation, but not guaranteed to be complete or up-to-date.\n        * API expected to change significantly, with no guarantee of backward compatibility.\n        * Performance not a primary focus at this stage.\n        * Security implications may not be fully understood.\n        * Support limited to community involvement.\n    * **Examples**:  Early prototypes of James features, research-oriented components.\n\n**Feature Quality Levels:**\n\nJames features will inherently inherit the quality level of the artifact they belong to.  For example, a feature implemented within a Production-level James Mail Server will also be considered Production.\n\n**Communication:**\n\n- We will clearly document the quality level of each artifact and feature within its respective README file, website documentation, and release notes. \n- Version numbers of artifacts may be used to further indicate stability (e.g., 1.x.x for Production, 0.x.x for Incubating\/Experimental).\n\nThis classification will provide transparency to James users, enabling them to make informed decisions regarding the suitability of different components for their specific needs and risk tolerance.\n","GenTime":"2024-08-28 12:27:03"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"text":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n\n##Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","MatchedIds":[3521,3853,1404,4224,5255],"Prediction":"## Decision\n\nWe will use Babel to transpile our JavaScript code. \n\n### Chosen Option: Babel + Preset Env + Linting + Bundling\n\nWe will integrate Babel into our build process to transpile modern JavaScript code into code compatible with older environments. This will allow developers to utilise the latest language features without worrying about compatibility issues with older browsers or environments. \n\n1. **Babel:** Babel is a key tool for transpiling modern JavaScript syntax (like arrow functions, classes, and destructuring) into code that older JavaScript engines understand.\n2. **Preset-Env:** We'll use Babel's `preset-env` to target specific JavaScript environments. This ensures that our code only includes the necessary polyfills and transformations, optimizing the final bundle size.\n3. **Linting:**  We will implement a linter like ESLint to enforce code style consistency and catch potential errors early in the development process. \n4. **Bundling:** We will use a bundler like Webpack or Rollup to combine and minify our JavaScript code for efficient delivery.\n\n### Configuration\n\n* **`.babelrc` (or Babel configuration file):**\n   ```json\n   {\n     \"presets\": [\n       [\n         \"@babel\/preset-env\",\n         {\n           \"targets\": \"> 0.5%, last 2 versions, ie 11\" \/\/ Adjust browser targets as needed\n         }\n       ]\n     ]\n   }\n   ```\n* **Linter Configuration:** Configure the linter (e.g., ESLint) to enforce best practices and catch common errors. \n* **Bundler Configuration:** Set up the bundler (e.g., Webpack) to process JavaScript files through Babel during the build process.\n\n### Positive Consequences\n\n* **Modern JavaScript:** Developers can write cleaner, more maintainable code using the latest JavaScript features (ES6+, ESNext).\n* **Broader Compatibility:** The codebase remains compatible with older browsers and JavaScript environments, expanding potential reach.\n* **Improved Development Experience:**  Linting helps identify code quality issues early on. \n* **Optimized Delivery:**  Bundling reduces the number of requests and the overall size of the JavaScript code, leading to faster page load times. \n\n### Negative Consequences\n\n* **Build Process Complexity:**  Adding Babel, a linter, and a bundler adds complexity to the build process.\n* **Potential for Polyfill Bloat:** If not carefully configured, `preset-env` could include unnecessary polyfills, increasing bundle size.\n\n### Mitigation\n\n* **Documentation and Training:** Provide clear documentation and training on the setup and usage of Babel, linting, and bundling for the development team.\n* **Regular Dependency Updates:** Keep Babel, linters, and bundlers updated to benefit from performance improvements and bug fixes. \n","GenTime":"2024-08-28 12:27:18"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"text":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n\n##Decision\nThe format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","MatchedIds":[1308,1293,1319,1291,1302],"Prediction":"## Decision\nWarnings will be saved as metadata using the `warnings` prefix with an array notation conforming to the established [array decision](array.md). This removes the arbitrary limitation of 100 warnings and enforces consistency in array representation.\n\nHere's an example of the new notation:\n\n```\nwarnings\/#0 = \"This is the first warning\"\nwarnings\/#1 = \"This is the second warning\"\n```\n\nThis approach offers several advantages:\n\n- **Consistency:**  Aligns warning representation with the standard array notation used within Elektra.\n- **Scalability:** Eliminates the previous limitation of 100 warnings, allowing for a more flexible and scalable solution. \n- **Clarity:** Provides a more understandable and consistent way to store and retrieve warnings. \n","GenTime":"2024-08-28 12:27:23"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"text":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n\n\n##Decision\nzsh should be used in place of bash.\\n","MatchedIds":[2968,1239,4336,1243,4497],"Prediction":"## Decision\n\nGiven that macOS Catalina and later will default to `zsh`, we will adopt `zsh` as the supported shell for our development and deployment environments going forward.\n\n### Justification\n\n* **Industry Standard:** Apple's decision to switch signifies a broader shift in the Unix-based world towards `zsh`. Aligning with this change future-proofs our systems and takes advantage of `zsh`'s improvements over `bash`.\n* **Improved Developer Experience:** `zsh` offers features like enhanced tab completion, plugin support, and theming, leading to increased productivity and a more enjoyable development experience.\n* **Reduced Maintenance Overhead:** Maintaining compatibility with both `bash` and `zsh` will add unnecessary complexity. Focusing on a single, modern shell streamlines our efforts.\n\n### Transition Plan\n\n* **Documentation:** Update all documentation and guides to reflect `zsh` as the default shell.\n* **Tooling:** Ensure all internal scripts and tooling are compatible with `zsh` and leverage its features where applicable.\n* **Developer Support:** Provide resources and assistance to help developers transition smoothly to using `zsh`. This can include workshops, documentation, and internal support channels.\n\n### Alternatives Considered\n\n* **Continuing to support `bash`:** This adds maintenance overhead and prevents us from fully embracing `zsh`'s benefits.\n* **Mandating `zsh` immediately:** This could be disruptive for developers who are unfamiliar with `zsh`.\n\n### Consequences\n\n* Developers will need to learn and adapt to `zsh`. However, the benefits outlined above outweigh the learning curve.\n* Existing scripts relying on specific `bash` features may require updates.\n* By proactively embracing this change, we ensure our systems remain current and take advantage of the latest advancements in shell environments. \n","GenTime":"2024-08-28 12:27:39"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"text":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n\n##Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","MatchedIds":[442,446,447,1243,443],"Prediction":"## Decision \n\nGiven the requirement to scrape data from the Koshvani platform and store it in a machine-readable format, we will utilize a combination of:\n\n- **Scrapy**: A Python-based framework ideally suited for web scraping tasks. Scrapy provides a robust and efficient mechanism for extracting data from websites.\n- **JSON**: As a data format, JSON offers excellent human readability while being highly suitable for machine processing. Scraped data will be structured and stored as JSON files. \n\nThis approach balances functionality and maintainability, allowing for efficient data extraction and readily accessible storage.\n","GenTime":"2024-08-28 12:27:43"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"text":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n\n\n##Decision\n- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","MatchedIds":[1067,4305,128,2703,118],"Prediction":"## Decision\n\n**Chosen option:** Implement a plugin system based on loading command definitions from various sources.\n\n**Reasoning:**\n\n- **Flexibility:** A plugin system provides the most flexibility for users to extend the CLI tool with their own commands and share them with others. \n- **Decoupling:**  This approach decouples the core CLI logic from the actual command implementations, making the tool more maintainable and extensible.\n- **Future-proof:**  Supporting various repository types (HTTP, Github, local) from the start ensures the tool can evolve alongside user needs and preferences.\n\n**Implementation details:**\n\n1. **Command Definition:** Define a standardized format for command definitions (e.g., using YAML or JSON) that includes the command name, description, arguments, options, and execution logic.\n\n2. **Plugin Loading:**\n   - **Remote Repositories:**  The CLI tool can download and cache command definitions from specified HTTP URLs or Github repositories. Versioning can be implemented using tags or branches.\n   - **Local Repositories:**  Users can specify local files or directories containing command definitions.\n\n3. **Plugin Management:**\n   - Implement commands like `add-repo`, `remove-repo`, `list-repos`, `update-repo` to manage the list of active repositories.\n   - Store repository information in a user configuration file (e.g., in the user's home directory) for persistence.\n\n4. **Security:**\n   - Implement security measures to validate the source and integrity of downloaded command definitions (e.g., checksum verification, code signing).\n   - Provide mechanisms for users to trust or distrust repositories.\n\n**Positive Consequences:**\n\n- Increased flexibility and extensibility.\n- Easier sharing and distribution of custom commands.\n- Improved maintainability of the core CLI tool.\n- Fosters a community around plugin development. \n\n**Negative Consequences:**\n\n- Increased complexity in the CLI tool's architecture.\n- Potential security risks if proper validation and security measures are not implemented. \n","GenTime":"2024-08-28 12:28:01"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"text":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\n\n##Decision\n1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","MatchedIds":[3889,2752,1376,3109,2968],"Prediction":"## Decision\nWe will use a combination of Namespaces and clearly defined Kubernetes resource specifications to achieve a balance between ease of use and flexibility for service teams.\n\n* **Namespaces:** Strict separation of environments (dev, prod, etc.) and service teams will be enforced using Kubernetes Namespaces. This ensures multi-tenancy at the resource and security level.\n* **Default Configurations:** A \"Golden Path\" using opinionated Helm Charts and sensible defaults will be provided to enable a PaaS-like experience for straightforward deployments. This allows for fast onboarding and quicker iteration cycles for teams.\n* **Full Customization:** Service teams will retain the ability to define their own Kubernetes resource specifications within their namespace. This provides granular control over pod configuration, network policies, resource limits, and other Kubernetes features for teams needing specialized configurations.\n* **Clear Documentation and Tooling:** Comprehensive documentation, tutorials, and potentially internal tooling will be provided to guide service teams on utilizing the platform and navigating the balance between simplicity and control.\n\nThis approach empowers service teams with both a streamlined \"get started\" experience and the flexibility to leverage the full power of Kubernetes when necessary.\n","GenTime":"2024-08-28 12:28:07"}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"text":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n\n\n##Decision\nThe most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","MatchedIds":[3416,1545,2183,3253,4747],"Prediction":"## Decision\n\n**Logic related to onboarding and processing of feed files should be implemented within the ETL services instead of the CDH, based on the following considerations:**\n\n* **Modularity and Maintainability**: Separating feed-specific logic within ETL promotes modularity, making it easier to maintain, update, and troubleshoot individual feeds without impacting other parts of the system or the core CDH functionality.\n\n* **Flexibility and Scalability**:  ETL services are better suited to accommodate the diverse and evolving requirements of individual data feeds. This allows for greater flexibility in handling feed-specific transformations, validations, and error handling.\n\n* **Clear Responsibility**:  Placing onboarding and processing logic within ETL establishes a clear separation of concerns. CDH can focus on its core responsibilities (data storage, management, access) while ETL manages the complexities of data ingestion and preparation.\n\n**Further Decision Points:**\n\n* **Generic Logic**:  Generic logic applicable across multiple feeds should be implemented as reusable components within the ETL framework to prevent code duplication and ensure consistency. \n* **Continuous Improvement**:  A process for identifying common patterns and migrating feed-specific logic into reusable components within the ETL framework should be established to promote maintainability and scalability in the long term.\n\n**This approach ensures a more robust, maintainable, and scalable data pipeline by leveraging the strengths of both CDH and ETL services.** \n","GenTime":"2024-08-28 12:28:16"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"text":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n\n\n##Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","MatchedIds":[4354,1985,1558,1944,4101],"Prediction":"## Decision\n\nCurrently, we decided **not to handle amnesia attacks**. This decision is based on the following factors:\n\n* **Complexity:** Implementing a robust solution for detecting and proving amnesia attacks would significantly increase the complexity of the light client and consensus mechanisms. \n* **Limited Practical Impact:** Amnesia attacks are difficult to execute in practice, especially in a live network with a diverse validator set. The attack relies on a high degree of network manipulation and requires the attacker to control a significant portion of the voting power.\n* **Mitigation by Other Mechanisms:** Existing mechanisms like the two-thirds voting power requirement and the evidence handling process already mitigate the risk of malicious forks and double signing, which are the primary goals of amnesia attacks.\n\n**Instead of directly addressing amnesia attacks**, we will focus on strengthening the existing security mechanisms:\n\n* **Improved Network Monitoring:** Implement better monitoring tools to detect unusual voting patterns and potential network manipulation attempts.\n* **Enhanced Evidence Handling:**  Continuously improve the efficiency and robustness of the evidence handling process described in ADR 47 to ensure timely detection and punishment of malicious behavior. \n\n**Future Considerations:** \n\nWe acknowledge that amnesia attacks pose a theoretical security risk. We will continue to monitor the evolution of the threat landscape and re-evaluate this decision if:\n\n* Amnesia attacks become a practical threat due to changes in the network environment or the emergence of new attack vectors.\n* More efficient and practical solutions for detecting and proving amnesia attacks become available.\n\nUntil then, we believe that focusing on strengthening existing mechanisms and improving network monitoring provides a more practical and effective approach to mitigating the risks associated with light client attacks. \n","GenTime":"2024-08-28 12:28:24"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"text":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n\n##Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","MatchedIds":[4021,2498,1792,1758,1660],"Prediction":"## Decision\n\nWe will be adopting a lightweight Data Access Layer (DAL) approach combined with a modular code structure.\n\n**Rationale:**\n\n* **Simplicity and Control:**  A custom DAL provides the most control over database interactions and avoids the overhead and potential complexity of a full-fledged ORM. This aligns with the rejection of existing ORM solutions due to learning curves and debugging concerns.\n* **Flexibility:** This approach allows for the potential integration of query builders like Knex.js in the future, should the need for more complex querying arise. \n* **Maintainability:**  A clear separation of concerns with a DAL improves code organization and makes maintenance easier as the codebase expands. \n\n**Implementation Details:**\n\n* **Data Access Layer:** Create a dedicated directory (e.g., `.\/dal` or `.\/repositories`) for data access logic. \n* **Data Access Objects (DAOs):** Define a DAO class for each entity (e.g., `UserDAO`, `ProductDAO`) to encapsulate all database operations for that entity (CRUD operations).\n* **Modular Structure:** Organize code into feature-based modules to improve maintainability. Each module can contain its own routes, controllers, business logic, and DAOs.\n\n**Consequences:**\n\n* **Increased Initial Development Time:** Writing a custom DAL will require more upfront work compared to directly using an ORM.\n* **Potential for Code Duplication:**  Without careful abstraction, some data access code might be duplicated across DAOs.\n* **Long-Term Benefits:** This approach promotes cleaner code, easier testing, and greater flexibility in the long run, outweighing the initial time investment.\n","GenTime":"2024-08-28 12:28:34"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"text":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n\n##Decision\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","MatchedIds":[446,447,442,445,138],"Prediction":"## Decision\nNumerous sub-sections from Expenditure will be scraped and analysed in a phased approach to create sustainable data pipelines.\\\n\\\n**Phase 1**: Focus on developing robust and reliable data extraction jobs for each sub-section, prioritizing sections that are most likely to provide accurate accounting head information. \n\\\n**Phase 2**: Develop data validation and cross-checking mechanisms using information from different sections. This will help identify and address discrepancies and ensure data quality.\n\\\n**Phase 3**:  Design and implement a sustainable data architecture that supports long-term data storage, processing, and analysis. This architecture should be scalable, maintainable, and adaptable to future requirements. \n\nThis phased approach will allow for iterative development, continuous improvement, and the creation of a robust data infrastructure for analyzing Uttar Pradesh fiscal data over the next three years. \n","GenTime":"2024-08-28 12:28:40"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"text":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n\n##Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","MatchedIds":[979,1575,311,3295,885],"Prediction":"## Decision\n\n**Chosen option:**  Adopt **Apache Kafka** as the message broker for SDR.\n\n**Reasoning:**\n\n* **Decoupling and Scalability:** Kafka's pub-sub model directly addresses the service coupling issue. Services publish events without knowing the consumers. This allows for easier addition and scaling of services without impacting others.\n* **Resilience:** Kafka's fault-tolerant architecture with distributed storage and replication ensures message persistence and availability even with node failures, mitigating the unreliable network concerns.\n* **Performance:**  Kafka is designed for high-throughput asynchronous messaging. This is beneficial for scenarios like event reporting mentioned in the problem statement and eliminates the need for inefficient polling mechanisms.\n* **Flexibility:** Kafka's ability to handle various message formats and topics allows it to support different communication patterns within SDR.\n\n**Additional Considerations:**\n\n* **Learning Curve:** While Kafka offers significant advantages, it has a steeper learning curve than simpler brokers. Allocate resources for training and knowledge sharing.\n* **Operational Complexity:** Kafka's robust architecture requires dedicated operational effort. Assess if existing infrastructure and expertise can support it or if adjustments are needed.\n\n**Implementation Strategy:**\n\n1. **Pilot Project:**  Start by integrating Kafka with a small subset of services to gain experience and demonstrate its value.\n2. **Gradual Migration:**  Incrementally migrate other services to Kafka, prioritizing those facing the most significant challenges with the current architecture.\n3. **Monitoring and Evaluation:**  Implement robust monitoring and logging to track Kafka's performance and identify any potential issues.\n\n**Alternatives Considered:**\n\n* **RabbitMQ:** A viable option, but Kafka's focus on high-throughput and scalability makes it a better fit for SDR's anticipated growth and the unreliable network environment.\n* **Redis:** While Redis can function as a lightweight message broker, it might not offer the same level of fault tolerance and scalability as Kafka.\n\nThis decision aims to address the core issues of service coupling, scalability, and resilience within SDR, laying the groundwork for a more robust and maintainable architecture as the system evolves.\n","GenTime":"2024-08-28 12:29:07"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"text":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\n\n##Decision\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","MatchedIds":[160,3521,4600,3187,3784],"Prediction":"## Decision\n\n**Chosen option:**  Implement a numerically based spacing scale.\n\n**Rationale:** A numerically based spacing system provides the most flexibility and avoids the issues inherent in the current relative system. \n\n**Implementation Details:**\n\n* **Define a scale:**  Establish a set of numerical values representing common spacing increments. This could be a linear scale (e.g., 4, 8, 12, 16, 20, 24...) or a more customized scale to suit specific design needs.\n* **Units:** Determine whether to use abstract units (e.g., 1, 2, 3...) or map directly to pixel values.\n* **Naming:** Utilize clear, descriptive names for each spacing value (e.g., spacing-1, spacing-2, spacing-xs, spacing-sm, etc.). This enhances code readability and allows for future adjustments to the scale without renaming variables throughout the codebase.\n\n**Positive Consequences:**\n\n* **Flexibility:** Easily accommodate new spacing values as design requirements evolve.\n* **Maintainability:**  Changing the base spacing unit or overall scale becomes simpler and less prone to errors.\n* **Clarity:**  Numerical values provide unambiguous definitions for spacing, improving communication between designers and developers.\n\n**Negative Consequences:**\n\n* **Initial Implementation Effort:**  Requires refactoring existing code to use the new spacing system. However, this effort is a worthwhile investment in the long-term maintainability and scalability of the codebase.\n\n**Additional Considerations:**\n\n* **Design System Integration:**  Document the new spacing system thoroughly within the design system or style guide to ensure consistency across projects.\n* **Tooling:**  Investigate and implement tooling (e.g., linters, design system extensions) that can enforce adherence to the defined spacing scale and streamline its usage. \n","GenTime":"2024-08-28 12:29:17"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"text":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n\n\n##Decision\nWe have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","MatchedIds":[399,1660,2625,485,3071],"Prediction":"## Decision\n\nTo avoid coupling our libraries to a specific JSON serialization library like Json.NET or `System.Text.Json`, we will adopt a strategy based on **abstraction and dependency inversion**. \n\nWe will define an **abstraction layer** (e.g., interfaces) for JSON serialization and deserialization operations. This abstraction layer will define the common methods our libraries need (e.g., `Serialize<T>`, `Deserialize<T>`), without depending on a specific implementation. \n\nThen, we will create **separate concrete implementations** of this abstraction layer for both Json.NET and `System.Text.Json`. This allows our libraries to accept any implementation of the abstraction through dependency injection.\n\nThis approach offers several benefits:\n\n* **Flexibility:** Developers using our libraries can choose the JSON serializer they prefer.\n* **Testability:** Our libraries can be easily tested with different serialization implementations.\n* **Future-proofing:** We can easily add support for new JSON libraries in the future by simply implementing the abstraction layer.\n\nThis approach adds a small amount of upfront complexity but provides significant benefits in terms of flexibility and maintainability in the long run.\n","GenTime":"2024-08-28 12:29:24"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"text":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n\n\n##Decision\nWe will not run IPSec in our BOSH deployments.\\n","MatchedIds":[216,2410,4047,2236,2240],"Prediction":"## Decision\nGiven the information that the IPSec release used for end-to-end encryption between gorouter and diego cells is no longer maintained, we need to re-evaluate our security posture and choose a replacement. The following options are considered:\n\n**1. Migrate to WireGuard:**\n\nWireGuard is a modern, fast, and secure VPN tunnel that can provide the same level of encryption as IPSec. It is actively maintained, has a smaller codebase, and is generally considered easier to configure and manage.  We will investigate the feasibility of migrating from IPSec to WireGuard for securing traffic between gorouter and diego cells.\n\n**2. Explore Native Cloud Foundry Solutions:**\n\nCloud Foundry is constantly evolving. We will investigate if newer versions of routing-release or other Cloud Foundry components offer alternative end-to-end encryption mechanisms that can replace IPSec without relying on external solutions. \n\n**3. Continue Using IPSec with Mitigation:**\n\nIf immediate migration is not feasible, we will thoroughly assess the security risks associated with using an unmaintained IPSec release.  This includes analyzing the specific vulnerabilities present in the outdated release and implementing mitigating controls to minimize the risk of exploitation. This option should be considered a temporary solution.\n\n**Status:** Proposed\n\n**Consequences:** \n\n* **Migrating to WireGuard:** This offers a more secure and maintainable solution for end-to-end encryption between gorouter and diego cells. However, it involves migration effort and potential compatibility testing.\n* **Exploring Native CF Solutions:**  Leveraging native solutions simplifies management and can potentially offer performance benefits. However, the availability of suitable alternatives needs further investigation. \n* **Continuing with IPSec:** This exposes the platform to potential security risks due to unpatched vulnerabilities. Thorough risk assessment and mitigation strategies are mandatory if this option is chosen as a temporary measure.\n\nWe will decide on a specific course of action after evaluating the feasibility, security implications, and potential impact of each option. \n","GenTime":"2024-08-28 12:29:34"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"text":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n\n##Decision\nWe will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","MatchedIds":[3214,2578,276,2470,577],"Prediction":"## Decision\n\nThe application will implement user authentication using a SAML-based approach integrated directly into the application, rather than relying on the traditional Apache httpd module (mod_shib). \n\n**Justification:**\n\n* **Flexibility:** Direct SAML integration provides greater control and flexibility over the authentication flow within the application.\n* **Containerization:**  This approach aligns with the existing containerized setup, simplifying deployment and management.\n* **Exploration of Alternatives:**  While not officially supported by MIT, in-app SAML allows exploring alternative solutions that may better suit the application's needs. \n\n**Considerations:**\n\n* **Maintenance:** In-app SAML implementation might require more maintenance compared to relying on a pre-configured module like mod_shib.\n* **Compatibility:** Ensure chosen SAML library\/solution is compatible with MIT's identity provider and supports necessary attributes. \n* **Security:** Implement rigorous security measures when handling user authentication data and communication with the identity provider.\n","GenTime":"2024-08-28 12:29:40"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"text":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n\n\n##Decision\nWe will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","MatchedIds":[2183,1866,2067,4454,581],"Prediction":"## Decision\n\nWe will host the experimenter docs hub under the URL `https:\/\/experimenter.data.gov.uk`. \n\nThis URL meets the following criteria:\n\n* **Memorable:** It is short, descriptive, and easy to recall.\n* **Stable:**  It is not tied to a specific technology or implementation detail, allowing for future changes without affecting the URL. \n* **Part of data information architecture:** Using `data.gov.uk` clearly places this documentation within the broader data information ecosystem. \n\nThis decision assumes `data.gov.uk` is a suitable and accessible domain. If not, a similarly appropriate domain should be used.\n","GenTime":"2024-08-28 12:29:45"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"text":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n\n\n##Decision\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","MatchedIds":[1346,3351,1339,3657,1255],"Prediction":"## Decision\nDeprecate and remove `wb_terms`. \n\nInstead of relying on the problematic `wb_terms` table, leverage Wikibase's existing MediaWiki caching mechanisms (such as memcached in the Wikidata production environment) to store and retrieve the data necessary for rendering short form displays of items and properties. \n\nThis decision is based on the following:\n\n* **Minimal performance gains:**  The introduction of `wb_terms` did not result in significant performance improvements, as evidenced by the lack of noticeable changes and discussions during its implementation.\n* **Existing caching mechanisms:** Wikibase already utilizes MediaWiki's robust caching infrastructure, which can be effectively used to store and retrieve short form display data, mitigating the need for a dedicated table. \n* **Maintenance burden:**  Maintaining `wb_terms` is proving to be increasingly challenging and unsustainable in the long run.\n* **Consistency with Lexeme handling:**  The WikibaseLexeme extension already loads full entity data for short form display without major performance issues. Aligning item and property handling with this approach ensures consistency across entity types.\n\nBy removing the dependency on `wb_terms` and leveraging existing caching mechanisms, Wikibase can simplify its architecture, reduce maintenance overhead, and maintain consistent performance for rendering short form displays across all entity types. \n","GenTime":"2024-08-28 12:29:53"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"text":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n\n\n##Decision\nType-check via [mypy](http:\/\/mypy-lang.org\/).\\n","MatchedIds":[2652,3853,4359,3863,1416],"Prediction":"## Decision\n\nAdopt static type checking using a tool like [MyPy](http:\/\/mypy-lang.org\/). This will allow us to leverage existing type annotations to detect type errors during development, improving code reliability and maintainability without significant changes to the existing codebase. \n","GenTime":"2024-08-28 12:29:55"}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"text":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n\n\n##Decision\nA simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","MatchedIds":[302,429,1089,428,1091],"Prediction":"## Decision\n\n**Microservices architecture style** will be adopted for this project. \n\nThis decision considers the following factors:\n\n* **Scalability and maintainability**: Microservices allow for independent scaling and deployment of individual components, improving the application's resilience and making it easier to maintain.\n* **Technology diversity**: Microservices offer flexibility in choosing different technologies for different components, optimizing for specific needs.\n* **Team autonomy**:  Microservices align well with smaller, independent teams, promoting faster development cycles and efficient resource allocation.\n\nHowever, the decision should be further validated by analyzing the specific requirements and constraints of the project. This includes:\n\n* **Application complexity**: Microservices might be overkill for simple applications, introducing unnecessary overhead. \n* **Team expertise**:  Successful implementation of microservices requires specific skillsets and experience within the team.\n* **Infrastructure requirements**: Microservices generally require more sophisticated infrastructure and monitoring tools compared to other options. \n\nA thorough evaluation of these factors will ensure the chosen architecture style aligns with the project's needs and contributes to its success.\n","GenTime":"2024-08-28 12:30:02"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"text":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n\n##Decision\nSpotify is the only viable alternative.\\n","MatchedIds":[2215,847,2637,3069,3715],"Prediction":"## Decision\n\nSince none of the APIs offer a straightforward solution using IMDB IDs, we will use a combination of services. We will fetch artist data from Music Brainz using its existing IMDB ID integration. For artist images, we will utilize Spotify's API. \n\nAlthough Spotify uses a custom ID system, we can perform an additional lookup using the artist's name obtained from Music Brainz to find their Spotify ID and retrieve the image. This approach offers the best compromise, leveraging the strengths of each API while mitigating their shortcomings. \n\nWe acknowledge that this solution introduces an additional API call. However, we prioritize the user experience and believe that displaying artist images significantly enhances the visual appeal of Korin. \n","GenTime":"2024-08-28 12:30:06"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"text":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n\n\n##Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","MatchedIds":[4415,200,779,3069,3159],"Prediction":"## Decision\n\nWe will follow a Global Versioning strategy by prefixing API versions in the URI.\n\n**Rationale:**\n\n* **Clear Version Identification:**  `\/v1\/`, `\/v2\/`, etc. clearly indicates the API version a client is interacting with, reducing confusion and potential errors.\n* **Independent Evolution:**  Allows significant changes between versions without impacting existing clients. New versions can leverage different frameworks or technologies (e.g., migrating from REST to GraphQL) while older versions continue to operate.\n* **Simplified Maintenance:**  Clear separation between versions simplifies maintenance and issue resolution. Teams can focus on specific versions without the risk of unintended side effects on other versions.\n\n**Implementation Details:**\n\n* **Version Prefix:**  All API endpoints will be prefixed with the version number, e.g., `\/v1\/users`, `\/v2\/products`. \n* **Default Version (Optional):** Consider offering a default version (e.g., latest stable) if no version is specified in the request.  This can improve user experience for new clients. \n* **Versioning Policy:** Define a clear versioning policy, outlining when new versions are released, how long older versions are supported, and the deprecation process.  \n* **Documentation:**  Clearly document each API version, highlighting changes, new features, and deprecations.\n\n**Considerations:**\n\n* **Version Proliferation:** Over time, supporting multiple versions can increase complexity.  Implement a sunsetting policy for older versions to manage this.\n* **Client Migration:**  Provide clear migration guides and support for clients to transition to newer API versions.\n\n\nThis decision addresses the need to handle significant changes across the entire API while minimizing disruption for existing clients. It provides a structured approach to versioning, enabling the API to evolve over time while maintaining stability and backward compatibility.\n","GenTime":"2024-08-28 12:30:16"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"text":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n\n##Decision\nWe'll package components with their own inline styles. No external dependencies needed.\\n","MatchedIds":[3031,2701,36,697,1151],"Prediction":"## Decision\n\nExported components should be packaged with **zero shared dependencies**. \n\n### Rationale\n\nRequiring consuming applications to have the same dependencies creates a  \"tight coupling\" scenario with several drawbacks:\n\n* **Version Conflicts:** Consuming applications might need different versions of the same dependency, leading to potential conflicts and breakages.\n* **Increased Bundle Size:** Consuming applications would need to bundle unnecessary dependencies, even if they only use a small portion of the component library.\n* **Reduced Flexibility:**  Consuming applications are locked into specific dependency versions, hindering their ability to upgrade or experiment independently.\n\n**Solution:**\n\nUtilize a bundler like Rollup or Webpack to package components as self-contained modules with all dependencies bundled within. This ensures:\n\n* **Isolation:** Components function independently without relying on shared dependencies.\n* **Predictability:** Consuming applications receive consistent component behavior regardless of their own dependency tree.\n* **Reduced Bundle Size:** Only the necessary code for each component is included in the final bundle.\n\n**Caveat:**\n\nWhile striving for zero shared dependencies is ideal, some exceptions might exist, such as fundamental utilities or polyfills required for basic component functionality. In these cases, clearly document these peer dependencies and their minimum required versions. \n","GenTime":"2024-08-28 12:30:23"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"text":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n\n\n##Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","MatchedIds":[4483,3187,138,3853,1193],"Prediction":"## Decision\n**Approved**: The proposed approach of developing an accessor method to retrieve data from Open edX events is a reasonable first step to mitigate the risk of PII leakage. \n\n**Rationale:**\n\n* **Centralized Access Control:** By funneling all data access through this accessor method, we introduce a single point where PII leakage checks can be implemented and maintained. This improves maintainability and reduces the likelihood of accidental PII exposure.\n* **Clear Error Handling:**  The decision to throw exceptions for missing required keys and log warnings for missing optional keys provides valuable feedback during development and in production environments. This helps in identifying potential issues with data mapping and event structure early on.\n* **Future Enhancement:** Exploring fuzzy comparison techniques for key matching is a valuable next step. This can further strengthen the PII protection by identifying potential PII leaks even when key names are slightly different or obfuscated.\n\n**Recommendations:**\n\n* **Comprehensive PII Key List:**  Maintain an exhaustive and well-defined list of PII keys. This list should be regularly reviewed and updated as the platform and data structures evolve.\n* **Automated Testing:** Implement automated tests to validate that the accessor method correctly throws exceptions for missing required keys, logs warnings for missing optional keys, and handles PII data appropriately.\n* **Code Review Focus:** During code reviews, pay particular attention to the usage of the accessor method and ensure that no PII keys are being accessed or exposed unintentionally.\n* **Documentation:**  Document the accessor method, its purpose, expected behavior, and any PII-related checks it performs. This documentation will be valuable for developers maintaining and extending the event transformation functionality.\n\nBy implementing these recommendations in conjunction with the approved decision, the `event-routing-backends` system will be better equipped to prevent PII leakage and maintain data privacy as new transformers are added. \n","GenTime":"2024-08-28 12:30:33"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"text":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n\n\n##Decision\n### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","MatchedIds":[2968,138,5009,4415,3488],"Prediction":"## Decision\n\nWe will introduce the concept of **resource lifecycles** to model the different states and transitions a resource can undergo within the platform.  This approach offers several advantages:\n\n* **Flexibility:** Resource lifecycles allow defining custom states and transitions for different resource types, catering to various use-cases without hardcoding logic.\n* **Abstraction:**  By abstracting lifecycle management, the platform remains agnostic to specific resource implementations and focuses on managing state transitions.\n* **Reusability:**  Common lifecycle patterns can be defined once and reused across different resource types, improving consistency and reducing development effort.\n* **Automation:**  Lifecycle definitions can trigger automated actions based on state transitions, such as resource provisioning, de-provisioning, or archival.\n\n### Implementation Details\n\n* **Lifecycle Definition:**  A dedicated configuration mechanism will be used to define resource lifecycles. This could involve YAML files, a database schema, or a graphical interface for user-friendliness.\n* **State Transitions:**  Transitions between states will be triggered by events, either internal to the platform (e.g., resource creation) or external (e.g., user requests).\n* **State Management:**  The platform will keep track of the current state of each resource, allowing for querying and filtering based on lifecycle status.\n* **Action Execution:**  Each state transition can trigger specific actions, implemented as functions or scripts, ensuring consistency and automation.\n\nThis approach enables a flexible and adaptable platform that can accommodate diverse resource types and their respective lifecycle requirements. \n","GenTime":"2024-08-28 12:30:41"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"text":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n\n##Decision\nAll pointer event interactions have a mouse event fallback.\\n","MatchedIds":[669,847,528,1890,4940],"Prediction":"## Decision\n\nDue to the lack of support for `PointerEvents`  with reliable `movementX` and `movementY` properties in Safari and iOS, we will use the following approach:\n\n- **Primary Implementation (Mouse Events):**  We will implement the core event handling logic using standard Mouse Events (`mousedown`, `mousemove`, `mouseup`). This will provide reliable functionality across all major desktop browsers, including Safari.\n\n- **Touch Event Fallback (for Mobile):**  To support mobile devices, we will add a fallback mechanism that utilizes Touch Events (`touchstart`, `touchmove`, `touchend`). Since mobile is a secondary concern, this implementation will focus on basic functionality. \n\n- **Coordinate Tracking:** Since `movementX\/Y` are unreliable, we will implement our own system to track previous coordinates and calculate distances within both the mouse and touch event handlers. \n\n- **Future Consideration (Pointer Events):** We will encapsulate the event handling logic in a way that allows for easy integration of Pointer Events in the future, when browser support improves. This will simplify transitioning to a unified approach. \n","GenTime":"2024-08-28 12:30:48"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"text":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n\n\n##Decision\nThe data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","MatchedIds":[4245,3253,3657,3831,398],"Prediction":"## Decision\nA central, versioned data repository will be created to store test data for Horace, Herbert and Euphonic.\n\nAs a consequence: \n\n- A suitable technology will be selected for this repository (e.g., Git LFS, an artifact repository, shared network drive). \n- Access permissions will be set to allow read access for the relevant CI\/CD pipelines and developers. \n- Versioning will be implemented to track changes and allow tests to be run against specific data snapshots. \n- A clear process for updating the test data and documenting any changes will be established.\n","GenTime":"2024-08-28 12:30:54"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"text":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n\n##Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","MatchedIds":[2837,856,78,631,3789],"Prediction":"## Decision\n\nKafkarator will introduce two new CRDs:\n\n1. **KafkaratorTopic:** This CRD will be used to define and configure Kafka topics that the application needs. This will include properties like topic name, partitions, replication factor, and any other Kafka-specific configurations.\n2. **KafkaratorConsumerGroup:** This CRD will be used to define consumer groups for applications that consume messages from Kafka topics. This will specify the consumer group ID, the topic to consume from, and other consumer-specific configurations. \n\nDecoupling these concerns into two CRDs provides the following advantages:\n\n* **Clear separation of concerns:** Topic configuration is distinct from consumer group configuration. This makes the setup easier to understand and maintain.\n* **Flexibility:** Developers can define multiple consumer groups for a single topic, or a single consumer group consuming from multiple topics, as needed.\n* **Alignment with NAIS:** Providing distinct CRDs aligns well with the existing NAIS deployment process, allowing developers to specify both resources during their deployments. \n\nBy implementing these two CRDs, Kafkarator can provide a clean and flexible API for application developers to interact with Kafka. \n","GenTime":"2024-08-28 12:31:01"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"text":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n\n##Decision\nWe will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","MatchedIds":[3914,3295,1924,4852,1927],"Prediction":"## Decision\n\nWe will develop a minimal log shipping component, deployed as a [sidecar][sidecar] container alongside applications, which will:\n\n1. **Receive** application log events in [syslog format][syslog] from the GOV.UK PaaS provided [syslog drain][drains].\n2. **Transform** the log events from [syslog format][syslog] to [AWS CloudWatch Logs format][cloudwatch].\n3. **Forward** the transformed log events to the GDS Cyber Security provided [AWS Kinesis][kinesis] stream.\n\nThis approach provides several benefits:\n\n* **Leverages existing infrastructure:** Utilizes the GOV.UK PaaS [syslog drains][drains] and GDS Cyber Security [AWS Kinesis][kinesis] stream, minimizing infrastructure management overhead. \n* **Simplified configuration:** The log shipping component will have a simple configuration, focused solely on connecting to the syslog drain and Kinesis stream.\n* **Testability:** The component's limited scope and clear inputs\/outputs will allow for straightforward testing strategies.\n* **Minimal application impact:** Applications will continue using the standard GOV.UK PaaS [syslog drain][drains] mechanism, minimizing changes to application code or deployment processes.\n\nWhile this approach does introduce a new component, its minimal footprint, ease of configuration, and testability outweigh the added complexity compared to alternatives. This solution balances utilizing existing infrastructure, minimizing application impact, and ensuring reliable log shipping to Splunk. \n","GenTime":"2024-08-28 12:31:10"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"text":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\n\n##Decision\n* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","MatchedIds":[78,70,1430,3635,5188],"Prediction":"## Decision\n\n### Self-Hosted `init`\n\n* **No automatic download**:  Instead of the `dapr init` command automatically downloading the Dapr runtime binaries, we will provide clear instructions and options for users to download the binaries themselves. This will give users more control over the specific version and source of the binaries they are using.\n* **Configuration-based setup:**  The `dapr init` command, when used in self-hosted mode, will focus primarily on generating the necessary configuration files for Dapr to run. This includes files like the default components YAML and potentially a configuration file for specifying the location of downloaded binaries.\n* **Explicit version selection:** If a user wants to use a specific version of Dapr, they can specify it during the `dapr init` process.  The CLI should guide users on where to obtain the desired binaries.\n* **Simplified installation:** Streamline the `init` process for self-hosted by minimizing external dependencies. The focus should be on setting up configuration and pointing Dapr to locally available binaries.\n\n### Self-Hosted `uninstall` \n\n* **Configuration and data removal:** `dapr uninstall` in self-hosted mode will remove Dapr's configuration files and data directories, but will not attempt to remove the Dapr runtime binaries themselves. \n* **Clear Communication:** Provide clear instructions to users on how to manually remove the Dapr runtime binaries if they choose to do so. This ensures users have complete control over their environment.\n\n### Benefits\n\n* **Flexibility:**  Users have more control over the Dapr runtime versions and where they obtain binaries.\n* **Transparency:** The installation process is more transparent as users are explicitly involved in obtaining and managing the Dapr binaries.\n* **Security:**  Downloading binaries directly gives users more control over the security of their setup. \n\n### Additional Considerations:\n\n* **Documentation:** Update documentation to reflect these changes clearly, guiding users through the self-hosted installation process.\n* **CLI Help:** Enhance the CLI help messages for the `init` and `uninstall` commands to accurately reflect the self-hosted behavior.\n* **Download Verification:** Explore options for providing guidance or mechanisms for users to verify the integrity of downloaded Dapr binaries, such as checksums or digital signatures.\n","GenTime":"2024-08-28 12:31:23"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"text":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n\n\n##Decision\nWe will use serverless infrastructure components where possible.\\n","MatchedIds":[2752,4937,1089,1182,302],"Prediction":"## Decision\n\nWe will adopt a serverless architecture for our application development to leverage its inherent advantages in scalability, flexibility, speed of deployment, and cost-effectiveness. This approach will allow us to focus on delivering business value and optimizing code rather than managing server infrastructure. \n","GenTime":"2024-08-28 12:31:25"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"text":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n\n\n##Decision\nIgnore missing `require`d keys (in help mode), but fail for every other error.\\n","MatchedIds":[1289,1308,1271,3571,1292],"Prediction":"## Decision\n\nIntroduce a dedicated flag `--help` (and its short form `-h`) for invoking the help message. This flag should bypass any other logic and print the help message regardless of any other errors or the state of `kdbOpen` or `kdbGet`. \n\nThis approach offers the following advantages:\n\n- **Clear Intent:** Using a dedicated flag clearly signals the user's intent to view the help message, separating it from other potential error states.\n- **Consistent UX:** Aligns with common command-line tool conventions, making the behavior predictable for users.\n- **Simplified Logic:** Avoids complex error handling and state management within the code by prioritizing the help flag. \n","GenTime":"2024-08-28 12:31:30"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"text":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n\n\n##Decision\nSecurity groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","MatchedIds":[621,2793,328,1372,2539],"Prediction":"## Decision\nWe have decided to use separate resources for security group rules in Terraform. While this introduces some additional boilerplate and separation between the group and rule resources, the benefits of more granular and efficient resource updates outweigh the drawbacks. This approach will minimize the blast radius of changes and prevent unnecessary resource re-creations, ultimately leading to faster and more predictable infrastructure updates. \n","GenTime":"2024-08-28 12:31:34"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"text":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n\n\n##Decision\nWe will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","MatchedIds":[2500,885,4896,429,2316],"Prediction":"## Decision\n\n**We will use a message broker to facilitate communication with integration events.** \n\nThis approach provides a centralized and reliable platform for:\n\n* **Asynchronous communication:** Decouples event producers and consumers, enhancing scalability and fault tolerance.\n* **Event persistence:** Ensures events are delivered even if consumers are temporarily offline.\n* **Scalability and flexibility:** Allows for easy addition of new event producers and consumers as the system evolves.\n\nThere are various message broker options available, such as RabbitMQ, Kafka, and others. The specific choice will depend on factors like expected message volume, desired performance characteristics, and integration with existing infrastructure. \n","GenTime":"2024-08-28 12:31:39"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"text":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n\n\n##Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","MatchedIds":[1883,1180,847,2181,4021],"Prediction":"## Decision\n\nWe will implement a Backend for Frontend (BFF) pattern to aggregate data from the BAG, BRK, and HR APIs. \n\n### Reasoning\n\n* **Data Aggregation:**  A BFF allows us to retrieve and combine data from multiple sources (BAG, BRK, HR) into a single, cohesive response tailored for the specific needs of the \"per address\" page. This reduces frontend complexity and minimizes the number of requests from the user's browser.\n* **Data Transformation:** The BFF can transform and format the data from the different APIs into a consistent structure optimized for display on the frontend. This simplifies frontend logic and ensures a unified user experience. \n* **Decoupling:** By having a dedicated BFF layer, we decouple the frontend from the complexities and potential changes in the underlying APIs. This allows for greater flexibility and easier maintenance in the future. \n\n### Implementation Details\n\n* The BFF will expose a single endpoint that accepts an address (or verblijfsobject identifier) as input.\n* It will then call the necessary endpoints of the BAG, BRK, and HR APIs to retrieve the relevant data.\n* The BFF will then aggregate, transform, and format the data into a JSON response tailored for the frontend's consumption.\n\nThis approach ensures efficient data retrieval, clean separation of concerns, and a more maintainable codebase. \n","GenTime":"2024-08-28 12:31:47"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n\n##Decision\nUse [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","MatchedIds":[1631,1495,1669,2725,2703],"Prediction":"## Decision\n\nWe will fork go-bindata and maintain our own internal copy, `go-internal-bindata`.  This will give us control over the dependency and allow us to simplify its usage to better suit our development processes. This decision sacrifices the ease of using an upstream dependency for increased control and reduced uncertainty. \n","GenTime":"2024-08-28 12:31:50"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"text":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n\n\n##Decision\nAfter discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","MatchedIds":[3917,3907,74,923,4847],"Prediction":"## Decision \n\nWe will **not** enforce a default image lifecycle policy on ECR repositories. \n\n### Justification\n\nWhile hitting the AWS limit of 1000 images per repository poses a risk, automatically deleting images without application teams' awareness can lead to:\n\n* **Loss of important image versions:** Teams may require older images for debugging, rollbacks, or auditing purposes.\n* **Broken pipelines:**  If build pipelines or deployment processes rely on specific image tags that are deleted unexpectedly, this can cause disruptions.\n* **Lack of control:** Teams should have the autonomy to manage their artifacts and implement cleanup strategies tailored to their specific needs and risk tolerance.\n\n### Recommendations\n\n1. **Empower teams:** Provide guidance and documentation to application teams on ECR best practices, including:\n    * **Image tagging strategies:** Encourage the use of meaningful tags that clearly indicate version, environment, and stability.\n    * **Lifecycle management:** Educate teams on ECR lifecycle policies and empower them to define and implement policies tailored to their applications.\n    * **Monitoring and alerts:** Advise teams on setting up monitoring and alerts for ECR storage usage to proactively manage their repositories.\n2. **Explore alternative solutions:** \n    * If storage limits become a concern, investigate increasing the ECR repository limit with AWS support.\n    * For long-term archival, consider solutions like Amazon S3 for storing older images. \n\nBy empowering teams and providing the right tools and information, we can ensure responsible ECR usage while avoiding the risks associated with a default image deletion policy. \n","GenTime":"2024-08-28 12:31:59"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"text":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n\n\n##Decision\nThe decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","MatchedIds":[861,847,3715,1472,3451],"Prediction":"## Decision\n\nThe Console UI will be developed using **React** as the main JavaScript framework.\n\nThis decision is based on the following factors:\n\n* **Popularity and Community Support:** React has a large and active community, providing access to a vast pool of resources, libraries, and skilled developers.\n* **Component-Based Architecture:** React's component-based architecture aligns well with the plugin-based extensibility design of the Console UI, enabling modularity and reusability.\n* **Performance:** React's virtual DOM and efficient rendering mechanisms contribute to a performant user experience.\n* **Alignment with Existing Expertise:**  While the context mentions limiting programming languages for core features, using a popular framework like React for the UI leverages existing front-end expertise within the development team or allows for easier onboarding of new developers.\n\nLibraries and tools for integrating React with the Go backend will be developed and maintained to ensure seamless communication and data exchange between the frontend and backend components. This approach balances the principle of limiting core technologies while leveraging the strengths of widely-adopted UI frameworks.\n","GenTime":"2024-08-28 12:32:06"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"text":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n\n\n##Decision\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","MatchedIds":[1924,3187,4644,160,4096],"Prediction":"## Decision\n\nChosen option: \"Structured logging with a shared schema\", because it addresses both driver requirements.\n\n### Positive Consequences\n* **Consistent format:**  A shared schema ensures all components output logs and error messages in a standardized, structured format (e.g., JSON). This makes parsing and analysis easier for both humans and machines, regardless of the originating language.\n* **Language agnostic:**  Structured logging can be implemented in Bash, Python, and Freemarker (Java) with relative ease, satisfying the first decision driver.\n* **Improved maintainability:** A well-defined schema and centralized logging logic reduces code duplication and simplifies maintenance.\n\n### Negative Consequences\n* **Initial implementation effort:**  Adapting existing components to adhere to the schema requires initial development effort. \n* **Potential performance impact:**  Structured logging may introduce a slight performance overhead compared to simple string formatting.  However, this can be mitigated through asynchronous logging and efficient serialization libraries.\n\n### Implementation Details\n\n1. **Define a schema:**  Establish a common schema for log messages, including fields for:\n    * Timestamp\n    * Severity level (DEBUG, INFO, WARNING, ERROR)\n    * Component name\n    * Message string\n    * Optional:  Error codes, stack traces, contextual data\n2. **Implement logging libraries\/functions:** Provide simple logging functions in each language (Bash, Python, Freemarker) that accept structured data and format it according to the schema.\n3. **Centralize log aggregation (optional):**  Consider a centralized log management system to aggregate logs from different components for easier analysis and monitoring.\n\nThis approach allows for a consistent and informative logging experience across Hamlet Deploy, improving both user understanding and developer debugging capabilities.\n","GenTime":"2024-08-28 12:32:16"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"text":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n\n\n##Decision\nTo use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","MatchedIds":[4848,2221,4011,4841,578],"Prediction":"## Decision\n* We will use an encrypted, signed cookie to store the session data client-side.\n* Encryption will be achieved using a symmetric key managed by the application.\n* We will leverage a Message Authentication Code (MAC) with the encryption key for data integrity and authenticity.\n* The key will be rotated regularly, and key rotation will be transparent to active user sessions. \n","GenTime":"2024-08-28 12:32:19"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"text":"## Context\\nWe discussed the relative merits of different JS package managers.\\n\n\n##Decision\nUse `yarn` instead of `npm` in all dev, test, and build environments.\\n","MatchedIds":[1202,1240,3031,2273,701],"Prediction":"## Decision\n\nWe will use **npm** for managing our JavaScript packages. \n\nWhile other package managers like **yarn** and **pnpm** offer benefits such as performance improvements and deterministic installs, **npm** remains the most widely used and understood package manager in the JavaScript ecosystem. \n\nThis decision prioritizes familiarity and ease of adoption for the majority of developers. We will re-evaluate this decision if our needs change in the future, such as encountering performance bottlenecks or requiring specific features offered by alternative package managers. \n","GenTime":"2024-08-28 12:32:22"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"text":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n\n\n##Decision\nUse session cookies.\\n","MatchedIds":[4848,4843,2221,3657,3802],"Prediction":"## Decision\nWe will use **storage-based cookies** for user sessions.\n\n**Reasoning:**\n\n* **Security:** The ability to remotely invalidate sessions on sign out is crucial for security. Cookie-based sessions are vulnerable to replay attacks if a user's cookie is compromised, even after they have signed out.\n* **Functionality:** Remote sign-out is a desirable feature for users and improves the overall user experience.\n* **Trade-offs:** While storage-based cookies introduce a slight performance overhead and reliance on additional infrastructure, the security and functionality benefits outweigh these drawbacks. \n* **Mitigation:** We can mitigate the risks of relying on external infrastructure by:\n    *  Using a highly available caching service like Redis for session storage to minimize performance impact and single points of failure.\n    *  Implementing robust monitoring and alerting for our session storage infrastructure.\n    *  Ensuring that sensitive data is never stored directly in the session and that session IDs have a short expiration time.\n\nBy using storage-based cookies, we prioritize security and provide a better user experience, accepting the manageable overhead of additional infrastructure. \n","GenTime":"2024-08-28 12:32:32"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"text":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n\n##Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","MatchedIds":[1661,1740,4832,3657,2520],"Prediction":"## Decision\n\n**Chosen option:** PostgreSQL (Postgres) \n\n**Reasoning:**\n\n* **Structured Data:**  The platform's reliance on structured data aligns well with PostgreSQL's strengths in handling relational data and complex queries.\n* **Query and Reporting Needs:** PostgreSQL's robust querying capabilities, including advanced indexing and its powerful query planner, make it well-suited for the platform's requirements for frequent queries and reports. \n* **Open Source and Community Support:** Both MySQL and PostgreSQL benefit from strong open-source communities. However, PostgreSQL is often praised for its adherence to SQL standards and its commitment to data integrity, which are valuable for a platform like tta smarthub.\n\n**Additional Considerations:**\n\n* **Specific Performance Needs:** While not detailed in the context, if there are very specific performance requirements, benchmarking both MySQL and PostgreSQL with realistic workloads would be advisable. \n* **Long-Term Maintenance:**  Factor in the team's existing expertise and the long-term maintenance aspects. Choose the database where your team feels more comfortable with administration and optimization.\n","GenTime":"2024-08-28 12:32:38"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"text":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n\n\n##Decision\nAn architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","MatchedIds":[1540,1173,4388,4642,1243],"Prediction":"## Decision\n\nAdopt the Application Factory pattern for structuring Flask applications. This involves:\n\n* **Creating a `create_app()` function:** This function will be responsible for creating the Flask app instance, configuring it, and registering blueprints and extensions. \n* **Configuration via Environment Variables and a `.env` file:**  Use environment variables for storing configuration values and a `.env` file (handled by a library like `python-dotenv`) for local development.\n* **Blueprints for Modularity:** Organize application functionality into separate blueprints (e.g., `auth`, `api`, `admin`) to improve code organization and maintainability.\n* **Centralized Extension Initialization:** Initialize all Flask extensions (like SQLAlchemy, Flask-Login, etc.) within the `create_app()` function after the app object is created.\n\n### Example:\n```python\nfrom flask import Flask\n\ndef create_app():\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_mapping(\n        # Default configuration options\n    )\n\n    # Load configuration from environment variables\n    app.config.from_pyfile('config.py', silent=True)\n\n    # Initialize extensions (example with SQLAlchemy)\n    from .models import db\n    db.init_app(app)\n\n    # Register blueprints\n    from .auth import bp as auth_bp\n    app.register_blueprint(auth_bp)\n\n    return app\n```\n\n### Benefits:\n\n* **Clean Structure:** Promotes a well-organized and maintainable codebase.\n* **Improved Configuration:** Simplifies configuration management for both the application and Flask extensions.\n* **Testability:**  Easier to write unit tests for individual parts of the application due to better modularization.\n* **Scalability:** Allows the application to grow organically as new features are added. \n","GenTime":"2024-08-28 12:32:48"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"text":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\n\n##Decision\nWe will use Python for Kafkarator.\\n","MatchedIds":[4761,3715,2393,472,3109],"Prediction":"## Decision\n\nWe will use **Python** to develop the Kubernetes operator. \n\nWhile Go is popular for writing operators, our existing experience with Python in the context of FIAAS makes it a more pragmatic choice. This reduces the learning curve and allows us to leverage existing knowledge and potentially even code. \n\nUsing Rust, while beneficial as a learning experience, might introduce unnecessary risk and complexity at this stage, especially given our time constraints and the need for a stable and maintainable operator. \n","GenTime":"2024-08-28 12:32:51"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"text":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n\n##Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","MatchedIds":[160,3521,4600,4766,1996],"Prediction":"## Decision\n\nChosen option: `lowercase-with-hypens.jpg`\n\n### Positive Consequences\n* Easy to process with any software that deals with filenames, not only Jekyll\n* No WTFs at creators that are used to this naming convention from slugs in URLs\n* Search engines consider hyphens as word separators.\n\n### Negative Consequences\n* None\n","GenTime":"2024-08-28 12:32:54"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"text":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n\n\n##Decision\nRejected (keep checks) due to time constraints\\n","MatchedIds":[2629,536,2687,4179,1258],"Prediction":"## Decision\n\nIntroduce a build flag that allows compiling the code with or without argument checking. In the default build, argument checking will remain active to ensure correctness. When performance is critical, the flag can be used to disable argument checks in the compiled code. This approach offers a balance between safety and performance. \n","GenTime":"2024-08-28 12:32:56"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"text":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n\n\n##Decision\nDescriptive metadata will not be stored by DOS.\\n","MatchedIds":[3369,3374,2859,3831,2183],"Prediction":"## Decision \nDOS will only store administrative metadata required for its functionality. Descriptive and structural metadata will be stored and managed by other systems, such as a repository or metadata aggregator. The integration with these systems will be handled through APIs or message queues, ensuring that DOS can access and utilize this metadata without duplicating storage.\n","GenTime":"2024-08-28 12:32:59"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"text":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n\n##Decision\nThese names will no longer be required by the base classes.\\n","MatchedIds":[3027,3981,1193,84,2968],"Prediction":"## Decision\nThe name member will be removed from the base classes of Driver, Processor, Hardware Platform, and Platform. \n\n**Reasoning:**\n\n* **Redundancy:** Storing names in both the base class and the driver registry introduces data duplication and potential inconsistencies. \n* **Unused Data:**  The names in Processor and Platform classes are unused, indicating unnecessary storage and complexity.\n* **Simplified Design:**  Removing the name member simplifies the base classes and streamlines the architecture by relying on the driver registry for name management where needed. \n\n**Impact:**\n\n* Code will need to be updated to remove the name member from the affected base classes.\n* References to the name member in derived classes and other parts of the codebase will need to be adjusted to utilize the driver registry or other appropriate mechanisms. \n","GenTime":"2024-08-28 12:33:04"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"text":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n\n\n##Decision\nWe chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","MatchedIds":[4397,3915,2968,2240,3497],"Prediction":"## Decision\n\nWe will address this circular dependency issue by implementing a two-stage readiness approach for Connaisseur:\n\n1. **Initial Readiness:** The Connaisseur Pods will initially report `Ready` when their internal services are up and running. This is similar to the current behavior and ensures the Pods can be deployed even before the MutatingWebhookConfiguration is active.\n\n2. **Webhook Readiness:** We will introduce a separate liveness probe specifically for the MutatingWebhookConfiguration. This probe, running within the Connaisseur Pod, will periodically verify the existence and functionality of the MutatingWebhookConfiguration. Only when this probe succeeds will Connaisseur be considered truly ready for processing admission requests.\n\nThis approach decouples the readiness states, allowing the MutatingWebhookConfiguration to be deployed after the Connaisseur Pods without causing a deadlock. \n\n### Positive Consequences:\n\n- Avoids circular dependencies and potential deadlocks during deployment.\n- Provides a clear distinction between the internal readiness of Connaisseur Pods and the availability of the MutatingWebhookConfiguration.\n- Enables monitoring systems to accurately track the operational status of Connaisseur.\n\n### Negative Consequences:\n\n- Introduces a small delay between the Connaisseur Pod reporting `Ready` and it being fully functional. This delay should be minimal as the webhook probe would run frequently.\n- Requires implementing and maintaining an additional liveness probe within Connaisseur.\n\nThis approach prioritizes robust deployment and clear status reporting over a minimal potential delay in full functionality. \n","GenTime":"2024-08-28 12:33:12"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"text":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n\n\n##Decision\nWe will use ports&adapters architecture style\\n","MatchedIds":[1094,2502,428,4896,3784],"Prediction":"## Decision \n\n**Insufficient information to make a decision.**\n\nThe context provided is too vague.  \"Adjacent to our architectural drivers\" is not a clear or specific requirement. To recommend an appropriate application architecture style, we need more information about:\n\n* **What are the architectural drivers?** \n    * Are there specific quality attributes that are most important, such as performance, scalability, maintainability, security, etc.?\n    * Are there any constraints related to technology, team skills, or budget?\n    * What are the functional and non-functional requirements of the application?\n* **What does \"adjacent\" mean in this context?** \n    *  Does it imply a preference for certain architectural styles over others? \n    *  Are there any existing systems or architectures that this application needs to integrate with?\n\n**Next Steps:**\n\n1. **Gather more information about the architectural drivers.**  Clearly define the goals, constraints, and priorities for the project.\n2. **Explore different architectural styles.** Once the drivers are understood, evaluate different styles against those drivers. Some common styles to consider include:\n    * Microservices\n    * Layered Architecture\n    * Event-Driven Architecture\n    * Space-Based Architecture\n    * Microkernel Architecture\n3. **Document the decision-making process.** Once a decision is made, create an Architectural Decision Record (ADR) to capture the rationale, consequences, and alternatives considered. \n \nProviding more context will enable a specific and well-informed decision about the most suitable application architecture style. \n","GenTime":"2024-08-28 12:33:20"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"text":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n\n\n##Decision\nTo standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","MatchedIds":[4361,3784,3488,2221,3070],"Prediction":"## Decision\n\nWe will not enforce strict architectural guidelines for this project. This will allow for flexibility and encourage experimentation with various technologies and design patterns.  However, we will maintain a lightweight documentation process using Architecture Decision Records (ADRs) to capture the rationale behind significant choices made during experimentation. This will provide a reference point for understanding the evolution of the project and facilitate knowledge sharing among team members. \n","GenTime":"2024-08-28 12:33:23"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"text":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[1410,1404,4224,1588,3734],"Prediction":"## Decision\nWe will use [ts-morph](https:\/\/github.com\/dsherret\/ts-morph#readme) to interact with the TypeScript Compiler API. It offers a more developer-friendly interface for navigating and manipulating TypeScript code than directly using the lower-level TypeScript Compiler API. This will allow us to build robust tooling and analysis features without needing to manage the complexities of the lower-level API. \n","GenTime":"2024-08-28 12:33:26"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"text":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n\n##Decision\nWe will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","MatchedIds":[3370,3127,779,1016,847],"Prediction":"## Decision\nWe will use WebSockets to provide live updates to the front-end. \n\nThis approach offers the following benefits:\n\n* **Real-time Updates:** WebSockets enable bidirectional communication between the client and server, facilitating instant updates on ingest sheet validation and ingest status without requiring constant polling. \n* **Efficient Resource Utilization:** Unlike polling, WebSockets maintain a persistent connection, reducing network overhead and server load.\n\nWe considered alternative solutions, including:\n\n* **Polling:**  While simpler to implement, polling can lead to delays in updates and unnecessary server requests, especially if the update frequency is high. \n* **Server-Sent Events (SSE):** SSE offers a unidirectional communication channel from server to client. While suitable for ingest status updates, it might not be ideal for interactive features like ingest sheet validation, where real-time two-way communication is beneficial. \n\nUltimately, the real-time, bidirectional nature of WebSockets makes it the most suitable solution for delivering a responsive and efficient user experience for live updates on the front-end. \n","GenTime":"2024-08-28 12:33:34"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"text":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n\n\n##Decision\nWe will use 32-bit floats to store any numbers.\\n","MatchedIds":[2634,2637,2629,4868,2687],"Prediction":"## Decision\nAll Scratch numeric values will be converted to 32-bit floats before being sent to the ftduino. Potential data loss and precision issues due to this conversion will not be addressed. We prioritize ftduino compatibility and runtime performance over maintaining complete numerical accuracy with the Scratch environment. \n","GenTime":"2024-08-28 12:33:36"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"text":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n\n\n##Decision\nWhen we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","MatchedIds":[4497,1193,1035,2360,4980],"Prediction":"## Decision\n\n### Problem\nSimply marking a method or class as `@Deprecated` without providing context or migration guidance creates confusion and friction for developers.  They are left in the dark about:\n\n* **Why is this deprecated?**  Is it buggy, insecure, or just superseded by a better approach?\n* **What should I use instead?**  Concrete alternatives are crucial for smooth transitions.\n* **When will it break?**  Knowing the deprecation timeline helps prioritize refactoring.\n\n### Solution\n\n**Mandate informative deprecation messages:**\n\n1. **Every `@Deprecated` annotation MUST have a `message` attribute.** This message should clearly explain:\n    * The reason for deprecation.\n    * The recommended alternative (with code examples, if possible).\n    * The planned removal version (e.g., \"Will be removed in v2.0\").\n\n2. **Consider using tools to enforce this.**  Linters or static analysis can detect bare `@Deprecated` usages and fail the build.\n\n3. **Document deprecated elements consistently.** Ensure API documentation renders deprecation notices prominently.\n\n### Example\n\n```java\n\/**\n * This method is deprecated because it relies on an insecure library.\n * Use {@link #secureAlternative(String)} instead. This method will be removed in v1.5.0.\n * @param data The input data\n * @deprecated\n *\/\n@Deprecated(message = \"Use secureAlternative(String) instead. Will be removed in v1.5.0.\")\npublic void insecureMethod(String data) {\n  \/\/ ...\n}\n```\n\n### Benefits\n\n* **Clear guidance for developers.**  Facilitates smooth transitions to newer APIs.\n* **Reduced technical debt.** Makes future refactoring efforts more predictable.\n* **Improved code maintainability.**  Communicates design intent and evolution clearly. \n","GenTime":"2024-08-28 12:33:45"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"text":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n\n\n##Decision\n### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","MatchedIds":[4507,3637,1855,2221,1193],"Prediction":"## Decision\n\n**Adopt the new configuration procedure leveraging the hdom context object for theme attributes.**\n\n### Rationale\n\n*   **Dynamic Theming:** Utilizing the hdom context object enables dynamic theming by allowing components to adapt their styling based on the current theme defined in the context.\n*   **Theme Switching:**  This approach simplifies theme switching as components can react to context changes and update their appearance without requiring explicit theme-specific logic.\n*   **Separation of Concerns:**  By using the context for stylistic attributes, we promote a cleaner separation between a component's behavior and its visual presentation.\n\n### Implementation Details\n\n*   Components should retrieve theme attributes from the hdom context object using the provided APIs.\n*   A mechanism for defining and updating the active theme in the context should be implemented.\n*   Existing components relying on the previous configuration approach (ADR-0002) may require refactoring to align with this new pattern.\n\n### Migration Plan\n\n*   Introduce the new context-based configuration alongside the existing approach to allow for gradual migration.\n*   Update documentation and examples to reflect the new theming strategy.\n*   Provide clear guidance and potentially tooling to help developers migrate existing components to leverage the hdom context for theming. \n\n### Potential Drawbacks\n\n*   Increased reliance on the hdom context object, potentially impacting performance if not implemented carefully.\n*   Refactoring effort required to migrate existing components to the new theming approach.\n\n### Conclusion\n\nThis decision favors a more flexible and maintainable theming solution by leveraging the capabilities of the hdom context object. While some migration effort is anticipated, the long-term benefits in terms of dynamic theming and separation of concerns outweigh the initial costs. \n","GenTime":"2024-08-28 12:33:54"}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"text":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n\n##Decision\nWe are going to use DSI as our single sign-on provider using the OIDC standard.\\n","MatchedIds":[2041,2405,2093,2410,2397],"Prediction":"## Decision\n\nWe will use [DfE Sign-in (DSI)](https:\/\/www.signin.education.gov.uk\/) for authenticating users due to the governance requirement. \n\nGiven the lack of tooling recommendations in the technical guidance, we will investigate using the DSI OpenID Connect (OIDC) implementation. This standard aligns with our desire for an open and modern security protocol, enabling potential future interoperability. \n\nWe acknowledge that using DSI might introduce complexities beyond a simple password-less mechanism. We will conduct a timeboxed technical spike to evaluate the effort required to integrate DSI and achieve the desired authentication experience. If the spike reveals significant challenges, we will document the findings and revisit alternative solutions with the DfE governance team.\n","GenTime":"2024-08-28 12:33:59"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"text":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n\n\n##Decision\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","MatchedIds":[1819,1823,669,2221,3529],"Prediction":"## Decision\n\nWe will implement a solution where experiment assignments are pre-fetched and stored locally on the device before the user's first run. This approach addresses the decision drivers in the following ways:\n\n**Chosen Option:** Pre-fetch and Local Storage\n\n**Implementation Details:**\n\n1. **Pre-fetching:**\n   - A lightweight process will run in the background (e.g., during app installation or update) to fetch the latest experiment definitions and default values from the Nimbus server.\n   - This data will be stored locally on the device in a persistent storage mechanism (e.g., shared preferences on Android, UserDefaults on iOS). \n\n2. **First Run Experience:**\n   - On the user's first run, the Nimbus SDK will prioritize using the pre-fetched experiment data.\n   - This allows experiments to be activated immediately without waiting for a network request to the Nimbus server.\n\n3. **Background Update:**\n   - The Nimbus SDK will continue to communicate with the server in the background to fetch the latest experiment data. \n   - If changes are detected, the local storage will be updated accordingly.\n\n4. **Idempotency:**\n   - The SDK will be designed to handle scenarios where pre-fetched data may be outdated or unavailable (e.g., first-time installation). \n   - In such cases, it will seamlessly fall back to fetching data from the server, ensuring idempotency and a consistent user experience.\n\n\n### Positive Consequences:\n\n* **Availability of Experiments on First Run:** This approach ensures that experiments are readily available from the moment a user launches the app for the first time.\n* **No Impact on Data Analysis:** Since the pre-fetched data mirrors the actual experiment configuration, there is no impact on data analysis.\n* **Flexibility in Creating Experiments:** This solution does not impose any new limitations on how experiments are designed or implemented.\n* **Fast Experiment Disabling:** Disabling an experiment on the server will immediately reflect on subsequent data fetches, minimizing any delay in turning off experiments.\n* **Simplicity of Releases:**  The pre-fetch mechanism can be integrated into existing app update or installation processes.\n* **Mobile's Expectations of Nimbus:** This solution aligns with the principle of idempotency, ensuring that the SDK functions reliably even in scenarios with limited or delayed network connectivity. \n\n\n### Negative Consequences:\n\n* **Increased App Storage Footprint:** Storing experiment definitions locally will consume a small amount of additional device storage. However, this is expected to be negligible for most applications.\n* **Potential for Data Staleness:**  There is a small window of time where pre-fetched data might become outdated if a user doesn't launch the app immediately after an update. However, the background update mechanism mitigates this risk.\n\n**Additional Considerations:**\n\n* Implement a mechanism to handle the infrequent scenario where the pre-fetched data becomes corrupted.\n* Monitor the performance impact of the pre-fetching process and optimize as needed. \n","GenTime":"2024-08-28 12:34:14"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"text":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n\n\n##Decision\nAccessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","MatchedIds":[3568,160,3037,3784,4699],"Prediction":"## Decision\n\n**Introduce redundant visual cues alongside color to represent information and distinguish record types.**\n\n**Reasoning:** \n\n* **Accessibility:** Relying solely on color excludes users with visual impairments, such as color blindness. Redundant cues ensure information is perceivable by everyone.\n* **Clarity and comprehension:** Visual cues like icons, patterns, or different font weights can reinforce meaning and make it easier to distinguish record types at a glance, even for users without visual impairments.\n* **Flexibility:** This approach allows users to personalize the tool's appearance (e.g., choosing color themes) without sacrificing information clarity.\n\n**Specific Examples:**\n\n* Instead of only using color to denote record status (e.g., green for Accepted, red for Rejected), add icons representing checkmarks, crosses, or pending symbols.\n* Use different border styles or background patterns to distinguish record types in lists or tables.\n* Utilize varying font weights or text styles to highlight important information within records.\n\n**Implementation:**\n\n* Review all instances where color is used as the primary differentiator.\n* Design and implement alternative visual cues based on the specific information being conveyed.\n* Conduct user testing with diverse users to validate the effectiveness and clarity of the chosen cues. \n","GenTime":"2024-08-28 12:34:21"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"text":"## Context\\nWe need to record the future architectural decisions made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), to document all future architectural decisions. \n","GenTime":"2024-08-28 12:34:23"}
{"File Name":"containers\/2-.md","Context":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"text":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n\n\n##Decision\n======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","MatchedIds":[923,3868,2968,771,469],"Prediction":"## Decision\n\nWhile Fedora offers a minimal base image and newer software versions, we will continue to utilize **Debian-based images** for the following reasons:\n\n* **Stability and Long-Term Support:** Debian is renowned for its stability and long-term support, which aligns well with our focus on operational reliability and minimizing maintenance overhead. \n* **Predictable Release Cycle:** Debian's predictable release cycle allows for better planning and reduces the risk of unexpected breaking changes during upgrades. \n* **Extensive Community Support and Documentation:** Debian boasts a large and active community, providing access to vast resources, documentation, and support channels.\n* **Existing Expertise:** Our team possesses significant experience with Debian, including automated deployment and configuration management using tools like preseeding. \n\nWhile Fedora's bleeding-edge software is appealing, the stability and familiarity of Debian outweigh the benefits in this context. We prioritize a robust and predictable foundation for our infrastructure over having the absolute latest software versions.\n","GenTime":"2024-08-28 12:34:29"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"text":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n\nThis will ensure that architectural decisions are:\n\n* **Documented:** Providing a single source of truth for the rationale behind architectural choices.\n* **Traceable:** Allowing us to understand the reasoning behind past decisions and how they impact current and future decisions.\n* **Communicable:** Facilitating communication and knowledge sharing among team members, both present and future.\n\nThis decision is driven by the need to:\n\n1. **Avoid repeating past mistakes:**  By documenting our decisions, we can avoid making the same mistakes in the future.\n2. **Onboard new team members efficiently:** ADRs provide valuable context for new team members, allowing them to quickly understand the architectural decisions that have shaped the project.\n3. **Maintain a clear and concise architectural history:** As the project grows and evolves, ADRs will serve as a valuable resource for understanding the architectural decisions that have been made.\n\nWe will aim to create an ADR for each significant architectural decision made on the project.\n","GenTime":"2024-08-28 12:34:36"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"text":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n\n\n##Decision\nThere will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","MatchedIds":[4234,2772,3192,4931,994],"Prediction":"## Decision\n\nThe SQW object will use **absolute indexing** to access pixel data. \n\nThis decision provides a simplified and consistent interface for users of the SQW object, regardless of the underlying file-backed implementation. Users will not need to be concerned with managing pages or translating between relative and absolute indices.\n\nTo handle the large pixel array, the SQW object will internally manage the loading and caching of pixel data pages as needed. This will be transparent to the user, who can access any pixel using its absolute index. \n\nThis approach prioritizes ease of use and a clean API while abstracting away the complexities of managing a file-backed pixel array. \n","GenTime":"2024-08-28 12:34:41"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"Done.\\n","tokens":42,"id":2997,"text":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n\n\n##Decision\nDone.\\n","MatchedIds":[2471,1376,3917,4358,4096],"Prediction":"## Decision\n\nUse AWS ECS with Fargate launch type. This removes the need to manage EC2 instances for running our Docker containers.  We will create an ECS cluster configured to use Fargate, and define our application as ECS tasks. These tasks will specify the Docker images to pull from AWS ECR (or potentially another registry if public access is required). \n\nThis approach minimizes operational overhead as AWS manages the underlying infrastructure, allowing us to focus on application development and deployment. We will use infrastructure-as-code tools like Terraform or AWS CloudFormation to define and manage our ECS cluster and tasks. \n","GenTime":"2024-08-28 12:34:44"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"text":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n\n##Decision\nWe will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","MatchedIds":[2526,302,3488,4747,2498],"Prediction":"## Decision\n\n**Chosen option:** \"Command Query Responsibility Segregation (CQRS)\", because it allows for separate models optimized for reads and writes, addressing both performance and maintainability concerns.\n\n### Positive Consequences\n\n* **Improved performance:** Separate read and write paths allow for optimized data structures and queries tailored for each concern. \n* **Scalability:** Reads and writes can be scaled independently, improving the system's ability to handle increased load.\n* **Maintainability:**  Simpler, more focused models are easier to understand, modify, and maintain. \n\n### Negative Consequences\n\n* **Increased complexity:** CQRS introduces additional architectural components and complexity, requiring careful design and implementation.\n* **Data consistency:** Asynchronous updates between read and write models can lead to eventual consistency challenges. This requires careful consideration and mitigation strategies.\n","GenTime":"2024-08-28 12:34:50"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"text":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n\n\n##Decision\nOnly support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","MatchedIds":[78,641,4424,2536,4497],"Prediction":"## Decision\n\n### Supported Versions\n\nWe believe in providing clear and predictable support for our software. This document outlines our version support policy to help you plan accordingly.\n\n* **Current Version:** The most recently released version (e.g., 1.2.3). We encourage all users to adopt the current version for the latest features, bug fixes, and security updates.\n* **Active Support:** We provide active support for the current version and the previous two minor releases (e.g., 1.2.3, 1.1.x, 1.0.x). This includes:\n    * Bug fixes\n    * Security patches\n    * Answering user questions and addressing issues\n* **End-of-Life (EOL):** Versions older than the actively supported releases reach EOL. We do not provide support for EOL versions.\n\n### Version Updates\n\n* **Major Releases (e.g., 1.x.x to 2.x.x):** Major releases introduce significant changes, potentially including breaking changes. We strive to provide ample notice and migration guides for major version updates. \n* **Minor Releases (e.g., 1.2.x to 1.3.x):** Minor releases contain new features, enhancements, and bug fixes, but maintain backward compatibility with previous minor releases within the same major version.\n* **Patch Releases (e.g., 1.2.3 to 1.2.4):** Patch releases focus on bug fixes and security updates without introducing new features. \n\n### Communication\n\nWe will communicate upcoming version changes and end-of-life dates through:\n\n* **Release Notes:** Each release will have detailed release notes outlining changes.\n* **Documentation:** Our documentation will clearly indicate the supported versions.\n* **Blog Posts\/Announcements:**  We may use blog posts or announcements for major version changes. \n\n### Recommendation\n\nWe strongly recommend users to stay up-to-date with the latest supported versions to benefit from new features, improvements, and security updates. \n","GenTime":"2024-08-28 12:35:00"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"text":"## Context\\nOpen Lobby must have open stable API.\\n\n\n##Decision\nOpen Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","MatchedIds":[4618,847,2181,1545,1180],"Prediction":"## Decision\n\nOpen Lobby's API will be versioned and follow the [Semantic Versioning](https:\/\/semver.org\/) specification (SemVer). This ensures:\n\n* **Predictability:** Consumers can clearly understand the impact of upgrading to a new version.\n* **Backward Compatibility:**  Major version changes (e.g., 1.x.x to 2.x.x) will be infrequent and will be communicated well in advance. Minor and patch versions will maintain backward compatibility wherever possible. \n* **Clear Communication:**  API documentation will clearly indicate the version of each endpoint and any breaking changes will be explicitly documented in release notes.\n\n### API Versioning Scheme\n\nThe API will be versioned using a URI-based approach:\n\n```\nhttps:\/\/api.openlobby.com\/v1\/ \n```\n\nThis allows for serving multiple API versions concurrently, ensuring older integrations remain functional during transitions. \n\n### Documentation\n\nAPI documentation will be generated using [Swagger](https:\/\/swagger.io\/) (OpenAPI Specification) and hosted publicly, providing a clear and interactive resource for developers. \n\nThis approach ensures Open Lobby's API remains stable, predictable, and developer-friendly, encouraging wider adoption and integration. \n","GenTime":"2024-08-28 12:35:07"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\\nDefine a strategy for deprecations.\\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"text":"## Context\\nDefine a strategy for deprecations.\\n\n\n##Decision\n### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","MatchedIds":[3863,3653,138,3853,4618],"Prediction":"## Decision\n\nWe will adopt a clear and structured deprecation policy with the following stages:\n\n**1. Identification & Announcement:**\n\n* Identify functionalities, classes, or methods slated for deprecation. Reasons could include:\n    * Architectural evolution\n    * Performance improvements\n    * Security concerns\n    * Adoption of superior alternatives\n* Announce deprecations clearly in the release notes, documentation, and ideally through code comments (e.g., using `@deprecated` tags with explanations and migration guidance).\n\n**2. Marking & Transition Period:**\n\n* Mark deprecated elements in the codebase. This might involve:\n    * Using deprecation annotations or attributes.\n    * Logging warnings on usage.\n    * Providing clear alternatives within the warnings\/documentation.\n* Allow a reasonable transition period based on the impact of the deprecation. This gives users time to migrate their code.\n\n**3. Removal:**\n\n* After the transition period, remove deprecated elements from the codebase.\n* This removal should align with a major or minor version release, as per semantic versioning principles.\n\n**Additional Considerations:**\n\n* **Severity Levels:** Introduce different deprecation levels (e.g., \"deprecated,\" \"strongly deprecated\") to signal urgency and potential future removal.\n* **Tooling:**  Leverage static analysis tools and IDE extensions to detect and warn about the usage of deprecated elements.\n* **Communication:** Maintain open communication channels with users (forums, mailing lists) to address concerns and provide support during the transition.\n* **Exception Handling:** In exceptional cases where removing a deprecated feature is impossible due to strong external dependencies, clearly document the reasons and potential future implications.\n\nThis strategy aims to balance maintaining a clean and modern codebase while respecting users' existing implementations and minimizing disruption.\n","GenTime":"2024-08-28 12:35:16"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"text":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n\n##Decision\nRemove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","MatchedIds":[1883,425,4308,3069,1599],"Prediction":"## Decision\n\n**Migrate away from using the Tech Docs Gem in the API catalogue website.**\n\n**Reasoning:**\n\n* **TDG is not a good fit:** The TDG is designed for documentation websites with markdown as the primary data source. Our project relies on a CSV data source and goes beyond serving static documentation.\n* **Lack of Flexibility:** Overriding inherited templates and adding page-specific customizations is difficult with TDG, limiting the site's design and functionality.\n* **Performance Issues:** TDG's Table of Contents rendering process is inefficient for our site, adding significant overhead to build times.\n* **Dependency Restrictions:** TDG's specific dependency versions hinder our ability to control upgrades, potentially leaving us vulnerable to security issues and outdated libraries. \n\n**Next Steps:**\n\n1. **Evaluate alternative static site generators:** Explore options better suited for our CSV data source and provide more flexibility and control over the site's design and functionality. Consider generators like Hugo, Jekyll, or Eleventy.\n2. **Implement custom solutions for search and navigation:** Develop a bespoke search functionality optimized for our CSV data and create a more efficient Table of Contents generation process.\n3. **Migrate existing content:** Transfer existing content from the TDG structure to the new platform, ensuring data integrity and proper formatting.\n4. **Thoroughly test the new implementation:** Conduct comprehensive testing to guarantee the site functions correctly, performs well, and meets all requirements before deploying.\n","GenTime":"2024-08-28 12:35:23"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"text":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n\n\n##Decision\nIn order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","MatchedIds":[2933,2932,2944,4299,899],"Prediction":"## Decision\nTo improve compatibility between different versions of the frameworks during model import, we will introduce a **node pre-processing stage** based on annotations.\n\nThis pre-processing stage will be responsible for applying transformations to the imported graph based on framework version and specific node annotations.\n\nHere is a breakdown of the approach:\n\n**1. Annotations:**\n\n* Introduce a set of annotations for graph nodes that can be used to specify:\n    * The original framework version the node was created with.\n    * The required framework version for compatibility.\n    * Specific transformations to be applied during upgrade.\n\n**2. Pre-processing Stage:**\n\n* Implement a dedicated pre-processing stage within the model import framework that will be executed before graph construction.\n* This stage will iterate through all nodes in the imported graph.\n* For each node, it will:\n    * Check for relevant annotations.\n    * If annotations are present, determine if a transformation is needed based on the supported framework versions.\n    * Apply the specified transformations to the node's definition or attributes.\n\n**3. Transformation Library:**\n\n* Create a library of predefined transformation functions that can be applied to nodes based on the annotations.\n* This library can be extended over time to support new framework versions and transformations.\n* Transformations can include:\n    * Replacing deprecated operations with equivalent alternatives.\n    * Adjusting node attributes to match updated specifications.\n    * Inserting additional nodes to handle changes in graph execution semantics.\n\n**4. Error Handling:**\n\n* Implement robust error handling during pre-processing to catch unsupported framework versions or incompatible annotations.\n* Provide informative error messages to guide users in resolving compatibility issues.\n\n**Benefits:**\n\n* **Automated Graph Upgrades:** Enables automatic adaptation of imported models to different framework versions, reducing manual intervention.\n* **Improved Compatibility:** Enhances support for a wider range of framework versions, ensuring smoother model import experiences.\n* **Extensibility:** The annotation-based approach allows for flexible extension to accommodate future framework changes and new transformation needs.\n\n**Considerations:**\n\n* **Maintenance:**  Maintaining the transformation library and updating it with each new framework version requires careful consideration and effort.\n* **Complexity:**  Designing and implementing a robust and comprehensive set of annotations and transformations might introduce complexity to the model import framework.\n\nThis approach aims to provide a balance between automation, flexibility, and maintainability in managing compatibility challenges during model import. \n","GenTime":"2024-08-28 12:35:36"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"text":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n\n##Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","MatchedIds":[4096,699,3657,771,4358],"Prediction":"## Decision\nChosen option: **Heroku Container Registry**, because:\n\n* **Ease of Use**: It simplifies the deployment process significantly. We do not need to manage our own cluster of machines or handle complex container orchestration setups. \n* **Free Tier**: Heroku's free tier allows us to get started with minimal cost, which aligns with our goal of a simple and potentially low-cost solution.\n* **Integrated Database**: The built-in PostgreSQL offering further streamlines the setup process by automatically handling database hosting and configuration.\n\n**Important Considerations:**\n\n* **Database Limitations**:  The 10,000-row limit on the free PostgreSQL database may require us to migrate to a paid tier as our data grows. We should factor this potential cost increase into our long-term planning.\n* **Scaling**: While Heroku simplifies initial deployment, we should research their scaling options to ensure the platform can handle our application's growth and performance needs in the future. \n","GenTime":"2024-08-28 12:35:42"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"text":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n\n##Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n","MatchedIds":[4354,1949,4130,1982,1979],"Prediction":"## Decision\nTendermint will maintain a **single RPC endpoint** exposed by the Tendermint process itself. While exposing a separate RPC server from the ABCI application is technically possible, it introduces unnecessary complexity and potential security risks. This decision prioritizes the following:\n\n* **Standardization:** Maintaining a single RPC interface simplifies client development and ensures consistent behavior across all Tendermint applications. Light clients, in particular, benefit significantly as they can rely on a standardized interface for proof verification.\n* **Security:**  Consolidating RPC endpoints to a single location simplifies security audits and reduces potential attack surfaces. Locking down and monitoring a single endpoint is more straightforward than managing access to multiple servers.\n* **Reduced Complexity:** Exposing Tendermint proofs through the primary RPC interface eliminates the need for application developers to implement and maintain their own proof-handling logic. This streamlines application development and reduces the potential for inconsistencies or errors.\n\nWhile querying the application state might require passing requests as `Query` objects through Tendermint, this approach provides a clean separation of concerns. Tendermint remains responsible for handling raw data and proofs, while the application logic focuses on interpreting and presenting the information. \n\nThis decision enables a more maintainable, secure, and developer-friendly architecture for Tendermint and its ecosystem of applications.\n","GenTime":"2024-08-28 12:35:50"}
{"File Name":"dp\/0009.md","Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"text":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n\n\n##Decision\nDimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","MatchedIds":[1510,3657,899,2802,4539],"Prediction":"## Decision\n\n**Option:** Maintain separate nodes for dimension options and code definitions in the graph database.\n\n### Reasoning:\n\nWhile there is some overlap in information, keeping these concepts as separate nodes provides several advantages:\n\n* **Clearer semantics and data model:**  Separating the nodes enhances the clarity and expressiveness of the graph data model. It explicitly distinguishes between a dimension option within the context of a specific dataset and its broader definition within a code-list. This distinction is valuable for data interpretation and querying.\n* **Flexibility and maintainability:** Separate nodes allow for easier updates and maintenance. Changes to a code definition won't directly impact the structure of dataset-specific dimension options, and vice versa. This reduces the risk of unintended consequences and simplifies data management.\n* **Potential for different lifecycles and evolution:**  Code-lists and dataset dimensions might evolve independently. A separate node structure accommodates potential differences in their lifecycles and versioning.\n\n### Positive Consequences:\n\n* Improved data model clarity and semantic richness.\n* Enhanced flexibility for updates and maintenance.\n* Better support for independent evolution of code-lists and datasets. \n\n### Negative Consequences: \n\n* Requires managing relationships between the separate nodes.\n* Might lead to some data redundancy.\n\n### Mitigation:\n\n* Implement robust relationship management within the graph database to link dimension options to their corresponding code definitions.\n* Explore data de-duplication techniques if redundancy becomes a significant concern. \n\nThis approach prioritizes a clear and maintainable data model that aligns with the distinct but related concepts of dimension options and code definitions. \n","GenTime":"2024-08-28 12:35:58"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"text":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n\n\n##Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","MatchedIds":[3657,2261,4699,4428,4408],"Prediction":"## Decision\n\nWe will move all Technical Documentation (Development & Infrastructure) to the dedicated Planet 4 space on Gitbook. This decision is driven by the superior features offered by Gitbook as a documentation platform, including enhanced UI\/UX, seamless integration with Github for version control and external contributions, and its adoption by other teams within Greenpeace. While keeping all P4 information consolidated in the Handbook has its merits, the limitations regarding internal perception, custom development needs, and styling constraints for technical content outweigh the benefits. This move aligns with modern documentation practices and fosters wider accessibility and contribution. \n","GenTime":"2024-08-28 12:36:02"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"text":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n\n##Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","MatchedIds":[2488,3784,3657,1180,276],"Prediction":"## Decision\n\nWe will follow a domain-driven design (DDD) approach to define the data model and application structure. This involves:\n\n1. **Identifying the core domains:** We'll analyze the requirements and identify the main functional areas (domains) of the application.\n2. **Defining bounded contexts:** Each domain will be encapsulated within a bounded context, establishing clear boundaries and responsibilities.\n3. **Creating entities, value objects, and aggregates:** Within each bounded context, we'll define the relevant entities, value objects, and aggregates to represent the data model and their relationships.\n4. **Designing the application layers:** We'll structure the application into distinct layers (e.g., presentation, application, domain, infrastructure) with well-defined responsibilities and dependencies.\n\nThis approach will lead to a more maintainable, scalable, and flexible application that can evolve independently as new features are added.\n\nTo guide this process, we will:\n\n- **Organize workshops:** We'll hold workshops with domain experts and developers to collaboratively identify and define the core domains, bounded contexts, and key data structures.\n- **Iterate and refine:** The initial data model and application structure will be iteratively refined as we gain a deeper understanding of the requirements and implement new features.\n- **Document our decisions:** We'll document the rationale behind our design decisions using ADRs to ensure transparency and maintainability.\n\nBy adhering to these principles, we can ensure a robust foundation for the application that can adapt to future changes and growth.\n","GenTime":"2024-08-28 12:36:10"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"text":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n\n##Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","MatchedIds":[1031,4747,1574,2261,2520],"Prediction":"## Decision\n* **Performance** - decision should support high throughput scenarios\n* **Maintainability** - amount of code needed to write should be minimized\n\nChosen option: \"**Optimistic locking**\", because it offers a good balance between performance and maintainability in most scenarios. \n\n### Positive Consequences\n* **Performance:** Optimistic locking introduces minimal overhead when no conflicts are present.\n* **Maintainability:** Implementation is usually straightforward, often relying on existing database features. \n\n### Negative Consequences\n* **Performance:** Requires careful handling of conflicts, which might lead to retries and impact performance under high contention. \n* **Maintainability:**  Application logic needs to be aware of potential conflicts and implement appropriate retry or rollback mechanisms.\n\n### Alternatives considered:\n* **Pessimistic Locking:** Provides strong consistency guarantees but introduces significant performance overhead, especially in high-concurrency scenarios. \n\n### Additional Considerations:\n* Specific implementation details of Optimistic locking (e.g., using version numbers or timestamps) will depend on the chosen persistence mechanism.\n* Consider using dedicated libraries or frameworks to simplify the implementation and handling of concurrent updates. \n","GenTime":"2024-08-28 12:36:17"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"text":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n\n##Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","MatchedIds":[4507,899,2823,745,3203],"Prediction":"## Decision\n\nWe will introduce a new concept of \"**fine-grained privilege dependencies**\" for modules. \n\nInstead of requiring a dependency on an entire module, which grants full access, modules can now define dependencies on specific privileges from other modules.\n\n### Implementation Details:\n\n1. **Privilege Definition:**  Modules will explicitly declare the ACL privileges they require using a standardized format within their manifest file.  \n\n   ```yaml\n   # Example module manifest.xml excerpt\n   ... \n   aclPrivileges:\n     - product:edit\n     - rule.builder:read\n     - media.manager:upload \n     ...\n   ```\n2. **Dependency Declaration:**  Modules requiring specific privileges will declare them as \"fine-grained\" dependencies.\n\n   ```yaml\n   # Example module manifest.xml excerpt\n   ...\n   dependencies:\n     # ... regular module dependencies\n   fineGrainedDependencies:\n     - module: rule_builder\n       privileges: \n         - rule.builder:read \n         - rule.builder:create\n     - module: media_manager\n       privileges:\n         - media.manager:upload\n   ```\n\n3. **Privilege Resolution:** The system will resolve these fine-grained dependencies, ensuring users have the necessary privileges to use components requiring them. If a user lacks a required privilege, access to specific components or functionalities will be restricted accordingly.\n\n### Advantages:\n\n- **Granular Control:**  Fine-grained control over privilege requirements, enhancing security and reducing the risk of unintended access.\n- **Modularity:** Improved modularity by decoupling components with specific privilege needs from the entire module.\n- **Maintainability:** Clearer dependency management and easier to understand privilege requirements for each module.\n\n### Considerations:\n\n- **Complexity:** Slightly increased complexity in dependency management, requiring careful planning and documentation.\n- **Performance:** Potential minor performance overhead due to the additional privilege checks, which should be benchmarked and optimized if needed. \n","GenTime":"2024-08-28 12:36:28"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"text":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n\n\n##Decision\n- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","MatchedIds":[498,528,4821,1193,1255],"Prediction":"## Decision\n\nWe will implement a subject browsing experience using a **persistent local cache** of subjects within the classifier. This approach provides the following benefits:\n\n1. **Enhanced browsing experience**: Volunteers can freely navigate forward and backward through the subject set using familiar browser-like navigation (previous\/next subject). \n2. **Improved subject selection**: Volunteers can revisit subjects in the set before deciding to classify, leading to more informed choices. \n3. **Bandwidth efficiency**:  Fetching subjects only once (or as needed) reduces server load and improves responsiveness for volunteers.\n\n**Implementation Details:**\n\n* **Local Cache:** A local cache (using browser storage or an in-memory data structure) will store a predefined number of subjects fetched from the Panoptes API. \n* **Cache Management:** The cache will be populated as volunteers browse the subject set. A strategy for cache eviction (e.g., least recently used) will be implemented to manage the cache size.\n* **Subject Selection:** When a volunteer selects a subject for classification, it is retrieved from the cache. If not present, the subject will be fetched from the Panoptes API and added to the cache.\n* **Synchronization:** The classifier will handle situations where the subject set changes on the server (e.g., new subjects added) and ensure the cache remains consistent.\n\nThis decision prioritizes a user-friendly browsing experience within Engaging Crowds while being mindful of performance and resource utilization. \n","GenTime":"2024-08-28 12:36:36"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"text":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n\n\n##Decision\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","MatchedIds":[4747,2520,583,3657,4285],"Prediction":"## Decision\n* **Flexibility:** High\n* **Integration with external sources:** High\n* **Implementation difficulty:** Moderate\n\n**Chosen option:  \"Snowflake\" (or similar cloud-based data warehouse like Google BigQuery)** \n\n**Reasoning:**  While building an in-house solution might offer the most flexibility, the time investment is prohibitive. Snowflake (or similar) offers a good balance:\n\n* **Flexibility:** Data warehouses are schema-on-read, making them highly flexible to changing data needs.\n* **Integration:** Snowflake has excellent connectors for various data sources, simplifying ingestion.\n* **Implementation:** Setting up Snowflake is relatively straightforward compared to building a full in-house solution.\n\n**Alternatives considered:**\n* **Building an in-house data pipeline:** Offers maximum flexibility but requires significant development time.\n* **Managed Hadoop solutions (e.g., Databricks):** Powerful but potentially overkill for current needs and could introduce complexity.\n\n**Positive Consequences:**\n* Quick setup allows for faster iteration and value delivery.\n* Scalability to accommodate future growth.\n* Robustness and managed infrastructure reduce operational overhead.\n\n**Negative Consequences:**\n* Cost might be higher than an in-house solution (but potentially lower than managing infrastructure).\n* Vendor lock-in, though mitigated by Snowflake's data portability features. \n","GenTime":"2024-08-28 12:36:47"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"text":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n\n\n##Decision\nImplement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","MatchedIds":[3681,138,3210,4538,4699],"Prediction":"## Decision\n\nWe will implement a rule-based routing engine. This engine will allow users to define routing rules using boolean expressions based on survey responses. \n\nEach rule will consist of:\n\n1. **Condition:** A boolean expression evaluating answers provided by the respondent.\n2. **Target:** The identifier of the next question or the end of the survey if the condition evaluates to true.\n\nThe engine will evaluate all defined rules in order. Upon encountering a rule whose condition evaluates to true, the respondent will be routed to the corresponding target. If no rule's condition is met, a default route (e.g., the next question in the survey) will be followed. \n\nThis approach provides flexibility and allows for complex routing scenarios based on user-defined conditions. We will provide a user-friendly interface for defining and managing these routing rules within the survey creation tool. \n","GenTime":"2024-08-28 12:36:52"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"text":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n\n\n##Decision\nDrains will not be considered as a reaction class.\\n","MatchedIds":[239,5142,3738,1747,4539],"Prediction":"## Decision\nDrains will be implemented as a separate class in Maud, independent of the existing `Reaction` class. \n\nWhile drains impact the ODE system similarly to reactions, their distinct characteristics and the benefits to post-processing warrant a separate implementation. \n\nSpecifically:\n\n* **Enzyme Independence:** Unlike reactions in Maud, which are defined with enzymes and modular rate laws, drains do not have an enzymatic basis. Decoupling them from the `Reaction` class avoids imposing an enzyme requirement on drains.\n* **Post-processing Clarity:**  Separating drains simplifies post-processing techniques that rely on enzyme concentrations. By design, drains lack this property, and their independent classification prevents ambiguity.\n* **Prior Integration:**  Classifying drains as priors ensures that prior predictive checks accurately reflect their impact on the experimental conditions. This contributes to a more robust and informative model analysis.\n\nThis approach maintains the existing enzyme-centric structure for reactions while accommodating the unique properties and behavior of drains within Maud.\n","GenTime":"2024-08-28 12:36:59"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"text":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n\n\n##Decision\nFrom now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","MatchedIds":[4507,4498,4508,4500,2513],"Prediction":"## Decision\n\n### Sorting Options in Administration\n\nA new section will be added to the administration to manage sorting options for product listings. This section allows administrators to:\n\n* **Create, edit, and delete custom sorting options:** Each sorting option will have a name and a unique identifier (e.g., `price-asc`, `newest-to-oldest`).\n* **Define sorting logic:**  Instead of defining services, administrators will use a simplified UI to define sorting rules based on product properties. The UI should support:\n    * Selecting properties for sorting (e.g., price, name, created at).\n    * Choosing the sort direction (ascending or descending).\n    * Defining multiple levels of sorting (e.g., sort by popularity first, then by price).\n* **Set system default sorting:** An option can be marked as the default sorting option for product listings. This sorting will be applied across the storefront unless overridden. \n* **Assign sorting options to Sales Channels:**  Allow assigning specific sorting options to individual sales channels, giving merchants greater control over sorting per channel.\n\n### Search Results and Suggest Route\n\n* **Top Results Default:** The search result page and suggest route will continue to use `Top Results` (sorting by `_score`) as the default sorting option.\n* **Custom Sorting Availability:** Custom sorting options created in the administration will also be available on search result pages and the suggest route.\n\n### Technical Implementation\n\n* **Database Storage:** Sorting options will be stored as entities in the database, allowing for easier management and association with sales channels.\n* **API Endpoint:**  A new API endpoint will provide access to the defined sorting options for the storefront and other clients.\n* **Frontend Integration:** The storefront will be updated to fetch and display available sorting options. A dropdown menu or similar UI element will allow users to select their preferred sorting.\n\n### Advantages of this Approach:\n\n* **User-friendly Administration:**  Shop owners can easily manage sorting options without technical knowledge.\n* **Flexibility and Customization:**  The UI allows for creating diverse sorting logics, catering to various business requirements.\n* **Improved Shopping Experience:** Customers can discover products based on their preferences, potentially leading to increased sales and customer satisfaction.\n\n### Deprecation\n\nThe `shopware.sales_channel.product_listing.sorting` service tag will be marked as deprecated and removed in a future major version. Existing implementations should be migrated to the new system. \n","GenTime":"2024-08-28 12:37:11"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","MatchedIds":[4271,1243,3681,1202,577],"Prediction":"## Decision\n\nWe will use the [Express](http:\/\/expressjs.com\/) web framework with Node.js. This aligns with the platform-level decision made for NHS UK and NHS Digital projects, ensuring consistency and leveraging existing expertise within the team. \n\nThis choice offers several benefits:\n\n* **Reduced Development Time:** Express.js provides a minimalist and unopinionated structure, minimizing the need to write boilerplate code and allowing us to focus on domain-specific logic.\n* **Familiarity and Support:** Using a framework the team is already experienced with will lead to faster development and reduce the learning curve.\n* **Large Community and Ecosystem:** Express.js boasts extensive documentation, a large community, and readily available resources for troubleshooting and support.\n\nThis decision will allow us to deliver a robust and efficient solution while adhering to the principle of minimizing effort spent on non-domain-specific problems.\n","GenTime":"2024-08-28 12:37:17"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\\nComplicated with uses of different css rules\\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"text":"## Context\\nComplicated with uses of different css rules\\n\n\n##Decision\nOnly uses styled-components for design\\n","MatchedIds":[157,1854,783,425,1240],"Prediction":"## Decision\n\nGiven the context that CSS rules are becoming complicated, we should introduce a CSS preprocessor to help manage the complexity and improve the maintainability of the stylesheets. \n\nHere's a breakdown of the decision and next steps:\n\n**Decision:**\n\nWe will adopt **Sass** (Syntactically Awesome Style Sheets) as our CSS preprocessor. \n\n**Rationale:**\n\n* **Improved Organization:** Sass allows us to organize CSS code into modular and reusable components using variables, mixins, and functions. This addresses the complication of scattered CSS rules.\n* **Increased Maintainability:** Changes become easier to implement and maintain with a more structured approach.\n* **Enhanced Readability:** Sass syntax is more concise and readable than plain CSS, making it easier for developers to understand and collaborate on styles.\n* **Widely Used and Supported:** Sass is a mature and popular choice, ensuring good documentation, community support, and integration with various tools. \n\n**Next Steps:**\n\n1. **Integration:** Determine the best way to integrate Sass into the existing development workflow (e.g., using a build tool like webpack or gulp).\n2. **Migration Strategy:**  Plan a gradual migration of existing CSS to Sass to minimize disruption.\n3. **Style Guide:** Establish a clear style guide for Sass usage to maintain consistency across the project. \n4. **Training (If Necessary):** Provide developers with the necessary training or resources to effectively use Sass. \n\nThis decision aims to alleviate the complexities arising from diverse CSS rules by introducing a structured and maintainable approach to styling.\n","GenTime":"2024-08-28 12:37:27"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"text":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n\n\n##Decision\nWe decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","MatchedIds":[4983,4556,3863,1193,4618],"Prediction":"## Decision\n\nWe should standardize how presenters interact with their conforming delegates to ensure consistency and predictability across the codebase. \n\nThere are two main approaches:\n\n**1. Define a shared protocol\/interface:**\n\n* Create a `PresenterDelegate` protocol that defines all the methods a conforming delegate should implement. \n* All presenters would then type-check their delegates against this protocol.\n* **Pros:** Enforces consistency, improves code readability, and facilitates better tooling support (e.g., autocompletion).\n* **Cons:** Introduces a level of coupling, as all presenters now depend on this shared protocol.\n\n**Example:**\n\n```swift\nprotocol PresenterDelegate: AnyObject {\n    func didTapSubmitButton(on presenter: Presenter)\n    func didSelectItem(at index: Int, on presenter: Presenter)\n    \/\/ ... other delegate methods\n}\n```\n\n**2. Utilize delegate-specific closures:**\n\n* Instead of a shared protocol, each presenter defines specific closure properties for actions it needs to delegate.\n* This allows for more flexibility, as presenters only expose the delegate methods they need.\n* **Pros:** More decoupled, avoids potential protocol bloat.\n* **Cons:** Can lead to less discoverable delegate methods, potentially impacting code readability.\n\n**Example:**\n\n```swift\nclass Presenter {\n    var didTapSubmit: (() -> Void)?\n    var didSelectItem: ((Int) -> Void)?\n\n    \/\/ ... presenter logic\n}\n```\n\n**Decision Rationale:**\n\nChoose the approach that best fits the project's needs:\n\n* **For a large codebase with many presenters:**  A shared `PresenterDelegate` protocol might be more suitable to ensure consistency and maintainability.\n* **For a smaller codebase with more specific delegate requirements:** Delegate-specific closures might offer better flexibility and avoid unnecessary overhead. \n\n**Additional Considerations:**\n\n* Document the chosen approach clearly for future developers.\n* Consider providing a default \"no-op\" implementation for delegate methods to simplify adoption. \n* Ensure consistent naming conventions for delegate methods (e.g., prefixing with \"did\" for actions). \n","GenTime":"2024-08-28 12:37:38"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"text":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n\n\n##Decision\nWe will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","MatchedIds":[3863,1193,2687,1854,4980],"Prediction":"## Decision\n\nInstead of directly modifying existing classes with interceptors, which is irreversible and prone to compatibility issues, we will adopt a composition-based approach. \n\nThis can be achieved by:\n\n1. **Creating wrapper classes:** For classes requiring interception, we define wrapper classes that implement the same interface. These wrappers will hold an instance of the original class and delegate calls to it. \n2. **Implementing interception logic within wrappers:**  The wrapper classes will contain the interception logic. Before or after delegating calls to the original object, they can execute additional code (e.g., logging, authorization, input\/output manipulation).\n3. **Exposing the wrapper:** Instead of directly using the original class, we will expose and use the wrapper class.\n\nThis approach offers several benefits:\n\n* **Non-intrusive:**  The original classes remain untouched, preventing unintended side effects or conflicts with other frameworks.\n* **Reversible:**  Since we are not directly modifying the original classes, we can easily switch between using the original class or the wrapped version. \n* **Testability:**  Interception logic resides within separate wrapper classes, simplifying unit testing and ensuring cleaner code.\n\nThis approach provides a more flexible, maintainable, and robust way to implement interception in ES5 without the drawbacks of directly modifying existing classes. \n","GenTime":"2024-08-28 12:37:46"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"text":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n\n\n##Decision\nAddressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","MatchedIds":[4636,779,1883,3845,4875],"Prediction":"## Decision\n\nWe will introduce a new pipeline that listens to the Content Publisher's event bus for content update events. For each event, the pipeline will:\n\n1. **Extract relevant data:**  This includes the content item's ID, the timestamp of the update, the user who made the change, and potentially a summary of changes (e.g., \"Title updated\").\n2. **Transform the data:** The extracted data will be transformed into a format suitable for the Content Items' dimension. \n3. **Load the data:** The transformed data will be loaded into the Content Items' dimension, updating the relevant record with the latest change information.\n\nThis approach offers several advantages:\n\n* **Real-time updates:** The Content Items' dimension will reflect the latest content changes in near real-time due to the event-driven nature of the pipeline.\n* **Loose coupling:** The pipeline remains decoupled from Content Publisher's internal implementation, relying only on the published events.\n* **Scalability:** The pipeline can be scaled independently to handle a high volume of content updates.\n\n**Technical Details:**\n\n* **Event Source:** Content Publisher's event bus\n* **Pipeline Implementation:** Consider using a message queue system like RabbitMQ or Kafka to process events asynchronously.\n* **Data Transformation:** Utilize a data processing library like Apache Spark or Apache Beam for efficient data transformation.\n* **Data Loading:**  Employ appropriate data loading techniques to ensure efficient and consistent updates to the Content Items' dimension.\n\n[1]: You forgot to provide the Trello card link. Please provide the link for a complete decision.\n","GenTime":"2024-08-28 12:37:54"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"text":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n\n##Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","MatchedIds":[4852,1924,2975,3159,1823],"Prediction":"## Decision\nGiven the decision drivers, the chosen option is to **include the fetched URL in the exception message** instead of adding a separate logging statement. \n\n**Rationale:**\n\n* **Improves Code Readability:**  Having the relevant information directly within the exception reduces the need for tracing back through logs and keeps debugging contextual. \n* **Follows Best Practices:** Adhering to the principle of enriching exceptions with context instead of relying solely on logging is a recognized best practice in exception handling. \n\n**Implementation Details:**\n\n* Modify the fetcher code to catch any exceptions thrown during the fetching process.\n* In the `catch` block, create a new exception (potentially a custom exception type for fetch errors).\n* Include the fetched URL in the message of the new exception. \n* Re-throw the newly created exception to preserve the stack trace and error propagation.\n\n**Example:**\n\n```python\nclass FetchException(Exception):\n    def __init__(self, url: str, message: str):\n        super().__init__(message)\n        self.url = url\n\ndef fetch_data(url: str):\n    try:\n        # ... actual fetching logic ...\n    except Exception as e:\n        raise FetchException(url, f\"Failed to fetch data from {url}: {str(e)}\")\n```\nThis approach ensures that debugging information is readily available within the exception itself, simplifying the debugging process and adhering to best practices. \n","GenTime":"2024-08-28 12:38:02"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"text":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n\n##Decision\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","MatchedIds":[1031,2498,2183,4021,3203],"Prediction":"## Decision\n\nWe will model the relationships using a flexible graph structure. \n\nHere's how it will address both use cases:\n\n* **Mother-Child Relationship during Delivery:** \n    * A \"Person\" node will represent both mothers and children.\n    * A \"Relationship\" node will have a \"type\" attribute (e.g., \"Mother\", \"Child\", \"Sibling\", \"Partner\", \"Head of Household\"). \n    * During delivery, we create new \"Person\" nodes for each child and link them to the mother with \"Relationship\" nodes of type \"Mother\" and \"Child\" respectively. \n    *  Navigation between mother and child during delivery will be facilitated by directly traversing these relationship nodes.\n\n* **Family Structure:**\n    * The \"Head of Household\" relationship type will be used to identify the primary individual in a family.\n    * Other family members will be linked to the Head of Household with appropriate relationship types (e.g., \"Partner\", \"Child\", \"Sibling\", \"Parent\").\n    * This structure allows for flexibility in representing various family structures beyond the traditional nuclear family.\n\n**Advantages of this approach:**\n\n* **Flexibility:**  Accommodates different relationship types and family structures.\n* **Extensibility:**  Easily adaptable to future requirements for additional relationship types or complex family structures.\n* **Intuitive Navigation:** Enables efficient querying and navigation between individuals based on their relationships. \n\n**Implementation Considerations:**\n\n* Choosing a suitable graph database or implementing a graph-like structure within an existing database.\n* Designing an intuitive UI for navigating and managing relationships.\n\nThis approach provides a robust and scalable solution for modeling relationships, catering to both immediate delivery needs and the broader concept of families. \n","GenTime":"2024-08-28 12:38:13"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"text":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n\n\n##Decision\nInstead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","MatchedIds":[3092,3099,317,3094,3090],"Prediction":"## Decision\n\nWe will modify the AtlasDB transaction protocol to release the immutable timestamp lock and the row\/cell locks asynchronously after a successful commit. This decision is based on the following factors:\n\n1. **Performance Improvement:**  Releasing the locks asynchronously removes the overhead of two lock service calls from the critical path of a transaction commit. This can lead to significant performance improvements, especially for write-heavy workloads.\n2. **Correctness is Maintained:** The transaction protocol remains correct even if locks are released asynchronously. The existing safety mechanism, where an AtlasDB client crash after `putUnlessExists` is considered a commit, ensures data integrity.\n3. **Simplified Implementation:** Asynchronous lock release can be implemented with less complexity compared to other alternatives like batched lock release, which would require changes to the lock service API.\n\n**Implementation Details:**\n\n- A background thread or a mechanism like a scheduled task can be used to release the locks asynchronously. \n- The asynchronous lock release mechanism should be designed to be robust and handle failures gracefully. For instance, if a lock fails to release, it should be retried periodically.\n- Monitoring and logging should be added to track the performance and health of the asynchronous lock release mechanism.\n\nThis change represents a relatively low-risk optimization that can yield performance benefits. We will carefully monitor the system after deployment to ensure the asynchronous lock release mechanism behaves as expected. \n","GenTime":"2024-08-28 12:38:20"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"text":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n\n\n##Decision\nThe sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","MatchedIds":[2181,84,1216,1180,2499],"Prediction":"## Decision\n\n### Expose Timing Information via API\n\nWe will expose message timing information via the API. While the principle of \"modeling time\" for time-based business logic remains valid, there are compelling reasons to surface this information:\n\n**Benefits of Exposing Timing Information:**\n\n* **Improved Observability:** Access to message timestamps enables better monitoring, troubleshooting, and performance analysis of message flows. This data is valuable for identifying bottlenecks, understanding latency, and debugging issues.\n* **Enhanced Auditing and Debugging:** Timestamps provide crucial context for auditing message journeys and diagnosing problems. They can help track message provenance and reconstruct event sequences.\n* **Flexibility for Future Use Cases:** Exposing timing information caters to potential future requirements that might necessitate this data, even if not immediately apparent.  \n* **Alignment with Industry Practices:** Many messaging systems and APIs offer access to message timestamps, indicating the value of this information.\n\n**Addressing Concerns:**\n\n* **Modeling Time:** The principle of \"modeling time\" primarily addresses business logic directly tied to specific time values.  It doesn't preclude the need for system-level visibility into message timing.\n* **API Complexity:** Exposing additional fields for timing information adds minimal complexity to the API, especially considering the benefits gained.\n\n**Implementation Considerations:**\n\n* **Consistency:** Timestamps should be consistently applied across all relevant API endpoints and message lifecycle events.\n* **Accuracy:** Ensure timestamps are accurate and reflect the appropriate stage in the message lifecycle.\n* **Documentation:** Clearly document the meaning and format of all exposed timestamps.\n\n**Conclusion:**\n\nProviding access to message timing information via the API offers significant benefits in terms of observability, debugging, and future-proofing, outweighing the minimal increase in API complexity.  This decision promotes transparency and empowers users with valuable insights into message flows within the system. \n","GenTime":"2024-08-28 12:38:31"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"text":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n\n##Decision\nTTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","MatchedIds":[3150,3370,1167,3469,583],"Prediction":"## Decision\n\nWe will implement a multi-layered monitoring solution for TTA Smart Hub, encompassing:\n\n**1. Application Performance Monitoring (APM):** We will utilize an APM tool such as New Relic, Dynatrace, or Datadog to gain deep insights into application performance, identify bottlenecks, and trace errors to their root cause. This will provide real-time visibility into the application's behavior and help ensure optimal performance.\n\n**2. Infrastructure Monitoring:** We will leverage a cloud provider's native monitoring tools (e.g., AWS CloudWatch, Azure Monitor) or a third-party solution like Prometheus to track the health and performance of the underlying infrastructure. This includes monitoring metrics such as CPU utilization, memory usage, disk I\/O, and network traffic.\n\n**3. Log Management and Analysis:**  A centralized log management system like ELK (Elasticsearch, Logstash, Kibana), Splunk, or Graylog will be implemented to collect, aggregate, and analyze logs from various sources within the TTA Smart Hub ecosystem. This allows for comprehensive error tracking, root cause analysis, and proactive identification of potential issues.\n\n**4. Synthetic Monitoring\/Uptime Monitoring:**  To ensure the availability of TTA Smart Hub, we will use a service like Pingdom, UptimeRobot, or a similar tool to perform periodic synthetic checks from different geographic locations. This will provide alerts in case of downtime or service degradation.\n\n**5. Alerting and Incident Management:** We will configure an alerting system that integrates with our chosen monitoring tools to notify relevant personnel of critical events. This system will trigger alerts based on predefined thresholds and allow for customizable escalation policies to ensure timely incident response and resolution.\n\nThis multi-layered approach provides a comprehensive and proactive monitoring solution for TTA Smart Hub, ensuring uptime, facilitating rapid error resolution, and ultimately enhancing the user experience. The specific tools chosen will be based on a detailed evaluation of their features, ease of integration, and cost-effectiveness. \n","GenTime":"2024-08-28 12:38:42"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"text":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n\n\n##Decision\n- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","MatchedIds":[3907,3416,4388,3794,3797],"Prediction":"## Decision\n\nWe will implement a routing mechanism that identifies IIIF manifest requests originating from Meadow. This can be achieved by:\n\n1. **Dedicated Path\/Subdomain:** Meadow-generated manifests can be served from a dedicated path (e.g., `\/meadow\/manifests\/`) or subdomain (e.g., `meadow-manifests.example.com`). This allows for straightforward routing based on the URL.\n\n2. **Metadata Tagging:**  Meadow can add a specific metadata tag to the manifests it generates. This tag can be used by the routing logic to identify and handle these manifests differently.\n\nOnce identified, requests for Meadow-generated manifests will bypass Donut and be directly fetched from their S3 location. \n\n**Additional Considerations:**\n\n* **Authentication & Authorization:** If Meadow-generated manifests need specific access control, implement appropriate authentication and authorization mechanisms.\n* **Caching:** Evaluate the need for caching Meadow-generated manifests. If caching is beneficial, consider a dedicated caching layer for these manifests to avoid conflicts with the existing Donut-based cache. \n* **Monitoring:** Monitor the performance and health of the new routing mechanism to ensure efficient and reliable manifest delivery. \n","GenTime":"2024-08-28 12:38:49"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"text":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n\n\n##Decision\nThe prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","MatchedIds":[5147,4804,3820,1145,4616],"Prediction":"## Decision\n\n**Adopt a strategy of using weakly informative priors by default.**\n\n## Rationale\n\n* **Least surprising for users:**  Most users, especially those without deep statistical expertise, would expect the analysis to be driven primarily by their data. Weakly informative priors minimize the influence of the prior on the posterior distribution, letting the data \"speak for itself\" as much as possible.\n* **Transparency and reproducibility:** Clearly document the specific weakly informative priors used as defaults. This allows users to understand the assumptions made and reproduce the results. \n* **Flexibility:**  Provide users with the option to specify their own custom priors. This empowers users with domain-specific knowledge or those seeking to explore different prior assumptions.\n* **Future Iteration:**  Start with weakly informative priors and offer guidance on how to choose and implement custom priors. This approach allows the SkyPy team to gather user feedback and potentially refine the default prior selection strategy over time.\n\n## Example Implementation\n\nOffer a set of commonly used weakly informative priors (e.g., uniform distributions over a plausible range, broad normal distributions) as the default for each parameter type.\n","GenTime":"2024-08-28 12:38:55"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"text":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n\n\n##Decision\nUse Jest for testing.\\n","MatchedIds":[452,4223,3323,1594,4359],"Prediction":"## Decision\nWe will use Jest as our testing framework. It offers familiarity, a strong track record, and benefits from a large, active community that ensures good documentation, readily available support, and a rich ecosystem of plugins and extensions. \n","GenTime":"2024-08-28 12:38:58"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"text":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n\n##Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","MatchedIds":[2834,3657,2471,1545,979],"Prediction":"## Decision\n\nKafkarator will be deployed on NAV's Kubernetes clusters and configured to interact with the Aiven-hosted Kafka. No on-premise Kafka installation will be managed by Kafkarator. \n\nTo facilitate secure and seamless communication between Kafkarator and Aiven:\n\n- Necessary credentials and connection details for the Aiven Kafka instance will be stored securely, leveraging Kubernetes secrets management.\n- Kafkarator will be responsible for managing topics and access control for applications on the Aiven Kafka instance based on the configuration provided by developers. \n- We will investigate and implement a robust monitoring and alerting system to track the health and performance of both Kafkarator and the Aiven Kafka instance.\n","GenTime":"2024-08-28 12:39:04"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"text":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n\n\n##Decision\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","MatchedIds":[1464,2256,4358,2471,3147],"Prediction":"## Decision\n\nTo enhance security and prevent accidental key leakage, we will adopt the following approach:\n\n* **For applications running on EC2 instances:** Utilize **IAM roles and instance profiles** to grant permissions and access AWS resources. This method removes the need to manage long-term credentials and leverages automatic credential rotation. \n* **For deployment tooling and Cloud Foundry components:**\n    * **Where possible:**  Migrate to solutions that integrate with AWS IAM roles directly. This may involve updates or configuration changes to existing tools like Concourse, Terraform, and BOSH.\n    * **If direct IAM role integration isn't feasible:** Explore solutions like **AWS Secrets Manager** or **HashiCorp Vault** to securely store and manage API credentials. These tools offer enhanced security features and reduce the risk of accidental exposure.\n\n**Rationale:**\n\n* IAM roles and instance profiles significantly reduce the risk of leaking credentials by eliminating the need to store them directly.\n* Utilizing established secrets management solutions provides a centralized and secure way to handle sensitive information while offering additional features like access control and auditing.\n\n**Implementation Notes:**\n\n* A thorough review of current tools and infrastructure is necessary to determine the feasibility of IAM role integration and select the appropriate secrets management solution.\n* Security best practices should be followed during implementation, including the principle of least privilege when assigning permissions.\n* Ongoing monitoring and auditing of credential usage should be implemented to ensure continued security.\n","GenTime":"2024-08-28 12:39:11"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"text":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n\n\n##Decision\nWe decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","MatchedIds":[1855,4361,4699,4428,4408],"Prediction":"## Decision\n\nWe will treat accessibility as a first-class citizen within Marsha's architecture by adopting a flexible and extensible metadata model for video content.\n\n### Metadata Model:\n\nInstead of embedding auxiliary content directly into the main video file, we will store it as separate, linked resources. This approach offers several advantages:\n\n* **Flexibility**: Supports various formats (audio, timed text, sign language video) without modification of the main video.\n* **Extensibility**: Allows adding new types of auxiliary content in the future without architectural changes.\n* **Accessibility**: Enables users to select and combine auxiliary content according to their needs.\n* **Bandwidth Efficiency**: Users only download the resources they need.\n\n### Implementation Details:\n\n1. **Video Metadata**: Each video will have metadata associated with it, including links to:\n    * Multiple audio tracks\n    * Multiple timed text tracks in different languages and formats\n    * Multiple sign language video tracks for different sign languages\n2. **Player Integration**: We will utilize a video player that supports:\n    * Selecting and switching between available audio tracks\n    * Displaying timed text tracks with customization options (font size, color, background)\n    * Playing sign language videos synchronized with the main video\n3. **Standardized Formats**: We will use established formats for accessibility:\n    * Timed text: WebVTT (preferred) or SRT\n    * Audio descriptions: Follow WCAG guidelines \n\nThis metadata-driven approach ensures Marsha is accessible from the outset, allowing for future expansion and adaptation to evolving accessibility standards. \n","GenTime":"2024-08-28 12:39:19"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","MatchedIds":[2462,2467,3873,1631,1193],"Prediction":"## Decision\n\nThe decision to not directly support or provide binary packages for google-cloud-cpp is reasonable. \n\nHowever, instead of completely rejecting any involvement, we can refine the approach to be more proactive in enabling the community to create and distribute binary packages:\n\n**Revised Decision:**\n\n1. **Do Not Officially Support Binary Packages:**  We will not officially provide or support binary packages for google-cloud-cpp. This means we won't:\n   - Build and distribute binary packages ourselves.\n   - Officially endorse any specific package manager or distribution.\n   -  Directly test binary distributions.\n\n2. **Facilitate Community-Driven Binary Packages:**  We will actively facilitate the creation and maintenance of high-quality, community-driven binary packages. This includes:\n    - **Providing Clear Packaging Guidelines:** Develop and maintain comprehensive documentation on how to package google-cloud-cpp for various package managers (dpkg, rpm, vcpkg, etc.).\n    - **Designing for Packageability:** Structure our codebase and build system to be easily packageable. This might involve:\n        - Using standard CMake variables and targets.\n        - Clearly separating public headers from internal implementation details.\n        - Avoiding complex dependencies or providing mechanisms to easily package them. \n    - **Collaborating with Community Packagers:** Be responsive to requests and issues filed by community members who are packaging google-cloud-cpp. \n    - **Potentially Hosting a List of Community Packages:** Consider hosting a curated list of community-maintained binary packages on the project website, along with their supported platforms and package managers. This provides a central location for users to find pre-built packages.\n\n## Revised Consequences\n\n- **Reduced Maintenance Burden:** Avoids the significant overhead of building, testing, and distributing binary packages for numerous platforms and package managers.\n- **Empowers the Community:** Encourages and enables community involvement in providing binary packages, leveraging their expertise and resources.\n- **Faster Adoption:** Makes it easier for users who prefer binary packages to adopt and use google-cloud-cpp. \n- **Potential for Fragmentation:**  Without official support, there's a risk of fragmentation with multiple, potentially conflicting, community-maintained packages.\n- **Quality Control Challenges:** Ensuring the quality and security of community-maintained packages can be more challenging.\n\n## Recommendation\n\nBy providing clear guidance, tooling, and a collaborative environment, we can leverage the power of the community to make google-cloud-cpp easily accessible through binary packages while minimizing the maintenance burden on the core development team. \n","GenTime":"2024-08-28 12:39:32"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"text":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\n\n##Decision\nWe will build distributables using Rollup.js.\\n","MatchedIds":[1631,1576,539,3827,1119],"Prediction":"## Decision\n\nWe will use Rollup to bundle the library for distribution. \n\nThis approach will allow us to: \n\n- **Support TypeScript and JavaScript consumers**: Rollup can transpile TypeScript to JavaScript and bundle it in various formats (CommonJS, ES modules) to accommodate both TypeScript and JavaScript projects.\n- **Create optimized bundles**: Rollup is known for its efficient tree-shaking capabilities, minimizing the final bundle size by eliminating unused code.\n- **Simplify the build process**: Rollup offers a straightforward configuration, streamlining the build process and making it easier to maintain.\n\nThis approach will enable us to reach a wider audience by providing a versatile library compatible with various JavaScript environments. \n","GenTime":"2024-08-28 12:39:42"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"text":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n\n##Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","MatchedIds":[3295,1767,2410,4047,3915],"Prediction":"## Decision\n\nThe service broker will be extracted from the `alphagov\/tech-ops` repository into its own repository. This will allow for independent versioning, release management, and clearer ownership for the service. \n\nThe pipeline for building and deploying the broker will be migrated from the Tech Ops multi-tenant Concourse to a dedicated Concourse pipeline within the new repository.  This provides a more self-contained and maintainable CI\/CD process for the broker. \n","GenTime":"2024-08-28 12:39:51"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"text":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n\n\n##Decision\nIn order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","MatchedIds":[2625,485,2585,1180,2802],"Prediction":"## Decision\n\nWe will use Jackson to serialize and deserialize workspaces to\/from JSON. \n\nThis decision is based on the following factors:\n\n* **Existing familiarity:** Jackson is a widely used and well-documented library with robust JSON processing capabilities. This minimizes the learning curve and potential for integration issues. \n* **Feature set:** Jackson offers extensive features for object mapping, custom serialization, and handling complex data structures, which aligns well with the potential intricacies of a workspace model.\n* **Performance:** Jackson is known for its performance, which is crucial when dealing with potentially large and complex workspaces. \n\nBy leveraging Jackson, we can ensure a reliable and efficient mechanism for interacting with remote and local workspaces in JSON format. \n","GenTime":"2024-08-28 12:39:55"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"text":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n\n\n##Decision\nFrom Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","MatchedIds":[1956,1949,1979,4097,1982],"Prediction":"## Decision\n\nStarting with Tendermint 1.0, we will consider the following to be our public API, and provide backwards compatibility guarantees accordingly:\n\n- **All `exported` identifiers in packages listed under `Public APIs` in the [README](https:\/\/github.com\/tendermint\/tendermint#versioning).** This includes functions, types, variables, constants, and methods.\n- **The behavior of all public APIs,** as documented in the GoDoc API documentation. This includes the expected input and output values, error handling, and side effects.\n\nAll other code is considered private API, and we make no guarantees about its backwards compatibility. This includes:\n\n- **All `private` identifiers,** even those in public packages.\n- **All code in packages not listed under `Public APIs` in the [README](https:\/\/github.com\/tendermint\/tendermint#versioning).**\n- **All code in the `internal\/` directory.**\n- **All test code,** even if it is in a public package.\n\n### Backwards Compatibility\n\nWe will follow the rules of semantic versioning for our public API:\n\n- **Major version changes (e.g. 1.0 to 2.0):** May introduce backwards-incompatible changes.\n- **Minor version changes (e.g. 1.0 to 1.1):** May introduce new features in a backwards-compatible manner.\n- **Patch version changes (e.g. 1.0.1 to 1.0.2):** May only include backwards-compatible bug fixes.\n\n### Recommendations\n\n- Developers should only rely on the documented public API for their external projects.\n- We should strive to document all public APIs clearly and comprehensively.\n- We should minimize breaking changes to the public API, and clearly communicate any such changes in release notes.\n- We should use the `internal\/` directory for code that is not intended for use by external projects.\n- We should consider using a tool like [Go API Checker](https:\/\/github.com\/kisielk\/go-tools\/tree\/master\/cmd\/go-api-checker) to help enforce our public API guarantees.\n\nBy clearly defining our public API and adhering to semantic versioning, we can provide a stable and predictable platform for developers building on Tendermint. \n","GenTime":"2024-08-28 12:40:06"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"text":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n\n\n##Decision\nDuring runtime, we will only depend on resources services from the Google Cloud Platform.\\n","MatchedIds":[1035,1631,929,3925,302],"Prediction":"## Decision\n\nWe will strive to minimize dependencies where it makes sense for the production environment, considering both the risk of outages and the cost of developing and maintaining services ourselves. \n\n**Specifically:**\n\n* **Production:** We will favor reducing external dependencies for critical functionalities where outages would have a significant impact on our system's availability. We will consider caching or copying functionality of 3rd party services where feasible and cost-effective.\n* **Build and Development:** We will prioritize leveraging 3rd party components and services to accelerate development and reduce costs. We will avoid investing in replicating their functionality in these environments unless strictly necessary for specific testing purposes.\n\nThis approach allows us to benefit from the agility and cost-effectiveness of external services during development while prioritizing stability and resilience in our production environment. We will document each decision regarding 3rd party dependencies, outlining the rationale, potential risks, and mitigation strategies. \n","GenTime":"2024-08-28 12:40:11"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"text":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n\n\n##Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","MatchedIds":[4041,4266,2410,3147,4056],"Prediction":"## Decision\n\nWe will migrate the GOV.UK MongoDB clusters to AWS DocumentDB. This approach aligns with the TechOps principle of leveraging AWS managed services where possible, reducing the operational overhead of managing MongoDB clusters directly.\n\n**Rationale:**\n\n- **Reduced Operational Burden:** Migrating to DocumentDB eliminates the need for GOV.UK teams to manage the underlying infrastructure, patching, backups, and scaling of the MongoDB clusters.\n- **Compatibility:** DocumentDB offers a high degree of compatibility with MongoDB, simplifying the migration process for existing applications.\n- **Scalability and Availability:** DocumentDB provides built-in scalability and high availability features, ensuring the performance and resilience of GOV.UK applications.\n- **Cost Optimization:** While DocumentDB may have a higher upfront cost compared to self-managed MongoDB, the reduced operational overhead and potential for right-sizing resources can lead to long-term cost savings.\n\n**Implementation Plan:**\n\n- **Assessment:** Conduct a thorough assessment of each MongoDB cluster, evaluating data size, usage patterns, and application dependencies to determine the appropriate migration strategy.\n- **Pilot Migration:** Migrate a non-critical application or a subset of data to DocumentDB as a pilot, validating the migration process and identifying any potential challenges.\n- **Phased Rollout:** Implement a phased migration approach, gradually moving applications and data to DocumentDB, minimizing disruption to GOV.UK services.\n- **Monitoring and Optimization:** Establish comprehensive monitoring and logging for the DocumentDB clusters, proactively identifying and addressing any performance or operational issues.\n\n**Consequences:**\n\n- **Dependency on AWS:** Migrating to DocumentDB introduces a dependency on AWS as the managed service provider.\n- **Potential Compatibility Issues:** While DocumentDB is highly compatible with MongoDB, there might be minor differences requiring code adjustments in some applications.\n- **Migration Effort:** Migrating existing data and applications to DocumentDB requires dedicated effort and resources. \n\n**Status:** Accepted\n\nThis decision will be revisited as part of the regular architecture review cycle or if significant changes occur in the technology landscape or GOV.UK's operational requirements.\n","GenTime":"2024-08-28 12:40:23"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"text":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n\n##Decision\nThe proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","MatchedIds":[1985,4113,4097,4099,4130],"Prediction":"## Decision\n\nWe introduce a new structure called `AppHash`, which encapsulates multiple application-related roots instead of just representing a single monolithic state root. \n\nThis `AppHash` structure will include, but is not limited to:\n\n* **StateRoot:** The existing root hash of the application state. This maintains backward compatibility and serves the primary purpose of the application hash.\n* **EventsRoot:**  A root hash of all events emitted during a block. This allows for efficient inclusion proofs for specific events.\n* **TxRoot:** A root hash of all transactions included in the block. This could be useful for applications that need to provide proofs related to transaction inclusion.\n\nThis `AppHash` structure will be included in the block header, replacing the existing single application hash. Light clients and other components can then utilize the relevant root hash from within `AppHash` for their specific verification purposes. \n\nThis approach provides the following benefits:\n\n* **Flexibility:**  Allows for future extensibility to include other application-specific roots as needed.\n* **Light Client Proofs:** Enables light clients to efficiently verify specific events, not just the entire application state.\n* **Backward Compatibility:** Maintains the existing functionality of the application hash through the `StateRoot` field.\n\nWe will define a clear process for evolving the `AppHash` structure in the future to accommodate new application requirements while minimizing disruption to existing implementations. \n","GenTime":"2024-08-28 12:40:30"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"text":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\n\n##Decision\nWe use git for version control and GitHub for collaboration.\\n","MatchedIds":[1117,4600,400,3070,241],"Prediction":"## Decision\n\nWe will use **Git** as our version control system and **GitHub** as our online hosting platform. \n\n**Reasons:**\n\n- **Git** is a distributed version control system, which means every developer has a complete copy of the repository history, making it robust against data loss and allowing for offline work.\n- **Git** is widely used and considered the industry standard for version control, making it easier for new developers to onboard and contribute.\n- **GitHub** is a popular platform for hosting Git repositories, offering features like issue tracking, pull requests, and a vast community of developers. \n- **GitHub** provides free public repositories for open-source projects and affordable plans for private repositories.\n- **GitHub's** extensive documentation, tutorials, and integrations make it easy to learn and use.\n\nThis combination of **Git** and **GitHub** provides a powerful and widely-adopted solution for version control and online collaboration, enabling us to maintain code integrity, facilitate teamwork, and engage with the broader development community. \n","GenTime":"2024-08-28 12:40:37"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"text":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n\n##Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","MatchedIds":[3571,2616,1416,2793,3069],"Prediction":"## Decision\nThe `help` subcommand will, by default, display the contents of the `__doc__` string from the corresponding Python script file. This encourages developers to document their code clearly within the script itself.\n","GenTime":"2024-08-28 12:40:40"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"text":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n\n\n##Decision\n[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","MatchedIds":[4276,3968,2802,1255,4274],"Prediction":"## Decision\n\nThe decision is to integrate with a postcode lookup service that can provide latitude and longitude coordinates for a given postcode. This will allow the application to consistently use lat\/lon coordinates for both services and search points, simplifying the process of calculating distances for proximity-based searches. \n\nSeveral options exist for postcode lookup services, including free and paid tiers, and the best choice will depend on factors like accuracy requirements, usage volume, and budget. \n","GenTime":"2024-08-28 12:40:46"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"text":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n\n\n##Decision\nFor choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","MatchedIds":[4395,351,1739,3915,2968],"Prediction":"## Decision\nWe decided to integrate a CI\/CD pipeline into the development process for building, signing, and pushing Connaisseur images. This pipeline will be triggered automatically on every pull request merge to the main branch. \n\n### Positive consequences\n\n* **Automation:** Removes the manual effort and responsibility from the maintainer, allowing any contributor to trigger a new release after their changes are reviewed and merged.\n* **Increased Release Frequency:**  Enables more frequent releases, aligning with the increase in contributions and ensuring the latest features and bug fixes are available to users quickly. \n* **Version Consistency:** Guarantees the uploaded Connaisseur image always corresponds to the version specified in the Helm chart, avoiding potential inconsistencies and deployment issues. \n* **Improved Development Agility:**  Contributes to a more agile development process, allowing faster iteration and delivery of value to users.\n\n### Negative consequences\n\n* **Initial Setup Cost:**  Requires initial investment in setting up and configuring the CI\/CD pipeline.\n* **Maintenance Overhead:**  Introduces a new system to maintain and troubleshoot. However, the benefits of automation and improved development workflow outweigh this overhead in the long run.\n\nThis approach aligns with our goals of fostering a more collaborative development environment, increasing release velocity, and ensuring a consistently reliable experience for Connaisseur users. \n","GenTime":"2024-08-28 12:40:55"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"text":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n\n##Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","MatchedIds":[1631,3203,3691,1035,2626],"Prediction":"## Decision\nTo enforce a cleaner separation between API and implementation and leverage the strengths of OSGi and Karaf, we will refactor the OpenHAB project structure as follows:\n\n1. **Split modules into API and Implementation:** Each module currently containing both API (`org.openhab.core.items`) and implementation (`org.openhab.core.items.internal`) will be split into separate modules\/bundles. For example, `openhab-core-items` (API) and `openhab-core-items-impl` (implementation).\n\n2. **Strict dependency management:**  We will utilize build tool capabilities (e.g., Maven's dependency scope or Gradle's configurations) to strictly manage dependencies. API modules will only declare dependencies required by the public API, excluding any transitive dependencies related to internal implementation.\n\n3. **Granular Karaf features:** Instead of relying on broad features like `openhab-core-base`, we will introduce fine-grained Karaf features representing individual functionalities or modules. This will allow for more selective dependency resolution, preventing the activation of unnecessary parts of the framework. For example, a feature for `openhab-core-items` would only include the `openhab-core-items` API bundle and its direct dependencies, while a feature for a specific item provider would additionally include the corresponding `openhab-core-items-impl` bundle and its dependencies.\n\n4. **Refactor module relationships:** The relationship between modules like `openhab-thing-core` and `openhab-rest-core` should be reviewed. If `openhab-rest-core` only depends on the API of `openhab-thing-core`, then the dependency should be updated to reflect that. If a deeper integration is needed, it should be clearly defined and potentially refactored for better separation.\n\nThis approach will result in:\n\n* **Cleaner architecture:** Clear separation of concerns between API and implementation, reducing coupling and improving maintainability.\n* **Reduced footprint:** By loading only the necessary modules, the overall memory and resource consumption can be optimized.\n* **Increased flexibility:** Easier to extend and customize the framework by selecting and integrating only the required functionalities.\n\nThis restructuring will require a significant refactoring effort but will ultimately lead to a more robust, modular, and maintainable OpenHAB architecture that takes full advantage of OSGi's strengths. \n","GenTime":"2024-08-28 12:41:09"}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"text":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n\n\n##Decision\nUse a jvm per airport\\n","MatchedIds":[4538,1545,3210,3815,2183],"Prediction":"## Decision\n\nWe will move away from a single, normalized model for all ports in DRTv2. Instead, we will embrace a more flexible approach where:\n\n* **Ports can define their own schemas:** This allows for specialized structures and data types tailored to the specific needs of each port. \n* **Commonalities can be extracted into shared modules:** To avoid unnecessary duplication, common data structures and functionalities can be defined once and reused across different ports.\n* **A core set of basic attributes will be maintained:**  This ensures a minimum level of consistency and interoperability between ports, even with the increased flexibility.\n\nThis approach balances the need for standardization with the flexibility required to support diverse port requirements, ultimately leading to a more maintainable and extensible system. \n","GenTime":"2024-08-28 12:41:14"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"text":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n\n\n##Decision\nAll schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","MatchedIds":[3092,3090,317,1558,2526],"Prediction":"## Decision\nWe will implement a locking mechanism using a dedicated table in Cassandra itself, named \"_locks\", to prevent concurrent table creation. Here's why:\n\n1. **Directly Addresses the Issue:** Utilizing a locking mechanism provides a straightforward solution to prevent concurrent table creation, directly addressing the root cause of the data loss scenario.\n2. **Leverages Existing Infrastructure:** By using Cassandra itself for the locking mechanism, we minimize external dependencies and leverage the existing cluster's capabilities.\n3. **Resiliency:** Cassandra's distributed nature and fault-tolerance extend to the \"_locks\" table, ensuring the locking mechanism remains operational even with node failures.\n\nWhile this approach introduces the need to manage the \"_locks\" table, the benefits of data integrity and a robust solution outweigh the added complexity.  This decision aligns with the need for a reliable and efficient solution to a critical data loss issue in Cassandra. \n","GenTime":"2024-08-28 12:41:21"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"text":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n\n\n##Decision\nWe use Ant Design as the UI Framework.\\n","MatchedIds":[1240,1243,3062,1477,4271],"Prediction":"## Decision\n\nWe will use **React** for our UI framework. \n\nReact has a large and active community, which translates into excellent documentation, readily available resources, and ample support for troubleshooting. This will significantly contribute to faster development times.  Additionally, React's component-based architecture and reusable components lend themselves well to building complex and performant UIs, ensuring good functionality for our needs. \n","GenTime":"2024-08-28 12:41:28"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"text":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n\n\n##Decision\nThe 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","MatchedIds":[2791,2526,2121,1606,4821],"Prediction":"## Decision\n\n**Chosen Option:** Creating a separate table for periodic tasks (Option 2).\n\n## Rationale\n\nWhile adding fields to the existing `job` table might seem simpler at first, separating periodic tasks into their own table offers several advantages:\n\n* **Clearer Separation of Concerns:**  Maintaining a distinct table for periodic tasks enhances maintainability and readability. It clearly segregates periodic tasks from immediate tasks, simplifying queries and reducing potential confusion. \n* **Optimized Data Structures:**  Periodic tasks have unique attributes like execution intervals, last run timestamps, and potential retry mechanisms. A separate table allows for a schema specifically optimized for these attributes, improving performance for periodic task management.\n* **Flexibility and Extensibility:** Separating periodic tasks anticipates future requirements. It allows for easier introduction of new features specific to periodic tasks, such as user-defined schedules, without cluttering the existing `job` table. \n* **Reduced Contention:**  By separating the tables, contention on the `job` table from regular task scheduling is minimized even during bursts of periodic task executions.\n\n## Implementation Notes\n\n1. **Periodic Task Table Schema:**\n    * `task_id`: Unique identifier for the periodic task.\n    * `task_name`:  Descriptive name of the task.\n    * `task_definition`:  Reference to the task's execution logic (e.g., function name).\n    * `schedule`:  Cron expression or similar defining the execution interval.\n    * `last_run`:  Timestamp of the last successful execution.\n    * `next_run`:  Calculated timestamp for the next scheduled execution.\n    * `status`: Current state of the task (e.g., Scheduled, Running, Error).\n\n2. **Periodic Task Scheduler:** \n    * A dedicated scheduler component will be responsible for managing the periodic tasks table. \n    * It will periodically query for tasks due to run, acquire a lock on the task to prevent concurrent execution, and then delegate the task execution to the job executor.\n\n3. **Job Executor Integration:** \n    * The existing job executor will need minimal modification to execute periodic tasks.\n    * It will receive the task definition and any required parameters from the periodic task scheduler and execute it accordingly.\n\nThis approach ensures that periodic tasks are managed efficiently and reliably within the system while maintaining a clean and scalable architecture. \n","GenTime":"2024-08-28 12:41:41"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"text":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n\n\n##Decision\n* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","MatchedIds":[3682,3722,1180,4021,779],"Prediction":"## Decision\n\nInstead of building a complete end-to-end solution immediately, we will adopt an iterative approach and break down the development process into smaller, more manageable milestones. This will allow us to:\n\n* **Deliver value incrementally**:  By focusing on smaller, functional slices, we can demonstrate progress and gather feedback early.\n* **Mitigate risk**: Breaking down complex tasks helps identify and address potential issues sooner rather than later.\n* **Increase flexibility**:  An iterative approach makes it easier to adapt to changing requirements or priorities.\n\nWe will prioritize these milestones based on criticality and dependencies.  Each milestone will represent a tangible outcome and will be followed by testing and integration before moving on to the next. \n","GenTime":"2024-08-28 12:41:46"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"text":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n\n\n##Decision\nConsidering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","MatchedIds":[1094,1089,1180,2359,1243],"Prediction":"## Decision\n\nWe will implement the Web APIs as **Asynchronous** using a RESTful pattern. \n\nThis decision is driven by the following factors:\n\n* **Improved Scalability and Responsiveness:** Asynchronous APIs allow the server to handle multiple requests concurrently without blocking on long-running operations. This significantly improves the application's ability to scale and remain responsive under high load. \n* **Enhanced User Experience:** By returning control to the user immediately, asynchronous APIs enable a more fluid and interactive user experience. Users are not kept waiting for long-running operations to complete and can continue interacting with the application.\n* **Flexibility in Handling Long-Running Tasks:** Asynchronous APIs provide a natural mechanism to handle tasks that may take a significant amount of time to complete. This can be achieved by utilizing background processes or queues to manage the execution of these tasks.\n\nWhile asynchronous APIs offer significant advantages, it's important to consider the added complexity they introduce in terms of error handling and managing asynchronous workflows. However, the benefits often outweigh these complexities, especially for applications requiring high scalability and responsiveness. \n","GenTime":"2024-08-28 12:41:55"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"text":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n\n##Decision\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","MatchedIds":[4937,2409,4025,745,1376],"Prediction":"## Decision\nWe will use [Infrastructure as Code (IaC)](https:\/\/en.wikipedia.org\/wiki\/Infrastructure_as_code) principles to manage the provision of servers and software for the Datalabs project. A suitable IaC tool, such as Terraform or Ansible, will be selected after evaluating their suitability against the project's specific requirements. This evaluation will consider factors such as:\n\n- **Ease of use:** The chosen tool should be straightforward for the team to learn and use.\n- **Community support:** A tool with strong community support will provide access to documentation, examples, and assistance when needed. \n- **Integration capabilities:** The tool should integrate seamlessly with other tools and services used in the project, such as cloud providers and version control systems.\n- **Cost:** The tool's cost, if any, should be aligned with the project budget.\n\nThis decision will be reviewed after an initial evaluation period to confirm its suitability for the Datalabs project.\n","GenTime":"2024-08-28 12:42:00"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"text":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n\n##Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n","MatchedIds":[3657,1740,1376,2471,4025],"Prediction":"## Decision\n\nWe will use an RDS instance for PostgreSQL instead of hosting it on the Puppetmaster itself. \n\nWhile this requires additional Terraform work for provisioning and minor Puppetmaster code updates, the benefits outweigh the effort:\n\n* **Reduced management overhead:** Offloading database administration to RDS simplifies our infrastructure and removes the need for Puppet code related to database configuration, backups, and maintenance.\n* **Increased reliability and scalability:** RDS provides a managed environment with built-in redundancy, automated backups, and easier scaling options compared to a self-managed database on the Puppetmaster. \n* **Better security:** RDS benefits from AWS security best practices and compliance certifications.\n\nWe will prioritize the Terraform work and Puppetmaster code updates to migrate to the RDS solution as soon as possible to capitalize on these advantages. \n","GenTime":"2024-08-28 12:42:07"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\\nI need to decided with which tools I build my react app.\\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"text":"## Context\\nI need to decided with which tools I build my react app.\\n\n\n##Decision\nI build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","MatchedIds":[1240,3715,1243,3035,2273],"Prediction":"## Decision\n\nI will use **Create React App (CRA)**. It's a widely adopted toolchain specifically designed for React applications, offering a pre-configured setup with:\n\n* **Webpack:** Handles bundling, transpiling, and development server.\n* **Babel:**  Allows using modern JavaScript features.\n* **ESLint:**  Maintains code style and quality.\n* **Jest:** Provides a testing framework.\n\nCRA allows me to focus on building the app's features without getting bogged down in configuration details. \n","GenTime":"2024-08-28 12:42:10"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"text":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n\n\n##Decision\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","MatchedIds":[4832,469,583,4642,1243],"Prediction":"## Decision: \nThe change proposed to the current implementation is to add an apache web server,  using the PHP-apache docker image, to host the php web application created in the php directory. \n\n### Consequences: \nWhile apache is a robust and well-documented web server, there might be performance implications when choosing it solely based on convenience. Using the bundled docker image simplifies setup but might lead to larger image sizes and potential security vulnerabilities if the image isn't regularly updated.  Consider exploring alternative lightweight web servers optimized for PHP applications like Nginx or Caddy for potential performance and resource efficiency improvements in the future. \n","GenTime":"2024-08-28 12:42:14"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"text":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n\n\n##Decision\nWhile we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","MatchedIds":[3768,1558,779,4354,1494],"Prediction":"## Decision\n\nWe will perform a phased removal of the `PartSetHeader` from the `BlockID` structure to simplify our commitment model and reduce redundancy.\n\n**Phase 1: Immediate Removal**\n\n* **Remove `PartSetHeader` from `BlockID` wherever it's used for data commitment verification.** This includes any logic that relies on `PartSetHeader` for validating the integrity of the block data. \n* **Update data verification logic to solely rely on `DataHash` within `BlockID`.** This ensures a single source of truth for block data integrity.\n\n**Phase 2: Retention for Consensus**\n\n* **Retain `PartSetHeader` within the `Proposal` struct.** This maintains the existing consensus mechanism relying on `PartSetHeader` for block propagation.\n\n**Phase 3: Future Removal from `Proposal`**\n\n* **Investigate alternative block propagation mechanisms during consensus that don't rely on `PartSetHeader`.** This could involve utilizing erasure coding techniques or other efficient data dissemination strategies.\n* **Once a suitable alternative is implemented and tested, remove `PartSetHeader` from the `Proposal` struct.**\n\n**Additional Notes:**\n\n* This phased approach allows us to simplify the system and remove redundancy without disrupting the existing consensus mechanism.\n* The removal of `PartSetHeader` from `BlockID` will require updates to various parts of the codebase, including data verification logic, serialization\/deserialization, and consensus-related components.\n* Thorough testing should be conducted at each phase to ensure the system's stability and the correctness of the changes. \n","GenTime":"2024-08-28 12:42:22"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"text":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n\n\n##Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","MatchedIds":[2763,1924,2270,2963,3192],"Prediction":"## Decision \n\nUse a byte-by-byte comparison of the UTF-8 representation of the strings to determine equality.\n\n**Rationale:**\n\n* **Simplicity and Performance:** Byte-by-byte comparison is the simplest and most performant string comparison method. Since the identifiers are already validated and in UTF-8, no additional parsing or normalization is required.\n* **Immutability:** As identifiers are mostly immutable and not derived from user input, concerns about normalization complexities or case-folding variations are minimized. \n* **Specificity:** This decision clearly defines the comparison method, eliminating ambiguity.\n\nThis approach optimizes for performance and clarity, aligning with the use case of pre-validated, largely immutable identifiers used within the system. \n","GenTime":"2024-08-28 12:42:27"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"text":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n\n\n##Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","MatchedIds":[4930,138,2215,4699,4428],"Prediction":"## Decision\n\nWe will develop a dedicated module responsible for parsing and interpreting the `*_ALL` files produced by the feature-compare process. This module will:\n\n* **Abstract the complexity** of handling different record types (`IDENTICAL`, `ADDED`, `MODIFIED`, `REMOVED`), non-unique primary keys, and the special logic associated with `WEGSEGMENT_ALL.DBF`.\n* **Provide a simplified and consistent API** for accessing change information, regardless of the underlying file format or specific primary key handling. \n* **Handle the translation** between temporary identifiers in the `*_ALL` files and permanent identifiers used internally.\n\nThis approach isolates the complexity of the feature-compare output and provides a clean interface for other parts of the system to consume and process change data. This promotes better maintainability, testability, and reduces the risk of errors related to the intricacies of the `*_ALL` file format.\n","GenTime":"2024-08-28 12:42:32"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"text":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n\n##Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","MatchedIds":[4857,5163,2093,3421,4841],"Prediction":"## Decision\nWe will use **Azure Active Directory (Azure AD)** as the identity management system for the frontend application. \n\n### Positive Consequences\n* **Single Sign-On (SSO):**  Leveraging Azure AD allows seamless integration with SEPA's existing Active Directory, providing a unified single sign-on experience for users. This enhances security by reducing password fatigue and improves usability with a single set of credentials for both internal systems and the CCP application. \n* **Vendor Synergy:**  Choosing Azure AD aligns with the existing proposal of using Azure Cloud Services and Microsoft Dynamics 365, promoting interoperability and maximizing vendor reuse within the Microsoft ecosystem.  \n* **Robust Feature Set:** Azure AD offers a comprehensive suite of identity and access management features including multi-factor authentication, conditional access policies, and role-based access control, ensuring robust security for the application.\n\n### Negative Consequences\n* **Vendor Lock-in:** Choosing Azure AD increases reliance on Microsoft services. Migrating away from Azure AD in the future might present complexities and require significant effort.\n* **Potential Licensing Costs:** While basic Azure AD features may be sufficient initially, utilizing advanced security features or higher user volume might necessitate upgrading to premium Azure AD licenses, leading to additional costs. \n","GenTime":"2024-08-28 12:42:39"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"text":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n\n\n##Decision\nWe will use RuboCop for linting our Ruby code\\n","MatchedIds":[458,400,3853,4359,4336],"Prediction":"## Decision\nWe will use RuboCop to enforce code style and best practices in our Ruby codebase.\n\nWe will configure RuboCop using a `.rubocop.yml` file, striving for sensible defaults and leveraging the team\u2019s prior experience with the tool. We will aim for automatic correction of as many offenses as possible.\n","GenTime":"2024-08-28 12:42:42"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"text":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n\n##Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n","MatchedIds":[1067,4644,4695,2914,2181],"Prediction":"## Decision\n\nChosen option: **\"Serverless architecture with a Function-as-a-Service (FaaS) backend and managed database\"**, because it best fits the requirements of being lightweight, fully managed, and easily integrated.\n\n### Positive Consequences\n\n* **Lightweight**: Serverless inherently promotes minimal code and resource usage, as functions only execute when triggered. This aligns with the need for a lightweight system.\n* **Fully Managed**:  FaaS providers manage the underlying infrastructure, scaling, and server management. This frees developers to focus solely on business logic and integrations.\n* **Easy Integration**:  Serverless platforms often integrate seamlessly with other managed services like databases, APIs, and authentication systems, simplifying development and deployment.\n* **Cost-effective**: Pay-as-you-go pricing models in serverless environments mean costs are directly proportional to usage, beneficial for a system built on ADRs and evolving developer needs.\n\n### Negative Consequences\n\n* **Vendor Lock-in**: Choosing a specific FaaS provider can lead to dependence on their ecosystem and potential challenges when switching.\n* **Cold Starts**:  Initial function invocations might experience latency due to cold starts. However, this can be mitigated through optimization techniques. \n* **Debugging and Monitoring**:  Traditional debugging tools might not be directly applicable, requiring adaptation to serverless paradigms.\n\n### Follow-up Decisions Required\n\n* **Specific FaaS Provider**: Evaluate and select a provider (e.g., AWS Lambda, Google Cloud Functions, Azure Functions) based on cost, features, and existing infrastructure.\n* **Managed Database Selection**: Determine the appropriate managed database service (e.g., AWS DynamoDB, Google Cloud Firestore) based on data structure and access patterns.\n* **API Gateway Implementation**:  Define an API gateway strategy to handle routing, authentication, and rate limiting for the FaaS backend.\n* **Monitoring and Logging Setup**: Implement robust monitoring and logging solutions to track system health, performance, and potential issues within the serverless environment. \n","GenTime":"2024-08-28 12:42:53"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"text":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n\n\n##Decision\nFiles are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","MatchedIds":[4234,1341,3657,1339,3831],"Prediction":"## Decision\n\nWikispeech will adopt a dedicated caching service to store and retrieve synthesized audio and its metadata. This addresses the limitations of the previous implementation and fulfills the specified requirements:\n\n1. **Decoupled Storage:** The caching service will be responsible for storing the synthesized audio and its metadata. This allows flexibility in choosing the storage backend (e.g., Swift for Wikimedia, local disk for others) through configuration.\n\n2. **Stateless Speechoid Service:**  Removing the storage responsibility from Speechoid allows it to remain stateless, aligning with the desired architecture. \n\n3. **Metadata Storage:** The caching service will store utterance metadata alongside the audio, enabling efficient retrieval and reuse without re-synthesizing. \n\n4. **Content Privacy (RevisionDelete):** Only utterance metadata (e.g., hash of the text) will be stored, not the actual text content. This ensures compliance with RevisionDelete policies and prevents unauthorized access to potentially sensitive information.\n\n5. **Cache Management:** The caching service will implement mechanisms for:\n    - **Time-based Expiry:**  Flushing utterances after a configurable duration.\n    - **Targeted Invalidation:** Instantly purging audio associated with modified segments or outdated lexicon entries.\n\n6. **Utterance Reusability:** The caching service will utilize a content-addressable storage approach (e.g., based on a hash of the utterance text). This facilitates audio reuse across revisions, pages, and even different sites, as identical text will map to the same cached audio.\n\nThis approach balances the need for efficient audio storage and retrieval with the flexibility, security, and maintainability requirements of the Wikispeech system. \n","GenTime":"2024-08-28 12:43:02"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"text":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n\n##Decision\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","MatchedIds":[4477,78,4424,2499,4436],"Prediction":"## Decision\n\n1. **Versioning:** Event transformers will be versioned using semantic versioning (SemVer) and embedded within the generated xAPI and Caliper events.\n\n2. **Version Format:** The version will follow the pattern: `event-routing-backends@vX.Y.Z`, where `X.Y.Z` represents the semantic version of the `event-routing-backends` package used for the transformation.\n\n3. **Version Placement:** \n    - **xAPI:** The transformer version will be included within the `context` object of the xAPI statement, under the `extensions` key using the URL `https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version`. \n      ```json\n      \"context\": {\n        \"extensions\": {\n          \"https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version\": \"event-routing-backends@v1.2.3\"\n        }\n      }\n      ```\n    - **Caliper:**  The version will be placed within the `extensions` object of the Caliper event under the key `transformerVersion`. \n      ```json\n      \"extensions\": {\n        \"transformerVersion\": \"event-routing-backends@v1.2.3\"\n      }\n      ``` \n\n4. **Version Increment Rules:**\n\n    * **Major version (X):** Incremented when:\n        * The transformer changes due to updates in the xAPI or Caliper specifications.\n        * A key is removed or renamed within an existing transformer.\n        * The value of a key is modified in an existing transformer, leading to a breaking change.\n    * **Minor version (Y):** Incremented when:\n        * A new key is added to an existing transformer.\n        * A new event type is added for transformation.\n    * **Patch version (Z):** Incremented when a bug within a transformer is fixed.\n\n5. **Changelog Maintenance:** Separate changelogs will be maintained for both xAPI and Caliper event transformations. These changelogs will document all modifications made to the transformers, including version updates and the rationale behind them.\n\n## Benefits\n\n* **Transparency and Traceability:** Embedding the transformer version directly within the xAPI and Caliper events provides a clear and readily accessible record of the transformation process. \n* **Debugging and Support:**  Versioning facilitates easier debugging by allowing developers to pinpoint the specific transformer version used for an event, aiding in issue identification and resolution.\n* **Consumer Compatibility:**  Clear versioning practices allow consumers of xAPI and Caliper events to adapt to changes in the transformed data structure or content, ensuring compatibility and preventing integration issues. \n* **Maintainability:**  Maintaining separate changelogs for each specification enhances maintainability by providing a structured history of changes, simplifying future updates and bug fixes. \n","GenTime":"2024-08-28 12:43:17"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"text":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n\n##Decision\n### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","MatchedIds":[2687,1067,1302,1588,3738],"Prediction":"## Decision\n\n\"Setup\" actions will register problem matchers.\n\nFor example:\n\n- `setup-nodejs` would register the `tsc` and `eslint` problem matchers. \n- `setup-python` would register the `flake8` and `pylint` problem matchers.\n\nThis approach offers several benefits:\n\n- **Improved User Experience:** Provides more informative and actionable error messages during CI builds. \n- **Familiarity:** Leverages the existing problem matcher model from VSCode, making it easier for users to adapt.\n- **Extensibility:** Allows for adding support for other languages and tools in the future.\n\nBy registering problem matchers within \"setup\" actions, we ensure that the appropriate matchers are automatically available for subsequent steps in the CI workflow.\n","GenTime":"2024-08-28 12:43:22"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"text":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","MatchedIds":[66,3853,4179,1258,1339],"Prediction":"## Decision\n\nSince both code samples utilize PHPUnit's API, and considering Sebastian Bergmann's recommendation in [this article][phpunit-exception-best-practices], we will favor the use of `expectException*()` methods for testing expected failures. This approach offers better readability and aligns with PHPUnit's recommended practices.\n\nWhile a large-scale refactor to replace existing `try-catch` blocks with `expectException*()` is not a priority, new code contributions should adopt this practice. \n\n### Status\n\nAccepted ([#1090][1090])\n\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061 \n","GenTime":"2024-08-28 12:43:28"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"text":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\n\n##Decision\nWe have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","MatchedIds":[631,3147,621,4424,714],"Prediction":"## Decision\n\nWe will implement a new, consistent naming convention for all service plans. This convention will:\n\n1. **Be concise and descriptive:** Clearly indicate the plan's purpose and key characteristics without extraneous information.\n2. **Use a hierarchical structure:**  Organize plans logically, potentially using categories and tiers. For example, \"compute-standard-small\" instead of \"Legacy Production Plan - Region A - Small Instance Type\".\n3. **Versioning (optional):** If deemed necessary, integrate versioning into the naming convention in a clear and unambiguous way (e.g., \"compute-v2-standard-small\"). This will depend on the anticipated frequency of introducing breaking changes to service plans. \n4. **Be documented and discoverable:** Provide clear documentation explaining the naming convention, its components, and how to interpret plan names.\n\n**Migration Plan:**\n\n* **New services:** All new services will adopt the new naming convention immediately.\n* **Existing services:** We will migrate existing service plans to the new naming convention gradually:\n    * **Option 1: Aliasing:**  Introduce aliases using the new naming convention for existing plans. This allows for a smoother transition for existing tenants. Deprecated plan names can be phased out over time.\n    * **Option 2: Direct Migration:** Define a migration window and communicate the change clearly to tenants. Provide tooling or scripts to assist with the migration, if necessary. This option offers a cleaner break but requires more coordination.\n\n**Tooling:**\n\n* Update any internal tools and dashboards to reflect the new naming convention.\n\n**Communication:**\n\n* Communicate the new naming convention and migration plan clearly to all stakeholders, including platform operators and tenants.\n\nThis approach will improve the usability and maintainability of service plans, reducing confusion and streamlining operations as the platform evolves. \n","GenTime":"2024-08-28 12:43:37"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"text":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n\n\n##Decision\nHost all environments excpet local development on BEIS own GOVUK PaaS account.\\n","MatchedIds":[4041,2093,1767,4056,631],"Prediction":"## Decision\n\nWe will be using the GOV.UK PaaS as the hosting platform for the service. This is in line with GDS policy for hosting government services and meets the service team's requirements. \n","GenTime":"2024-08-28 12:43:39"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"text":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n\n\n##Decision\nWith static pages the above can be achieved. Easily.\\n","MatchedIds":[1599,4940,1240,4271,3721],"Prediction":"## Decision\n\nTo achieve the goals of simplicity, minimal dependencies, high performance, accessibility, and ease of deployment, I will build this website using **plain HTML, CSS, and possibly a minimal amount of vanilla JavaScript**. \n\nThis approach allows for:\n\n* **Complete control over the codebase:**  Avoiding unnecessary dependencies reduces bloat and potential security vulnerabilities while increasing maintainability.\n* **Optimal performance:**  A smaller footprint translates to faster loading times, especially important for users on slower connections or devices.\n* **Improved understanding:**  Writing functionalities from scratch offers deeper insights into web development fundamentals and potential optimizations.\n* **Seamless deployment via GitHub Pages:**  Static HTML, CSS, and JavaScript files are directly deployable to GitHub Pages, simplifying the hosting process. \n\nWhile this means forgoing the convenience of frameworks, the trade-off is a faster, leaner, and more maintainable website that aligns perfectly with the project's core values.\n","GenTime":"2024-08-28 12:43:45"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"text":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n\n\n##Decision\nWe will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","MatchedIds":[4937,2409,3144,1464,2752],"Prediction":"## Decision\nWe will use Terraform to manage the infrastructure for the Data Submission Service. This offers a number of advantages:\n- **Cloud Agnostic:** Terraform allows us to define our infrastructure once and deploy it to multiple cloud providers if needed, avoiding vendor lock-in.\n- **Infrastructure as Code:** Terraform treats infrastructure as code, enabling us to version control our infrastructure changes, easily roll back to previous versions, and manage it alongside our application code. \n- **Simplified Management:** Terraform automates the provisioning and management of infrastructure resources, reducing manual effort and minimizing the risk of errors.\n- **Widely Adopted:** Terraform is a popular and well-supported tool with a large, active community. This means there are ample resources available for learning and troubleshooting. \n","GenTime":"2024-08-28 12:43:58"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"text":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n\n##Decision\nOur structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","MatchedIds":[1180,693,4644,3069,2183],"Prediction":"## Decision\n\nChosen option: **OpenAPI 3.0 (formerly known as Swagger)**\n\n### Reasons\n\n* **Industry Standard:** OpenAPI is a widely adopted standard for defining RESTful APIs in a machine-readable format. This makes it easy for developers and tools to understand and interact with the API.\n* **Human Readable:**  While machine-readable, OpenAPI specifications are also designed to be easily understood by humans. This is crucial for collaboration and documentation purposes. \n* **Tooling and Ecosystem:** A rich ecosystem of tools exists around OpenAPI for tasks like generating documentation, client SDKs, server stubs, and even for automated testing. \n* **Contract-Driven Development:**  Using OpenAPI allows for a contract-first approach to API development. This means defining the API contract upfront and using it as a guide for both frontend and backend development, improving consistency and reducing integration issues.\n\n### Positive Consequences\n\n* **Clear API Contract:**  Provides a single source of truth for the API definition, improving communication between teams and stakeholders.\n* **Faster Development Cycles:**  Auto-generation of documentation, client code, and server stubs reduces development time.\n* **Improved API Quality:**  Standardized specifications lead to more consistent and well-defined APIs.\n* **Easier Integration:**  Clients can be easily built in various programming languages thanks to the availability of client generators.\n\n### Negative Consequences\n\n* **Learning Curve:** Some initial effort is required to learn the OpenAPI specification and associated tooling.\n* **Maintenance Overhead:**  Changes to the API require updating the OpenAPI specification, which might add some overhead. However, the benefits often outweigh this cost. \n","GenTime":"2024-08-28 12:44:11"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"text":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n\n\n##Decision\nLaravel 7\\n","MatchedIds":[4271,1477,1243,1240,3718],"Prediction":"## Decision\nWe will use the **Laravel** framework for this project. Laravel provides an elegant syntax, a robust ecosystem of tools and packages, and a large and active community. This will allow us to leverage pre-built components, streamline development, and ensure maintainability in the long run. \n","GenTime":"2024-08-28 12:44:14"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"text":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n\n##Decision\nWe use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","MatchedIds":[2181,4644,693,3159,4021],"Prediction":"## Decision\nWe will describe our APIs using the OpenAPI 3.0 specification. \n\n### Positive Consequences\n* **Standardized Structure**:  OpenAPI provides a widely recognized and standardized format for defining APIs. This ensures consistency, understandability, and interoperability.\n* **Tooling and Ecosystem**:  Numerous tools support OpenAPI, including documentation generators (e.g., Swagger UI), client code generators, and API mocking tools, which can accelerate development and improve productivity.\n* **Collaboration and Communication**:  Having a clear, machine-readable API specification facilitates communication and collaboration between frontend and backend teams, or even across different teams working on separate services.\n\n### Negative Consequences\n* **Learning Curve**: There might be a slight learning curve for team members unfamiliar with OpenAPI. However, the specification is well-documented, and ample resources are available to aid in the learning process.\n* **Maintenance Overhead**:  Maintaining the OpenAPI specification will require some effort as the API evolves. However, this effort is outweighed by the long-term benefits of having a clear and up-to-date specification.\n\n### Implementation Details (Follow-up Decisions)\n\n* **Version Control**:  We will version our OpenAPI specification using Semantic Versioning (SemVer) to manage changes effectively.\n* **Tooling**:  We will choose appropriate tools for API documentation (e.g., Swagger UI), code generation, and potentially API mocking based on our specific needs.\n* **Integration with Development Workflow**:  We will integrate the OpenAPI specification into our development workflow, ensuring that changes to the API are reflected in the specification and vice versa.\n","GenTime":"2024-08-28 12:44:22"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"text":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n\n\n##Decision\nWe will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","MatchedIds":[1255,485,3571,2191,1745],"Prediction":"## Decision\n\nWe will implement a caching mechanism to store and retrieve EasyVista object names based on their GUIDs. \n\nHere's a breakdown:\n\n1. **Caching Mechanism:**\n    * We'll use a local cache (e.g., a dictionary-like structure) within the cmdlet's runtime. \n    * Upon the first request for a specific GUID, the cmdlet will query the EasyVista API to retrieve its friendly name. \n    * The GUID and friendly name pair will be stored in the cache.\n    * Subsequent requests for the same GUID will retrieve the name from the cache, avoiding unnecessary API calls.\n\n2. **Autocomplete Integration:**\n    * The autocomplete functionality will leverage this cache. \n    * As the user types a parameter value, the cmdlet will:\n        * Check if the input is a potential GUID. \n        * If so, it will query the cache for matching friendly names.\n        * The autocomplete suggestions will display these friendly names, making it easier for users to select the correct option.\n\n3. **Cache Refresh:** \n    * To handle potential changes in EasyVista (e.g., a friendly name update), we'll implement a cache refresh mechanism. This could be time-based (e.g., refresh every hour) or event-driven (e.g., refresh when a specific cmdlet or parameter is used).\n\n**Benefits:**\n\n* **Improved User Experience:**  Autocomplete with friendly names will significantly enhance the usability of the cmdlets.\n* **Reduced API Calls:** Caching will minimize API requests, improving performance and reducing load on the EasyVista system. \n* **Flexibility:** This approach does not rely on environment variables and can easily adapt to various cmdlet parameters and EasyVista objects. \n\n**Considerations:**\n\n* **Cache Size:**  We need to consider the potential size of the cache and implement appropriate limits or eviction strategies if necessary.\n* **Cache Invalidation:** The chosen cache refresh mechanism should effectively handle changes in EasyVista data to ensure accuracy.\n","GenTime":"2024-08-28 12:44:32"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"text":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n\n\n##Decision\nWe'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","MatchedIds":[4454,581,4436,4179,1258],"Prediction":"## Decision\n\nWe will develop automated integration tests that exercise the functionality of our service provider against the Verify compliance tool. \n\nThese tests will:\n\n* **Validate our implementation:** Ensure our service provider correctly implements the SAML profile as defined by Verify.\n* **Act as examples:** Provide clear, working examples for relying parties to understand how to interact with and test against the compliance tool. This will simplify the onboarding process for new relying parties.\n* **Highlight usability issues:**  By interacting with the compliance tool programmatically, we can identify potential areas for improvement in its user interface and reporting mechanisms. This feedback can be relayed to the Verify team to enhance the overall developer experience. \n\nThe tests will be integrated into our existing CI\/CD pipeline to ensure ongoing compliance and catch regressions early. We will use a tool like `RSpec` or a similar testing framework to implement these integration tests. \n","GenTime":"2024-08-28 12:44:37"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"text":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n\n\n##Decision\nWe have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","MatchedIds":[4271,3062,4359,1243,1035],"Prediction":"## Decision\n\nWhile leveraging a framework can be beneficial, the specific choice is highly dependent on the project's needs, the team's familiarity with various technologies, and the overall architectural vision. \n\nWe will conduct a comparative analysis of potential frameworks considering factors such as:\n\n* **Language Support:**  Alignment with the chosen programming language for the project.\n* **Project Requirements:** Assessing whether the framework's strengths align with the project's core functionalities (e.g., web application, data processing, machine learning).\n* **Community & Support:** Evaluating the framework's maturity, community size, and available documentation\/support resources.\n* **Learning Curve:** Considering the team's existing expertise and the effort required to become proficient with the framework. \n* **Performance & Scalability:**  Analyzing benchmarks and the framework's ability to scale with future growth.\n\nBased on this evaluation, we will select the framework that provides the optimal balance of productivity, maintainability, and alignment with our project goals. \n","GenTime":"2024-08-28 12:44:44"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"text":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n\n##Decision\n- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","MatchedIds":[2062,4937,328,3144,771],"Prediction":"## Decision\nWe will use Terraform to manage the Cloud Foundry spaces, services, and deployments, leveraging the available Terraform Cloud Foundry provider. This will allow us to manage our infrastructure as code, ensuring reproducibility and maintainability, and removing the reliance on custom shell scripts. \n","GenTime":"2024-08-28 12:44:46"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"text":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\n\n##Decision\nBased on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","MatchedIds":[33,816,839,3692,827],"Prediction":"## Decision\n\nTo foster a thriving ecosystem of composable Cosmos SDK modules and address the security concerns stemming from the lack of proper object-capability model adherence, we propose the following:\n\n**1. Prioritize and Stabilize Module Interfaces:**\n\n* **Define a clear path towards Cosmos SDK v1.0:** This involves finalizing core module interfaces and ensuring backward compatibility for a significant period.  \n* **Introduce stricter versioning and deprecation policies:** Communicate breaking changes well in advance and provide clear migration paths for module developers. \n* **Invest in robust testing and formal verification:**  This helps detect and prevent accidental breaking changes to module interfaces.\n\n**2. Implement a Fine-grained Object-Capability Model:**\n\n* **Introduce capability objects:**  Replace the simplistic string-based permissions with unique, unforgeable capability objects. These objects represent specific permissions, such as the ability to mint a certain type of token or access a particular subset of bank functionality.\n* **Enforce capability-based access control:** Refactor module keepers to strictly enforce access control based on the possession of relevant capability objects.  For example,  the `x\/bank` keeper's `SetBalance` method would require a specific capability object, effectively limiting its use to authorized modules.\n* **Manage capabilities securely:** Define mechanisms for issuing, granting, revoking, and verifying capability objects.  This could involve using a dedicated capability management module or integrating it into the existing authz module.\n\n**3.  Redesign `x\/bank` for OCAP Compliance:**\n\n* **Decouple privileged operations:**  Refactor `x\/bank` into smaller, more specialized modules with well-defined capabilities. For instance, a dedicated `x\/token` module could manage token issuance and burning, while `x\/bank` focuses on account management and transfers.\n* **Restrict `SetBalance` usage:**  Limit direct access to `SetBalance` and instead provide capability-gated methods for specific operations like minting, burning, and transferring tokens.\n* **Implement capability-based checks:**  Enforce capability checks within all `x\/bank` keeper methods to ensure that only authorized modules can perform privileged actions.\n\n**4.  Promote OCAP Best Practices:**\n\n* **Update documentation and tutorials:**  Provide clear guidance and examples on building and using OCAP-compliant modules.\n* **Develop tooling and libraries:**  Offer tools and libraries that simplify the process of working with capability objects and enforcing OCAP principles.\n* **Foster community collaboration:** Encourage the development and sharing of OCAP-compliant modules and best practices within the Cosmos SDK ecosystem.\n\n**Implementation Timeline:**\n\nImplementing a comprehensive object-capability model is a significant undertaking. Therefore, we recommend a phased approach:\n\n* **Phase 1 (short-term):** Stabilize core module interfaces and introduce stricter versioning practices.\n* **Phase 2 (mid-term):**  Design and implement the core capability object infrastructure and integrate it with the authz module.\n* **Phase 3 (long-term):** Refactor critical modules like `x\/bank` to be fully OCAP compliant.\n\n**Conclusion:**\n\nBy embracing a well-defined object-capability model and prioritizing module interface stability, we can foster a more secure and thriving ecosystem of composable modules for the Cosmos SDK, paving the way for broader adoption and innovation. \n","GenTime":"2024-08-28 12:45:03"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"text":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n\n##Decision\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","MatchedIds":[3129,2520,3657,2500,885],"Prediction":"## Decision\n\nWe will use `SqlStreamStore` for storing events in our system.\n\n### Reasons:\n\n* **Catch-up subscriptions:** SqlStreamStore supports catch-up subscriptions, which is crucial for our async event handler architecture. This outweighs the ease of debugging offered by our handwritten SQL solution in Wegwijs.\n* **MSSQL Integration:** SqlStreamStore integrates with MSSQL, addressing our concerns about storing business-critical data outside our existing infrastructure and aligning with AIV's preferences.\n* **Reduced Operational Burden:** Opting for SqlStreamStore eliminates the need to host and manage GetEventStore, simplifying our infrastructure and reducing operational overhead.\n* **Active Community & Proven Track Record:** The active community and reported production usage of SqlStreamStore provide confidence in its stability and reliability. \n\n### Potential Drawbacks:\n\n* **Learning Curve:**  Migrating from handwritten SQL to SqlStreamStore will require familiarization with the library.\n* **Potential Vendor Lock-in:** Utilizing an OSS library might lead to vendor lock-in. However, the active community and open-source nature mitigate this risk compared to a proprietary solution. \n\n### Mitigation Strategies:\n\n* **Allocate time for learning and training on SqlStreamStore.**\n* **Evaluate alternative solutions periodically** to ensure SqlStreamStore remains the best fit for our needs and avoid long-term vendor lock-in. \n","GenTime":"2024-08-28 12:45:11"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"text":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n\n##Decision\nWe will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","MatchedIds":[3681,3722,1576,276,4370],"Prediction":"## Decision\nWe will use a task runner to automate the setup and running of the prototype kit.  Gulp is a well-established and robust tool within the Javascript ecosystem that can handle a wide range of tasks. Using Gulp will allow us to define all the necessary steps in a repeatable and organized manner. This will streamline the development process, making it easier for users to get the prototype kit up and running quickly, regardless of their experience level. \n","GenTime":"2024-08-28 12:45:15"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"text":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n\n\n##Decision\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","MatchedIds":[362,4644,1758,2502,429],"Prediction":"## Decision\n\n**Chosen option:** Adopt a Node.js framework such as NestJS or Express.js with a set of consistent architectural guidelines. \n\n**Reasoning:**\n\nWhile both frameworks offer robust features for building well-structured applications, NestJS, with its opinionated architecture inspired by Angular, provides a more streamlined and standardized development experience out-of-the-box. This aligns well with the goal of creating a consistent code architecture across FxA backend services.\n\n**Positive Consequences:**\n\n- **Documented application architecture:** Both frameworks promote good architectural practices. NestJS in particular, encourages a modular structure with clear separation of concerns (Controllers, Services, Modules), making the application easier to understand, maintain, and evolve. Documentation for these frameworks is readily available and can be supplemented with FxA-specific guidelines.\n- **Tooling and reduced boilerplate:** Both frameworks come with CLI tools that automate common tasks and reduce boilerplate code. This allows developers to focus on business logic rather than writing repetitive setup code.\n- **Modern paradigms and DI:** Both frameworks support dependency injection, making testing easier and promoting code reusability. \n- **Training materials:**  The popularity of these frameworks ensures a wealth of online resources, tutorials, and courses that can be leveraged for training new developers. \n- **Handles required situations:**\n    - **Exception handling:** Both frameworks offer middleware and other mechanisms for global exception handling. \n    - **Validation:**  Libraries like class-validator can be easily integrated to enforce data validation.\n    - **Service instantiation (DI):**  Both frameworks have built-in DI systems.\n    - **Authentication:** Existing authentication libraries (Passport.js, for example) can be easily integrated into both frameworks.\n    - **RESTful APIs & OpenAPI:** Both frameworks have excellent support for building RESTful APIs. Libraries like `swagger-ui-express` or `@nestjs\/swagger` can be used to automatically generate OpenAPI documentation.\n    - **GraphQL:** While NestJS has a dedicated GraphQL module, both frameworks can be integrated with GraphQL servers.\n    - **Use of components in scripts:**  The modular structure of both frameworks allows for easy importing and utilization of components\/services within scripts.\n\n**Migration strategy:**\n\nWhile a big-bang migration of legacy applications might not be feasible, a gradual approach can be adopted:\n\n1. **Start with new services:** Implement the chosen framework and architectural guidelines in the four new FxA back-end services.\n2. **Incremental migration:**  Identify key components or modules from legacy applications that can be extracted and refactored to align with the new architecture.\n3. **Documentation and training:** Document the new architecture and best practices, and provide training to existing developers to facilitate a smooth transition.\n\n**Negative Consequences:**\n\n- **Learning curve:** While documentation and training can mitigate this, there will inevitably be a learning curve associated with adopting a new framework.\n- **Potential for over-engineering:**  It's important to strike a balance and avoid over-engineering solutions for simple problems.\n\n**Conclusion:**\n\nAdopting a well-established Node.js framework with a clear architectural vision will address the current challenges faced by the FxA backend services. This decision prioritizes maintainability, developer productivity, and scalability in the long run. \n","GenTime":"2024-08-28 12:45:31"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"text":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n\n##Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","MatchedIds":[2498,2502,4271,2307,641],"Prediction":"## Decision\n\nWe will write our own validation logic and implement it at the **use case layer**. \n\n**Justification:**\n\n* **Avoiding Library Coupling:** While established libraries offer powerful features, our validation needs are currently simple. Writing our own logic reduces external dependencies and avoids binding our domain layer to a specific library. This promotes flexibility and simplifies future changes.\n\n* **Simplicity and Maintainability:** The validation requirements are currently straightforward (mainly required fields). Introducing a library for such basic needs would add unnecessary complexity. Writing our own logic keeps the codebase leaner and easier to understand for the team.\n\n* **Control and Flexibility:** Implementing validation within the use case layer provides a clear separation of concerns. It allows us to tailor validation rules specific to each use case without cluttering the domain model or presentation layer. This approach also allows for easier evolution of validation logic as the application grows.\n\n* **Learning Curve:** While using a library might seem quicker initially, writing custom validation logic will deepen the team\u2019s understanding of validation principles and their application within the specific domain.\n\n**Future Considerations:**\n\nAs the application evolves and validation needs become more complex, we will re-evaluate the decision to use an external library.  At that point, the trade-off between custom code and a feature-rich library might shift. We will prioritize libraries with strong community support and a proven track record of stability.\n","GenTime":"2024-08-28 12:45:43"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"text":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n\n\n##Decision\n### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","MatchedIds":[1962,13,4623,2256,4099],"Prediction":"## Decision\n\n### Key Derivation Standard\n\nThe Oasis Network **should adopt BIP-32** (Hierarchical Deterministic Wallets) as the standard for key derivation. This standard is widely used and well-supported in the blockchain ecosystem.\n\n**Rationale:**\n\n- **Interoperability:** Using BIP-32 ensures compatibility with a wide range of existing wallets and tools, enabling users to manage their Oasis accounts seamlessly.\n- **Flexibility:** BIP-32 allows for generating multiple keys from a single seed phrase, enabling features like multi-signature wallets and account recovery.\n- **Security:**  BIP-32, when combined with best practices, promotes secure key management, especially for cold storage.\n- **Familiarity:**  The use of standard 24-word mnemonics aligns with common practices in the ecosystem, improving user experience and reducing confusion.\n\n### Specific Considerations\n\n* **Derivation Path:**  The Oasis Network should define a specific derivation path (BIP-44 or BIP-43) to ensure that keys are generated consistently across different applications and prevent potential conflicts.\n* **Key Types:** The specific elliptic curve used for key pairs (e.g., ed25519 or secp256k1) needs to be clearly specified and aligned with the network's requirements. \n* **Address Derivation:**  The process of deriving the `oasis1...` address from the public key should remain consistent. The chosen key derivation standard should not impact the existing address format.\n\n### Implementation\n\n- **Libraries:** Encourage the use of established libraries that implement BIP-32, such as those available for various programming languages (e.g., `bip32` for Python, `hdkey` for JavaScript).\n- **Documentation:** Provide clear and comprehensive documentation for developers outlining the chosen standard, derivation path, and any Oasis-specific implementation details.\n\n### Future Considerations\n\n- **Hardware Wallet Support:**  Work with hardware wallet manufacturers to ensure native support for Oasis accounts derived using the chosen standard.\n- **Multi-Coin Wallets:** Explore integration with popular multi-coin wallet applications that already support BIP-32 to further enhance user convenience and adoption.\n\nBy adopting a widely recognized standard like BIP-32, the Oasis Network can benefit from improved interoperability, security, and user experience, fostering a more robust and accessible ecosystem. \n","GenTime":"2024-08-28 12:45:58"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"text":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n\n\n##Decision\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","MatchedIds":[302,2502,3488,929,429],"Prediction":"## Decision\n\n**Approved**. This structure provides a consistent and comprehensive approach to documenting architectural principles, directly linking them to business strategy. This will improve communication, design consistency, and provide a valuable resource for both current and future development efforts. \n","GenTime":"2024-08-28 12:46:01"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"text":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n\n##Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","MatchedIds":[4061,317,207,1462,699],"Prediction":"## Decision\n\nGiven the limitations of RDS and the complexities of PostgreSQL's permission model, we will implement a shared user approach for managing tenant database access:\n\n* **Shared Application User:** A single database user will be created per RDS instance. This user will be granted full ownership and privileges on all tables and schemas within the database.\n* **Application Responsibility:** Each application binding to the RDS instance will receive the credentials for this shared user.  It becomes the responsibility of each application to manage schema and table modifications in a way that ensures compatibility and avoids conflicts with other bound applications.\n* **Migration Tooling and Coordination:** To mitigate the risks associated with a shared user, we will:\n    * Strongly encourage the use of a robust migration tool like Flyway or Liquibase. This will enforce order and consistency in database schema changes.\n    * Provide clear documentation and guidelines for developers to follow when making database changes in a multi-application environment. This should include recommendations for schema naming conventions and communication strategies to coordinate migrations. \n\n### Justification\n\nThis approach strikes a balance between security, usability, and maintainability:\n\n* **Security:** While a shared user might seem less secure, the alternative (granting excessive permissions or relying on unsupported features) poses a greater risk.  \n* **Usability:** This approach is transparent to the tenant, simplifying the binding and unbinding process.\n* **Maintainability:**  By clearly defining responsibilities and providing the right tools, we can empower developers to manage the shared database effectively.\n\n### Considerations\n\n* **Communication is Key:** Clear communication and coordination between development teams are essential to prevent conflicts and ensure smooth database migrations.\n* **Monitoring and Logging:** Implement robust monitoring and logging to track database changes and quickly identify any issues.\n* **Future Exploration:**  Continue to investigate alternative solutions and potential improvements in RDS capabilities that might allow for more granular permission management in the future. \n","GenTime":"2024-08-28 12:46:12"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"text":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n\n##Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","MatchedIds":[2896,2145,2897,2880,3831],"Prediction":"## Decision\nGiven the inefficiency of the `exist` method for ObjectStorage, we will introduce a probabilistic approach to reduce duplicate writes while avoiding the overhead of a full check for every blob:\n\n**Implement a Bloom filter-based mechanism in front of the ObjectStorage blob store.**\n\n* **How it works:**\n    * A Bloom filter will be maintained in-memory within the James application. \n    * Before writing a blob to ObjectStorage, its hash will be checked against the Bloom filter. \n    * If the hash is present, indicating the blob *might* already exist, we skip writing to ObjectStorage.\n    * If the hash is not present, indicating the blob is likely new, we write it to ObjectStorage and add its hash to the Bloom filter.\n* **Benefits:**\n    * **Reduced ObjectStorage writes:** By avoiding writing potentially duplicate blobs, we minimize storage costs and latency associated with these operations.\n    * **Minimal performance impact:** Bloom filters offer fast lookups and low memory footprint. \n    * **Tunable accuracy:** The size of the Bloom filter can be adjusted to control the false positive rate (i.e., cases where a blob is considered existing when it isn't). A larger filter offers higher accuracy but consumes more memory.\n* **Considerations:**\n    * **False Positives:** The Bloom filter can result in false positives, leading to some potential duplicates remaining unhandled. However, this trade-off is acceptable given the performance and cost benefits.\n    * **Bloom Filter Management:** Implement a strategy to manage the Bloom filter, including its size, eviction policy, and persistence during restarts. \n\nThis approach balances the need to reduce duplicate writes with the performance and cost constraints of ObjectStorage, ultimately improving efficiency and reducing operational expenses. \n","GenTime":"2024-08-28 12:46:22"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"text":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n\n##Decision\nStart with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","MatchedIds":[36,276,1631,4609,1117],"Prediction":"## Decision\n\nChosen option: **Split components into individual repositories.**\n\n## Decision Drivers\n\n- **Scalability and Maintainability:** Separate repositories for individual components enhance code organization, maintainability, and scalability as the platform grows.\n- **Independent Development and Deployment:** Decoupling components allows teams to work independently on specific functionalities, enabling faster development cycles and independent deployments.\n- **Clear Ownership and Responsibility:** Individual repositories promote clear ownership and responsibility for each component, simplifying maintenance and issue resolution.\n- **Reusability and Collaboration:** Isolating components in separate repositories increases their potential for reuse across different projects and facilitates collaboration with external contributors.\n\n## Positive Consequences\n\n- Improved code organization and maintainability.\n- Faster development cycles and independent deployments.\n- Clearer ownership and responsibility for components.\n- Enhanced reusability and potential for external contributions.\n\n## Negative Consequences\n\n- Increased management overhead for multiple repositories.\n- Potential for code duplication if not managed properly.\n- Requires robust tooling and processes for managing inter-repository dependencies.\n\n## Caveats\n\n- Implement a robust monorepo-like tooling or strategy to manage dependencies and ensure consistent code style across repositories.\n- Establish clear communication channels and processes to facilitate collaboration between teams working on different components.\n- Carefully consider component boundaries to minimize code duplication and ensure a cohesive platform architecture. \n","GenTime":"2024-08-28 12:46:40"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"text":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n\n\n##Decision\nUsing a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","MatchedIds":[2041,2405,5024,577,4443],"Prediction":"## Decision\n\nWe will use the Google Login API for the initial login implementation for Sprint 1. This approach offers a rapid solution for meeting the user story deadline and provides a tested framework we can reference when implementing our own database-backed authentication system. \n\nWe acknowledge this introduces a dependency on Google Login and may not be suitable as a permanent solution. We will prioritize replacing Google Login with a more robust, internally-managed authentication system in a future sprint. This will give us greater control over user data and security in the long term.\n","GenTime":"2024-08-28 12:46:43"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"text":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n\n\n##Decision\nWe will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","MatchedIds":[4974,142,2968,3440,1575],"Prediction":"## Decision\n\nChosen option: **NetworkManager**.\n\n### Reasons\n\n* **Feature Rich:** NetworkManager offers more advanced features compared to SystemD's native tools, especially beneficial for complex network setups.\n* **DBus Interface:** The DBus interface enables easy integration and control of NetworkManager from various applications and scripts.\n* **Widely Used:** NetworkManager enjoys broader adoption and community support compared to Wicd or other alternatives. This translates to better documentation, troubleshooting resources, and stability.\n\n### Positive Consequences \n\n* Increased flexibility and control over network configuration.\n* Facilitates dynamic network management.\n* Easier integration with existing tools and services due to DBus support.\n\n### Negative Consequences\n\n* Slightly larger footprint compared to SystemD's native tools.\n* Potential for conflicts if other applications directly manage network settings.\n","GenTime":"2024-08-28 12:46:49"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"text":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n\n\n##Decision\n### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","MatchedIds":[3488,4025,2520,4096,3109],"Prediction":"## Decision\n\nWe will implement a backup solution for super eks to cover resources not managed by IaC. \n\n### Solution Choice\n\nWe will use AWS Backup to back up the EBS volumes attached to the EKS control plane nodes. This choice leverages native AWS services for a robust and integrated solution.\n\n### Justification\n\n* **Integration:** AWS Backup is tightly integrated with EKS and EBS, simplifying management and automation.\n* **Security:** Backups are stored securely in S3, adhering to our security best practices. \n* **Cost-effectiveness:** We can leverage lifecycle policies to move backups to cheaper storage tiers over time. \n* **Ease of Use:** AWS Backup offers a centralized console and APIs for simplified backup and restore operations. \n\n### Scope\n\nThe following resources will be included in the backup strategy:\n\n* EBS volumes attached to EKS control plane nodes.\n\n### Implementation Details\n\n* **Schedule:** Daily backups will be scheduled using AWS Backup. \n* **Retention:** Backups will be retained for 30 days. \n* **Monitoring:**  AWS Backup's monitoring capabilities will be used to track backup status and receive alerts on failures.\n\n### Negative Consequences\n\n* **Cost:** Implementing AWS Backup will incur additional storage costs. However, this is mitigated by configuring appropriate lifecycle policies.\n* **Complexity:** While AWS Backup simplifies backup management, there's still a learning curve associated with configuring and managing backups.\n\n### Next Steps:\n\n1. **Evaluation:** Conduct a cost analysis and evaluate AWS Backup features in detail.\n2. **Implementation:** Configure AWS Backup to back up the specified resources.\n3. **Documentation:** Document the backup strategy, including procedures for backup and restoration. \n4. **Training:** Train relevant personnel on AWS Backup procedures. \n","GenTime":"2024-08-28 12:46:58"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"text":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n\n\n##Decision\n- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","MatchedIds":[3657,2183,2824,4539,359],"Prediction":"## Decision\n\nWe will implement multitenancy using a **single database with a discriminator column**. This approach offers the best balance between simplicity, maintainability, and data isolation, addressing the given constraints and concerns.\n\n**Rationale:**\n\n* **Data Isolation:** Adding a discriminator column (e.g., `tenant_id`) to each table ensures strict data separation. Queries and operations will always include this column, preventing accidental data leakage between organizations.\n* **Ease of Maintenance:**  A single database simplifies schema management and upgrades. Common metadata can reside in shared tables, with organizations having the option to override specific values in dedicated tables linked by the `tenant_id`.\n* **Low Data Volume:** Given the low data volume per organization, a single database approach is efficient and avoids the overhead of managing multiple databases or schemas.\n* **Cost-Effective:**  Using a single database reduces resource consumption and simplifies administration compared to managing multiple databases or schemas. \n\n**Implementation Details:**\n\n1. **Discriminator Column:**  Add a `tenant_id` column to all relevant tables.\n2. **Global Metadata:** Store common metadata in shared tables.\n3. **Tenant-Specific Overrides:** Allow organizations to override metadata using separate tables linked by the `tenant_id`.\n4. **Data Access Layer:** Enforce tenant isolation at the data access layer, ensuring all queries and operations include the `tenant_id`.\n5. **Application Logic:** Design application logic to transparently handle multitenancy using the `tenant_id`, ensuring data separation throughout the application.\n\nThis approach provides a secure, maintainable, and cost-effective solution for multitenancy in the openchs-server cloud deployment. \n","GenTime":"2024-08-28 12:47:09"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"text":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n\n\n##Decision\nRun a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","MatchedIds":[2582,2397,3214,577,4443],"Prediction":"## Decision\n\nWe will implement a proxy layer in front of a single GeoServer instance. This proxy will be responsible for authenticating users against our chosen authentication system and then managing GeoServer permissions based on user roles.  \n\nThis approach offers several benefits: \n\n* **Single GeoServer instance:** This simplifies deployment, management, and data publication. \n* **Fine-grained access control:** The proxy can enforce access control rules at the layer level, ensuring that only authenticated users with appropriate permissions can access restricted layers.\n* **Flexibility:** Decoupling authentication from GeoServer allows us to easily switch authentication mechanisms in the future without impacting GeoServer configuration.\n* **Improved user experience:**  Pre-authentication by the proxy eliminates the need for users to directly authenticate with GeoServer, providing a seamless experience.\n\nWe will investigate and select a proxy solution that integrates well with our technology stack and chosen authentication system. Potential solutions include:\n\n* **OAuth2 Proxy:** A lightweight and configurable option that supports a variety of authentication providers.\n* **Keycloak Gatekeeper:** Offers robust features and integration with the Keycloak Identity and Access Management platform.\n* **Custom Proxy Implementation:**  Building a custom proxy provides complete control but requires additional development effort.\n\nThe specific proxy implementation and integration details will be documented in a separate ADR after further investigation. \n","GenTime":"2024-08-28 12:47:16"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"text":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n\n\n##Decision\nAmended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","MatchedIds":[138,4699,4428,4408,3119],"Prediction":"## Decision\nWe will adopt the practice of `amending` Architecture Decision Records for minor changes and clarifications.  A new ADR will only be created when superseding an existing decision entirely. This approach will maintain a clear audit trail of architectural decisions while reducing unnecessary documentation overhead. \n","GenTime":"2024-08-28 12:47:19"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"text":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n\n\n##Decision\n* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","MatchedIds":[160,4308,1067,2616,2975],"Prediction":"## Decision\n\nChosen option: **Client-side rendering with MathJax**\n\n**Positive Consequences:**\n\n* **Fast rendering:**  MathJax is a JavaScript library that renders LaTeX client-side, meaning no server-side processing is required for quick display.\n* **Widely used and Supported:** MathJax is a standard for rendering math formulas online, ensuring compatibility across browsers.\n* **Interactive Exploration:** Users can right-click on rendered formulas to explore their LaTeX source, aiding learning and troubleshooting.\n\n**Negative consequences:**\n\n* **JavaScript dependency:**  The website needs to load and execute JavaScript, which might be blocked by some users or environments. \n* **Initial Rendering Flash:** A slight delay might be noticeable before formulas are rendered, especially for pages with many examples.\n\n**Alternatives considered:**\n\n* **Server-side rendering with LaTeX tools:** While accurate, this approach introduces server-side complexity and potential performance bottlenecks.\n\n**Reasoning:**\n\nClient-side rendering with MathJax offers the best trade-off between ease of implementation, performance, and interactivity for presenting LaTeX examples. It leverages a widely adopted solution and allows users to seamlessly explore both the source code and rendered output. While the JavaScript dependency might be a minor drawback, its benefits outweigh this concern in most cases. \n","GenTime":"2024-08-28 12:47:26"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"text":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n\n\n##Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","MatchedIds":[4442,4448,4454,581,78],"Prediction":"## Decision\n\nWe will send the verify-service-provider version number in an **encrypted SAML extension** within the SAML AuthnRequest.\n\n**Reasoning:**\n\n* **Security:** Encrypting the extension ensures the version number is not exposed in cleartext within the SAML messages. This addresses the security concern raised.\n* **Manageability:** Including the version number in the AuthnRequest allows us to track which version a relying party is using directly. This simplifies version management significantly.\n* **Existing Standards:**  Leveraging a SAML extension, even if encrypted, keeps the implementation aligned with existing SAML standards.\n\n**Alternatives Considered:**\n\n* **Custom User-Agent String:**  While simpler to implement, this method could be unreliable and easily missed. It also doesn't address the security concern.\n* **Unencrypted SAML Extension:**  This option lacks the necessary security to protect the version number information.\n\n**Implementation Details:**\n\n*  The encrypted extension will be added to the `Extensions` element of the `AuthnRequest`.\n*  The encryption will use the existing public key infrastructure established for communication between the verify-service-provider and the Verify Hub.\n*  The verify-service-provider will need to be updated to include logic for adding and encrypting the version number in the extension.\n*  The Verify Hub will need to be updated to decrypt and log the version number from the extension.\n\nThis approach balances the need for version tracking with security considerations while remaining aligned with established standards.\n","GenTime":"2024-08-28 12:47:35"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"text":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n\n\n##Decision\n**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","MatchedIds":[1240,1907,1151,3483,4271],"Prediction":"## Decision\n\nWe will use **Single SPA** to manage our micro-frontend composition. \n\nHere's why:\n\n- **Reduced Boilerplate:** Writing custom boilerplate code for micro-frontend orchestration can become complex and difficult to maintain. Single SPA provides a battle-tested framework, allowing us to focus on building our micro-frontends rather than reinventing the wheel.\n- **Established Conventions:**  Single SPA enforces a lifecycle for applications (bootstrap, mount, unmount). This brings structure and consistency to our micro-frontend integrations, making them more predictable and easier to reason about.\n- **Community & Support:**  Single SPA has an active community and good documentation, which is crucial for troubleshooting and staying up-to-date with best practices. \n- **Flexibility:** While we don't anticipate needing them immediately, Single SPA offers features like shared routing and application isolation that might become valuable as our micro-frontend architecture evolves.\n\nWhile building our own solution might offer slightly more control initially, the long-term benefits of using a dedicated framework like Single SPA outweigh the costs in terms of development speed, maintainability, and scalability. \n","GenTime":"2024-08-28 12:47:44"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"text":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n\n##Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","MatchedIds":[2410,2386,4875,1924,3469],"Prediction":"## Decision\nWe will stop using the `PublicActivity` gem and remove it from the project. A new, purpose-built mechanism for recording and displaying user edits will be developed. This new approach will be designed specifically to meet the needs of our application and its users, ensuring that the information recorded is relevant, useful, and easily accessible.\n","GenTime":"2024-08-28 12:47:48"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"text":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n\n##Decision\nEach driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","MatchedIds":[3023,1193,539,3203,4538],"Prediction":"## Decision\n\nDriver interfaces will be migrated to use namespaces for organization. Associated types and enums that were previously defined within structs will be moved into the respective driver interface's namespace. \n\nInstead of inheriting from structs like `embvm::tof`, driver interfaces will be defined directly within their namespace (e.g., `embvm::tof::TimeOfFlight`). This eliminates unnecessary inheritance and leverages the existing language feature of namespaces for better organization and clarity.\n\nThis change simplifies the inheritance hierarchy and improves code readability by using namespaces to logically group related functionality.\n","GenTime":"2024-08-28 12:47:51"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"text":"## Context\\nWe need to choose main programming language for this project.\\n\n\n##Decision\nI'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","MatchedIds":[4761,3715,1243,1240,861],"Prediction":"## Decision\n\nWe need more information to make an informed decision about the main programming language.  To choose effectively, please provide details about the following:\n\n* **Project goals and scope:** What kind of application are we building? What are its core functionalities? \n* **Team expertise:** What programming languages are the team members most comfortable with?\n* **Performance requirements:** Are there specific speed or resource constraints?\n* **Scalability:** How important is it for the application to scale efficiently?\n* **Ecosystem and libraries:** Are there specific libraries or frameworks essential for the project? \n\nOnce we have a clearer picture of these factors, we can evaluate the best programming language for the project.\n","GenTime":"2024-08-28 12:47:56"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"text":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n\n\n##Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","MatchedIds":[1307,1173,1310,1614,3572],"Prediction":"## Decision\nTo address nested key access and improve default value handling, we propose introducing a dedicated configuration module.\n\nThis module will:\n\n1. **Provide a function for accessing nested keys:** This function will accept a list of keys and safely traverse the nested configuration structure. If any key along the path is missing, it will raise an informative error, similar to `Application.fetch_env!\/2`.\n\n   ```elixir\n   # Example usage\n   config(:rig, :required_key, :required_subkey)\n\n   # In case of missing key:\n   # ** (ConfigurationError) Missing configuration key: [:rig, :required_key, :required_subkey]\n   ```\n\n2. **Centralize default values:**  Default values for all configuration options will be defined within this module, making them easily discoverable and maintainable. The function from point 1 will use these defaults if a key is not present in the actual configuration.\n\n   ```elixir\n   # Example within the configuration module\n   def default(:rig, :required_key, :required_subkey), do: \"default_value\" \n   ```\n\nThis approach offers several benefits:\n\n* **Improved developer experience:** Provides a consistent and safe way to access configuration values, eliminating potential errors from manual nesting.\n* **Enhanced maintainability:** Centralizing defaults makes it easy to find, update, and reason about the default behavior of the application. \n* **Improved testability:** The configuration module can be easily mocked and tested, facilitating different configuration scenarios in tests. \n\nWe will update all configuration accesses in the codebase to use this new module and deprecate direct usage of `Application.fetch_env\/2` for configuration retrieval.\n","GenTime":"2024-08-28 12:48:09"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"text":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n\n\n##Decision\nUse DynamoDB for persistent storage\\n","MatchedIds":[4041,3142,2410,4047,1480],"Prediction":"## Decision\n\nWe will use AWS RDS as the persistent storage solution for this application.\n\n* **Managed Service:** RDS is a managed service, aligning with the MoJ strategy of utilizing managed services. This reduces the operational overhead of managing database infrastructure.\n* **Security:** RDS provides robust security features, essential for sensitive data like user credentials and access control information. This includes encryption at rest and in transit, network access control, and integration with other AWS security services.\n* **Scalability and Availability:** RDS offers various instance sizes and configurations to meet the application's performance and availability requirements. It also supports features like automated backups, point-in-time recovery, and multi-availability zone deployments for high availability.\n\nWe will **not** store user credentials directly within the database. Instead, we will explore integration with a dedicated user management system or leverage AWS Cognito for secure authentication and authorization. \n","GenTime":"2024-08-28 12:48:15"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"text":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n\n\n##Decision\n- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","MatchedIds":[334,1854,2667,2221,1240],"Prediction":"## Decision\n\nGiven the context, we will adopt the following strategy for managing different types of state:\n\n**Local State:** Continue leveraging React's built-in `useState` Hook. This approach is sufficient for managing state local to a component and doesn't introduce unnecessary complexity.\n\n**Shared State:**  We will adopt a tiered approach based on the complexity and scale of the application:\n\n* **For simpler applications or features with minimal shared state:**  Utilize React Context along with optimization techniques like memoization to mitigate \"Provider Hell\" and unintentional re-renders.\n* **For larger applications with significant shared state and complex state management needs:** Consider `recoil` or `zustand`. These libraries offer a more structured and scalable approach to shared state management compared to React Context alone, with less boilerplate than Redux.\n\n**Remote State:**  We recommend using `@tanstack\/react-query` for managing server-side state. React Query excels in handling data fetching, caching, synchronization, and optimistic updates. This dedicated library simplifies asynchronous operations and eliminates the need to manage this complexity within our shared state solution.\n\n**Justification:**\n\n* **Avoiding Overkill:** For many applications, the full Redux ecosystem introduces unnecessary boilerplate and complexity. We favor a more lightweight and targeted approach using React's built-in capabilities and specialized libraries.\n* **Scalability:** The tiered approach allows us to start simple and scale our state management solution as the application grows and requirements evolve. \n* **Developer Experience:**  Libraries like `recoil`, `zustand`, and `@tanstack\/react-query` offer a more ergonomic and intuitive developer experience compared to traditional Redux setups.\n* **Performance:** Utilizing specialized libraries for remote state management and optimizing our use of React Context helps maintain a performant user experience.\n\nThis approach allows us to choose the right tool for the job, balancing simplicity, scalability, performance, and developer experience. \n","GenTime":"2024-08-28 12:48:38"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"text":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n\n\n##Decision\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","MatchedIds":[1721,1292,4422,3159,2582],"Prediction":"## Decision Drivers\n* **OpenID Connect Specification:** The OpenID Connect Core 1.0 specification explicitly states that symmetric signatures should not be used by public clients.\n* **Client Type:** This library is intended for use by public (non-confidential) clients.\n* **Security & Complexity:**  Supporting both symmetric and asymmetric key validation adds complexity. Ignoring symmetric keys aligns with the specification and common practice for public clients.\n\n## Decision\nThe library will **not** support validation of ID Tokens signed with symmetric keys.  Any attempt to validate such a token will result in the token being marked as invalid.\n\n## Positive Consequences\n* **Increased Security:**  Eliminates the risk of improperly handling symmetric keys in a public client context.\n* **Reduced Complexity:** Simplifies the library's validation logic and reduces code maintenance.\n* **Standards Compliance:** Enforces the OpenID Connect specification's recommendation for public clients.\n\n## Negative Consequences\n* **Limited Functionality:**  The library will not be compatible with ID Token issuers that exclusively use symmetric key signing for public clients (though this is non-standard according to OpenID Connect). \n\n## Alternatives Considered\n* **Supporting both symmetric and asymmetric validation:**  This would add significant complexity and potential security risks. Given the specification's guidance and the library's target audience, the added complexity outweighs the benefits.\n\n## Conclusion\nThis decision prioritizes security, simplicity, and standards compliance by focusing on the common use case of public clients and asymmetric key validation.  While it slightly limits functionality, this trade-off is justified by the increased security and reduced complexity. \n","GenTime":"2024-08-28 12:48:48"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"text":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n\n\n##Decision\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","MatchedIds":[3715,3718,1240,1477,472],"Prediction":"## Decision\nStick with JavaFX\/TornadoFX for now as the rendering\/animation framework. \n\n## Rationalization\nWhile other frameworks or engines like Unity or OpenGL might offer better performance and features for heavyweight rendering, the time investment required to learn and integrate them is currently not feasible. JavaFX, with its relatively gentler learning curve and existing integration with the project, offers a more pragmatic solution for the time being. We can re-evaluate this decision if performance becomes a critical bottleneck in the future. \n","GenTime":"2024-08-28 12:49:01"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"text":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n\n\n##Decision\nWe decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","MatchedIds":[4285,2791,4025,4061,317],"Prediction":"## Decision\n\nGiven the need for maintaining state during the RDS restore process and considering the tradeoffs of each option, the following decision is made:\n\n**Store the state in a lightweight, persistent key-value store.**\n\n**Rationale:**\n\n* **Simplicity:** Using a k\/v store offers a relatively simple solution for persisting the state across RDS broker restarts and handling multiple instances. This approach avoids the complexity of managing background routines or integrating with SNS\/SQS.\n* **Performance:** Key-value stores are generally performant for read\/write operations, which are the primary interactions needed for tracking the restore process.\n* **Resiliency:** Replicated key-value stores, like etcd or Consul, provide resilience against node failures, ensuring state consistency.\n* **Ease of Integration:** Many cloud providers offer managed key-value stores, simplifying the integration effort. \n\n**Implementation Details:**\n\n* The RDS broker will use the chosen k\/v store to store the state of the restore operation for each instance.\n*  A unique key will be generated for each restore operation, referencing the instance being restored.\n* The value will contain information about the current step of the restore process (e.g., \"restoring from snapshot\", \"updating parameters\", \"resetting passwords\"), timestamps, and any relevant error information.\n* Upon receiving a `LastOperation` request, the RDS broker will consult the k\/v store using the instance identifier. Based on the stored state, the broker will either return the appropriate status to the Cloud Controller API or proceed with the next step in the restore process.\n\n**Consequences:**\n\n* **Added Dependency:** This introduces a dependency on an external k\/v store service. \n* **Operational Overhead:** Monitoring and managing the k\/v store will be required.\n\n**Alternatives Considered:**\n\n* **Background Housekeeping Routine:** This approach would require significant effort to ensure resilience and handle multiple RDS broker instances. It introduces complexity for managing the routine's lifecycle and potential race conditions.\n* **SNS\/SQS Integration:** While offering event-driven capabilities, integrating with SNS and SQS would significantly increase the complexity of the RDS broker. It would require managing topics, queues, message handling logic, and error scenarios. This option might be worthwhile exploring in the future if more sophisticated eventing becomes necessary.\n\nThis decision favors a pragmatic approach by introducing minimal complexity while ensuring the RDS broker can reliably manage the multi-step restore process. The chosen solution balances simplicity, performance, and resilience considerations.\n","GenTime":"2024-08-28 12:49:13"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"text":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\n\n##Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","MatchedIds":[3217,4414,2582,3421,3872],"Prediction":"## Decision\n* **Adopt pac4j for handling Azure AD authentication and token validation.** \n    * This leverages a mature library that already addresses the nuances of Azure AD's multi-tenant setup, simplifying our codebase and reducing potential for errors.\n\n## Status\n* Proposed\n\n## Consequences\n* **Reduced Development Effort:**  Eliminates the need to maintain custom filter and token validation logic.\n* **Improved Maintainability:**  Relies on a dedicated security library for updates and vulnerability patching.\n* **Potential Learning Curve:**  Requires familiarization with the pac4j library and its API. \n","GenTime":"2024-08-28 12:49:18"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"text":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\n\n##Decision\nUpgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","MatchedIds":[3789,2968,621,1666,631],"Prediction":"## Decision\nTo ensure a smooth migration from `Corvus.Tenancy` v2 to v3 without disrupting existing applications, we will implement a two-phased approach:\n\n**Phase 1: Backfill Missing Containers and Introduce Compatibility Layer**\n\n1. **Container Backfilling:**\n    * Introduce a new service, deployed alongside existing v2 applications, responsible for detecting and backfilling any missing storage containers for existing tenants. This service will use the application's existing logic for defining the necessary containers based on tenant configuration.\n2. **Compatibility Layer:**\n    * Modify `Corvus.Tenancy` v2 to include a compatibility layer that can handle requests for both v2 and v3 container structures. This layer will abstract the differences between the two versions, ensuring continued functionality for applications still on v2.\n\n**Phase 2: Application Migration and Decommissioning of Legacy Components**\n\n1. **Application Upgrade:**\n    * Update applications individually to `Corvus.Tenancy` v3. This includes implementing the logic for creating all necessary containers during client onboarding, as per the new v3 specification.\n2. **Monitoring and Validation:**\n    * Deploy comprehensive monitoring to ensure the new application logic for container creation functions as expected.\n3. **Decommission Legacy Components:**\n    * Once all applications have been migrated to v3 and validated, decommission the container backfilling service and remove the compatibility layer from `Corvus.Tenancy`.\n\nThis phased approach allows for a controlled migration, minimizing disruption by:\n\n* **Gradual Rollout:**  Addressing existing tenants' inconsistencies without requiring immediate application upgrades.\n* **Backward Compatibility:** Ensuring continuous functionality during the migration process through a compatibility layer.\n* **Independent Application Migration:** Allowing applications to migrate to v3 at their own pace without impacting others.\n\nThis strategy ensures a smooth transition to `Corvus.Tenancy` v3, maintaining system integrity and minimizing downtime for applications and their users. \n","GenTime":"2024-08-28 12:49:29"}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"text":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n\n\n##Decision\nSASS will not be used in the Registraties project. Instead, styled components will be used.\\n","MatchedIds":[5090,157,2655,1992,783],"Prediction":"## Decision\n\nWe will use **styled-components** for styling this project. \n\n### Justification\n\n1. **Avoiding Global Scope Conflicts:** The current SASS setup leads to global scope CSS, increasing the risk of conflicts and making maintenance difficult as the codebase grows. Styled-components offer scoped styles, eliminating this problem and leading to more maintainable CSS.\n\n2. **Component-Level Styling:** Styled-components promote a component-based styling approach, aligning well with modern frontend development practices and improving code organization.\n\n3. **Integration with Amsterdam Component Library:** Since the Amsterdam reusable component library already utilizes styled-components, adopting this approach ensures consistency and simplifies integration with the library.\n\n4. **Reduced Need for Naming Conventions:** Styled-components inherently provide scope, lessening the reliance on strict naming conventions to prevent specificity issues that are common with global CSS.\n\n5. **Improved Developer Experience:** Styled-components offer a more intuitive developer experience by co-locating styles with their corresponding components, making it easier to understand, modify and maintain the codebase.\n\n\nWhile migrating away from SASS might require some initial effort, the long-term benefits of improved maintainability, reduced risk of style conflicts, and better integration with the component library outweigh the costs. \n","GenTime":"2024-08-28 12:49:36"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"text":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n\n##Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","MatchedIds":[2520,4747,3657,1574,5118],"Prediction":"## Decision\n* **High availability\/Fault tolerance**: Required for production-level systems.\n* **Learning curve**: Impacts development speed and potential hiring needs.\n* **Maintenance overhead**:  Affects operational costs and complexity.\n* **Vendor lock-in**: Limits future flexibility.\n\n**Chosen option**: \"Neo4j\" (or a similar open-source graph database like JanusGraph, ArangoDB)\n\n**Reasoning**:\n\n* **Native RDF Support**: Simplifies development by directly storing SKOS taxonomies without complex serialization\/deserialization.\n* **Open-source**: Minimizes vendor lock-in and provides flexibility.\n* **Active Community**: Offers support resources and reduces the learning curve.\n\n**While open-source graph databases might require more setup and configuration compared to managed solutions, their native RDF handling and avoidance of vendor lock-in outweigh these factors in this scenario.** \n\n### Positive Consequences\n* Simplifies data storage and retrieval due to native RDF support.\n* Reduces development time and effort.\n* Avoids vendor lock-in and provides long-term flexibility. \n\n### Negative Consequences\n* Potentially higher initial setup and configuration effort.\n* Requires learning a graph database technology.\n\n**Mitigation**: Investing in training and leveraging community resources can mitigate the learning curve and operational challenges associated with open-source graph databases. \n","GenTime":"2024-08-28 12:49:45"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"text":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n\n##Decision\nWe will adopt Kotlin as a Gradle build's definition language.\\n","MatchedIds":[159,2626,3860,872,1477],"Prediction":"## Decision \n\nWe will be using Kotlin (`gradle.kts`) for writing Gradle build scripts.\n\n**Positive Consequences:**\n\n* **Readability & Maintainability:** Kotlin's syntax is generally considered more readable and less verbose than Groovy, leading to build scripts that are easier to understand and maintain.\n* **IDE Support:** IntelliJ IDEA's robust support for Kotlin provides excellent code completion, navigation, and refactoring tools for `gradle.kts` files, enhancing developer productivity. \n* **Type Safety:**  Kotlin's static typing helps catch errors during compile time rather than runtime, resulting in more reliable builds. \n* **Modern Language:**  Adopting Kotlin for build scripts aligns with using a modern language throughout the project, promoting consistency.\n\n**Negative Consequences:**\n\n* **Learning Curve:** Team members unfamiliar with Kotlin may require some time to learn the language and its application in Gradle scripts.  \n\n**Mitigation:**\n\n* Provide resources and training for team members to learn Kotlin basics and its use in Gradle builds. \n","GenTime":"2024-08-28 12:49:50"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"text":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n\n\n##Decision\nTo avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","MatchedIds":[1278,3372,1216,1558,3815],"Prediction":"## Decision\n\nEnforce file attachment during record creation. This can be achieved by:\n\n- **Mandatory file upload:** The system will require users to upload at least one file during the record creation process. The record creation action will only be successful if at least one valid file is provided.\n- **Record validation:** Implement a validation step within the record creation workflow. This validation will check for the presence of an attached file. If no file is found, the record creation process will be aborted, and the user will be prompted to attach a file. \n\nThis decision ensures that all new records are meaningful and contain the actual data represented by the metadata.  This prevents the creation of \"empty\" records, enhancing data integrity and usability. \n","GenTime":"2024-08-28 12:49:55"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"text":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n\n\n##Decision\nIn the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","MatchedIds":[452,4179,1258,1758,3572],"Prediction":"## Decision\nWe will use randomly created databases for each test. This approach provides better isolation between tests, preventing potential race conditions and ensuring that each test interacts with a clean, predictable database state. While slightly more complex to implement than defining databases in tests, the increased reliability and reduced risk of flaky tests justify the added effort. \n","GenTime":"2024-08-28 12:49:58"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"text":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n\n\n##Decision\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","MatchedIds":[733,620,3905,216,50],"Prediction":"## Decision\n\nWe will configure a Kubernetes Service of type `LoadBalancer` to route traffic to the Ingress controller. \n\n**However**, recognizing the constraints outlined in the decision drivers, we will **also** provide the capability to configure Compliant Kubernetes to use an existing load balancer. This will be achieved by allowing users to specify the address of their load balancer during deployment, which will then be used to configure the Ingress controller accordingly.\n\n## Status\n\nAccepted\n\n## Consequences\n\n* **Principle of Least Astonishment:**  Most cloud providers and managed Kubernetes offerings natively support `LoadBalancer` services, making it a familiar approach for many users.\n* **Hybrid Cloud Flexibility:**  The ability to leverage existing load balancers directly addresses the limitations of bare-metal and BYOVM deployments.\n* **Simplicity:**  This dual approach offers a simple, default path for most users while preserving flexibility for more complex scenarios.\n\n## Tradeoffs\n\n* **Configuration Complexity:**  Supporting both Kubernetes-provisioned and external load balancers introduces some additional configuration complexity. However, this is mitigated by providing clear documentation and tooling to guide users through the process. \n","GenTime":"2024-08-28 12:50:05"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"text":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n\n\n##Decision\nWe will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","MatchedIds":[4618,472,2294,2473,1400],"Prediction":"## Decision\n\nWe will implement continuous input by consuming our existing input event stream from `SDL_PollEvent` and transitioning our own \"InputState\" based on \"KeyPressed\" and \"KeyReleased\" events. \n\n## Rationalization\n\nWhile using `SDL_GetKeyboardState` seems simpler at first glance, maintaining our own \"InputState\" offers several advantages:\n\n* **Type Safety and Reduced Complexity:** By explicitly defining our \"InputState\" and how it transitions based on events, we leverage the type system to limit possible states and transitions. This reduces complexity and potential errors compared to interpreting the raw keyboard state from `SDL_GetKeyboardState` every frame.\n* **Flexibility and Control:** Managing our own \"InputState\" allows for greater flexibility in the future. We can easily add support for new input types, handle complex input combinations, or implement features like input buffering and key repeat without relying on SDL's behavior. \n* **Clearer Logic:** Transitioning \"InputState\" based on discrete events leads to more understandable and maintainable code. The logic for handling each input event is localized, making it easier to reason about and debug. \n\nAlthough this approach might require slightly more upfront work, the benefits of increased type safety, flexibility, and code clarity outweigh the costs in the long run.\n","GenTime":"2024-08-28 12:50:13"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"text":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n\n##Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","MatchedIds":[1931,1240,4308,3827,3721],"Prediction":"## Decision\n\n**Choose 11ty as the static site generator.**\n\nWhile it has some drawbacks regarding the frontmatter parser and production-ready features, these can be addressed with additional development effort. \n\n**Reasons:**\n\n* **Alignment with Project Goals:**  11ty's predictable static file output and support for HapiJS align perfectly with the desired architecture of serving a statically generated blog. \n* **Simplicity and Control:** The smaller codebase and easy setup offer greater control over the site's structure and functionality, fitting with the desire for a streamlined, understandable project. \n* **Flexibility for Improvement:** Addressing the cons, like SEO and image optimization, are achievable within the 11ty ecosystem and present opportunities to tailor the site to specific requirements.\n\n**Alternative Solutions:**\n\n* **Gatsby:**  Disqualified due to its client-side JavaScript reliance, contradicting the goal of a purely static site served efficiently by HapiJS.\n* **Hugo:** Though performant, the need for Go introduces a new language and potentially increases the project's complexity, deviating from the preference for a JavaScript-based stack. \n\n**Next Steps:**\n\n1. **Investigate Frontmatter Workarounds:** Explore solutions for the frontmatter parser issues, such as using alternative parsers or adjusting content structure.\n2. **Implement Production-Ready Enhancements:** Integrate SEO optimizations, image processing, and other necessary features for a polished website.\n3. **Develop Custom HapiJS Server:**  Create a minimal HapiJS server specifically tailored to serve the static files generated by 11ty. \n","GenTime":"2024-08-28 12:50:25"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"text":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n\n\n##Decision\n### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","MatchedIds":[2725,4600,4766,1996,697],"Prediction":"## Decision\n\nWe propose a base directory structure for GitHub repositories, aiming for consistency and clarity across projects. This structure provides a common ground, leaving room for adjustments based on specific project needs.\n\n### Base Structure\n\n```\n\u251c\u2500\u2500 src              # Source code for the project\n\u2502   \u251c\u2500\u2500 components    # Reusable UI components\n\u2502   \u2502   \u2514\u2500\u2500 Button\n\u2502   \u2502       \u251c\u2500\u2500 Button.tsx\n\u2502   \u2502       \u2514\u2500\u2500 Button.module.css\n\u2502   \u251c\u2500\u2500 services      # Business logic and API interactions\n\u2502   \u2502   \u2514\u2500\u2500 api.ts\n\u2502   \u251c\u2500\u2500 utils         # Helper functions and utilities\n\u2502   \u2502   \u2514\u2500\u2500 string.ts\n\u2502   \u2514\u2500\u2500 types        # Type definitions and interfaces\n\u2502       \u2514\u2500\u2500 index.ts\n\u251c\u2500\u2500 tests           # Unit and integration tests\n\u2502   \u2514\u2500\u2500 services\n\u2502       \u2514\u2500\u2500 api.test.ts\n\u251c\u2500\u2500 public           # Publicly accessible assets\n\u2502   \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 scripts          # Utility scripts for development tasks\n\u2502   \u2514\u2500\u2500 build.js\n\u251c\u2500\u2500 .github         # GitHub-specific configurations\n\u2502   \u2514\u2500\u2500 workflows\n\u2502       \u2514\u2500\u2500 ci.yml\n\u2514\u2500\u2500 docs              # Project documentation\n    \u2514\u2500\u2500 index.md\n\n```\n\n###  Explanation\n\n* **src**:  This is the heart of your project, containing all the source code. We encourage further structuring this directory based on feature or domain, promoting modularity and maintainability.\n* **tests**:  A dedicated space for your test suite, mirroring the structure of your `src` directory for clarity.\n* **public**: Houses static assets directly copied to the final build output, like HTML files or images.\n* **scripts**:  Reserved for custom scripts aiding in development, building, or deployment tasks.\n* **.github**: Contains configuration files specifically for GitHub actions and workflows.\n* **docs**: The home for your project documentation, ensuring it remains centralized and accessible.\n\n### Exceptions and Adaptations\n\nThis structure serves as a template.  Flexibility is key!  Here are some scenarios where adaptations might be necessary:\n\n* **Project Type:**  Backend projects might not require a `public` directory. Libraries might benefit from a `dist` or `build` directory for compiled output.\n* **Tooling:** Frameworks or build tools may enforce certain conventions. In such cases, prioritize alignment with those tools while maintaining the spirit of this structure.\n\n### Positive Consequences\n\n* **Consistency:**  Reduces cognitive overhead when switching between projects, making onboarding smoother.\n* **Clarity:**  A structured approach improves code organization, making it easier to find and manage files.\n* **Maintainability:** Promotes best practices for scalability and long-term project health. \n","GenTime":"2024-08-28 12:50:44"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"text":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n\n\n##Decision\nThe ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","MatchedIds":[3637,1193,1067,3682,2914],"Prediction":"## Decision\nChange the initialization of the synchronous utils to be lazy. That is:\n1. Remove the initialization from the constructor. \n2. Create a method, maybe named `initContext`, that will load the context. This method will be called explicitly by the command logic, before using any other utility method that relies on the context.\n3. For the `init` command, this method (`initContext`) will not be called, as there is no pre-existing context to load. \n\nThis approach avoids loading the context unnecessarily, thus improving performance, and more importantly, solves the functional issue where the `init` command was failing. It also allows for more flexibility and control over when and how the context is loaded for each command. \n","GenTime":"2024-08-28 12:50:52"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"text":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n\n\n##Decision\nCreate a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","MatchedIds":[3794,1540,771,3416,4358],"Prediction":"## Decision\n\nWe will create a Dockerized service for processing GridLabD jobs. \n\nHere's a breakdown of the decision and its implications:\n\n**Rationale:**\n\n* **Reusability:** Docker containers offer a self-contained environment for the GridLabD processor, making it easily reusable across different projects and infrastructures.\n* **Flexibility:**  Projects can choose to either deploy the Docker image as a standalone service (e.g., using AWS ECS or Kubernetes) or integrate it directly into their existing Flask applications.\n* **Isolation:**  Containerization provides isolation, ensuring that dependencies and potential conflicts within the GridLabD processor don't impact other parts of the project. \n* **Scalability:** Docker makes it easier to scale the GridLabD processing horizontally by running multiple instances of the container.\n\n**Implementation Details:**\n\n1. **Docker Image:** Create a Dockerfile to define the environment, install dependencies, and package the GridLabD processing logic.\n2. **Job Queue:** Implement a job queuing system (e.g., Celery, Redis Queue) to handle asynchronous, long-running GridLabD jobs.\n3. **Artifact Storage Abstraction:**  Create a generic mechanism for the processor to store artifacts (using environment variables or configuration files), allowing projects to specify their preferred storage (e.g., S3 bucket location).\n4. **API Endpoints (Optional):**  If deployed as a standalone service, define API endpoints for projects to submit jobs, monitor progress, and retrieve results.\n\n**Benefits:**\n\n* **Simplified Deployment:**  Deploying the processor becomes as simple as pulling and running the Docker image.\n* **Project Agnostic:**  The core GridLabD processing logic remains independent of specific client project requirements, promoting maintainability.\n* **Scalability and Reliability:**  Containerization and a job queue enable efficient scaling and handling of varying workloads. \n\n**Considerations:**\n\n* **Learning Curve:** Teams will need familiarity with Docker and potentially container orchestration tools if deploying as a standalone service.\n* **Resource Management:**  Monitoring and managing resources (CPU, memory) within the Docker containers becomes important, especially in a multi-tenant environment. \n","GenTime":"2024-08-28 12:51:03"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"text":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n\n\n##Decision\nTo aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","MatchedIds":[4442,2181,4454,581,5009],"Prediction":"## Decision\n\nWe will implement a qualification validation service that will be used to verify a claimant's qualifications against the eligibility criteria.\n\n### Service Functionality:\n\n1. **Eligibility Criteria Retrieval:** The service will retrieve the required qualifications for the specific grant\/benefit a claimant is applying for. This could involve fetching data from a configuration file, database, or dedicated eligibility API.\n\n2. **Qualification Data Retrieval:** The service will fetch the claimant's qualification data. The source of this data could be:\n    *  Direct user input during the application process.\n    *  Integration with external qualification verification services.\n    *  Data provided by the claimant through uploaded documents.\n\n3. **Qualification Matching:** The service will compare the claimant\u2019s provided qualifications against the retrieved eligibility criteria.  This comparison should be flexible enough to handle:\n    * **Exact matching:** Checking for identical qualifications (e.g., specific degree title and institution).\n    * **Equivalent matching:**  Allowing for equivalent qualifications (e.g., accepting different but related degrees).\n    * **Partial matching:** Identifying cases where the claimant possesses some, but not all, of the required qualifications.\n\n4. **Validation Result:** The service will return a validation result indicating:\n    * **Success:** The claimant\u2019s qualifications meet the eligibility criteria.\n    * **Failure:**  The claimant\u2019s qualifications do not meet the eligibility criteria, specifying the discrepancies.\n    * **Partial Match:**  The claimant partially meets the eligibility criteria, detailing the fulfilled and unfulfilled requirements. \n\n### Additional Considerations:\n\n* **Data Privacy and Security:** Implement appropriate measures to protect sensitive claimant qualification data throughout the validation process.\n* **Maintainability and Extensibility:** Design the service with future changes in mind. The system should easily adapt to new qualification types, validation rules, and data sources.\n* **Error Handling and Logging:** Implement robust error handling to gracefully manage invalid data or service disruptions. Detailed logs will be crucial for debugging and monitoring. \n* **User Feedback:** Provide clear and informative feedback to the claimant on the validation results, explaining any discrepancies or required actions.\n","GenTime":"2024-08-28 12:51:15"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"text":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n\n\n##Decision\nWe will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","MatchedIds":[399,2585,282,2626,3071],"Prediction":"## Decision\nWe will use **Jackson** for deserialization of the `project.json` file. While GSON offers similar functionality, choosing Jackson leverages the team's existing experience with the library and capitalizes on its feature richness. Additionally, opting for Jackson's object mapping capabilities over org.json's manual object creation simplifies the codebase and improves maintainability. \n","GenTime":"2024-08-28 12:51:18"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"text":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n\n\n##Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","MatchedIds":[4804,3820,1145,4616,3546],"Prediction":"## Decision\n\n**Decision:**  Without specific details about the context and forces at play, it's impossible to make a concrete architectural decision.  \n\n**Rationale:**  Architectural decisions are driven by the specific challenges and constraints of a project. To make an informed decision, we need more information about:\n\n* **Technological forces:** What technologies are currently in use? What are their limitations? What new technologies are available?\n* **Political forces:** Are there any stakeholders who have a vested interest in a particular outcome? Are there any budget constraints? \n* **Social forces:** How will this decision impact users? Are there any ethical considerations?\n* **Project-local forces:** What is the timeline for this project? What are the team's skills and experience?\n\n**Next Steps:**\n\n1. **Gather more information:**  Please provide specifics about the forces mentioned in the context section. \n2. **Analyze trade-offs:** Once we have a clear understanding of the context, we can analyze the trade-offs of different options. \n3. **Make a recommendation:** Based on the analysis, I can recommend a specific architectural decision.\n\n**Example:**\n\n**Context:**\n\n> **Technological forces:** Our current system relies heavily on a monolithic architecture, which is proving difficult to scale and maintain. Microservices are gaining popularity for their flexibility and scalability.\n>\n>**Project-local forces:** We have a team of experienced developers who are eager to learn new technologies. However, we have a tight deadline for the project.\n\n**Decision:**\n\n**Decision:** We will adopt a hybrid approach, gradually migrating towards a microservices architecture for new features while maintaining the existing monolithic structure for core functionalities. This approach balances the need for scalability with the constraints of time and resources. \n","GenTime":"2024-08-28 12:51:27"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"text":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n\n\n##Decision\nWe identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","MatchedIds":[2703,3554,3853,3653,697],"Prediction":"## Decision\n\n### General Rules\n\n* **Purpose Alignment:** Choose the organization that best aligns with the primary purpose and stakeholders of the repository.\n* **Ownership & Governance:**  Consider the team or entity ultimately responsible for the repository's maintenance and development. \n* **Open Source vs. Internal:** Clearly distinguish between open-source repositories intended for public contribution and internal repositories with restricted access.\n\n### Specific Cases:\n\n* **Organization-Specific Projects:** Repositories directly related to a specific organization's activities or projects should reside within that organization's GitHub space.\n* **Cross-Organization Collaborations:** For projects involving multiple organizations, consider:\n    * Creating a dedicated shared organization (e.g., \"orgA-orgB-collaboration\")\n    * Hosting the repository under the organization with primary ownership, granting appropriate access to collaborators from other organizations.\n* **Open Source Projects:**\n    * Publicly released open-source projects generally benefit from their own dedicated organization for greater visibility and community engagement.\n    * Consider using a neutral organization name (if applicable) to emphasize open governance and reduce potential vendor lock-in perception.\n* **Personal Projects:** Developers can maintain personal projects under their individual GitHub accounts.\n\n### Additional Considerations:\n\n* **Naming Conventions:** Use clear and descriptive organization names to facilitate easy identification and discovery. \n* **Documentation:** Maintain documentation that outlines the organization selection criteria and any specific rules or exceptions.\n* **Review Process:** Implement a lightweight review process for new repository creation requests to ensure adherence to the defined guidelines.\n\n### Example:\n\nLet's say we have these organizations:\n\n* **AcmeCorp:** The main organization for Acme Corporation.\n* **AcmeOpenSource:**  Dedicated to open-source projects initiated by Acme Corp.\n* **AcmeCorp-PartnerSolutions:** For collaborative projects with partners.\n\nHere's how we'd apply the rules:\n\n* A repository for an internal Acme Corp. marketing project \u2192 **AcmeCorp** organization.\n* An open-source library developed by Acme Corp. \u2192 **AcmeOpenSource** organization.\n* A project developed jointly with ExampleTech Inc. \u2192 **AcmeCorp-PartnerSolutions** organization (or potentially \"AcmeCorp-ExampleTech\" for a more balanced representation). \n","GenTime":"2024-08-28 12:51:38"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"text":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n\n\n##Decision\nIntegrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","MatchedIds":[1430,1431,2706,3159,2823],"Prediction":"## Decision\n\nWe will move away from using config options like `CONF_MONITORED_CONDITIONS` for selecting data points exposed by integrations. Instead, we will adopt the following:\n\n**1. Expose All Relevant Entities by Default:**\n\nIntegrations should, by default, expose all data points as individual entities if they are deemed useful or potentially valuable for the user. This simplifies the initial setup and avoids users needing to understand data points upfront.\n\n**2. Provide Entity Customization Options:**\n\nUsers will have the flexibility to customize and fine-tune entity visibility and behavior through the following means:\n\n* **Lovelace UI:**  The primary method for users to select and manage entities displayed in their dashboards.\n* **Entity Registry:** Offers advanced control to disable or rename entities, providing a centralized location to manage entity properties.\n* **Helpers:**  Integrations can offer helper entities (e.g., input_boolean) to allow users to dynamically enable\/disable certain functionalities or data fetching without modifying the integration configuration itself.\n\n**Rationale:**\n\n* **Improved User Experience:** Simplifies the integration setup process, especially for new users. No need to understand cryptic data point options upfront.\n* **Flexibility and Control:** Users retain control over displayed data through familiar mechanisms (Lovelace UI, Entity Registry) and can fine-tune behavior with helpers.\n* **Consistency:**  Aligns with Home Assistant's approach of exposing functionality through entities.\n\n**Transition Plan:**\n\n* **New Integrations:** Adopt this approach immediately.\n* **Existing Integrations:** Gradually phase out `CONF_MONITORED_CONDITIONS` and similar options. Provide clear migration instructions for users during the transition period. Encourage users to utilize the Entity Registry and Lovelace UI for managing entity visibility.\n\n**Additional Considerations:**\n\n* **Performance:**  Integrations should be mindful of potential performance impacts from fetching and exposing additional data by default. Employing efficient data retrieval and caching mechanisms can mitigate this.\n* **Documentation:**  Clearly document the available entities and their functionality for each integration. Provide guidance on customizing entity visibility and behavior using the Lovelace UI, Entity Registry, and helpers. \n","GenTime":"2024-08-28 12:51:49"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"text":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n\n\n##Decision\nFor every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","MatchedIds":[1985,3692,4354,1949,405],"Prediction":"## Decision\n\nIntroduce a new endpoint in the API to allow querying past validator sets by block height. This endpoint will return the validator set that was active at the specified height, enabling reconstruction of historical validator information. \n\nThis new endpoint will require persisting validator set changes along with their corresponding block heights. This data can be stored efficiently and accessed quickly for historical queries. \n","GenTime":"2024-08-28 12:52:24"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"text":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n\n##Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","MatchedIds":[899,2488,4644,2498,1035],"Prediction":"## Decision\n\nWe will use **Spring Framework** as it provides both dependency injection and robust support for relational databases through **Spring JDBC** or **Spring Data JPA**. \n\n### Positive Consequences:\n\n- **Spring's Inversion of Control (IoC) container** will manage object creation and dependency injection, leading to loosely coupled and easily testable code. \n- **Spring JDBC** simplifies database interactions by handling boilerplate code like connection management and exception handling. \n- **Spring Data JPA** offers a higher level of abstraction with an object-relational mapping (ORM) approach, further reducing boilerplate code and improving developer productivity.\n- Utilizing a widely adopted framework like Spring brings a large, active community and readily available resources for troubleshooting and best practices.\n\n### Negative Consequences:\n\n- Introducing Spring adds a dependency to the project, increasing the learning curve for developers unfamiliar with the framework.\n- While Spring offers flexibility, choosing between Spring JDBC and Spring Data JPA requires careful consideration based on project complexity and team expertise.\n- Overuse of Spring's features without proper design can lead to overly complex code, negating some benefits of dependency injection. \n","GenTime":"2024-08-28 12:52:30"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"text":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n\n\n##Decision\nWe will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","MatchedIds":[2933,3746,4307,1031,2945],"Prediction":"## Decision\n\nWe will adopt **Protocol Buffers (Protobuf)** as the new standard for SameDiff model serialization. This decision is driven by the following factors:\n\n1. **Language Interoperability**: Protobuf offers native support for Java and C++, ensuring seamless cross-language compatibility for loading and saving models. This addresses the requirement for compatibility between C++ and Java. \n\n2. **Schema Evolution**: Protobuf's schema-based approach allows for backward and forward compatibility. This means that models saved with older versions can still be loaded by newer versions, and vice versa. This mitigates potential issues with future updates and maintains flexibility.\n\n3. **Performance**: Protobuf is known for its efficiency in both serialization\/deserialization speed and compact message size. This is crucial for both storage space and network transfer of models, potentially improving upon the limitations of the existing FlatBuffers implementation.\n\n4. **Widely Used and Supported**: Protobuf is a mature and widely adopted serialization framework with extensive documentation, tooling, and community support. This eases development, debugging, and ensures long-term maintainability.\n\n**Migration Plan**:\n\n1. **New Implementation**: Develop a new serialization\/deserialization mechanism based on Protobuf, defining appropriate message structures for SameDiff graph components.\n2. **Backward Compatibility**:  Investigate mechanisms to provide backward compatibility with existing FlatBuffers-based model files, either through conversion tools or dual-format loading support during a transition period.\n3. **Testing and Benchmarking**: Thoroughly test the new Protobuf implementation for correctness, performance, and compatibility across different languages and platforms.\n4. **Documentation and Release**: Update documentation to reflect the new serialization format and release the updated SameDiff library with comprehensive instructions for users.\n","GenTime":"2024-08-28 12:52:40"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"text":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n##Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","MatchedIds":[1799,779,3127,3099,2467],"Prediction":"## Decision\n\nWhile retrying all requests by default simplifies the developer experience for the majority of cases, the potential for unexpected behavior with non-idempotent operations is a significant concern. \n\nTherefore, the library will **not retry any requests by default**. Instead:\n\n* **Clear documentation and examples** will be provided to guide developers on implementing retry logic for idempotent operations. \n* The library **will offer a mechanism** for developers to explicitly configure retries for specific operations or groups of operations. This mechanism will provide flexibility to define retry strategies, including:\n    * Specifying the HTTP status codes that should trigger a retry.\n    * Setting maximum retry attempts and backoff intervals.\n    * Optionally marking specific operations as idempotent to enable automatic retries for them.\n\n**Consequences**:\n\n* **Improved correctness:** By avoiding automatic retries for non-idempotent operations, the library minimizes the risk of unintended side effects and ensures predictable behavior.\n* **Developer responsibility:** Developers are given explicit control and responsibility for implementing retry logic according to the specific needs of their applications.\n* **Slightly steeper learning curve:** Developers need to invest more effort in understanding and configuring retry mechanisms, but this is mitigated by comprehensive documentation and examples. \n\nThis approach strikes a balance between ease of use and safeguarding against potentially harmful unintended consequences. \n","GenTime":"2024-08-28 12:52:47"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"text":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n\n\n##Decision\nChosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","MatchedIds":[3504,699,4179,1258,4868],"Prediction":"## Decision\n\nInclude SWI Prolog in the CircleCI environment. This can be achieved by:\n\n**Chosen Option:**  Utilize a Docker image that has SWI Prolog pre-installed for the CircleCI jobs that require running these tests.\n\n### Positive Consequences\n\n* **Simplifies CI configuration:** No need for manual installation steps within the CI script. \n* **Consistent environment:** Ensures all dependencies, including the correct SWI Prolog version, are present.\n* **Improves build speed:**  Pulling a pre-built image is generally faster than installing dependencies in each build.\n\n### Implementation Details (optional)\n\n1. **Select\/Create Docker Image:** Choose an existing Docker image with SWI Prolog or create a custom one based on the project's needs. \n2. **Update CircleCI Config:** Modify the CircleCI configuration file (`config.yml`) to use the chosen Docker image for the relevant jobs. \n3. **Adjust Test Commands:** Update test execution commands in the CI script to utilize the SWI Prolog installation within the Docker container. \n","GenTime":"2024-08-28 12:52:54"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"text":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n\n\n##Decision\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","MatchedIds":[4857,4747,1545,1574,4804],"Prediction":"## Decision\n\n**Option 1:** Implement a Two-Factor Authentication (2FA) system with Single Sign-On (SSO) capabilities.\n\n* **Security Level:** Protect\n* **Data Retention:**  Retain user data indefinitely unless explicitly deleted by the user or administrator due to legal or regulatory requirements. Implement automatic reminders for account review after a period of inactivity (e.g., 1 year) to allow for potential data purging. \n\n**Rationale:**\n\n* **Ease of Use:** SSO simplifies login for journalists, while 2FA provides an extra security layer. \n* **Security:**  \"Protect\" level strikes a balance between security and usability. \n* **Data Retention:** Indefinite retention ensures data is available when needed, while inactivity reminders allow for data minimization practices.\n* **Complexity:**  SSO and 2FA solutions are widely available and can be integrated with existing systems.\n\n**Option 2:**  Implement a Role-Based Access Control (RBAC) system with granular permission levels.\n\n* **Security Level:**  Restrict\n* **Data Retention:**  Retain data based on content sensitivity. Public content retention can be indefinite, while sensitive content should follow a defined retention policy based on legal and operational requirements. \n\n**Rationale:**\n\n* **Ease of Use:** RBAC ensures journalists only access the information relevant to their roles. \n* **Security:**  \"Restrict\" level offers the highest security, crucial for sensitive journalistic content. \n* **Data Retention:**  Flexible retention based on content sensitivity ensures compliance and minimizes unnecessary storage.\n* **Complexity:** RBAC implementation may require deeper integration with existing systems and more configuration. \n\n\n### Positive Consequences\n\n* **Enhanced Security:** Both options significantly improve security compared to a system without these features.\n* **Improved User Experience:** SSO and 2FA (Option 1) streamline authentication for journalists. RBAC (Option 2) tailors access and simplifies information management.\n* **Compliance:** Data retention policies ensure compliance with legal and industry regulations.\n\n### Negative Consequences\n\n* **Implementation Cost:** Implementing these solutions may require initial investment and development time.\n* **User Training:** Journalists might require training on new security procedures, potentially causing temporary disruption.\n* **System Complexity:**  Integrating these solutions adds complexity to the overall system architecture.\n\n\n**Final Decision:** The optimal choice depends on the specific security needs and resources available. \n\n* **For high-security environments with sensitive data:** Option 2 (RBAC with \"Restrict\" level) is recommended. \n* **For a balance between security and usability:** Option 1 (2FA with SSO and \"Protect\" level) is suitable. \n","GenTime":"2024-08-28 12:53:08"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"text":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n\n\n##Decision\n* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","MatchedIds":[3598,2703,4766,1996,4096],"Prediction":"## Decision\n\nWe will adopt a standardized naming convention for our Docker images to ensure consistency and clarity across our projects. The format will be as follows:\n\n**`[repository]\/[project]-[component]:[tag]`**\n\n* **`repository`**: The name of the Docker repository (e.g., `quay.io\/my-organization`).\n* **`project`**: The name of the project this image belongs to (e.g., `my-awesome-project`).\n* **`component`**:  The specific component or service within the project (e.g., `frontend`, `api`, `worker`). \n* **`tag`**: A descriptive tag for the image. This could be:\n    * A version number (e.g., `1.0.0`, `v2.3.1`).\n    * A build identifier (e.g., `build-1234`).\n    *  A combination of both (e.g., `1.2.3-build456`).\n    * For images built for specific architectures, append the architecture to the tag  (e.g., `1.0.0-amd64`, `latest-arm64`).\n\n**Examples:**\n\n* `quay.io\/my-organization\/my-awesome-project-api:1.2.3`\n* `my-registry.com\/internal-tools\/user-service:latest`\n* `public-repo\/my-project-frontend:v2.1.0-arm64`\n\n**Rationale:**\n\n* **Consistency:** Using a standardized format makes it easier to identify and manage images across different projects and repositories.\n* **Clarity:** The naming convention clearly communicates the image's purpose, version, and architecture.\n* **Automation:**  A consistent format enables the automation of building, tagging, and deploying Docker images.\n\n**Additional Considerations:**\n\n* We will encourage the use of semantic versioning for the `tag` portion of the image name.\n* Image tags should be kept short and descriptive for readability.\n* This naming convention should be documented and enforced through automated processes where possible. \n","GenTime":"2024-08-28 12:53:21"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"text":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n\n\n##Decision\nChosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","MatchedIds":[2091,2914,1545,4644,1614],"Prediction":"## Decision\n\nWe will adopt a modular architecture that facilitates both current and future feature development. \n\n### Initial Architecture\n\n1. **Core ADR Library:**  A language-agnostic core library ([e.g., Python, JavaScript] responsible for:\n    - Defining the ADR data structure and validation.\n    - Parsing and serializing ADRs from\/to Markdown files.\n    - Providing basic operations like create, read, update (if applicable to the chosen ADR format), and list.\n\n2. **Command-Line Interface (CLI):** Built upon the core library, providing commands for:\n    - `create`: Generates a new ADR file with a standardized template.\n    - `list`:  Displays a list of existing ADRs.\n    - `preview` (optional):  Renders a local preview of ADRs using the core library and a templating engine (e.g., Jinja2, Handlebars).\n\n3. **Static Site Generator Integration:** \n    - Utilize a static site generator (e.g., Jekyll, Hugo, Gatsby) to generate a static website from the ADRs.\n    - The core library can be used to feed ADR data into the static site generator.\n\n4. **Local Development Server:** A simple development server (can be part of the CLI or a separate module) for previewing the website locally.\n\n### Future Extensions\n\n- **Web UI (Local\/Remote):** A separate web application module can be developed to interact with the core library via an API. This allows for future expansion to remote hosting and multi-repository support.\n- **VSCode Extension:**  A VSCode extension can utilize the core library's API to provide in-editor ADR creation and editing functionalities.\n- **Multi-Repository Aggregation:** Develop a separate service or extend the web application to fetch and aggregate ADRs from multiple Git repositories using their respective APIs. \n\n### Rationale\n\n- **Modularity:** Separating concerns into distinct modules ensures maintainability, testability, and flexibility for future growth.\n- **Core Library as the Foundation:** A well-defined core library fosters consistency across different components and reduces code duplication.\n- **Gradual Enhancement:** This approach allows us to start with essential features and progressively add more complex functionalities as needed.\n\n### Open Questions and Follow-up Decisions\n\n- Choice of programming language for the core library and other components.\n- Specific static site generator selection.\n- Definition of API endpoints for potential web UI and VSCode extension interactions.\n- Strategy for handling ADR versioning and potential conflicts.\n- Security considerations for remote access and multi-repository aggregation. \n","GenTime":"2024-08-28 12:53:35"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"text":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n\n\n##Decision\n* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","MatchedIds":[4498,3171,4500,1855,1125],"Prediction":"## Decision\n\nGiven the limitations of using feature flags directly within Storefront SCSS, we will adopt the following approach:\n\n1. **Feature-Specific CSS Classes:** Instead of directly checking for feature flags within SCSS, we will create feature-specific CSS classes. For example, for a feature named \"new-header\", we would create classes like `.new-header` and `.legacy-header`.\n\n2. **Conditional Class Application:** We will apply these classes conditionally in the Twig templates using feature flag checks. This allows us to control the visibility and styling of elements based on feature flag status.\n\n3. **Minimize Redundant CSS:** While SCSS for hidden features will still be compiled, we will strive to minimize its footprint.  We can achieve this by:\n    * **Scoping Feature-Specific Styles:**  Encapsulate styles related to a specific feature within a dedicated SCSS file or clearly marked section.\n    * **Using Placeholders:** For larger feature sets, consider using SCSS placeholders  (`%feature-styles`)  that are only extended and compiled when the feature flag is active.\n\n4. **Breaking Changes and Backward Compatibility:** For breaking changes within SCSS functions, mixins, or variables, we will follow a two-step process:\n    * **Introduce Versioned Alternatives:** Create new versions of the affected functions, mixins, or variables with appropriate names (e.g., `my-mixin-v2`).\n    * **Conditional Usage in Twig:** Use feature flags within Twig to conditionally include the appropriate version of the SCSS function, mixin, or variable.\n\nThis approach allows us to leverage feature flags for styling while mitigating the challenges of directly integrating them into SCSS. It ensures a clean separation of concerns and maintains backward compatibility during feature development.\n","GenTime":"2024-08-28 12:53:46"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"text":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n\n##Decision\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","MatchedIds":[3526,3187,1193,4645,1545],"Prediction":"## Decision\n**Chosen option:** Configure the GTM container used by tracking consent via data attributes instead of separate bundles. \n\n**Justification:** This decision prioritizes simplicity and ease of use for service developers, aligning with the key decision drivers.\n\n### Positive Consequences\n* **Simplified Integration:** Service developers will no longer need to manage separate bundles for GTM configuration, streamlining the integration process and reducing potential errors.\n* **Improved Developer Experience:**  Data attributes offer a more intuitive and discoverable way to configure tracking consent, making it easier for developers to understand and implement.\n* **Reduced Operational Overhead:** Consolidating configuration within data attributes simplifies deployments and reduces the potential for inconsistencies across environments.\n\n### Negative Consequences\n* **Potential for Code Bloat:**  While data attributes offer convenience, excessive use could lead to slightly larger HTML payloads. This should be monitored and mitigated if it becomes a concern.\n* **Migration Effort:**  Existing services using the separate bundle approach will require migration to the new data attribute-based configuration. \n\n**Mitigation:**\n\n* Provide clear migration guidance and tools to minimize the effort required for existing services to adopt the new configuration approach.\n* Carefully consider the structure and organization of data attributes to ensure readability and maintainability within the HTML codebase. \n","GenTime":"2024-08-28 12:53:53"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"text":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n\n\n##Decision\nWe're using MTL style.\\n","MatchedIds":[2480,3715,311,2482,2290],"Prediction":"## Decision\n\nLet's go with the MTL style for managing side effects. It seems like a good fit for our use case based on my research and the success of `dino-rush`. This will give us a chance to gain experience with MTL while keeping our code relatively straightforward. \n","GenTime":"2024-08-28 12:53:58"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"text":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n\n##Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","MatchedIds":[1574,4747,3071,1510,3657],"Prediction":"## Decision\n* **Cost:** Opt for a cost-effective solution given the system's simplicity.\n* **Backup capabilities:** Ensure data security and recoverability with reliable backup features.\n\nChosen option: **Azure Cosmos DB with the Core (SQL) API**.\n\n### Rationale:\n- **Cost-effective:**  While Azure SQL offers a relational model, Cosmos DB's Core (SQL) API provides a similar document-oriented approach suitable for less complex systems, with potential cost savings.\n- **Backup capabilities:** Azure Cosmos DB offers automatic backups and point-in-time restore capabilities, ensuring data security and recovery.\n- **Familiar Interface:** Using the Core (SQL) API allows leveraging existing SQL skills for querying and data manipulation. \n\n### Positive Consequences:\n* Reduced cost compared to Azure SQL for a less complex system.\n* Reliable data protection with built-in backup and restore features.\n\n### Negative Consequences: \n* Potential learning curve if unfamiliar with Cosmos DB and its API.\n* May not be suitable for highly relational data models. \n","GenTime":"2024-08-28 12:54:04"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"text":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n\n\n##Decision\nTo use a monorepo to keep all code together and deploy it at the same time.\\n","MatchedIds":[1117,4889,1035,3488,302],"Prediction":"## Decision\n\n**We will adopt a monorepo approach for our microservices architecture.** \n\nThis decision is driven by the following factors:\n\n* **Simplified Dependency Management:**  A monorepo helps us maintain a single source of truth for all microservices, simplifying dependency management and reducing the risk of incompatibilities arising from different update cycles.\n* **Improved Testability:** Having all microservices within the same repository allows for comprehensive integration testing, ensuring that changes in one service do not negatively impact others.\n* **Facilitated Refactoring:**  A monorepo makes it easier to perform large-scale refactoring across multiple services simultaneously, reducing code duplication and promoting consistency.\n* **Streamlined Development Workflow:**  Developers can work on multiple services without switching repositories, simplifying the development workflow and fostering collaboration.\n\nHowever, it is crucial to acknowledge potential downsides:\n\n* **Increased Build Times:**  Building and testing the entire codebase can become time-consuming as the monorepo grows. Implementing efficient build tools and strategies will be essential to mitigate this.\n* **Learning Curve:** Developers will need to adapt to the monorepo workflow, which may require training and adjustments to existing processes.\n* **Tooling Complexity:** Managing a large monorepo effectively may necessitate utilizing specialized tools for code organization, dependency management, and build orchestration.\n\n**To address potential drawbacks, we will:**\n\n* **Investigate and implement appropriate tooling:** Evaluate and integrate tools specifically designed for monorepo management, such as Nx, Bazel, or Lerna.\n* **Establish clear code organization guidelines:** Define a well-structured directory layout and naming conventions to maintain code clarity and searchability.\n* **Optimize build and test processes:** Implement strategies like parallel builds, caching, and targeted testing to minimize build times and improve development efficiency.\n* **Provide comprehensive documentation and training:** Ensure that all developers understand the benefits, challenges, and best practices of working with a monorepo. \n\nBy proactively addressing potential challenges and leveraging the advantages of a monorepo, we can establish a more robust, maintainable, and scalable microservices architecture. \n","GenTime":"2024-08-28 12:54:15"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n","Decision":"None yet\\n","tokens":59,"id":4382,"text":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n\n\n##Decision\nNone yet\\n","MatchedIds":[1193,282,2122,4980,2500],"Prediction":"## Decision\n- **Upgrade**: Upgrade to the latest version of the CloudEvent Java SDK where this issue ([#196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196)) is addressed. \n- **Contribution**: If the issue persists in the latest version, consider contributing a fix to the CloudEvent Java SDK to eliminate the unnecessary work during conversion. This approach ensures a long-term solution for Occurrent and benefits the entire CloudEvent community. \n- **Workaround**: If upgrading or contributing are not immediately feasible, implement a temporary workaround within Occurrent. This could involve optimizing the conversion process or selectively disabling certain features that trigger the unnecessary work. However, this should be considered a temporary solution, and the root cause should be addressed in the CloudEvent SDK eventually. \n","GenTime":"2024-08-28 12:54:20"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"text":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n\n##Decision\nWe will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","MatchedIds":[996,5213,1767,1784,1786],"Prediction":"## Decision \n\nWe will use GOV.UK Notify to send notifications to suppliers using the Data Submission Service. \n\nThis approach is consistent with other CCS digital services and leverages the existing GOV.UK Notify infrastructure, eliminating the need to build and maintain a separate notification system.  \n\nUsing GOV.UK Notify offers several advantages:\n\n* **Cost-effective:** Email notifications are free.\n* **Scalable and reliable:**  GOV.UK Notify is a robust service designed to handle high volumes of notifications. \n* **Flexibility for the future:** The service supports both email and SMS, allowing us to add SMS notifications later if required.\n* **Integration with existing systems:** The API-first approach of GOV.UK Notify allows for seamless integration with the Data Submission Service.\n\nThis approach also ensures a consistent experience for suppliers across different CCS services that utilize GOV.UK Notify. \n","GenTime":"2024-08-28 12:54:25"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"text":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n\n\n##Decision\nWe will build cloud-native solutions.\\n","MatchedIds":[2723,4937,631,1182,2062],"Prediction":"## Decision\nWe will adopt a cloud-native architecture for the application. This will allow us to leverage serverless computing and software-defined infrastructure to streamline our development process and focus on delivering business value. \n","GenTime":"2024-08-28 12:54:28"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"text":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 12:54:31"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"text":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[1854,1416,2687,3827,1855],"Prediction":"## Decision\n\nWe will favor the use of arrow functions over traditional function expressions throughout the codebase. This promotes conciseness and readability, especially in situations involving callbacks and functional programming paradigms. However, developers should be mindful of the differences in behavior and scoping compared to traditional functions, particularly regarding the `this` keyword.\n","GenTime":"2024-08-28 12:54:35"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"text":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n\n##Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","MatchedIds":[425,4878,2403,3037,783],"Prediction":"## Decision\n\nTo streamline the process of building accessible forms compliant with the GOV.UK Design System, we will use the `govuk-formbuilder` gem.\n\nThis gem offers several advantages:\n\n* **Simplified form building:**  It provides helpers and abstractions to generate the correct HTML structure, class names, and ARIA attributes required by the GOV.UK Design System, reducing manual effort and potential for errors. \n* **Accessibility by default:** The gem ensures forms are built with accessibility in mind, aligning with the design system's best practices. \n* **Consistent validation error display:** It integrates with Rails' validation framework to display errors using the GOV.UK Design System pattern, improving usability and accessibility. \n\nWe will integrate `govuk-formbuilder` into our application and refactor existing forms to utilize its features. This will ensure consistency, reduce development time, and promote accessibility. \n","GenTime":"2024-08-28 12:54:41"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"text":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n\n\n##Decision\nEvery new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","MatchedIds":[2574,1979,4538,1956,2261],"Prediction":"## Decision\n\nWe will adopt NetworkManager for all new network requests, ensuring consistency across the codebase. \n\nTo manage the transition, we will:\n\n1. **Assess Impact:** Analyze the existing usage of the Fetchable protocol and identify potential challenges and effort required for migration.\n2. **Migration Plan:** Define a phased approach to gradually replace Fetchable with NetworkManager. This might include creating adapters for compatibility during the transition period.\n3. **Deprecation Strategy:**  Mark Fetchable as deprecated and provide clear documentation and examples for migrating to NetworkManager. \n4. **Monitoring & Evaluation:** Continuously monitor the impact of the transition and gather feedback from developers to address any issues that may arise. \n\nThis approach ensures a smooth transition while capitalizing on the benefits of NetworkManager for future development. \n","GenTime":"2024-08-28 12:54:46"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n\n##Decision\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","MatchedIds":[3053,389,334,381,362],"Prediction":"## Decision\n\n**Chosen option:** Adopt GraphQL and Apollo Client for the Settings Redesign project. \n\n**Reasoning:**\n\nWhile leveraging our existing REST architecture is entirely possible, GraphQL offers compelling advantages for the Settings Redesign project that align with our outlined decision drivers:\n\n* **Performance:** GraphQL's ability to precisely request only the data needed minimizes over-fetching, common in RESTful APIs, leading to reduced data transfer and potentially faster loading times. This granular control can significantly benefit performance, particularly in scenarios with limited bandwidth.\n* **Ease of Setup and Development Speed:** Apollo Client, coupled with GraphQL, streamlines client-side API interactions. The declarative nature of GraphQL queries simplifies data fetching and management within the React application, potentially leading to cleaner, more maintainable code. Additionally, features like caching provided by Apollo Client further enhance performance and development speed.\n* **Clarity and Tooling:** GraphQL schemas offer a clear contract for data structures and interactions between the client and server. This explicitness enhances code clarity, making it easier for developers to understand and work with the API. Furthermore, the robust tooling ecosystem around GraphQL and Apollo, including features like introspection and code generation, contribute to improved developer experience and productivity.\n\n**Caveats:**\n\n* **Initial Setup:** Implementing GraphQL and Apollo Client will require an initial investment in setting up the GraphQL server and defining the schema. However, this upfront effort can be outweighed by the long-term benefits in terms of development speed, maintainability, and performance.\n* **Learning Curve:** There will be a learning curve associated with GraphQL and Apollo for the development team. However, the well-documented nature of both technologies, combined with their widespread adoption, makes this learning curve manageable.\n\n**Implementation Details:**\n\n* We will use Apollo Server to implement the GraphQL API, connecting to our existing backend services and databases.\n* The Settings Redesign application will use Apollo Client to interact with the GraphQL API.\n* Strong typing will be enforced through TypeScript and GraphQL schema definitions to ensure data consistency and improve developer experience. \n\n**Conclusion:**\n\nAdopting GraphQL and Apollo Client for the Settings Redesign project aligns with our goals for performance, developer experience, and long-term maintainability. This approach positions us to build a more efficient, scalable, and developer-friendly application.\n","GenTime":"2024-08-28 12:54:59"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"text":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n\n\n##Decision\nReverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","MatchedIds":[1317,4868,4245,3841,5171],"Prediction":"## Decision\n\n**Revert back to the previous ODE solver.** \n\nWhile the adjoint ODE solver offered performance gains, its instability and silent failures pose a significant risk to the reliability of the model. Ensuring correctness and stability takes precedence over performance optimizations in this case. \n\n**Additionally**:\n\n* **Investigate the instability:**  Allocate time to understand the root cause of the adjoint solver's instability. This may involve debugging, profiling, or consulting relevant literature\/experts. \n* **Explore alternative solvers:** Research and benchmark other ODE solvers that might offer a better balance between stability and performance.  Document these findings for future reference. \n","GenTime":"2024-08-28 12:55:03"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"text":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n\n\n##Decision\nGiven that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","MatchedIds":[4388,1012,8,2471,3416],"Prediction":"## Decision\n\nWe will leverage AWS Elastic MapReduce (EMR) to create a long-running Spark cluster. To optimize cost and performance, we will adopt the following strategies:\n\n* **Utilize a combination of instance fleets**: The cluster will use a core instance fleet for stable, consistent processing and a spot instance fleet for cost-effective scaling during peak loads. \n* **Implement EMR auto-scaling**: Configure auto-scaling policies based on YARN queue load and\/or custom metrics to dynamically adjust the number of instances within each instance fleet. \n* **Leverage Spot Instances**: Utilize a high percentage of Spot Instances, especially for the scaling fleet, to minimize costs. Implement appropriate interruption handling and fallback mechanisms to maintain processing resilience.\n* **Fine-tune Spark Configurations**: Optimize Spark configurations, such as executor memory, core allocation, and network settings, to maximize resource utilization and minimize processing bottlenecks. \n* **Consider Managed Spark Services**: Evaluate the potential of managed Spark services like AWS EMR Serverless or Databricks as alternatives to further simplify cluster management and potentially reduce costs. \n\nThis approach provides a balance between performance and cost by maintaining a baseline cluster capacity while allowing for elastic scaling based on workload demands. Continuous monitoring and refinement of the auto-scaling policies and Spark configurations will ensure optimal performance and cost efficiency. \n","GenTime":"2024-08-28 12:55:11"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"text":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n\n\n##Decision\n### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","MatchedIds":[839,1944,1495,4623,3768],"Prediction":"## Decision\n\nWe propose implementing a **Decentralization Incentive Mechanism** to disincentivize over-delegation to large validators and promote a more decentralized validator set. This mechanism will consist of two key components:\n\n**1. Dynamic Commission Adjustment:**\n\n* **Concept:** Validators will have their base commission rate dynamically adjusted based on their share of the total staked tokens.  \n* **Implementation:**\n    * Define a \"Centralization Threshold\" - a percentage of total staked tokens beyond which a validator is considered overly centralized.\n    * Introduce a \"Centralization Penalty\" - a dynamically calculated percentage increase to a validator's commission rate based on how far they exceed the Centralization Threshold. \n    * Validators exceeding the Centralization Threshold will have their effective commission rate increased by the Centralization Penalty. This penalty will decrease as their stake share moves closer to the threshold.\n    * Conversely, validators below the threshold could receive a bonus, further incentivizing delegators to choose them.\n\n**2. Delegator Reward Adjustment:** \n\n* **Concept:**  Delegators delegating to validators exceeding the Centralization Threshold will receive slightly reduced rewards.\n* **Implementation:**\n    * Implement a \"Decentralization Multiplier\" - a value ranging from 0 to 1 applied to delegator rewards.\n    * Delegators choosing validators below the Centralization Threshold receive the full reward (multiplier = 1).\n    * Delegators choosing validators exceeding the threshold will have their rewards multiplied by a factor less than 1, with the reduction scaling with the validator's stake share above the threshold.\n\n**Additional Considerations:**\n\n* **Parameter Tuning:**  The Centralization Threshold, Centralization Penalty, and Decentralization Multiplier require careful calibration to find an optimal balance between incentivizing decentralization without excessively punishing existing large validators.\n* **Gradual Implementation:** To avoid sudden disruptions, the mechanism should be introduced gradually, allowing validators and delegators time to adapt their strategies.\n* **Network Monitoring:** Continuously monitor the effectiveness of the mechanism and fine-tune parameters as needed to ensure its long-term sustainability and alignment with network goals.\n\nThis Decentralization Incentive Mechanism aims to make delegators more aware of the impact of their delegation choices on network health. By making the cost of centralization more transparent, it encourages active participation in maintaining a more secure and resilient Proof of Stake network. \n","GenTime":"2024-08-28 12:55:23"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"text":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n\n\n##Decision\nWe are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","MatchedIds":[3094,3092,1193,2121,2122],"Prediction":"## Decision\n\nInstead of fetching all columns and timestamps for a row at once, we will modify the sweep implementation to:\n\n1. **Fetch a limited number of columns and timestamps per request.** This will prevent Cassandra from loading an excessive amount of data into memory at once. \n2. **Iterate through the results in a paginated fashion.** This allows us to process large datasets without overwhelming Cassandra or the sweep process itself.\n\nThis approach will allow sweep to handle rows with large values and high write frequency without encountering memory issues. We will carefully evaluate the performance implications of this change and adjust the pagination parameters as needed to optimize for both memory efficiency and sweep speed. \n","GenTime":"2024-08-28 12:55:27"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"text":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n\n##Decision\nWe will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","MatchedIds":[2181,2756,4021,2502,1180],"Prediction":"## Decision\n\nThe domain structure for the opg-data service will be based on **business capabilities** rather than specific products or applications. \n\nThis approach offers several advantages:\n\n* **Product Agnosticism:** Decoupling domains from specific products allows for flexibility and adaptability as new products and services are introduced or existing ones evolve.\n* **Consistency:**  A consistent domain structure across the opg-data service fosters clarity, reduces ambiguity, and improves communication among teams.\n* **Reusability:** Business capabilities often have commonalities that can be leveraged across multiple products and services.  A domain-driven approach encourages the identification and reuse of these common capabilities.\n* **Scalability:**  As the opg-data service grows, a domain-based structure provides a clear framework for organizing and managing new functionalities and services.\n\n### Identifying Business Capabilities\n\nWe will conduct a series of workshops involving domain experts, product owners, and technical stakeholders to identify and define the core business capabilities of the opg-data service.  These capabilities will represent the high-level functions that the service provides, such as:\n\n* **Data Management**\n* **User Authentication and Authorization**\n* **Reporting and Analytics**\n* **Integration Services**\n* **Search and Discovery**\n\n### Organizing Domains\n\nOnce the core business capabilities are identified, we will organize them into a hierarchical domain structure. This structure will likely consist of:\n\n* **Core Domains:** Represent the essential and foundational capabilities of the opg-data service.\n* **Supporting Domains:**  Provide functionalities that support the core domains.\n* **Generic Domains:**  Encompass cross-cutting concerns like logging, security, and communication.\n\n### Documentation\n\nThe identified business capabilities and the resulting domain structure will be thoroughly documented and maintained in a central repository.  This documentation will serve as a single source of truth for all teams working within the opg-data service. \n","GenTime":"2024-08-28 12:55:39"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"text":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n\n\n##Decision\nSticking to `yarn` for `web` for now.\\n","MatchedIds":[2924,3841,241,3497,1631],"Prediction":"## Decision\n\nRemove `yarn.lock` and standardize on using `npm` and `package-lock.json` for dependency management. This ensures consistency and avoids potential conflicts between the two lock files. \n\nWe should also add a pre-commit hook or CI check to prevent  `yarn.lock` from being reintroduced accidentally.\n","GenTime":"2024-08-28 12:55:45"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"text":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n\n\n##Decision\nWe will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","MatchedIds":[4359,4179,1258,3853,4868],"Prediction":"## Decision\nWe will use `pytest` as our testing framework for PySpark scripts due to its popularity and familiarity within the Python ecosystem. \n\nTo speed up development and provide clear examples within our tests we will utilize the `pyspark-test` library. This library offers the ability to create a local Spark session, allowing for faster iteration during development without relying on the full data platform environment. We will use this library's functionality to define input data within our tests and assert the output from our transformations matches expectations. \n\nThis approach of defining example input and expected output within our tests will serve as documentation for the PySpark scripts, making it clearer for developers to understand the intended behavior of each transformation.\n\nBy adopting this testing strategy we aim to reduce the feedback loop during development, leading to faster development of PySpark scripts and a higher likelihood of catching defects before they reach the staging environment. \n","GenTime":"2024-08-28 12:55:50"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"text":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n\n\n##Decision\nThe content of each shard will be in Markdown format.\\n","MatchedIds":[2508,4385,3568,1924,2968],"Prediction":"## Decision\n\nStore shards as files in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\n\n*   Markdown can be generated and edited using common command line tools. \n*   It is readable in the terminal using tools like `cat` or `less`.\n*   Basic formatting and links are supported.\n*   Many text editors provide syntax highlighting and preview capabilities for Markdown. \n","GenTime":"2024-08-28 12:55:53"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"text":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n\n\n##Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","MatchedIds":[211,210,3497,3915,4397],"Prediction":"## Decision\n\n**Option 2** will be implemented. While it requires a process for creating and managing SSH keys, it best addresses the decision drivers:\n\n* **Avoids Service User Costs:**  Aligns with the requirement of not incurring costs for Service User accounts.\n* **Secure Key Management:** Leverages Vault for secure storage and rotation of private keys.\n* **Automation Potential:**  The process of key creation, deployment, and Vault storage can be automated using Terraform, reducing manual overhead.\n\n**Justification:**\n\nAlthough Option 2 requires some initial setup and automation, it provides a robust and secure solution for long-term access to GitHub configuration repos. It leverages existing infrastructure (Vault) and avoids recurring costs or policy violations. \n\n**Action Items:**\n\n1. **Define Process:** Document the process for creating SSH key pairs, adding public keys to GitHub repos, and storing private keys in Vault.\n2. **Automate with Terraform:** Develop Terraform code to automate the key management process.\n3. **Implementation and Testing:** Implement the solution and thoroughly test key generation, deployment, and rotation. \n4. **Documentation:** Update relevant documentation with the chosen approach and procedures. \n","GenTime":"2024-08-28 12:56:07"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"text":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n\n\n##Decision\n1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","MatchedIds":[2440,2968,216,2997,631],"Prediction":"## Decision\n\u9274\u4e8e\u4ee5\u4e0a\u95ee\u9898\uff0c\u51b3\u5b9a\u8fdb\u884c\u6570\u636e\u5e93\u5347\u7ea7\u53ca\u67b6\u6784\u4f18\u5316\uff0c\u5206\u9636\u6bb5\u5b9e\u65bd\uff1a\n\n**\u7b2c\u4e00\u9636\u6bb5 - \u89e3\u51b3\u6027\u80fd\u74f6\u9888\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09**\n\n1. **\u66ff\u6362\u78c1\u76d8\uff1a** \u5c06\u6570\u636e\u5e93ECS\u5b9e\u4f8b\u7684\u666e\u901a\u4e91\u76d8\u66ff\u6362\u4e3a\u66f4\u9ad8IOPS\u7684\u4e91\u76d8\uff0c\u4f8b\u5982ESSD\u4e91\u76d8\uff0c\u4ee5\u89e3\u51b3IOPS\u74f6\u9888\u95ee\u9898\u3002\n2. **\u8bfb\u5199\u5206\u79bb\uff1a**  \u5229\u7528\u73b0\u6709\u4e00\u4e3b\u4e24\u4ece\u67b6\u6784\uff0c\u914d\u7f6e\u6570\u636e\u5e93\u8bfb\u5199\u5206\u79bb\uff0c\u5c06\u8bfb\u8bf7\u6c42\u5206\u6d41\u5230\u4ece\u6570\u636e\u5e93\uff0c\u964d\u4f4e\u4e3b\u5e93\u538b\u529b\u3002\u4e1a\u52a1\u7aef\u9700\u8981\u8fdb\u884c\u76f8\u5e94\u6539\u9020\uff0c\u533a\u5206\u8bfb\u5199\u8bf7\u6c42\u3002\n\n**\u7b2c\u4e8c\u9636\u6bb5 - \u63d0\u5347\u53ef\u7528\u6027\u548c\u53ef\u7ef4\u62a4\u6027**\n\n3. **\u6570\u636e\u5e93\u5347\u7ea7\uff1a** \u5c06\u6570\u636e\u5e93\u7248\u672c\u5347\u7ea7\u81f3\u8f83\u65b0\u7248\u672c\uff08\u4f8b\u5982MySQL 8.0\uff09\uff0c\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5e76\u89e3\u51b3\u65e7\u7248\u672c\u5b58\u5728\u7684\u5df2\u77e5\u95ee\u9898\u3002\n4. **\u9ad8\u53ef\u7528\u67b6\u6784\uff1a**  \u5f15\u5165\u6570\u636e\u5e93\u9ad8\u53ef\u7528\u65b9\u6848\uff0c\u4f8b\u5982\u642d\u5efa\u57fa\u4e8e\u6570\u636e\u5e93\u81ea\u8eab\u590d\u5236\u529f\u80fd\u7684\u4e3b\u5907\u5207\u6362\u67b6\u6784\uff0c\u6216\u91c7\u7528\u4e91\u5382\u5546\u63d0\u4f9b\u7684\u6570\u636e\u5e93\u9ad8\u53ef\u7528\u670d\u52a1\u3002\n5. **\u8fde\u63a5\u65b9\u5f0f\u4f18\u5316\uff1a**  \u4e1a\u52a1\u7aef\u4e0d\u518d\u4f7f\u7528IP\u76f4\u8fde\u6570\u636e\u5e93\uff0c\u6539\u4e3a\u901a\u8fc7\u57df\u540d\u6216\u8d1f\u8f7d\u5747\u8861\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u4fbf\u4e8e\u540e\u7eed\u7ef4\u62a4\u548c\u6269\u5c55\u3002\n\n**\u7b2c\u4e09\u9636\u6bb5 - \u957f\u671f\u4f18\u5316**\n\n6. **\u6570\u636e\u5e93\u76d1\u63a7\uff1a** \u5efa\u7acb\u5b8c\u5584\u7684\u6570\u636e\u5e93\u76d1\u63a7\u4f53\u7cfb\uff0c\u76d1\u63a7\u6570\u636e\u5e93\u6027\u80fd\u6307\u6807\u3001\u53ef\u7528\u6027\u3001\u9519\u8bef\u65e5\u5fd7\u7b49\uff0c\u53ca\u65f6\u53d1\u73b0\u5e76\u89e3\u51b3\u6f5c\u5728\u95ee\u9898\u3002\n7. **\u5b9a\u671f\u7ef4\u62a4\uff1a** \u5236\u5b9a\u6570\u636e\u5e93\u5b9a\u671f\u7ef4\u62a4\u8ba1\u5212\uff0c\u5305\u62ec\u6570\u636e\u5e93\u5907\u4efd\u3001\u6027\u80fd\u4f18\u5316\u3001\u5b89\u5168\u626b\u63cf\u7b49\uff0c\u4fdd\u969c\u6570\u636e\u5e93\u7a33\u5b9a\u8fd0\u884c\u3002\n\n**\u5b9e\u65bd\u5efa\u8bae**\n\n*  \u4f18\u5148\u89e3\u51b3IOPS\u74f6\u9888\u95ee\u9898\uff0c\u968f\u540e\u9010\u6b65\u8fdb\u884c\u5176\u4ed6\u4f18\u5316\u3002\n*  \u5347\u7ea7\u6570\u636e\u5e93\u7248\u672c\u548c\u67b6\u6784\u8c03\u6574\u9700\u8fdb\u884c\u5145\u5206\u6d4b\u8bd5\uff0c\u786e\u4fdd\u4e1a\u52a1\u5e73\u7a33\u8fc7\u6e21\u3002\n*  \u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u5e93\u9ad8\u53ef\u7528\u65b9\u6848\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u914d\u7f6e\u548c\u6d4b\u8bd5\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u63aa\u65bd\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5f53\u524d\u6570\u636e\u5e93\u5b58\u5728\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6570\u636e\u5e93\u6027\u80fd\u3001\u53ef\u7528\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u4e3a\u4e1a\u52a1\u53d1\u5c55\u63d0\u4f9b\u7a33\u5b9a\u53ef\u9760\u7684\u6570\u636e\u652f\u6491\u3002\n","GenTime":"2024-08-28 12:56:18"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"text":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n\n\n##Decision\nWe will use Emotion as our CSS-in-JS library.\\n","MatchedIds":[5090,1125,157,1992,3035],"Prediction":"## Decision\n\nWe will use **Emotion** for styling our React components. \n\n### Justification\n\nWhile Styletron offers the smallest CSS output, its limitations in supporting common CSS features like nesting and sibling selectors outweigh its benefits.  \n\nEmotion provides a good balance between features and performance:\n\n- **Feature Rich:** Offers both styled components and direct style manipulation via the `css` prop. \n- **Smaller Footprint:** Its smaller size compared to Styled Components contributes to better performance.\n- **Familiarity:**  Its API is similar to Styled Components, making it easy to learn if we need to switch or onboard developers familiar with either library.\n\nThis choice allows us to write maintainable and performant styles while still allowing for expressive CSS when necessary. \n","GenTime":"2024-08-28 12:56:22"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"text":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n\n\n##Decision\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","MatchedIds":[583,3173,276,4096,4388],"Prediction":"## Decision\n\n**Chosen option:** **Gunicorn** \n\n**Reasoning:**\n\n* **Production Ready:** Gunicorn (Green Unicorn) is a widely used Python WSGI HTTP server known for its stability, performance, and robustness in production environments. \n* **Reverse Proxy Compatibility:**  Gunicorn is designed to work seamlessly behind reverse proxies like Nginx and Apache. This setup is standard practice for serving Flask applications in production, offering benefits such as load balancing, security enhancements, and static file serving.\n* **Simple Flask Integration:** Gunicorn provides a straightforward way to serve Flask applications with minimal configuration.  Its WSGI compliance ensures compatibility with Flask.\n* **Performance:** Gunicorn's pre-fork worker model allows it to handle multiple requests concurrently, enhancing the performance of the Flask API.\n\n**Alternatives Considered:**\n\n* **Waitress:** Another capable WSGI server, but Gunicorn is generally preferred for its wider adoption and perceived performance advantages in production settings. \n* **uWSGI:** A high-performance option often used with larger deployments. However, for a Flask API where simplicity is desired, Gunicorn strikes a good balance.\n\n**Additional Considerations:**\n\n* The specific configuration of Gunicorn (number of workers, worker class, etc.) will depend on factors like expected traffic and server resources.\n* Implementing a process manager like Supervisor is recommended to manage the Gunicorn process, ensuring automatic restarts in case of failures. \n","GenTime":"2024-08-28 12:56:30"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"text":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n\n\n##Decision\nThe Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","MatchedIds":[2539,621,3144,2793,2968],"Prediction":"## Decision\n\nWe will update all LIC modules to utilize Terraform version constraints and minimum provider versions. This will improve security by preventing incompatibility issues with future Terraform and provider releases, as well as ensure consistent and predictable deployments. \n\nThis will involve:\n\n* **Defining acceptable Terraform version ranges:**  We'll establish a process to define and document acceptable version ranges for Terraform itself, considering factors like stability, feature requirements, and organizational upgrade cycles. \n* **Specifying minimum provider versions:** Each module will explicitly declare minimum required versions for all used providers. This ensures compatibility and leverages new features or bug fixes in a controlled manner.\n* **Documentation and communication:** The benefits and procedures related to version constraints will be communicated to developers. This includes updating documentation and potentially creating automated checks for compliance.\n\nWe acknowledge that this will require an initial effort to update existing modules. However, the long-term benefits of improved stability, security, and maintainability outweigh the initial investment.\n","GenTime":"2024-08-28 12:56:36"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"text":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n##Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","MatchedIds":[1660,1659,1167,4832,2520],"Prediction":"## Decision\nChosen option: \"Microsoft SQL\", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me. This leverages existing skills and infrastructure within the company. \n","GenTime":"2024-08-28 12:56:39"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"text":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n\n##Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","MatchedIds":[2184,779,1574,4644,3205],"Prediction":"## Decision\n\nChosen option: **Use custom HTTP verbs with clear and descriptive names, along with HATEOAS to guide API clients.**\n\n### Justification\n\nWhile REST doesn't explicitly define how to handle non-CRUD operations, using custom verbs provides several advantages aligned with the decision drivers:\n\n* **Flexibility**:  Custom verbs like `APPROVE`, `REJECT`, `TRANSITION`, `EXECUTE`, etc., offer a more expressive and flexible way to represent diverse actions compared to forcing them into CRUD operations. This avoids ambiguity, especially when dealing with actions having the same name but different contexts (e.g., two different state transitions named \"APPROVE\").\n* **Clarity**:  Well-named verbs improve API readability and understanding. It becomes immediately clear what action an endpoint performs, leading to a more intuitive developer experience.\n* **HATEOAS Integration**:  Include links in the response payload to guide clients on the available actions for a given resource state. For instance, after retrieving an order, the response could include links for \"cancel\", \"process\", \"ship\", reflecting the allowed transitions. \n* **Extensibility**:  New custom verbs and corresponding links can be easily added to the API as functionalities evolve without impacting existing endpoints.\n\n### Implementation Details\n\n* **Verb Naming:** Use uppercase, semantically meaningful verbs that clearly describe the action performed. Follow a consistent naming convention throughout the API.\n* **HATEOAS Implementation:**  Utilize a standard hypermedia format like HAL, JSON:API, or Siren to embed links in the response, indicating the URL and HTTP method for each available action.\n* **Documentation:**  Clearly document the purpose, expected input, and output for each custom verb endpoint in the API documentation.\n\n### Positive Consequences\n\n* Enhances API clarity and expressiveness.\n* Facilitates a self-documenting and discoverable API through HATEOAS.\n* Provides flexibility and extensibility for future actions.\n\n### Negative Consequences\n\n* Deviates from strict REST interpretations that discourage custom verbs.\n* Requires careful design and documentation to ensure consistency and understandability.\n\n### Conclusion\n\nThis approach strikes a balance between RESTful principles and practicality for handling complex operations. It prioritizes clarity, discoverability, and extensibility, ultimately contributing to a more robust and developer-friendly API.\n","GenTime":"2024-08-28 12:56:52"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"text":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n\n\n##Decision\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","MatchedIds":[3483,1119,4645,4766,1996],"Prediction":"## Decision\n**Chosen Option:** Nx by Nrwl \n\n**Rationale:** Nx offers a robust, flexible, and scalable solution well-suited for managing monorepos, particularly for teams with a JavaScript\/TypeScript focus.\n\n* **Low Complexity and Overhead:** Nx provides a streamlined development experience with sensible defaults and clear conventions. Its plugin ecosystem reduces boilerplate and enforces best practices. \n* **Fit for our Stack:**  Assuming a JavaScript\/TypeScript stack (given the prior examples), Nx seamlessly integrates.\n* **Optimized CI\/CD:** Nx excels in this area:\n    * **Dependency Graph:** Nx understands project dependencies, enabling targeted builds and tests of only affected parts, significantly speeding up CI.\n    * **Caching:** Nx's computation caching further accelerates CI by storing and reusing build artifacts, saving considerable time.\n* **Flexible:** Nx accommodates diverse project structures and doesn't enforce rigid architectural patterns, allowing gradual adoption and evolution.\n\n**Alternatives Considered:**\n* **Lerna:** Simpler and suitable for smaller monorepos, but might lack the advanced CI\/CD optimization features of Nx as projects scale.\n* **Yarn Workspaces\/npm workspaces:** Offer basic dependency management but may require more manual configuration and tooling for sophisticated CI\/CD pipelines.\n\n**Positive Consequences:**\n- Improved developer experience through faster builds, better code organization, and streamlined CI\/CD.\n- Increased development speed and efficiency.\n- Facilitated code sharing and consistency across projects.\n\n**Negative Consequences:**\n- A learning curve is involved, although Nx provides excellent documentation and resources.\n- Potential for over-engineering if not carefully managed.\n\n**Additional Notes:**\nThe decision assumes a JavaScript\/TypeScript-heavy stack. If the technology stack differs significantly, re-evaluation might be necessary. \n","GenTime":"2024-08-28 12:57:02"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"text":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n\n\n##Decision\nThe ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","MatchedIds":[8,672,1606,1434,317],"Prediction":"## Decision\n\nTo avoid the inefficiency of restarting the entire ETL process from scratch after an interruption, we will implement a **checkpointing** mechanism. \n\nThis will involve:\n\n1. **Dividing the ETL process into smaller, logical stages.**  Each stage will represent a unit of work that can be independently completed and potentially restarted.\n2. **Persisting the state of the ETL process at the end of each stage.** This state will include things like data processed, transformations applied, and any intermediate results. \n3. **Upon restart, the ETL process will identify the last successful checkpoint and resume execution from that point.** This eliminates the need to reprocess data that was already handled. \n\nThe specific implementation of checkpointing will depend on the tools and technologies used in the ETL pipeline. Options include leveraging database transactions, storing state information in a persistent queue, or utilizing features provided by the ETL framework itself. \n\nThis approach will significantly reduce the time required to recover from interruptions and ensure more robust and efficient ETL operations.\n","GenTime":"2024-08-28 12:57:07"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"text":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n\n\n##Decision\nScratch images\/sprites and image\/sprite related blocks are not supported.\\n","MatchedIds":[2634,2631,2629,5198,4308],"Prediction":"## Decision\n\nWe will not support images\/sprites in the ftduino extension for Scratch. \n\nWhile Scratch supports images\/sprites, the ftduino hardware lacks a display capable of rendering them. Attempting to emulate this functionality through alternative means would likely be resource-intensive and detrimental to the ftduino's core functionality. \n","GenTime":"2024-08-28 12:57:12"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"text":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n\n\n##Decision\nAt a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","MatchedIds":[5178,4245,5172,239,4234],"Prediction":"## Decision\n\nTo enable interoperability between Python and Matlab within PACE, whilst maintaining performance and avoiding conflicts between Python interpreters, we will adopt the following approach:\n\n1. **Inter-Process Communication (IPC):** Communication between the Matlab and Python components of `pyHorace` will be achieved using inter-process communication mechanisms. This avoids launching multiple Python interpreters within the same process and allows both Matlab and the existing Python interpreter to run independently. \n\n2. **Message Passing:** A message passing protocol, such as ZeroMQ or TCP sockets, will be implemented to exchange data and commands between the Matlab and Python processes. \n\n3. **API Definition:** A well-defined API will be established for communication between Matlab and Python. This API will specify the format of messages, data types, and function calls, ensuring seamless interaction.\n\n4. **Performance Considerations:** The chosen IPC mechanism will be benchmarked to ensure minimal performance overhead. Asynchronous messaging and data serialization optimizations will be explored to maximize efficiency.\n\nThis approach provides several advantages:\n\n* **Avoids Interpreter Conflicts:** By running Python and Matlab in separate processes, conflicts arising from multiple interpreters are eliminated.\n* **Performance:** IPC mechanisms can achieve high throughput, minimizing the performance penalty compared to `OutOfProcess` execution.\n* **Flexibility:**  This approach enables flexible integration of existing Python libraries and tools into the PACE ecosystem.\n\nFurther investigations into suitable IPC libraries and performance optimizations will be conducted during the implementation phase. \n","GenTime":"2024-08-28 12:57:21"}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"text":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n\n##Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","MatchedIds":[4539,3159,1319,4656,1924],"Prediction":"## Decision\n\nWe will use a custom `Result<T, OckamError>` type for error handling throughout the Ockam library and its extensions. \n\nHere's how this addresses the requirements:\n\n**Agile and Convenient:**\n\n* **Custom Error Type:** Define `OckamError` as an enum with variants representing different error categories. This allows for fine-grained error handling tailored to Ockam's needs. \n* **Contextual Information:** Each variant can hold relevant data (e.g., error codes, internal messages) to aid debugging and provide useful information to users.\n* **Error Trait Implementations:** Implement standard traits like `std::error::Error`, `From`, and `Display` for `OckamError`. This ensures seamless integration with Rust's error handling ecosystem.\n\n**Portable:**\n\n* **Platform-Agnostic:** The enum-based approach avoids platform-specific error representations.\n* **Abstraction Layer:** The `OckamError` type acts as an abstraction, allowing underlying error types from different environments (e.g., network, cryptography) to be mapped to a consistent representation.\n\n**Developer and User-Friendly:**\n\n* **Clear Error Categories:** The enum variants provide a structured and understandable way to handle errors.\n* **Helpful Error Messages:**  Well-crafted error messages within each variant guide developers towards solutions.\n* **Error Propagation:** The `?` operator and the `From` trait streamline error propagation across function boundaries.\n\n**Example:**\n\n```rust\n#[derive(Debug)] \npub enum OckamError {\n    \/\/ Network errors\n    ConnectionFailed(String),\n    Timeout,\n    \/\/ Cryptographic errors\n    InvalidKey,\n    EncryptionFailed, \n    \/\/ ... other categories\n}\n\n\/\/ Implement std::error::Error for detailed error reporting \nimpl std::error::Error for OckamError { \n    \/\/ ... implementation ...\n}\n\n\/\/ Example usage\nfn send_message(data: &[u8]) -> Result<(), OckamError> {\n    \/\/ ... attempt to send data\n    if send_failed {\n        return Err(OckamError::ConnectionFailed(\"Unable to establish connection\".to_string()));\n    }\n    Ok(())\n} \n```\n\nThis approach provides a flexible, portable, and user-friendly error handling mechanism that can adapt to the diverse needs of the Ockam ecosystem. \n","GenTime":"2024-08-28 12:57:33"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"text":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n\n\n##Decision\nAxon Framework will be used as DDD library.\\n","MatchedIds":[1193,4896,2696,4741,1035],"Prediction":"## Decision\n\n**We will incorporate a dedicated DDD library to handle the creation and management of DDD building blocks.** \n\nThis decision is driven by the following factors:\n\n* **Reduced Boilerplate:** Using a DDD library eliminates the need for writing and maintaining custom classes for standard DDD constructs like entities, value objects, aggregates, and repositories. This frees up developers to focus on implementing business logic.\n* **Increased Maintainability:** DDD libraries typically provide well-tested and documented implementations of DDD patterns, contributing to a more stable and maintainable codebase.\n* **Improved Consistency:** A library enforces consistent implementation of DDD principles across the project, reducing the risk of inconsistencies and errors. \n* **Faster Development:** Leveraging pre-built components speeds up the development process, allowing teams to deliver value more rapidly.\n\n**Library Selection:**\n\nThe choice of a specific DDD library will depend on the project's programming language, existing tech stack, and specific requirements.  Factors to consider include:\n\n* **Language Support:**  Ensure the library is compatible with the project's primary programming language.\n* **Features:** Evaluate the library's support for core DDD concepts like entities, value objects, aggregates, domain events, and repositories.\n* **Maturity and Community:** Opt for well-established libraries with active communities to ensure long-term support and access to resources. \n* **Integration:** Assess how well the library integrates with the project's existing frameworks and tools.\n\n**Implementation Notes:**\n\n* **Gradual Migration:** If the project already has custom DDD classes, a phased approach to migration is recommended. Start by introducing the library for new features and gradually replace custom implementations over time.\n* **Training and Knowledge Transfer:** Ensure the development team is adequately trained on the chosen library and DDD principles to ensure effective adoption.\n\nBy incorporating a DDD library, we can streamline our development process, enhance code quality, and promote better adherence to DDD principles within the project. \n","GenTime":"2024-08-28 12:57:43"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"text":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n\n\n##Decision\nThe project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","MatchedIds":[1837,1510,3253,4021,2181],"Prediction":"## Decision\n\nGiven the lack of a unified data model, we opted to develop a custom RDF serialization for the existing SDBM database schema. This approach offered several advantages:\n\n* **Rapid Implementation:** Leveraging the existing database schema allowed us to quickly create an RDF export without waiting for a unified model to be finalized.\n* **Flexibility:** The custom serialization could be easily adapted as the MMM project's data model evolved.\n* **Comprehensive Data Export:** By mapping the existing schema, we ensured that all relevant SDBM data could be incorporated into the MMM project.\n\n### Positive Consequences\n\n* Enabled timely participation in the MMM project despite the absence of a predefined data model.\n* Provided a foundation for future alignment with a unified data model as it emerged.\n\n### Negative Consequences\n\n* Potentially increased complexity in merging SDBM data with data from other contributing organizations due to schema discrepancies.\n* Might necessitate future refactoring to align with a standardized data model adopted by the MMM project.\n","GenTime":"2024-08-28 12:57:50"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"text":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n\n\n##Decision\nWe've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","MatchedIds":[3144,621,4847,1464,631],"Prediction":"## Decision\n\n### Multiple AWS Accounts\n\nTo reduce blast radius and improve scalability, we will transition from a single AWS account for the Modernisation Platform to a multi-account strategy. This will involve:\n\n1. **Account Structure:**\n    * **Management Account:** Houses central resources like shared networking, security tooling, and account management tooling.\n    * **Development Account:** Dedicated to development and testing environments.\n    * **Production Account:** Hosts production-ready applications.\n    * **Additional Accounts:**  May be created for specific purposes like data isolation, compliance requirements, or individual service isolation.\n\n2. **Account Separation:**\n    * Teams will be assigned to specific AWS accounts based on their environment (development or production) and service boundaries.\n    * Resources within each account will be further isolated using tools like AWS Organizations and Service Control Policies (SCPs).\n\n3. **Automation and Tooling:**\n    * Implement Infrastructure-as-Code (IaC) tools like Terraform to manage the creation and configuration of accounts, resources, and permissions.\n    * Leverage CI\/CD pipelines to automate deployments and manage infrastructure changes across different accounts.\n    * Utilize account and resource tagging strategies for cost allocation, security auditing, and resource management.\n\n4. **Security and Access Control:**\n    * Implement a centralized Identity and Access Management (IAM) system to control access to AWS resources across all accounts.\n    * Use role-based access control (RBAC) to grant granular permissions based on job function and responsibilities.\n    * Enforce strong security policies, such as multi-factor authentication (MFA) and least privilege access, across all accounts.\n\n### Benefits of Multi-Account Strategy:\n\n* **Reduced Blast Radius:** Isolating applications and resources into separate accounts limits the impact of security incidents or operational errors.\n* **Improved Scalability:** Separating development, testing, and production environments allows for independent scaling and resource management.\n* **Enhanced Security Posture:** Enforcing granular access controls and security policies at the account level enhances overall security.\n* **Simplified Compliance:**  Segregating data and resources based on compliance requirements becomes easier with dedicated accounts.\n* **Improved Cost Management:** Clearer resource attribution and cost allocation are possible with account-level billing.\n\n\nThis multi-account strategy will be implemented incrementally, prioritizing applications and environments with the highest need for isolation and scalability. Regular reviews and adjustments will be made to the account structure and security policies as the Modernisation Platform evolves.\n","GenTime":"2024-08-28 12:58:03"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"text":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n\n\n##Decision\nAdd support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","MatchedIds":[3566,2337,3069,1193,3571],"Prediction":"## Decision\n\n**Option 1: Integrate with External Example Repositories**\n\n* **Description:** cljdoc could link to or embed examples from external repositories like GitHub, GitLab, or dedicated example hosting platforms. \n* **Rationale:** Leverages existing platforms and encourages community contribution. Reduces duplication of effort in maintaining examples.\n* **Implementation:** Define a standard format for referencing external examples within cljdoc's project metadata (e.g., `project.clj`, `deps.edn`). Implement functionality to fetch and display these examples within cljdoc.\n* **Pros:** Simple integration, leverages community efforts.\n* **Cons:** Relies on external platforms, might require additional tooling for those platforms.  \n\n**Option 2:  Built-in Example Hosting within cljdoc**\n\n* **Description:**  Provide a mechanism for library authors to directly include examples within their project documentation that would be hosted and displayed by cljdoc.\n* **Rationale:**  Offers a seamless and integrated experience for both library authors and users. Allows for tighter control over example presentation and consistency.\n* **Implementation:** Develop a specific syntax or structure for authors to include examples within their documentation. cljdoc would need to parse, store, and render these examples.\n* **Pros:** Tight integration, full control over presentation and experience.\n* **Cons:** Requires more development effort for cljdoc itself, potential for examples to become outdated if not maintained.\n\n**Decision:**\n\nA hybrid approach combining **Option 1 (integration with external repositories) as the initial step** and **Option 2 (built-in example hosting) as a potential future enhancement** is recommended. \n\n**Reasoning:**\n\n* Starting with external repository integration provides a faster path to delivering value to users with minimal development overhead for cljdoc. \n* It leverages the existing ecosystem and encourages community involvement in providing and maintaining examples. \n* As cljdoc matures, built-in example hosting can be explored to offer a more integrated and controlled experience, potentially addressing limitations identified during the initial phase. \n","GenTime":"2024-08-28 12:58:17"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n\n##Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","MatchedIds":[3602,3604,3917,74,2968],"Prediction":"## Decision\n\nWe will use a managed Docker Image Registry service from a cloud provider (e.g., AWS ECR, Azure Container Registry, Google Container Registry). This approach minimizes the maintenance burden on our IT resources, as the cloud provider is responsible for the uptime, scaling, and security of the service. \n\nThis approach offers several advantages:\n\n* **Reduced IT overhead**: We offload the management and maintenance of the registry to the cloud provider. \n* **Improved scalability and performance**: Cloud providers offer robust, scalable, and globally distributed registries.\n* **Enhanced security**: Cloud-based registries often come with built-in security features like vulnerability scanning and image signing. \n* **Fine-grained access control**:  We can leverage the cloud provider's identity and access management (IAM) solutions to control access at both the 'internal' and 'protected' tiers. \n\nWe will carefully evaluate the features, pricing, and security offerings of each cloud provider to select the best fit for our specific requirements.\n","GenTime":"2024-08-28 12:58:22"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\\nWe have to make a decision on the branching strategy for development.\\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"text":"## Context\\nWe have to make a decision on the branching strategy for development.\\n\n\n##Decision\n[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","MatchedIds":[3171,4539,472,3657,2261],"Prediction":"## Decision\n\nWe will adopt a **trunk-based development** strategy with **short-lived feature branches**. \n\nThis means:\n\n* All developers work primarily on the `main` branch, integrating their changes frequently.\n* Feature branches are created for specific features or bug fixes, but are kept small and merged back into `main` as soon as possible (ideally, within a day or two). \n* Continuous Integration (CI) is crucial for this approach, with automated tests running on every push to the `main` branch to ensure stability and catch regressions early.\n\n**Reasoning:**\n\n* **Faster delivery:** Trunk-based development encourages frequent integration, reducing merge conflicts and integration issues, leading to a faster development cycle.\n* **Improved collaboration:**  Working on a shared branch promotes visibility and collaboration among developers.\n* **Reduced risk:** Smaller, more frequent releases reduce the risk associated with large, infrequent deployments.\n* **Better alignment with Continuous Integration and Continuous Delivery (CI\/CD):**  Trunk-based development is essential for implementing effective CI\/CD pipelines.\n\nWe believe this approach strikes a balance between flexibility and stability, enabling us to deliver value quickly and iteratively while maintaining a healthy and reliable codebase. \n","GenTime":"2024-08-28 12:58:31"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"text":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n\n##Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","MatchedIds":[61,2405,1252,2397,5157],"Prediction":"## Decision\n\nWe will use a readily available and easy-to-integrate authentication service like Auth0 for our initial development and testing phases. This will allow us to quickly build and iterate on the application without being blocked by the integration with SRS and G-Suite. \n\nOnce the application's core functionalities are mature and stable, and the integration with SRS and G-Suite is feasible, we will migrate to these services as our primary authentication methods for suppliers and CCS staff respectively. \n\nThis two-phased approach balances the need for rapid development with our long-term goal of using the standard CCS user sign-on services. \n\n### Positive Consequences\n\n* Faster development cycle in early stages.\n* Ability to test and iterate application without waiting for complex integrations.\n* Reduced administrative overhead in the long run by using existing CCS services.\n\n### Negative Consequences\n\n* Additional development effort required for migration to CCS services later.\n* Potential for inconsistency in user experience between the initial and final authentication methods.\n\n### Mitigation\n\n* Design the application's authentication layer with a clear separation of concerns to facilitate future migration. \n* Use clear communication with users about the planned change in authentication methods. \n","GenTime":"2024-08-28 12:58:39"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"text":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n\n##Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","MatchedIds":[1289,1309,1307,1318,1292],"Prediction":"## Decision\nDefine an order in which keys are:\n- iterated \n- looked up (for maps)\n\nThe order should be first based on their position in the array\/map. Afterwards, the standard key name ordering can be used to resolve ambiguities, e.g., two elements with the same index.\n\nThe defined order needs to be:\n- deterministic\n- documented\n- tested \n\nApplications must follow the defined order for consistent behavior.\n","GenTime":"2024-08-28 12:58:44"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"text":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n\n\n##Decision\nUse XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","MatchedIds":[3827,4323,1240,3071,3715],"Prediction":"## Decision\nUse **JSON**. While not ideal for representing hierarchical data, its simplicity, native support in Node, and ease of editing outweigh the drawbacks. \n\nWe acknowledge that a tree structure is a more intuitive representation for bookmarks. However, the added complexity of XML parsing libraries and potential whitespace issues with YAML make JSON a more pragmatic choice for this application. \n","GenTime":"2024-08-28 12:58:51"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","MatchedIds":[118,3456,3328,4225,4179],"Prediction":"## Decision \n\nThe decision is to adopt GitHub Actions for CI in this repository. Given its impending general availability, tight integration with GitHub, and potential for broader workflow automation, GitHub Actions represents a strategic choice for the future. This repository's basic use case will serve as a low-risk introduction to the service.\n","GenTime":"2024-08-28 12:58:55"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"text":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n\n##Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","MatchedIds":[429,3374,1094,2183,1797],"Prediction":"## Decision\n\nGiven the focus on Java Web stack and the educational purpose of the Online Bank application, we will adopt the following technology stack:\n\n**Backend:**\n\n* **Language:** Java \n* **Framework:** Spring Boot - provides a robust, well-documented, and easy-to-use platform for building web applications.\n* **Database:** PostgreSQL -  a powerful, open-source object-relational database system known for its reliability and data integrity. \n\n**Frontend:**\n\n* **Framework:** React - A popular JavaScript library for building user interfaces, offering flexibility and a rich ecosystem.\n* **Styling:** Material UI - A React component library implementing Google's Material Design, providing pre-built components for a consistent and visually appealing UI.\n\n**Tooling:**\n\n* **Build Tool:** Maven -  A widely used build tool for Java projects, providing dependency management and build automation.\n* **CI\/CD:** Jenkins - An open-source automation server for implementing continuous integration and continuous delivery pipelines.\n\n**Justification:**\n\nThis technology stack is chosen for the following reasons:\n\n* **Familiarity:** The Java Web stack is a widely adopted and understood technology, aligning with the educational goals of ODT. \n* **Ease of Use:** Spring Boot simplifies the development process with its auto-configuration and embedded server capabilities. \n* **Open Source:** Using open-source technologies like PostgreSQL, React, and Jenkins reduces costs and avoids vendor lock-in.\n* **Community Support:** All chosen technologies have large and active communities, providing ample resources and support. \n\nThis stack provides a solid foundation for building the Online Bank application while offering a valuable learning experience for participants of the ODT University classes. \n","GenTime":"2024-08-28 12:59:04"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"text":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n\n\n##Decision\nUse TypeScript 3.4.5\\n","MatchedIds":[1404,4224,2687,1302,5087],"Prediction":"## Decision\n\nWe will use TypeScript 3.4.5 for now. Although we are introducing a minor version mismatch between our used TypeScript version and the latest available,  using a version unsupported by the ESLint plugin could lead to unexpected issues and make debugging more difficult. \n\nWe will create a ticket to investigate upgrading to a newer version of the ESLint plugin that supports TypeScript 3.5.1 or higher when time allows, to benefit from the latest improvements and bug fixes. \n","GenTime":"2024-08-28 12:59:09"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"text":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n\n\n##Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","MatchedIds":[4847,4358,4937,3794,771],"Prediction":"## Decision\n\n### Orchestration & Tooling\n\nWe will use a combination of **Terraform** and **AWS Systems Manager (SSM)** to achieve a balance between automation and operator control during the bootstrapping process. \n\n1. **Terraform** will be responsible for:\n    * Provisioning the foundational AWS infrastructure (e.g., VPC, subnets, IAM roles, S3 buckets)\n    * Deploying core services (e.g., EC2 instances, RDS databases, load balancers)\n    * Outputting necessary information for subsequent steps (e.g., instance IDs, security group IDs) to be consumed by SSM.\n\n2. **AWS Systems Manager (SSM)** will be used for:\n    * Executing scripts or commands on provisioned EC2 instances for software installation, configuration, and deployment.\n    * Leveraging SSM Run Command's capability to provide real-time logging and status updates, allowing the operator to monitor the progress and identify potential issues.\n    * Defining manual approval steps in SSM Automation documents, enabling operators to validate the environment's state at critical junctures before proceeding.\n\n\n### Process Flow\n\nThe bootstrapping process will adhere to the following steps:\n\n1. **Initialization:** An operator triggers the Terraform script, providing any required variables (e.g., environment name).\n2. **Infrastructure Provisioning:** Terraform provisions the fundamental AWS infrastructure components.\n3. **Core Services Deployment:** Terraform deploys the core services necessary for the application to function.\n4. **SSM Orchestration:**  An SSM Automation document is invoked, utilizing the outputs from the Terraform deployment:\n    * **Software Installation & Configuration:** SSM Run Command executes scripts on EC2 instances to install dependencies, configure software, and deploy applications. Real-time logs are streamed to the SSM console.\n    * **Operator Validation:** The automation pauses at predefined steps, presenting the operator with the option to review logs, access instances, or perform manual checks. The operator can then approve continuation or halt for troubleshooting. \n    * **Data Restoration:** Once approved, the automation executes scripts to retrieve and restore data backups from designated sources.  \n5. **Finalization:**  After successful data restoration, the environment is considered operational. \n\n### Benefits\n\nThis approach offers several advantages:\n\n* **Automation:** Terraform automates the infrastructure setup and initial service deployment, while SSM streamlines software installation and configuration.\n* **Control & Visibility:**  SSM empowers operators with granular control and visibility through manual approval gates and real-time logging.\n* **Repeatability & Consistency:**  The combination of Terraform and SSM ensures consistent and repeatable environment bootstrapping.\n\n\nThis process effectively blends automation with manual intervention points, granting operators the ability to oversee and validate critical stages of environment provisioning while maintaining a high level of automation.\n \n","GenTime":"2024-08-28 12:59:24"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"text":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n\n\n##Decision\nOption 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","MatchedIds":[128,2859,697,2191,3831],"Prediction":"## Decision\n\nImplement a **federated repository management system** within the winery-repository. \n\nThis system will:\n\n* **Allow users to define and register remote TOSCA repositories** using their respective URLs. This registration process could involve verifying the repository's structure and compatibility.\n* **Provide a unified view of available TOSCA elements** by aggregating data from both the local and registered remote repositories.\n* **Enable transparent access to TOSCA elements**, regardless of their physical location. When a user requests an element, the system will determine its source repository and retrieve it accordingly.\n* **Implement caching mechanisms** to optimize performance and reduce network dependency for frequently accessed remote elements.\n* **Incorporate a synchronization mechanism (optional)** to periodically update the local cache with changes from registered remote repositories. This ensures users have access to the latest element versions.\n* **Consider security implications** by allowing administrators to control which remote repositories are allowed and potentially implementing authentication mechanisms for accessing specific repositories or elements. \n\nThis approach offers several advantages:\n\n* **Improved developer experience:** Easier access to a wider range of TOSCA elements.\n* **Flexibility and scalability:** Ability to incorporate and manage elements from various sources.\n* **Decoupling and modularity:** Reduced reliance on a single repository and easier integration with external systems.\n* **Potential for collaboration:** Facilitates sharing and reuse of TOSCA elements across different teams and organizations. \n","GenTime":"2024-08-28 12:59:32"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"text":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n\n\n##Decision\nEvery screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","MatchedIds":[2502,5082,2626,428,429],"Prediction":"## Decision\n\nWe will adopt the Model-View-Intent (MVI) architecture pattern for the Android application. This entails separating the UI (View) from business logic by introducing a unidirectional data flow based on Intents, Models, and View states.\n\nThis approach offers the following advantages:\n\n* **Improved testability:** By decoupling business logic from Android framework components, we can write pure Kotlin unit tests that run quickly on the JVM.\n* **Enhanced maintainability:**  MVI enforces a clear separation of concerns, making the codebase easier to understand, maintain, and evolve. \n* **Predictable state management:** The unidirectional data flow makes it easier to reason about application state and reduces the likelihood of bugs.\n\n### Consequences\n\n* **Increased code complexity:** MVI introduces new components and concepts, potentially increasing the learning curve for developers unfamiliar with the pattern.\n* **Potential for over-engineering:** For simple screens, the overhead of implementing MVI might outweigh its benefits.\n\n### Status\n\nAccepted\n\n### In Retrospect\n\nWe will closely monitor the implementation of MVI and re-evaluate its suitability based on factors like team familiarity, code complexity, and testability improvements. \n","GenTime":"2024-08-28 12:59:44"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"text":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n\n\n##Decision\nWe decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","MatchedIds":[4609,1193,4644,4610,4497],"Prediction":"## Decision\n\nTo ensure compatibility between the current SDK and the next-gen SDK while minimizing maintenance costs and ensuring ongoing functionality of existing scenes, we propose the following approach:\n\n**Phase 1: Bridge the Gap**\n\n1. **Develop a Compatibility Layer:** Create a new module within the next-gen SDK that acts as a bridge between the old and new architectures. This layer will translate calls from the `DecentralandInterface` to their equivalents in the next-gen SDK.\n2. **Maintain DecentralandInterface:**  Keep the `DecentralandInterface` as a public API endpoint, ensuring existing scenes remain functional.\n3. **Deprecation Warnings:** Issue clear deprecation warnings for all `DecentralandInterface` functions, encouraging developers to migrate to the next-gen SDK.\n\n**Phase 2: Incentivize Migration**\n\n1. **Feature Parity:**  Prioritize achieving feature parity between the `DecentralandInterface` bridge and the next-gen SDK. This incentivizes migration by offering compelling features and performance improvements in the new SDK.\n2. **Documentation and Support:** Provide comprehensive documentation and support resources outlining migration paths and best practices for adopting the next-gen SDK.\n3. **Community Engagement:** Actively engage with the developer community, addressing concerns and providing assistance throughout the migration process.\n\n**Phase 3: Long-Term Backwards Compatibility**\n\n1. **Evaluate Maintenance Costs:** Continuously assess the maintenance burden of the compatibility layer.\n2. **Sunset Plan:**  If maintenance costs become prohibitive, develop a clear sunset plan for the `DecentralandInterface` bridge, providing developers ample time to migrate to the next-gen SDK. \n\n**Key Considerations:**\n\n* **Versioning:** Utilize clear versioning for both SDKs, allowing developers to choose the appropriate version for their needs.\n* **Testing:** Implement rigorous testing throughout the development process, ensuring seamless compatibility and preventing regressions.\n* **Transparency:** Communicate openly with the community about the roadmap, timelines, and any potential breaking changes.\n\nThis approach prioritizes backwards compatibility while paving the way for a smooth transition to the next-gen SDK, ultimately fostering a sustainable and thriving ecosystem. \n","GenTime":"2024-08-28 12:59:56"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"text":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n\n\n##Decision\nWe choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","MatchedIds":[21,1957,1962,1558,28],"Prediction":"## Decision\n\nTo enable meta-transactions for users leveraging smart contract wallets, we will adopt a two-pronged approach:\n\n**1. Introduce a dedicated Relayer for Smart Contract Wallets:**\n\n*  A specialized relayer will be developed to handle meta-transactions originating from smart contract wallets. \n*  This relayer, upon receiving a meta-transaction request, will verify that the signing address is authorized by the smart contract wallet.\n*  Instead of directly sending the user's signature to the target contract, the relayer will generate a new signature. This new signature will authorize the smart contract wallet itself to execute the desired action on behalf of the user. \n*  This signature will be generated using the relayer's private key and will include necessary details such as the target function, parameters, and the original user's signature as proof of authorization.\n\n**2. Modify Smart Contracts to Accommodate Relayer:**\n\n*  Smart contracts that currently support meta-transactions will be updated to recognize and process transactions coming from the designated relayer. \n*  An additional check will be implemented within these contracts to verify the relayer's signature and ensure it accurately represents the user's intended action.\n*  Upon successful verification, the smart contract will execute the requested function, attributing the action to the smart contract wallet's address (the actual asset owner) rather than the individual signer.\n\n**Advantages of this approach:**\n\n* **Maintains Compatibility:** This solution avoids requiring widespread changes to existing meta-transaction infrastructure or demanding users to manage separate accounts for interactions.\n* **Enhanced Security:** By incorporating a dedicated relayer and signature verification steps, we enhance security and mitigate potential risks associated with directly handling user signatures on-chain.\n* **Improved User Experience:** Users with smart contract wallets can enjoy a seamless meta-transaction experience without needing to deviate from their existing wallet setup or understand the underlying complexities. \n\n**Implementation Details:**\n\n* A thorough security audit of the relayer's codebase will be conducted to guarantee the integrity and robustness of its signature generation and verification processes.\n* The relayer's public key will be registered and auditable on-chain to ensure transparency and trust.\n* Documentation and user guides will be updated to reflect the modified meta-transaction flow for smart contract wallets. \n","GenTime":"2024-08-28 13:00:08"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"text":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n\n\n##Decision\n### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","MatchedIds":[1318,13,2256,1310,1279],"Prediction":"## Decision\n\nThe crypto plugin should use a Key Derivation Function (KDF) to derive encryption keys and IVs from a master password provided by the user. \n\nHere's a breakdown:\n\n**Key Derivation:**\n\n- **Master Password:** The user provides a strong master password.\n- **Salt:**  A unique, randomly generated salt should be used for each KeySet to prevent rainbow table attacks. This salt should be stored alongside the encrypted data.\n- **KDF:** A robust KDF like Argon2id or PBKDF2 should be employed to derive both the encryption key and IV from the master password and salt.\n- **Key and IV Length:**  The KDF output should be sufficient to satisfy the requirements of the chosen encryption algorithm (e.g., 256 bits for AES-256). \n\n**Storage and Usage:**\n\n- **Never store the master password directly.**\n- The salt should be stored alongside the encrypted data.\n- Upon accessing a KeySet, the user provides their master password.\n- The plugin uses the stored salt and provided master password with the KDF to regenerate the encryption key and IV.\n- The derived key and IV are then used with the chosen cryptographic library (OpenSSL or libgcrypt) to decrypt the data.\n\n**Additional Considerations:**\n\n- **Library Choice:** The crypto plugin should use well-established and maintained cryptographic libraries like OpenSSL or libgcrypt.\n- **API Design:** The plugin's API should be designed to minimize the risk of misuse, encouraging secure defaults.\n- **Documentation:** Clear and concise documentation is crucial to guide developers on how to securely use the crypto plugin.\n\nThis approach leverages well-established cryptographic practices to provide a secure mechanism for key and IV management within the crypto plugin. \n","GenTime":"2024-08-28 13:00:19"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"text":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n\n##Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","MatchedIds":[1574,2520,4909,1510,4266],"Prediction":"## Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\nChosen option: **PostgreSQL**, because it's a very common technology, with a big community, a lot of learning resources, and great integration with most programming languages. It also delivers great performance for structured data.\n### Positive Consequences \n* Fast development due to ease of use and great documentation.\n* Lots of libraries and integrations available.\n* Good performance for structured data.\n### Negative Consequences \n* If data outgrows the capabilities of a single PostgreSQL instance, scaling can be more complex and require additional tools and infrastructure. \n","GenTime":"2024-08-28 13:00:24"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"text":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n\n##Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","MatchedIds":[4415,1742,3657,4061,1167],"Prediction":"## Decision\n\nBookit will utilize AWS RDS Aurora MySQL for its persistence layer.  Key factors in this decision include:\n\n* **SQL\/RDBMS Approach:**  The team consensus points to a relational database as a good fit for the application. \n* **Managed\/PaaS Solution:** Using RDS allows the team to focus on application development rather than database administration.\n* **Open Source and Cost Effective:**  MySQL meets the criteria of being both open source and freely licensed.\n* **Performance and High Availability:** While not strictly required at this time, the minimal cost increase for Aurora MySQL provides significant performance and availability benefits.\n* **PostgreSQL Deferral:** Although PostgreSQL is a viable option, the Aurora PostgreSQL offering is in Preview.  We will revisit this decision if\/when Aurora PostgreSQL becomes a production-ready service. \n","GenTime":"2024-08-28 13:00:29"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"text":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n\n\n##Decision\nOur recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","MatchedIds":[3789,4096,3794,1376,1669],"Prediction":"## Decision\n\nWe will migrate our Docker image building process from DockerHub to **Cloud.gov's own buildpack-based build system**. \n\n**Reasons:**\n\n* **Security:** Leveraging Cloud.gov's built-in build system eliminates the security concerns associated with third-party platforms like DockerHub. Cloud.gov's infrastructure and processes are already FedRAMP certified, simplifying the ATO process.\n* **Maintain Benefits of Buildpacks:** Cloud.gov's build system leverages buildpacks, allowing us to retain their benefits \u2013  pre-configured dependencies, simplified builds, and existing FedRAMP documentation.\n* **Integration:** Utilizing Cloud.gov's native build system streamlines the deployment process, ensuring tighter integration with the platform and potentially reducing operational overhead.\n\n**Implementation Considerations:**\n\n* **Migration Effort:** Assess the effort required to adapt existing Dockerfiles and build processes to Cloud.gov's buildpack system. \n* **Feature Parity:** Ensure Cloud.gov's buildpacks support all required dependencies and configurations currently used in the DockerHub builds.\n* **Documentation:** Update internal documentation to reflect the new build process on Cloud.gov.\n\n**Alternatives Considered:**\n\n* **Self-hosted Build System:** While potentially offering more control, this option introduces significant infrastructure management overhead and complexity in achieving ATO compliance.\n* **Other Third-Party Build Systems:**  Similar to DockerHub, security documentation and ATO compliance might be challenging.\n\nThis decision prioritizes security and regulatory compliance while preserving the efficiency and benefits of buildpacks. \n","GenTime":"2024-08-28 13:00:39"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"text":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n\n\n##Decision\nWe will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","MatchedIds":[847,1855,157,783,425],"Prediction":"## Decision\n\nThis ADR replaces Spectre CSS with Bootstrap CSS framework for the following reasons:\n\n* **Improved mobile responsiveness:** Bootstrap provides a robust grid system and pre-built responsive utility classes that will ensure the demo site renders well across different screen sizes. \n* **Enhanced form input experience:** Bootstrap styles form elements to be easily usable on mobile devices, addressing the specific concern with Spectre's rendering of buttons and fields.\n* **Ease of implementation:**  Migrating to Bootstrap will require minimal code changes due to its wide adoption and well-documented components.\n\nWe considered the following alternatives:\n\n* **Continuing to use Spectre CSS and manually overriding styles:**  This approach would be time-consuming, error-prone, and could lead to inconsistent styling across the application.\n* **Implementing a custom CSS solution:** This option would require significant development effort and might not offer the same level of responsiveness and cross-browser compatibility as a mature framework like Bootstrap.\n\nBy using Bootstrap, we can ensure a user-friendly experience on mobile devices for the pilot participants, enabling them to effectively evaluate the demo site. This decision aligns with the project's goal of providing a realistic preview of the wallet's future functionality and user interface. \n","GenTime":"2024-08-28 13:00:51"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\n\n##Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","MatchedIds":[1992,157,802,1240,783],"Prediction":"## Decision\n\n**Chosen Option:** Continue using SCSS modules with a structured approach, leveraging ITCSS principles and incorporating established best practices for organization and maintainability. \n\n**Reasoning:** \n\nWhile various CSS-in-JS libraries offer attractive features, the existing codebase and team familiarity point towards evolving our current SCSS implementation. This approach strikes a balance between leveraging existing knowledge and adopting a more robust, scalable CSS architecture.\n\n### Positive Consequences\n\n- **Leverages existing knowledge:**  The team's familiarity with SCSS minimizes the learning curve and allows for a smoother transition. \n- **Structured approach with ITCSS:**  Implementing ITCSS principles brings a clear organizational structure, promoting scalability, maintainability, and reducing style conflicts. \n- **Improved Reusability:**  SCSS modules encourage component-based styling, leading to more reusable and DRY code.\n- **Control and Customization:** Direct control over CSS provides flexibility in tailoring styles to specific design requirements, including the 8px grid system and Mozilla Protocol color palette. \n\n### Negative Consequences\n\n- **Potential for Style Bloat:** Without proper discipline and adherence to ITCSS, there's a risk of CSS bloat. \n- **Manual Optimization Required:** Unlike some CSS-in-JS solutions with built-in optimizations, this approach requires manual optimization for performance.\n\n### Implementation Details\n\n1. **Adopt ITCSS:** Organize SCSS files following the ITCSS methodology (Settings, Tools, Generic, Elements, Objects, Components, Utilities).\n2. **Enforce SCSS Modules:** Utilize SCSS modules for all component-level styles to encapsulate styles and prevent global conflicts.\n3. **Establish Naming Conventions:** Define clear and consistent naming conventions (e.g., BEM) for classes within modules.\n4. **Centralize Global Styles:**  Maintain a centralized location for global styles (like typography, resets) within the ITCSS structure. \n5. **Documentation and Examples:** Create clear documentation and examples to guide developers on the adopted structure and best practices.\n6. **Performance Considerations:** Implement performance optimizations like critical CSS extraction, minification, and efficient selector usage. \n\n### Future Considerations\n\n-  Re-evaluate CSS-in-JS solutions in the future, especially as the project scales or team expertise grows.\n-  Explore tooling and linters to enforce code style, conventions, and prevent potential CSS issues. \n","GenTime":"2024-08-28 13:01:03"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"text":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n\n\n##Decision\nUse a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","MatchedIds":[2515,4533,5163,3372,74],"Prediction":"## Decision\n\nWe will implement a hierarchical storage structure within preservation buckets to allow for duplicate file names and efficient object management. \n\nHere's the proposed structure:\n\n* **Bucket:** Each bucket represents a top-level container for preservation storage.\n* **Work ID:**  Within each bucket, a unique Work ID will be used as a directory to store all files associated with a specific Work object. \n* **FileSet ID:** Each FileSet associated with a Work object will have a unique FileSet ID assigned, creating a subdirectory within the Work ID directory.\n* **Filename:**  Files within a FileSet will retain their original filenames. Duplicate filenames within a FileSet are allowed.\n\n**Example:**\n\n```\npreservation-bucket\/\n  - <Work ID 1>\/\n    - <FileSet ID 1>\/\n       - file1.txt\n       - image.jpg\n    - <FileSet ID 2>\/\n       - file1.txt  \n       - document.pdf \n  - <Work ID 2>\/\n    - <FileSet ID 3>\/\n       - data.csv\n```\n\n**Benefits:**\n\n* **Duplicate Filenames:**  This structure allows for duplicate filenames within a Work object by separating files based on FileSet ID.\n* **Efficient Retrieval:**  Retrieving files is efficient as it requires traversing a well-defined path based on Work ID and FileSet ID.\n* **Organization:**  Provides a logical and organized way to manage a large number of files associated with various Work objects.\n* **Scalability:** The hierarchical structure ensures scalability as the number of Work objects and FileSets grows. \n\n**Implementation Details:**\n\n* The application will handle the generation and assignment of unique Work IDs and FileSet IDs.\n* We will leverage cloud storage APIs for upload, download, and management of objects within the defined structure. \n\nThis approach provides a robust and scalable solution for storing objects in preservation buckets while accommodating the need for duplicate filenames and efficient object management. \n","GenTime":"2024-08-28 13:01:14"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"text":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n\n\n##Decision\nUse Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","MatchedIds":[2619,923,2968,469,3657],"Prediction":"## Decision\nWe will use Debian as the base operating system for this project. This decision is driven by the following factors: \n\n* **Simplicity and Stability:** Debian's \"install and forget\" nature makes it ideal for a system where minimal intervention is desired. This aligns with our need for a low-maintenance solution. \n* **Customization and Control:** While Ubuntu's auto-installation is appealing, Debian's flexibility allows for greater control over the installation process. The ability to remaster the ISO with a preseed configuration provides a streamlined and reproducible one-button installation experience.\n* **Avoidance of Controversial Technologies:** The use of `snapd` in Ubuntu presents a potential concern. Opting for Debian allows us to sidestep this issue and maintain a more traditional and widely accepted package management approach.\n* **Previous Success and Familiarity:** Prior successful experience using Debian for this specific role instills confidence in its suitability. Leveraging existing knowledge and experience will contribute to a smoother implementation. \n\nThis decision prioritizes long-term stability, ease of maintenance, and avoids potential complications associated with Ubuntu's specific features and package management choices. \n","GenTime":"2024-08-28 13:01:25"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"text":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n\n##Decision\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","MatchedIds":[3109,861,631,3915,3147],"Prediction":"## Decision\n\nWe will implement a hybrid approach that leverages the advantages of both cluster-wide and namespaced operator models, while mitigating their respective drawbacks. \n\n**Here's how:**\n\n1. **Centralized Controller (Cluster-wide):** \n    - A single, lightweight controller will be deployed with cluster-wide read access. This controller will be responsible for:\n        - **Discovery and Inventory:** Identifying and keeping track of all namespaces designated for our application. This could be achieved using labels or annotations on namespaces.\n        - **Event Watching:** Observing events related to resources in managed namespaces (e.g., deployments, pods) to trigger necessary actions by namespaced operators.\n        - **Cross-Namespace Orchestration:** Managing cross-namespace features like enterprise license distribution, cross-cluster search\/replication. This controller won't directly manipulate resources in managed namespaces but will delegate those tasks to namespaced operators. \n    - **Minimal Permissions:** This controller will operate with minimal RBAC permissions, primarily read-only access for discovery and event watching. It will not require cluster-wide admin privileges.\n\n2. **Namespaced Operators:**\n    - Individual operators will be deployed in each namespace designated for our application. \n    - **Focused Responsibilities:** These operators will have admin permissions *only within their assigned namespace*. They will be responsible for:\n        - **Resource Management:** Directly managing application-specific resources (deployments, services, etc.) within their namespace.\n        - **Responding to Events:** Reacting to events from the centralized controller and performing necessary actions within their namespace.\n    - **Scalability and Isolation:** This approach allows for scaling by deploying additional operators as new managed namespaces are added. The isolation provided by namespaces ensures that one operator's performance doesn't impact others.\n\n**Rationale:**\n\n* **Scalability:** The centralized controller remains lightweight, ensuring scalability for large installations. Namespaced operators allow for horizontal scaling as needed.\n* **Security:** Least privilege principle is adhered to by granting the central controller only necessary read permissions. Namespaced operators only have admin rights within their designated namespace, minimizing the blast radius. \n* **Cross-Namespace Functionality:** The central controller handles cross-namespace concerns, while namespaced operators focus on individual deployments.\n* **Management Complexity:**  While slightly more complex than a purely cluster-wide or namespaced approach, this hybrid model offers a balance that addresses the identified drawbacks. \n\n**Positive Consequences:**\n\n* Improved scalability and resource efficiency.\n* Enhanced security through reduced permission requirements.\n* Support for cross-namespace features.\n* Clear separation of concerns and responsibilities.\n\n**Negative Consequences:**\n\n* Increased architectural complexity compared to a single-operator model.\n* Requires careful design of communication and coordination between the central controller and namespaced operators. \n\n**Mitigation:**\n\n* Employ clear API contracts and robust error handling for communication between components.\n* Implement comprehensive monitoring and logging to identify and resolve potential issues swiftly.\n* Document the architecture and interactions thoroughly for maintainability and knowledge sharing. \n","GenTime":"2024-08-28 13:01:39"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"text":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n\n##Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","MatchedIds":[2067,1193,2221,539,847],"Prediction":"## Decision\n\nTo support both web and existing clients with minimal disruption and code duplication, we will adopt a hybrid approach:\n\n**1. Create a Stateless Nimbus Core Library:**\n\n- Extract the core experiment evaluation and feature branching logic from the existing Nimbus SDK into a standalone, stateless Rust library (let's call it `libnimbus-core`). This library will be agnostic to platform or client type. \n- Implement UniFFI bindings for `libnimbus-core`, enabling usage from both Rust (existing clients) and Python (web service).\n\n**2. Develop a Separate Web Service (Cirrus):**\n\n- Create a new service called Cirrus, responsible for handling web-specific logic like:\n    - User enrollment and experiment assignment.\n    - Persistence of experiment state (if required).\n    - Communication with your existing experimentation platform.\n- Cirrus will utilize the Python bindings for `libnimbus-core` to leverage the experiment evaluation and feature branching logic. \n- Define a well-documented API for Cirrus, exposing endpoints like those listed in the problem statement (enroll user, fetch feature values, etc.).\n\n**3.  Adapt Existing Clients:**\n\n- Update existing clients to utilize the new stateless `libnimbus-core` directly. \n- Clients will be responsible for handling their own experiment state management (likely by persisting assignments locally) and communication with the appropriate backend services.\n\n**4. Deprecate Old SDK Functionality:**\n\n- Gradually deprecate the stateful components and platform-specific logic within the original Nimbus SDK. \n- Provide clear migration paths for existing clients to adopt the new architecture.\n\n### Justification:\n\n- **Reusability:**  This approach maximizes code reuse by centralizing the core experiment logic in `libnimbus-core`, accessible to both web and existing clients.\n- **Maintainability:** Separating concerns between `libnimbus-core` and Cirrus simplifies maintenance and allows for independent evolution of each component.\n- **Flexibility:** The stateless nature of `libnimbus-core` provides flexibility for various client architectures and future integrations. \n- **Backward Compatibility:** Existing clients can gradually transition to the new architecture, minimizing breaking changes.\n\n### Trade-offs:\n\n- **Increased Complexity:** The introduction of a new service and library adds some complexity to the overall architecture.\n- **Migration Effort:**  While minimized, some effort will be required to adapt existing clients to the new architecture. \n","GenTime":"2024-08-28 13:01:52"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"text":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n\n\n##Decision\nWe have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","MatchedIds":[2240,2243,2968,50,4705],"Prediction":"## Decision\n\nWe will adopt a phased approach to enable STRICT mTLS for all components:\n\n**Phase 1: Enable STRICT mTLS globally but exclude incompatible components**\n\n* **Global STRICT mTLS:** Configure the Istio mesh for STRICT mTLS mode.\n* **Namespace Exclusion:** Identify the namespaces where CAPI and Log-cache are deployed. Exclude these namespaces from STRICT mTLS enforcement using Istio's namespace-level configuration. This allows other components on the mesh to benefit from STRICT mTLS immediately. \n\n**Phase 2: Address incompatibilities in CAPI and Log-cache**\n\n* **CAPI:** \n    *  **Preferred Solution:** Collaborate with the CAPI team to adjust the init container to support mTLS connections, potentially by delaying database migrations until the sidecar is ready. Track progress on the existing issue ([https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12)).\n    *  **Alternative Solution (if collaboration is not feasible):** Explore options to disable mTLS specifically for the CAPI database connection. This is less secure but can be a temporary workaround until a proper fix is available. \n* **Log-cache:**\n    * **Investigate:**  Determine the root cause of the TLS incompatibility between Log-cache's existing configuration and Istio's mTLS. \n    * **Reconfigure:**  Adjust Log-cache's TLS settings to align with Istio's mTLS requirements. This might involve disabling Log-cache's own TLS setup and relying solely on Istio's mTLS.\n\n**Phase 3:  Enable STRICT mTLS for CAPI and Log-cache**\n\n* Once the incompatibilities are addressed, remove the namespace exclusions for CAPI and Log-cache. This enforces STRICT mTLS for all components on the mesh.\n* Thoroughly test the system after enabling STRICT mTLS for these components to ensure smooth operation. \n\n**Monitoring and Communication:**\n\n*  Throughout the process, actively monitor the system for any issues related to mTLS.\n*  Maintain clear communication with the relevant teams (CAPI, Log-cache) about the planned changes, timelines, and any potential impacts.\n\nThis phased approach prioritizes enabling STRICT mTLS for the majority of the system quickly while providing a roadmap to address the incompatibilities in CAPI and Log-cache. \n","GenTime":"2024-08-28 13:02:06"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"text":"## Context\\nAuthenticode signing of binaries.\\n\n\n##Decision\n* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","MatchedIds":[1962,2256,2968,2963,4119],"Prediction":"## Decision\n\n### General Approach\n\nBinaries distributed by the project SHOULD be Authenticode signed. This provides several benefits:\n\n* **Improved Security:** Signing verifies the publisher's identity and ensures the binary hasn't been tampered with after signing. This increases user trust and reduces the risk of malicious code execution.\n* **Reputation Management:** Signed binaries are less likely to be flagged by anti-virus software, improving the user experience and protecting the project's reputation.\n* **Windows SmartScreen:** Authenticode signing helps binaries pass Windows SmartScreen checks, further enhancing security and user confidence.\n\n### Implementation Details\n\n* **Signing Certificate:** A dedicated code signing certificate from a recognized Certificate Authority (CA) SHOULD be obtained. This ensures the certificate is trusted by operating systems and browsers.\n* **Timestamping:** Signed binaries SHOULD be timestamped. This extends the validity of the signature beyond the certificate's expiration date, preventing future issues with expired signatures.\n* **Signing Process:** \n    * A robust and secure signing process MUST be established. This includes storing the signing certificate securely, using strong passwords or hardware security modules (HSMs) for protection, and defining clear roles and responsibilities for signing.\n    * Consider using a dedicated build machine with restricted access for signing. \n* **Verification:** Users SHOULD be provided with instructions and tools to verify the authenticity of signed binaries.\n* **Documentation:** The project documentation SHOULD include information about the code signing policy, how users can verify signatures, and what to do in case of signature verification failures.\n\n### Exceptions\n\nWhile generally recommended, there might be exceptions where Authenticode signing is not feasible or desirable. These exceptions should be evaluated on a case-by-case basis and documented accordingly:\n\n* **Open Source Projects:** Distributing unsigned source code alongside signed binaries could be a suitable approach for open source projects, allowing users to build and verify binaries themselves.\n* **Development Builds:** Internal or development builds might not require signing if strict access controls and security measures are in place. However, it's good practice to sign all releases intended for public distribution.\n\n### Review and Update\n\nThis policy SHOULD be reviewed and updated periodically or whenever significant changes occur, such as switching to a new signing certificate or modifying the signing process. \n","GenTime":"2024-08-28 13:02:17"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"text":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n\n\n##Decision\nFirefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","MatchedIds":[3171,3653,1048,3229,3215],"Prediction":"## Decision\n\nWe will adopt the following versioning strategy for Firefox TV releases to ensure clarity and consistency:\n\n**Version Format:**\n\nWe will follow the semantic versioning scheme (SemVer) of `MAJOR.MINOR.PATCH`, aligning with industry standards and enhancing understanding.\n\n* **MAJOR:** Incremented for significant, backward-incompatible changes.\n* **MINOR:** Incremented for backward-compatible feature additions or enhancements.\n* **PATCH:** Incremented for backward-compatible bug fixes.\n\n**Release Channels:**\n\nWe will maintain distinct release channels for different build purposes:\n\n* **Nightly:** Continuously built from the latest source code. These builds are for testing purposes only and not for general use. Nightly builds will not follow the SemVer format.\n* **Beta:** Pre-release versions for broader user testing and feedback. They will use the SemVer format, incrementing the `MINOR` version for each new beta release.\n* **Release:** Stable and publicly available versions of Firefox TV. They will use the SemVer format, incrementing the `MAJOR` or `MINOR` versions based on the changes introduced.\n* **Long-Term Support (LTS):** We will designate specific release versions as LTS releases, providing extended support for a defined period. These versions will follow the standard SemVer format.\n\n**Rollback Strategy:**\n\nIn the event of critical issues requiring a rollback, we will release a new patch version (incrementing the `PATCH` version) based on the previous stable release branch. This ensures continuity and minimizes feature regressions.\n\n**Version Communication:**\n\nAll releases, including their versions and release channels, will be clearly documented in the Firefox TV release notes, available on the GitHub repository. We will also use this platform to communicate any deviations from the standard versioning or release processes.\n\n**Feature Rollback Handling:**\n\nIf features are temporarily rolled back in a specific version due to issues, this information will be explicitly mentioned in the corresponding release notes, explaining the rationale and expected reinstatement timeline.\n\nThis comprehensive versioning strategy for Firefox TV aims to provide transparency, consistency, and predictability for both developers and users. It will ensure better management of releases, simplified communication, and a smoother overall experience for everyone involved. \n","GenTime":"2024-08-28 13:02:30"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"text":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n\n\n##Decision\nAfter some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","MatchedIds":[697,1067,4167,5244,4695],"Prediction":"## Decision\n\n**Chosen option: Develop a command-line interface (CLI) tool as the MVP.** This tool will focus on the core functionality of generating Anki decks from RTK data. \n\n**Justification:**\n\n- **Simplicity and Focus:** A CLI tool allows for rapid development and iteration, focusing on the core functionality of data processing and deck generation without the overhead of a graphical user interface. \n- **Flexibility:** The generated deck can be easily imported into Anki, allowing users to leverage Anki's existing features for studying. \n- **Lower Entry Barrier:**  CLIs are generally easier to develop than GUIs, especially when working with a complex framework like QT. This allows for a quicker release and faster feedback loop.\n\n### Positive Consequences\n\n* **Faster Development:** Reduced scope leads to quicker delivery of a functional MVP.\n* **Clear Value Proposition:**  Demonstrates core functionality without distractions.\n* **Potential for Future Expansion:** The CLI can serve as a foundation for future development, such as a GUI or Anki plugin.\n\n### Negative consequences \n\n* **Limited User Appeal:** CLI tools may be less approachable for users unfamiliar with command-line interfaces.\n* **Potential for Feature Creep:**  It's important to strictly adhere to the MVP scope to avoid adding unnecessary complexity.\n\n\n### Follow-up Decisions Required\n\n* Define the specific commands and options for the CLI tool.\n* Determine the format and structure of the generated Anki deck.\n* Explore options for distributing the CLI tool to users. \n","GenTime":"2024-08-28 13:02:40"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"text":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n\n##Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n","MatchedIds":[2968,3497,4096,699,3604],"Prediction":"## Decision\n\nWe will implement a combination of the proposed ideas:\n\n**1. User Alignment Flag:** Introduce a flag (e.g., `--align-user`) to macondo commands. When enabled:\n    -   The command will run the container with the host user's UID and GID. This ensures files created\/modified within the container are owned by the host user.\n    -   The host user's HOME directory will be mounted to the container's HOME directory, ensuring consistent environment variables and configurations.\n\n**2. Simplified Working Directory Mounting:**  Along with the user alignment flag:\n    -  Macondo will automatically mount the host's current working directory (PWD) to a specific location inside the container (e.g., `\/mnt\/workspace`).\n    -  This makes the host's PWD immediately accessible within the container without manually specifying volume mounts.\n\n**Rationale:**\n\n- **Flexibility:** The flag allows users to choose whether they need user alignment or prefer the default Docker behavior.\n- **Ease of Use:**  Automatically mounting both the HOME directory and PWD simplifies the user experience, especially for commands intended to interact with files on the host system.\n- **Security:** Running containers as root should be avoided when possible. By aligning the user, we mitigate potential security risks associated with root-owned files on the host system. \n\n**Implementation Notes:**\n\n- The user alignment feature will require macondo to access and use the host user's UID\/GID.\n- Clear documentation and examples will be provided to guide users on leveraging this feature effectively. \n","GenTime":"2024-08-28 13:02:49"}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"text":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n\n\n##Decision\nWe removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","MatchedIds":[3827,3806,425,1576,3841],"Prediction":"## Decision\n\nTo avoid future versioning conflicts, we will standardize how we include Bootstrap in our projects. \n\nMoving forward, Bootstrap will be included **solely as an npm package**. This eliminates the dual-dependency issue and allows us to update all frontend dependencies consistently using a single package manager. \n\nThis approach simplifies dependency management and reduces the risk of encountering similar security vulnerabilities in the future. We will remove the Bootstrap gem from our projects and migrate any existing functionality to leverage the npm package.\n","GenTime":"2024-08-28 13:02:54"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n\n\n##Decision\nWe will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","MatchedIds":[2740,2963,2181,1618,1180],"Prediction":"## Decision\n\nWe will utilize the **Uniform Resource Locator (URL)** format for all resource identifiers. URLs, as a subset of URIs, offer a widely recognized and standardized structure that ensures uniformity and extensibility through its hierarchical naming scheme (e.g., `https:\/\/`).  \n\nThis decision aligns with common web practices and leverages existing infrastructure and tooling designed for URL parsing and resolution.\n","GenTime":"2024-08-28 13:02:57"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"text":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n\n\n##Decision\nThis is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","MatchedIds":[1417,85,1239,1431,2968],"Prediction":"## Decision\n\nThis ADR defines the \"**Home Assistant Yellow**\" installation method as **supported**.\n\n### Home Assistant Yellow\n\n* **Description:** Home Assistant installed on aarch64 hardware, using the Home Assistant Yellow image.\n* **Target Users:** End-users who prioritize ease of use and a streamlined experience with pre-selected hardware. \n* **Maintenance:** Maintained and supported by the Home Assistant team. \n* **Release Cadence:** Follows the same release schedule as other supported installation methods (e.g., Home Assistant OS).\n* **Update Mechanism:** Automated updates delivered through the Supervisor.\n* **Add-ons:** Supports officially maintained add-ons that are compatible with the aarch64 architecture.\n\n### Justification\n\nThe Home Assistant Yellow offers a user-friendly out-of-the-box experience, catering to users who desire a simple setup. Its support for aarch64 architecture expands hardware compatibility and aligns with the increasing availability of powerful ARM-based devices. Official maintenance ensures stability and timely updates.\n","GenTime":"2024-08-28 13:03:03"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"text":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","MatchedIds":[3456,187,2393,4285,2471],"Prediction":"## Decision\n\nWe will evaluate alternative hosted CI\/CD solutions like CircleCI, GitHub Actions, and GitLab CI.  The evaluation will focus on the following criteria:\n\n* **Pipeline Flexibility:**  Ensure the chosen solution supports the desired pipeline stages and workflows without undue limitations.\n* **Docker Support:** Verify robust support for building, testing, and deploying Dockerized applications.\n* **Ease of Migration:** Consider the effort required to migrate existing Travis CI pipelines and integrations.\n* **Pricing and Scalability:** Evaluate the pricing model and ensure it aligns with our budget and future growth plans. \n\nBased on the evaluation results, we will select the most suitable hosted CI\/CD platform to replace Travis CI for our projects. \n","GenTime":"2024-08-28 13:03:08"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"text":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n\n\n##Decision\nWe decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","MatchedIds":[779,2829,1558,3192,2215],"Prediction":"## Decision\n\nThe update date of a sample **should** be updated when a relationship is created with it, even if it's the target of the relationship. This ensures data consistency and accurately reflects the last time a sample was modified, either directly or indirectly through relationship changes. \n\nThis decision addresses the inconsistency highlighted in BSD-1088 and promotes the following:\n\n* **Data Integrity:**  Maintaining accurate update timestamps for all entities ensures data integrity and provides a reliable audit trail.\n* **Consistency:**  Consistently updating timestamps across all operations, including relationship creation, avoids confusion and potential issues with data synchronization or dependency tracking.\n* **Traceability:**  Accurate timestamps are crucial for tracking data changes, understanding data lineage, and debugging potential issues.\n\nThe relationship inversion process should be modified to update the target sample's update date. This might require changes to the data model, database triggers, or application logic to ensure that the update timestamp is consistently updated whenever a relationship is created or modified. \n","GenTime":"2024-08-28 13:03:14"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"text":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n\n\n##Decision\nMake things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","MatchedIds":[1854,2298,1193,802,4539],"Prediction":"## Decision\n\n**Octagon components SHOULD be stateless wherever possible.**\n\n### Reasoning\n\n- **Promotes a declarative API:** Consumers can directly control the visual state of Octagon components through props, aligning with the \"config via props\" principle. This enhances clarity and predictability.\n- **Avoids state management conflicts:**  Relying on application-level state management for things like active navigation items ensures consistency and avoids potential issues with state duplication or becoming out of sync.\n- **Improves testability:** Stateless components are inherently easier to test, as their output is solely determined by their inputs (props). \n- **Simplifies Octagon's scope:**  By focusing on being a pure view layer, Octagon can be more lightweight and maintainable, leaving complex state management to the consuming applications.\n\n### Exceptions\n\nWhile the general rule is statelessness, limited internal state might be permissible for:\n\n- **Ephemeral UI state:**  For instance, a toggle switch might briefly track its own \"is-hovered\" state solely for visual feedback, without impacting the application's core state.\n- **Performance optimization:**  In rare cases, caching derived values within a component's state could offer performance benefits, provided it doesn't contradict the overall architecture. \n\nThese exceptions should be carefully evaluated to ensure they don't undermine the overall goal of a declarative, state-managed-by-consumer approach for Octagon. \n","GenTime":"2024-08-28 13:03:22"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"text":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","MatchedIds":[68,3853,1339,1418,3863],"Prediction":"## Decision\nSince there is no clear benefit to adopting `self` over `$this` for PHPUnit assertions, and given: \n\n- PHPUnit documentation itself uses `$this`.\n- `$this` is much more widely used than `self` in this context in the community.\n- All existing Infection code uses `$this`. \n\nThe decision is to **keep the usage of `$this` over `self`** in the codebase. While technically \"incorrect\", this usage does not break anything and promotes consistency with PHPUnit itself and the wider community.\n\n### Status\nAccepted ([#1061][1061])\n\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061 \n","GenTime":"2024-08-28 13:03:30"}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"text":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n\n\n##Decision\n1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","MatchedIds":[2459,215,4821,946,2968],"Prediction":"## Decision\n\n\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u8d44\u6e90\u4f7f\u7528\u4e0d\u5747\u8861\u548c\u8d44\u6e90\u6d6a\u8d39\u7684\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u91c7\u53d6\u4ee5\u4e0b\u63aa\u65bd\uff1a\n\n1. **\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u5206\u6790 & \u8fc1\u79fb\u8ba1\u5212:** \n    * \u5bf9\u6240\u6709\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u4f7f\u7528\u7387\u8fdb\u884c\u8be6\u7ec6\u76d1\u63a7\u548c\u5206\u6790\uff0c\u786e\u5b9a\u9ad8\u4f4e\u8d1f\u8f7d\u673a\u5668\u7684\u670d\u52a1\u7c7b\u578b\u3002\n    * \u5236\u5b9a\u8fc1\u79fb\u8ba1\u5212\uff0c\u5c06\u4f4e\u8d1f\u8f7d\u673a\u5668\u4e0a\u7684\u670d\u52a1\u8fc1\u79fb\u81f3\u9ad8\u8d1f\u8f7d\u673a\u5668\uff0c\u6216\u5c06\u9ad8\u8d1f\u8f7d\u670d\u52a1\u8fc1\u79fb\u81f3\u8d44\u6e90\u5145\u8db3\u7684\u673a\u5668\uff0c\u4ee5\u5747\u8861\u8d44\u6e90\u5229\u7528\u7387\u3002\n2. **\u8c03\u6574\u673a\u5668\u914d\u7f6e:**\n    *  \u9488\u5bf9 CPU \u4f7f\u7528\u7387\u4f4e\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u9ad8\u7684\u673a\u5668\uff0c\u8bc4\u4f30\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u5185\u5b58\u6216\u8c03\u6574\u670d\u52a1\u914d\u7f6e\u6765\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002\n    *  \u9488\u5bf9 CPU\u3001\u5185\u5b58\u3001\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\u7684\u673a\u5668\uff0c\u8bc4\u4f30\u662f\u5426\u53ef\u4ee5\u8c03\u6574\u914d\u7f6e\uff0c\u4f8b\u5982\u5347\u7ea7\u5185\u5b58\u3001\u66f4\u6362\u66f4\u9ad8\u6027\u80fd\u7684\u786c\u76d8\u7b49\u3002\n3. **\u4f7f\u7528\u66f4\u5408\u9002\u7684\u5b58\u50a8\u65b9\u6848:**\n    * \u5bf9\u4e8e\u6570\u636e\u5e93\u3001SVN \u7b49\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u5c06\u666e\u901a\u4e91\u76d8\u5347\u7ea7\u4e3a SSD \u4e91\u76d8\u6216\u5176\u4ed6\u9ad8\u6027\u80fd\u5b58\u50a8\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u670d\u52a1\u6027\u80fd\u3002\n4. **\u5efa\u7acb\u8d44\u6e90\u7533\u8bf7\u89c4\u8303:**\n    *  \u5236\u5b9a\u8d44\u6e90\u7533\u8bf7\u89c4\u8303\uff0c\u8981\u6c42\u7533\u8bf7\u8005\u63d0\u4f9b\u670d\u52a1\u7684\u9884\u8ba1\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\uff0c\u5305\u62ec CPU\u3001\u5185\u5b58\u3001\u786c\u76d8\u3001\u7f51\u7edc\u7b49\u65b9\u9762\u7684\u9700\u6c42\uff0c\u907f\u514d\u62cd\u8111\u95e8\u51b3\u5b9a\u914d\u7f6e\u3002\n    *  \u5f15\u5165\u8d44\u6e90\u4f7f\u7528\u8bc4\u4f30\u673a\u5236\uff0c\u5bf9\u7533\u8bf7\u7684\u8d44\u6e90\u8fdb\u884c\u5408\u7406\u6027\u8bc4\u4f30\uff0c\u907f\u514d\u8fc7\u5ea6\u914d\u7f6e\u548c\u8d44\u6e90\u6d6a\u8d39\u3002\n5. **\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u914d\u7f6e:**\n    *  \u6839\u636e\u670d\u52a1\u7684\u5b9e\u9645\u53d1\u5c55\u60c5\u51b5\uff0c\u5b9a\u671f\u8bc4\u4f30\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\uff0c\u5e76\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u3002\u5bf9\u4e8e\u77ed\u671f\u5185\u4e0d\u9700\u8981\u4f7f\u7528\u7684\u8d44\u6e90\uff0c\u53ef\u4ee5\u8003\u8651\u91ca\u653e\u6216\u964d\u4f4e\u914d\u7f6e\uff0c\u4ee5\u8282\u7ea6\u6210\u672c\u3002\n\n**\u5b9e\u65bd\u6b65\u9aa4:**\n\n1. **\u77ed\u671f\u76ee\u6807:**  \u4f18\u5148\u89e3\u51b3\u8d44\u6e90\u4f7f\u7528\u7387\u6781\u5ea6\u4e0d\u5747\u8861\u7684\u95ee\u9898\uff0c\u5c06\u9ad8\u4f4e\u8d1f\u8f7d\u673a\u5668\u4e0a\u7684\u670d\u52a1\u8fdb\u884c\u8fc1\u79fb\u6574\u5408\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u3002\n2. **\u4e2d\u671f\u76ee\u6807:**  \u6839\u636e\u670d\u52a1\u9700\u6c42\u8c03\u6574\u673a\u5668\u914d\u7f6e\uff0c\u4f18\u5316\u8d44\u6e90\u642d\u914d\uff0c\u5e76\u9010\u6b65\u5c06\u5173\u952e\u670d\u52a1\u8fc1\u79fb\u81f3\u9ad8\u6027\u80fd\u5b58\u50a8\u65b9\u6848\u3002\n3. **\u957f\u671f\u76ee\u6807:**  \u5efa\u7acb\u5b8c\u5584\u7684\u8d44\u6e90\u7ba1\u7406\u673a\u5236\uff0c\u5305\u62ec\u8d44\u6e90\u7533\u8bf7\u89c4\u8303\u3001\u8bc4\u4f30\u673a\u5236\u3001\u52a8\u6001\u8c03\u6574\u673a\u5236\u7b49\uff0c\u5b9e\u73b0\u8d44\u6e90\u7684\u5408\u7406\u5206\u914d\u548c\u9ad8\u6548\u5229\u7528\u3002\n\n\n\u901a\u8fc7\u4ee5\u4e0a\u63aa\u65bd\uff0c\u6211\u4eec\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5f53\u524d\u8d44\u6e90\u4f7f\u7528\u4e0d\u5747\u8861\u548c\u8d44\u6e90\u6d6a\u8d39\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u4e3a\u670d\u52a1\u7684\u7a33\u5b9a\u8fd0\u884c\u63d0\u4f9b\u4fdd\u969c\u3002 \n","GenTime":"2024-08-28 13:03:46"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"text":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n\n\n##Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","MatchedIds":[1949,4354,1956,1979,1985],"Prediction":"## Decision\n\nTo address the limitations of JSON encoding and lack of error handling in the communication between Tendermint and remote signers, the following changes will be implemented:\n\n1. **Switch from JSON to Protocol Buffers:** Instead of encoding messages as JSON strings, we will utilize Protocol Buffers (Protobuf) for serialization. This offers several advantages:\n    * **Efficiency:** Protobuf is a binary format, leading to smaller message sizes and faster encoding\/decoding compared to the textual JSON.\n    * **Hardware Wallet Compatibility:** Protobuf's structured format is better suited for hardware wallets, which often have limitations in parsing complex JSON structures.\n    * **Smart Contract Integration:** Protobuf's binary representation is more suitable for usage within Ethereum smart contracts.\n\n2. **Introduce Distinct Request and Response Messages:**  Instead of using the same message type for both requests and replies, we will create separate Protobuf messages for each direction. This will improve clarity and allow for specific fields in responses, such as error codes or status messages.\n\n3. **Implement Error Handling:** The new response messages will include a dedicated field for error codes and optional error messages. This allows remote signers to communicate errors encountered during the signing process back to Tendermint.\n\n**Implementation Details:**\n\n* Define new Protobuf messages for each request and response type (e.g., `VoteRequest`, `VoteResponse`, `ProposalRequest`, etc.). These messages will contain the necessary data fields and error handling fields.\n* Update the `privval\/socket.go` code to utilize the new Protobuf messages for encoding and decoding messages exchanged with remote signers.\n* Refactor the signing process to handle the new request\/response structure and propagate any errors received from the remote signer.\n\nThis approach provides a more robust, efficient, and secure communication protocol between Tendermint and remote signers, addressing the issues with JSON encoding and lack of error handling. \n","GenTime":"2024-08-28 13:03:56"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n\n##Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","MatchedIds":[1784,5213,1767,2523,996],"Prediction":"## Decision\n\nUse GovUK Notify for sending emails from Keycloak. This aligns with the decision to use GovUK Notify for other system emails and leverages the benefits of a centralized email service, such as cost-effectiveness and standardized delivery. \n","GenTime":"2024-08-28 13:03:59"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"text":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n\n\n##Decision\n- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","MatchedIds":[5009,1799,2098,138,1255],"Prediction":"## Decision\n\nWe will implement the \"carry over\" functionality by copying the existing \"Apply again\" approach, adapting it to handle the specific scenario of transitioning between recruitment cycles. \n\n### Rationale\n\nThis approach leverages existing functionality, minimizing development effort. It also provides a familiar user experience for candidates who have used the \"Apply again\" feature.\n\n### Implementation Details:\n\n1. **Eligibility Check:** Upon the conclusion of a recruitment cycle (September 18th, 2020), the system will identify eligible applications based on their status (excluding those under provider consideration or with active offers). \n2. **Application Copy:** Eligible applications will be copied, preserving all existing data, including references in their current state (not started, in progress, completed).\n3. **New Cycle Designation:**  Copied applications will be tagged as belonging to the new recruitment cycle. \n4. **Application Reset:** All copied applications, regardless of their previous Apply 1 or Apply again status, will be reset to an \"Apply 1\" state in the new cycle.\n5. **Course Choice:** Candidates will be prompted to select a course from the new cycle's offerings.\n6. **Notification:**  Candidates with eligible applications will be notified about the carry-over process and guided to complete their new applications. \n\n### Considerations:\n\n- **Data Integrity:**  Ensure that data copying maintains the integrity of candidate information and reference data.\n- **User Communication:** Provide clear and concise communication to candidates throughout the carry-over process to minimize confusion.\n- **Testing:**  Thoroughly test the implementation, focusing on various application states and reference scenarios to ensure smooth transitions between cycles. \n","GenTime":"2024-08-28 13:04:07"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"text":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n\n##Decision\nThe new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","MatchedIds":[1494,1500,1956,861,1982],"Prediction":"## Decision\nWe will adopt a clear separation of concerns by implementing a single, immutable global state within each Holochain agent. This state will be managed through a centralized, event-driven architecture, effectively decoupling modules and reducing implicit dependencies.  This approach will improve maintainability and testability while still accommodating the distributed nature of the Holochain network.\n","GenTime":"2024-08-28 13:04:10"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"text":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n\n##Decision\nOnly store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","MatchedIds":[1303,1293,1307,1306,1289],"Prediction":"## Decision\n\nIntroduce a **string interning mechanism** for key names within Elektra's core data structures. This approach would store each unique key name only once in a dedicated string pool.  Key objects would then reference these interned strings, reducing memory usage for long, repetitive key names.\n\n**Benefits:**\n\n- **Reduced Memory Consumption:**  Significantly decreases memory usage, especially in configurations with many keys sharing common prefixes.\n- **Improved Performance:**  Potentially faster key comparison and lookup operations due to pointer comparisons instead of string comparisons.\n\n**Considerations:**\n\n- **Implementation Complexity:** Requires the implementation and management of a string intern pool.\n- **Potential for Memory Fragmentation:**  A mitigation strategy (e.g., compacting the pool) may be necessary for long-running processes.\n\n**Evaluation Criteria:**\n\n- Measure memory consumption before and after implementation in typical use cases.\n- Benchmark performance impact on key manipulation operations. \n\n**Alternatives:**\n\n- **Short Key Name Aliases:** Introduce an optional mechanism to use short, generated aliases internally while preserving original key names for user interaction. This approach might be less effective than interning but simpler to implement.\n- **Data Structure Optimization:** Explore alternative data structures (e.g., tries) optimized for storing key-value pairs with potential for further memory and performance improvements.\n\nThis decision prioritizes memory efficiency while considering potential performance implications. The chosen approach and its impact should be thoroughly evaluated against the defined criteria. \n","GenTime":"2024-08-28 13:04:18"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"text":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n\n\n##Decision\nSince we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","MatchedIds":[4741,1031,2261,4699,4428],"Prediction":"## Decision\n\nChosen option: **Option 3: Each project has its own stream, with relational resources organized using multiple key-items.**\n\n**Reasoning:**\n\nWhile option 1 offers granular data fetching and storage efficiency, the potential for hundreds of queries for deeply nested structures makes it inefficient and complex. Option 2, on the other hand, minimizes queries but introduces significant storage and memory overhead due to duplicating the entire project tree for each change.\n\nOption 3 offers the best balance. By organizing relational resources within the project stream using multiple key-items, we achieve:\n\n* **Efficient querying:** Retrieving a specific resource or a subset of related resources can be done with a single, targeted query.\n* **Minimal storage footprint:** Updates only affect the modified resource within the project stream, minimizing data duplication. \n* **Manageable memory usage:**  Fetching the entire project tree is unnecessary, leading to lower memory consumption compared to option 2.\n\nThis approach ensures performant data retrieval, efficient storage utilization, and manageable memory footprint, making it the most suitable solution for managing project, subproject, and workflow data. \n\n**Additional Considerations:**\n\n* **Database choice:** This approach is particularly well-suited for document databases like MongoDB or Couchbase, which natively support storing and querying complex, nested data structures.\n* **Indexing strategy:** Implementing appropriate indexes on the key-items within the project stream is crucial for optimal query performance.\n* **Data consistency:** Consider implementing optimistic locking or similar mechanisms to handle concurrent updates to the project stream and maintain data consistency. \n","GenTime":"2024-08-28 13:04:27"}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n\n##Decision\nThe \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","MatchedIds":[1786,1767,5213,2410,996],"Prediction":"## Decision\n\nIt was decided to keep the Development GovUK Notify service in \"trial mode\", but make the other environments live.\n\nThis is because the Development environment will not be used to send large volumes of emails and is expected to remain below the \"trial mode\" thresholds. The other environments may be used more heavily, particularly during performance testing, so will need to be live. \n","GenTime":"2024-08-28 13:04:29"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"text":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n\n\n##Decision\nAs I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","MatchedIds":[3469,3440,2997,1701,1372],"Prediction":"## Decision\n\nWe will use the existing Grafana dashboard as a starting point to visualize etcd cluster metrics. This will allow us to quickly gain visibility into the cluster's performance without investing significant time in developing a custom solution. \n\nWhile the existing dashboard might not fully implement the USE and RED methods, we can iteratively adapt and extend it over time. We will prioritize adding key metrics related to USE (utilization, saturation, errors) for the underlying system and RED (rate, errors, duration) for the gRPC communication within the etcd cluster.\n\nThis approach balances the need for a quick solution with the long-term goal of having a comprehensive monitoring dashboard based on USE and RED principles. \n","GenTime":"2024-08-28 13:04:34"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"text":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\n\n##Decision\nWe chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","MatchedIds":[1855,3722,4361,3784,1193],"Prediction":"## Decision\nInstead of hard-coding the `max-height` property, we will dynamically calculate and set the `max-height` of the SubstepsList using Javascript. \n\n1. When the SubstepsList is toggled to be shown:\n    - We record the computed height of the SubstepsList.\n    - We set the `max-height` of the SubstepsList to this computed height.\n    - This allows the animation to expand the SubstepsList to its full height.\n\n2. When the SubstepsList is toggled to be hidden:\n    - We set the `max-height` of the SubstepsList back to `0`.\n    - This allows the animation to collapse the SubstepsList.\n\nThis approach provides the flexibility needed to accommodate dynamic content within the SubstepsList, such as ErrorMessages of varying heights, while still achieving the desired animation effect. \n","GenTime":"2024-08-28 13:04:39"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"text":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 13:04:41"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\\nState handling in React becomes very complicated when you have too many states\\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"text":"## Context\\nState handling in React becomes very complicated when you have too many states\\n\n\n##Decision\nWe implement Redux to handle state\\n","MatchedIds":[2298,2667,334,4596,1890],"Prediction":"## Decision\n\n- **For Local component state:** Continue using React's `useState` hook for simple state management within a component. However, refactor components with excessive local state into smaller, more focused components. This promotes reusability and simplifies state management.\n- **For Shared state across the application:**  Adopt a centralized state management solution like `recoil` or `zustand`. These libraries provide a more structured way to manage and share state across components, reducing complexity compared to prop drilling or heavily nested Context providers.  \n- **Evaluate the need for state:**  Before adding new state, consider if it's truly necessary or if it can be derived from existing state or props.  Can the component be stateless? Reducing overall state complexity is key. \n- **Caching for derived state:** Utilize memoization techniques like `useMemo` to cache expensive calculations and prevent unnecessary re-renders, especially when dealing with derived state.\n- **For Remote data fetching:** Utilize libraries like `@tanstack\/react-query` or `SWR` to handle caching, background updates, and error handling, simplifying data fetching logic and state management related to asynchronous operations.\n\nBy strategically applying these approaches, we can mitigate the complexity associated with managing numerous states in a React application, leading to a more maintainable and scalable codebase. \n","GenTime":"2024-08-28 13:04:48"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"text":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n\n##Decision\nThe use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","MatchedIds":[1924,3081,3469,3290,3488],"Prediction":"## Decision\n\nWe will use a distributed tracing system to track user requests across application layers. This will involve:\n\n* **Generating unique request IDs:** Each incoming request will be assigned a unique ID that is propagated through all downstream services and components.\n* **Instrumenting application code:** We will use a library or framework to instrument our application code to log events with the request ID, including:\n    * Incoming requests\n    * Outgoing calls to other services or databases\n    * Important application logic events\n* **Collecting and storing trace data:** We will choose a tracing backend to collect and store the generated trace data. This could be a managed service like AWS X-Ray, Jaeger, or an open-source solution like Zipkin.\n* **Visualizing and analyzing traces:** The chosen tracing backend should provide tools for visualizing and analyzing the collected traces. This will allow us to see the flow of requests, identify bottlenecks, and debug issues.\n\n**Chosen Approach:**\n\nWe will leverage **OpenTelemetry** for instrumenting our application code and propagating trace context. OpenTelemetry is a vendor-neutral standard that offers flexibility in choosing a tracing backend. This allows us to start with a simple solution and migrate to a more robust system as our needs evolve.\n\n**Benefits:**\n\n* **End-to-end request tracing:**  Allows us to trace a user request across multiple services and components.\n* **Performance analysis:** Identify performance bottlenecks by visualizing request flow and timings.\n* **Improved debugging:** Easier to identify the root cause of errors by following request paths.\n* **Flexibility and vendor neutrality:** OpenTelemetry allows us to switch tracing backends without significant code changes.\n\n**Considerations:**\n\n* **Instrumentation overhead:** Instrumenting application code adds some overhead. We need to carefully monitor performance and adjust instrumentation as needed.\n* **Data storage costs:** Storing tracing data can be expensive. We need to consider sampling strategies and data retention policies.\n\nThis approach provides a comprehensive solution for tracing user requests across our application. It provides valuable insights into application behavior, performance, and helps us to quickly identify and resolve issues. \n","GenTime":"2024-08-28 13:05:05"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"text":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n\n\n##Decision\nWe will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","MatchedIds":[4388,3497,929,279,2515],"Prediction":"## Decision\n\n**Implement a multi-queue Sidekiq setup to categorize and prioritize background jobs.**\n\nThis approach addresses the challenge of managing various job types with different priorities and resource requirements:\n\n1. **Categorization:** Create separate queues for each distinct category of background jobs:\n    * `user_requests`: For time-sensitive tasks initiated by users, like file ingestion or derivative generation.\n    * `cleanup`: For tasks related to maintaining data integrity after user actions.\n    * `metadata_refresh`: For periodic updates from external systems like Voyager\/PULFA.\n    * `bulk_ingest`: For large-scale content ingestion processes.\n    * `routine_maintenance`: For scheduled tasks like cleaning up guest users.\n    * `fixity_check`: For ensuring the integrity of stored files. \n\n2. **Prioritization:** Configure Sidekiq workers to prioritize queues based on their importance:\n    * Allocate more workers or higher process priority to the `user_requests` queue to ensure quick turnaround for user-facing actions.\n    * Balance worker allocation for other queues based on their frequency and resource consumption. For example, `fixity_check` might require fewer workers than `cleanup`.\n\n3. **Monitoring & Scaling:** Utilize Sidekiq's monitoring capabilities to track queue sizes and worker performance. This data can inform decisions to scale worker resources for specific queues as needed.\n\n**Advantages:**\n\n* **Improved performance:** Ensures that high-priority tasks are processed quickly, even during periods of high load from other job categories.\n* **Resource optimization:** Allows for fine-grained control over resource allocation for different job types.\n* **Enhanced maintainability:** Simplifies troubleshooting and performance tuning by isolating different types of jobs.\n\n**Implementation Notes:**\n\n* Use Sidekiq's queue naming conventions (e.g., `critical`, `default`, `low`) to assign queues to different priority levels.\n* Leverage Sidekiq's retry mechanism to handle transient errors gracefully within each queue.\n* Regularly review and adjust queue configurations based on evolving application needs and performance data. \n","GenTime":"2024-08-28 13:05:16"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"text":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n\n\n##Decision\nWe have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","MatchedIds":[745,4056,3104,3909,731],"Prediction":"## Decision \n\nWe have decided to adopt the following approach for externally accessible services, while minimizing our public footprint and maintaining security:\n\n**1. Public\/Private Subnets and Network Segmentation:**\n\n* All servers requiring external access will reside within a dedicated public subnet, separated from the main application servers in private subnets. \n* The Bastion server will remain the only access point to the private subnet, ensuring no direct external access to internal applications.\n\n**2. Load Balancing and Reverse Proxying:**\n\n* A load balancer will be provisioned in the public subnet, acting as the single entry point for external traffic. \n* The load balancer will be configured to forward traffic to the appropriate service instances in the public subnet based on pre-defined rules. \n* We will utilize a reverse proxy server, sitting behind the load balancer, to provide an additional layer of security and flexibility. This will handle SSL termination, request filtering, and caching to improve performance.\n\n**3. Secure Communication:**\n\n* All communication between the load balancer, reverse proxy, and backend services will be encrypted using TLS\/SSL certificates. \n* We will utilize strong cipher suites and regularly rotate certificates to maintain optimal security.\n\n**4. Firewall Configuration:**\n\n* Strict firewall rules will be implemented to allow only necessary traffic to reach the load balancer and subsequently the backend services.\n* Access to specific ports and protocols will be tightly controlled and regularly audited.\n\n**5. Service Discovery and Health Checks:**\n\n* The load balancer will be integrated with a service discovery mechanism to dynamically update its configuration as new service instances are launched or terminated. \n* Health checks will be implemented to ensure only healthy instances receive traffic, automatically removing unhealthy instances from the pool.\n\nThis approach will allow us to expose only necessary services, limit our attack surface, and maintain a secure environment for our applications. \n","GenTime":"2024-08-28 13:05:27"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"text":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n\n\n##Decision\nTo make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","MatchedIds":[4234,5171,5178,3203,3253],"Prediction":"## Decision\n\nTo mitigate the risks associated with tight coupling between Horace and Herbert, we will implement a Continuous Integration (CI) pipeline that builds and tests both projects together. This pipeline will:\n\n* **Build Herbert artifacts on every commit pushed to either the Horace or Herbert repositories.** \n* **Run Horace's integration and MATLAB tests using the newly built Herbert artifacts.**\n* **Alert developers of any breaking changes immediately.**\n\nThis approach ensures that any changes to Herbert are immediately validated against Horace, reducing the likelihood of integration issues and broken functionality. \n","GenTime":"2024-08-28 13:05:31"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"text":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n\n\n##Decision\nWe use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","MatchedIds":[631,4096,3920,3598,1879],"Prediction":"Please provide me with the content of the document \"2. Overall context and container platform.md\" so I can make an informed decision. I need the specific details outlined in that document to understand the context and problem statement. \n","GenTime":"2024-08-28 13:05:33"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"text":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n\n\n##Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","MatchedIds":[5172,5178,2951,241,78],"Prediction":"## Decision\n\nWe will use a Git branch-based approach for managing documentation versions. \n\n1. **Documentation branches:**  For each PACE project, a dedicated branch will be created for each major.minor release of the software (e.g., `v1.0-docs`, `v1.1-docs`). \n2. **Default branch documentation:** The `main` branch of each project's documentation will host the documentation for the latest released version.\n3. **Tagging Documentation Releases:**  When a new version of a PACE project is released, the corresponding documentation branch will be tagged with the full semantic version (e.g., `v1.0.1`, `v1.1.0`).\n4. **GitHub Pages Deployment:** GitHub Pages will be configured to build and deploy documentation from these version-specific branches, allowing users to easily access documentation for their desired version.\n\n**Rationale:**\n\n* **Clear Versioning:** This approach aligns documentation versions directly with the corresponding software releases.\n* **Simplified Maintenance:**  Managing documentation within the respective project repositories simplifies version control and avoids conflicts.\n* **Flexibility:**  Allows for potential future integration of Sphinx plugins like `sphinx-multiversion` for projects where it is suitable. \n","GenTime":"2024-08-28 13:05:40"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n\n\n##Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","MatchedIds":[996,2050,1767,2410,1786],"Prediction":"## Decision\n\nWe will use GOV.UK Notify to send email and SMS notifications for this service. This aligns with Hackney's existing usage of GOV.UK Notify, leveraging its benefits of reliability and cost-effectiveness.  \n\nThis decision should be documented in the Hackney API Playbook  (https:\/\/github.com\/LBHackney-IT\/API-Playbook) to promote consistency and knowledge sharing across Hackney's services. \n","GenTime":"2024-08-28 13:05:43"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"text":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n\n\n##Decision\n### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","MatchedIds":[78,4847,2968,85,699],"Prediction":"## Decision\n\n### Test Infrastructure\n\n* **Kubernetes-based:** Leverage a Kubernetes cluster for running E2E tests. This provides a production-like environment and scalability for testing various Dapr features. Options include:\n    * **Managed Kubernetes:** Utilize a managed Kubernetes offering like Google Kubernetes Engine (GKE), Azure Kubernetes Service (AKS), or Amazon Elastic Kubernetes Service (EKS) for simplified cluster management and scalability.\n    * **Ephemeral Clusters:** Use tools like KinD (Kubernetes in Docker) to spin up lightweight Kubernetes clusters on demand for testing, ensuring environment isolation and faster test execution.\n\n### Test Execution\n\n* **Test Runner:** Employ a robust testing framework like pytest or Go's built-in testing package for organizing, running, and reporting on E2E tests. \n* **CI\/CD Integration:** Integrate E2E tests into the CI\/CD pipeline to automatically execute tests on every pull request and before merging to the main branch. Popular CI\/CD tools like GitHub Actions, Jenkins, or CircleCI can be used.\n\n### Test Reporting\n\n* **Standardized Reporting:** Utilize a standardized format like JUnit XML for test results. This ensures compatibility with a wide range of reporting tools and dashboards.\n* **Dashboard Visualization:** Integrate with a test reporting dashboard like ReportPortal, TestRail, or a custom solution to visualize test results, track trends, and identify flaky tests.\n* **Failure Notifications:** Configure notifications to alert developers about test failures through channels like Slack, email, or issue trackers, enabling rapid response and issue resolution.\n\n### Additional Considerations\n\n* **Environment Variables:** Use environment variables to configure test parameters like the target Kubernetes cluster, Dapr version, and test-specific settings.\n* **Test Data Management:** Implement a strategy for managing test data. Consider using test fixtures, mock data, or dedicated test databases to ensure test consistency and isolation.\n* **Resource Cleanup:**  Automate the cleanup of resources created during testing, such as Kubernetes deployments and services, to prevent resource leaks and ensure a clean testing environment.\n\nThis approach ensures consistent, reliable, and informative E2E testing for Dapr, contributing to the overall quality and reliability of the project.\n","GenTime":"2024-08-28 13:05:54"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"text":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n\n##Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","MatchedIds":[3526,1654,1754,4843,847],"Prediction":"## Decision\n\n**Chosen option:** \"Enhance the `hmrcTimeoutDialog` component to periodically poll a new endpoint on the service it is loaded on.\"\n\n**Rationale:** This approach addresses the problem while best satisfying the decision drivers:\n\n* **Minimises code changes for service teams:** Service teams only need to upgrade the `play-frontend-hmrc` library to benefit from the updated component. \n* **Avoids additional routing rules and configuration:** The endpoint can utilize existing routing mechanisms and configurations related to session management.\n* **Limits duplication of knowledge:** The endpoint can leverage the existing logic within `SessionTimeoutFilter` in `bootstrap-play` to determine and provide the remaining session time.\n* **Maintains loose coupling:**  The polling mechanism resides within the `hmrcTimeoutDialog` component itself. There is no reliance on other frontend microservices.\n* **Endpoint does not affect the session:** A dedicated endpoint can be implemented with session management disabled, ensuring it doesn't refresh the session upon polling.\n\n### Positive Consequences\n\n* Improved user experience by aligning timeout warnings with actual session expiry.\n* WCAG 2.1 compliance regarding timing adjustable interactions.\n* Minimal disruption and effort for service teams to adopt the solution.\n* Improved maintainability and reduced complexity by leveraging existing components and knowledge.\n\n### Negative Consequences\n\n* Increased network traffic due to periodic polling. However, this can be mitigated by implementing appropriate polling intervals and request caching strategies.\n* Potential for slight discrepancies in the reported remaining session time due to network latency and polling intervals. However, these discrepancies are expected to be minimal and should not significantly impact the user experience. \n\n### Implementation Details\n\n1. **New Endpoint:**  A new endpoint (`\/session-timeout-remaining`) will be added to services utilizing `bootstrap-play`. This endpoint will:\n    * Be excluded from session management.\n    * Utilize the logic within `SessionTimeoutFilter` to determine the remaining session time.\n    * Return the remaining time in a structured format (e.g., JSON).\n2. **`hmrcTimeoutDialog` Enhancement:**  The component will be enhanced to:\n    * Periodically poll the `\/session-timeout-remaining` endpoint.\n    * Update its internal state based on the received remaining session time.\n    * Adjust the displayed warning accordingly.\n\nThis solution provides a good balance between effectiveness, ease of implementation, and minimal impact on existing systems. \n","GenTime":"2024-08-28 13:06:06"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"text":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n\n\n##Decision\n### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","MatchedIds":[4354,4119,3764,1962,4130],"Prediction":"## Decision\n\n### Genesis File Initialization\n\nWe will define a new section in the genesis file dedicated to consensus parameters. This section will hold a serialized representation of the parameters using a format like JSON or Protobuf.  This allows for human-readability and future extensibility.\n\nExample:\n\n```json\n{\n  \"consensus_params\": {\n    \"max_block_size\": 1048576, \n    \"max_tx_size\": 1048576,\n    \"evidence_validity_period\": \"1000h\", \n    \"checkpoint_frequency\": 1000 \n  }\n}\n```\n\n### ABCI Exposure\n\nTwo new methods will be added to the ABCI:\n\n1. **`QueryConsensusParams`**: This method will allow clients to query the current consensus parameters. This is a read-only operation and has no impact on consensus.\n\n2. **`SetConsensusParams`**:  This method will allow authorized entities (likely via governance mechanisms) to propose changes to the consensus parameters. To avoid breaking changes, `SetConsensusParams` will only be allowed during network upgrades, signaled by a specific block height or protocol version in the header. \n\n   - The proposal will include the new set of consensus parameters. \n   - The change will only be applied at a specific future block height, allowing validators time to prepare for the change. \n\n### Parameter Management \n\n1. **Initialization**: On startup, the consensus engine will load the parameters from the genesis file. \n2. **Querying**: The `QueryConsensusParams` ABCI method will return the current values of the parameters.\n3. **Updates**:\n    -  A governance proposal will be required to initiate an update to the consensus parameters.\n    -  The proposal will specify the new parameter values and the future block height for the change to take effect.\n    -  Upon proposal approval, the `SetConsensusParams` ABCI method will be called at the designated block height to update the parameters.\n\n### Considerations\n\n* **Versioning**: A versioning scheme for the parameters might be necessary to manage future changes and ensure backward compatibility.\n* **Security**:  The `SetConsensusParams` method must be adequately protected to prevent malicious or unauthorized modifications to the consensus parameters. This likely involves requiring supermajority consensus among validators and incorporating the changes into the light client verification process. \n* **Complexity**:  Introducing this mechanism will increase the complexity of the consensus engine and potentially the light client implementation. \n\nThis approach allows for flexible configuration of consensus-critical parameters, ensuring they can be tailored to specific network needs and evolve over time without requiring hard forks.\n","GenTime":"2024-08-28 13:06:19"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"text":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n\n##Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","MatchedIds":[445,442,447,443,1016],"Prediction":"## Decision\nBased on the requirement to **\"Scrape data to be consumed in machine readable format\"**, **Platform 2: Koshvani web** will be the primary source of data. The website's structure allows for scraping and analysis of budget data.  \n\n**Platform 1: Budget - Government of Uttar Pradesh** will be used as a secondary source. This platform will primarily provide budget documents that can be analyzed to understand the accounting information and context for the data scraped from Koshvani web. \n\nThis two-pronged approach will provide both the raw data and the necessary context to understand Uttar Pradesh's fiscal data effectively. \n","GenTime":"2024-08-28 13:06:24"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"text":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n\n\n##Decision\n-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","MatchedIds":[2298,334,1854,2958,1193],"Prediction":"## Decision\nWe will introduce the React Context API to share data across components without prop drilling. The `edition` property, as demonstrated in the spike, is a good candidate for this approach. We will start with this specific use case and evaluate extending the use of Context API for other shared data as needed. \n","GenTime":"2024-08-28 13:06:26"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"text":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n\n\n##Decision\nKeys should be minted on a per-feature, per-application basis.\\n","MatchedIds":[211,4848,13,5057,578],"Prediction":"## Decision\nTo minimize the impact of compromised keys or tokens, we will adopt the following strategies:\n\n* **Principle of Least Privilege:**  Keys and tokens will be granted only the minimum permissions necessary to perform their intended function. This limits the scope of potential damage if a key is compromised.\n* **Short-Lived Credentials:**  Keys and tokens will have short expiration times, requiring regular rotation. This reduces the window of vulnerability and limits the usefulness of a stolen key. \n* **Scoped Tokens:** Where possible, we will issue scoped tokens that grant access to specific resources or actions, rather than broad permissions.\n* **Centralized Key Management:** We will utilize a centralized key management system to handle key generation, rotation, and revocation. This allows for easier auditing and control over key lifecycles.\n* **Rate Limiting and Anomaly Detection:** Implement rate limiting and anomaly detection mechanisms to identify and block suspicious activity that may indicate a compromised key.\n\nBy implementing these practices, we can significantly reduce our vulnerability to compromised keys and tokens, even in the case of a security breach. \n","GenTime":"2024-08-28 13:06:33"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"text":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n\n\n##Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","MatchedIds":[4689,1117,3602,1151,1240],"Prediction":"## Decision\n\n- **Meet all requirements listed above:** This is a must-have. Any considered library should be capable of rendering the diverse charts and graphs required by our projects.\n- **API quality:**  A well-designed, intuitive API is crucial for developer productivity and ease of maintenance.  \n- **Pricing:** As this is for the whole repository, we should prioritize free and open-source solutions or those with a permissive license for our use case.\n- **Bundle size:**  A smaller bundle size is always preferable for faster page load times, especially in a multi-project environment.\n- **Typescript support:**  Typescript greatly enhances code quality and maintainability, so strong Typescript support is important. \n\n**Chosen option:** We need to research and compare available charting libraries based on the criteria above. Potential candidates could be:\n\n* **Recharts:**  A popular React-based library with a good balance of features, a relatively small bundle size, and excellent Typescript support. \n* **Chart.js:** A lightweight and very popular JavaScript charting library. While not React-specific, it's flexible and has a good ecosystem. \n* **Nighcharts:** A commercially supported library with a focus on interactivity and aesthetics. It may require evaluating its pricing model.\n\n**Next Steps:**\n\n1. **Research & Comparison:** Conduct a more thorough comparison of the shortlisted libraries (and potentially others) based on our specific requirements and the listed decision drivers.\n2. **Prototyping:** Develop small prototypes using the top contenders to get a hands-on feel for their API, performance, and how well they integrate into our existing tech stack. \n3. **Decision & Documentation:**  Document the chosen library, the reasons behind the decision, and provide clear guidelines and examples for its use across island.is. \n","GenTime":"2024-08-28 13:06:42"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"text":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n\n\n##Decision\nAll agreed with this decision\\n","MatchedIds":[3488,1094,3925,429,1089],"Prediction":"## Decision\nWe will use a microservices architecture for the application. This decision is driven by the need for scalability, reliability, and performance. By decomposing the application into smaller, independent services, we can achieve better fault isolation, independent deployments, and granular scaling. \n\nHowever, we acknowledge that adopting a microservices architecture introduces complexities in terms of inter-service communication, data consistency, and operational overhead.  Therefore, we will mitigate these challenges by:\n\n* **Implementing asynchronous communication patterns** where appropriate to reduce coupling and improve resilience.\n* **Employing robust service discovery and registration mechanisms** to ensure seamless service-to-service communication.\n* **Leveraging containerization and orchestration tools** like Docker and Kubernetes to simplify deployment and management of microservices.\n\nThis approach allows us to leverage the benefits of microservices while mitigating potential drawbacks. \n","GenTime":"2024-08-28 13:06:48"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\\nI need to decide in which language I implement the frontend.\\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"text":"## Context\\nI need to decide in which language I implement the frontend.\\n\n\n##Decision\nI use Javascript.\\n","MatchedIds":[1240,472,1243,3721,4761],"Prediction":"## Decision\nFor the frontend, I will use **JavaScript** with **React**. React is a popular JavaScript library for building user interfaces and has a large community and extensive documentation, making it easier to find solutions and support. While other options exist, focusing on a widely used combination like React and JavaScript will allow for quicker development and potentially attract other developers familiar with these technologies. \n","GenTime":"2024-08-28 13:06:57"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","MatchedIds":[4549,4547,4910,4918,4544],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n\nWe will implement a message queue system using RabbitMQ to handle asynchronous communication between our microservices. This choice leverages RabbitMQ's reliability, flexible routing options, and mature feature set to improve the scalability and fault tolerance of our architecture. \n","GenTime":"2024-08-28 13:07:01"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"text":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n\n\n##Decision\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","MatchedIds":[2520,4747,4266,4859,583],"Prediction":"## Decision\n* Easy to use storage\n* **Fast storage**\n* **Dynamic and\/or schemaless storage**\n\nChosen option: **Redis**, because it is an in-memory data store known for its speed and flexibility. \n\n### Positive Consequences\n* **Performance:**  Redis excels at handling high-speed read\/write operations, crucial for time-sensitive actions like missile launch commands.\n* **Flexibility:** Its dynamic, schemaless nature allows for storing various data types and evolving data structures as operational needs change.\n\n### Negative Consequences\n* **Data Persistence:** Being primarily in-memory, Redis requires careful consideration for persistence strategies to prevent data loss in case of failures.\n* **Data Complexity:** While Redis supports various data structures, complex relationships between missiles, launch sites, and other operational data might be better suited for a traditional database if those relationships become highly intricate. \n","GenTime":"2024-08-28 13:07:08"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"text":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n\n##Decision\nDistributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","MatchedIds":[2888,2896,2145,2880,2897],"Prediction":"## Decision\n\nImplement a new distributed Mail Queue leveraging existing components to address limitations of current implementations:\n\n1. **Leverage RabbitMQ**: Continue using RabbitMQ for its core queuing capabilities, ensuring efficient message queuing and delivery.\n2. **Cassandra as Metadata Store**: Store mail metadata (sender, recipient, state, timestamps, etc.) in Cassandra.  Avoid using Cassandra for queue-like structures due to tombstone issues. Instead, utilize a key-value approach for storing and retrieving mail metadata.\n3. **Object Storage for Mail Content**: Continue storing the actual email content in Object Storage (Swift or S3) to leverage its scalability and cost-effectiveness for large data storage.\n4. **Introduce Mail Metadata Index**: Implement a separate indexing mechanism optimized for querying and browsing mail metadata. This could leverage technologies like Elasticsearch or a dedicated Cassandra table optimized for range queries.\n5. **Asynchronous Processing for Administration Operations**: Implement `browse`, `getSize`, and `arbitrary mail removal` as asynchronous background tasks. This will allow for handling large mail queues efficiently without impacting core mail delivery performance.\n6. **Expose Administration API**: Provide a centralized API for administrators to interact with the distributed Mail Queue. This API will allow managing mail queues across all James servers from a single point.\n\nThis approach leverages the strengths of each component while addressing their limitations. It provides a scalable, performant, and administrator-friendly solution for distributed mail queuing in James. \n","GenTime":"2024-08-28 13:07:15"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"text":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n\n##Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","MatchedIds":[4890,2703,4899,3521,398],"Prediction":"## Decision\n\nTo maintain a clean and informative Git history, we will adopt the following strategies:\n\n**1. Git History Structure:**\n\n* **Semi-linear history with explicit merge commits:** Feature branches will be merged into the main branch using the \"no-fast-forward\" option (`--no-ff`). This creates a clear visual separation of features and allows us to easily understand when and how a feature was integrated.\n* **Feature branches for new features and bug fixes:** All development work will be done in dedicated feature branches. This keeps the main branch clean and stable.\n* **Rebase interactive for clean feature branches (optional):** Encourage developers to use `git rebase -i` to maintain a clean and logical commit history within feature branches. This step, while optional, can significantly improve the readability and comprehension of a feature's development process.\n\n**2. Commit Message Format:**\n\n* **Conventional Commits:** We will adopt the [Conventional Commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) specification for writing commit messages. This provides a standardized structure that makes commit messages more readable and easier to parse for automated tools. \n    * **Example:**  \n     ```\n     feat(authentication): implement user login functionality\n\n     This commit implements the user login functionality, including:\n\n     - Creating the login form\n     - Handling user authentication with the backend\n     - Redirecting the user to the dashboard upon successful login\n\n     Resolves #123\n     ```\n\n* **Meaningful commit messages:** Each commit message should clearly and concisely describe the change being made. Avoid vague messages like \"fixed bug\" or \"updated code.\"\n* **Detailed descriptions when necessary:** For more complex changes, provide a more detailed explanation in the commit message body. Explain the reasoning behind the change and any significant implementation details.\n\n**Benefits:**\n\n* **Improved code review:** A clean and well-structured Git history makes it easier for reviewers to understand the changes being made.\n* **Easier debugging:** When a bug is discovered, a clear Git history can help pinpoint the source of the issue more quickly.\n* **Automated changelog generation:** Conventional Commits allow us to automate the process of generating changelogs, making it easier to track releases and communicate changes to users.\n* **Better collaboration:** A consistent format for commit messages improves communication and collaboration within the team.\n\n**Tools:**\n\n* Consider using tools like pre-commit hooks to enforce commit message conventions and ensure a consistent Git history.\n\nBy adhering to these guidelines, we can ensure that our Git history remains a valuable resource for understanding the evolution of our codebase. \n","GenTime":"2024-08-28 13:07:31"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"text":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n\n\n##Decision\nAPI authentication and access control will use JWTs.\\n","MatchedIds":[2397,2256,1721,3950,577],"Prediction":"## Decision\n\nFlowKit will use JSON Web Tokens (JWTs) for authentication and authorization for the new API. \n\n### Reasons:\n\n* **Granular Access Control:** JWTs allow us to define fine-grained permissions for each token, enabling access to specific resources or actions within FlowKit. This is crucial for providing secure access to sensitive data and functionality. \n* **Centralized Management:**  JWTs can be issued and managed from a central location, simplifying administration and reducing the overhead of managing authentication across different components of FlowKit.\n* **Security:** The cryptographic signing of JWTs ensures their integrity and authenticity. Tampering attempts are detectable, enhancing the security of the API. \n* **Ephemeral Nature:** JWTs have a built-in expiration time, limiting the potential damage from stolen tokens and promoting better security practices.\n* **Low Overhead:** JWTs are lightweight and easy to implement, minimizing the impact on performance and user experience.\n* **Wide Support:** JWTs are an industry standard with extensive support in various languages and frameworks, ensuring compatibility and easing future development.\n\n### Alternatives Considered:\n\n* **Traditional Login System:** While feasible, this approach lacks the fine-grained access control offered by JWTs and introduces complexity in managing sessions.\n* **Docker Host Authentication:** This tightly couples FlowKit authentication to the Docker host environment, limiting flexibility and portability. \n* **Public Key Authentication:** This approach is more complex to implement and manage compared to JWTs for this use case.\n\n**By choosing JWTs, FlowKit benefits from enhanced security, fine-grained access control, and simplified management, aligning with the goal of providing a secure and flexible API.** \n","GenTime":"2024-08-28 13:07:39"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"text":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n\n\n##Decision\nWe will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","MatchedIds":[5177,3237,241,771,536],"Prediction":"## Decision\nProjects will use [Poetry](https:\/\/python-poetry.org\/) to manage virtual environments and dependencies. \n\nPoetry offers a number of advantages:\n* **Actively maintained**: While Pipenv suffers from a lack of maintenance, Poetry is under active development.\n* **`pyproject.toml` support**: Poetry embraces the emerging standard for Python packaging, simplifying project configuration. \n* **Smooth dependency management:** Poetry excels at handling dependencies and creating reproducible builds.\n\nWhile Poetry may not be universally standardized, its feature set and active development make it a suitable choice for new projects.\n","GenTime":"2024-08-28 13:07:45"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"text":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n\n\n##Decision\nIn order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","MatchedIds":[2945,2932,2944,899,1193],"Prediction":"## Decision\n\nTo enable eager shape computation during model import in Nd4j, we will introduce a new operational mode within the model import framework. This mode will be responsible for calculating and propagating shape information as each node in the imported graph is processed.\n\nThis approach will involve the following:\n\n1. **Shape Inference Functions:** For each supported operation type, define a corresponding shape inference function. This function will take the input shapes (or symbolic representations of shapes) and operation attributes as input, and return the output shape(s) based on the operation's semantics.\n\n    ```kotlin\n    interface ShapeInferenceFunction {\n        fun inferShape(inputShapes: List<LongArray?>, attributes: Map<String, Any>): List<LongArray?>\n    }\n    ```\n\n2. **Eager Shape Computation Context:** Introduce a context or environment within the model import framework to manage shape information during eager shape computation. This context will store a mapping of variable names to their corresponding shapes (or symbolic representations).\n\n3. **Shape Propagation:** During the graph traversal and node processing phase of model import:\n    * For each node, retrieve the shape inference function corresponding to its operation type.\n    * Fetch the input shapes from the eager shape computation context based on the node's input variable names.\n    * Invoke the shape inference function with the input shapes and node attributes.\n    * Store the computed output shape(s) in the context, associated with the node's output variable names.\n\n4. **Fallback Mechanism:** Implement a fallback mechanism for cases where shape inference is not possible or not implemented for a particular operation. This could involve using a placeholder shape, raising an error, or deferring shape resolution to a later stage.\n\nThis approach will enable Nd4j's model import framework to resolve shapes eagerly, facilitating the creation of well-defined graph representations from diverse frameworks. It will also lay the groundwork for potential future extensions, such as fully eager computation in Nd4j. \n","GenTime":"2024-08-28 13:07:56"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"text":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n\n\n##Decision\nWe've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","MatchedIds":[4841,13,947,3147,1480],"Prediction":"## Decision\n\nWe will standardize on **AWS Secrets Manager** as the solution for securely storing secrets across the Modernisation Platform. \n\nThis decision is based on the following factors:\n\n* **Alignment with MoJ strategy:** Leveraging managed services like Secrets Manager is consistent with the MoJ's existing strategy and reduces our operational overhead.\n* **Robust Security:** Secrets Manager provides robust security features including encryption at rest and in transit, granular access control, and integration with AWS CloudTrail for auditing.\n* **Developer Experience:** Secrets Manager offers convenient SDKs and integrations, making it easy for developers to interact with and manage secrets within their applications.\n* **Cost-Effectiveness:** Secrets Manager provides a cost-effective solution, especially when considering the reduced operational burden and potential cost savings from not having to manage our own secrets infrastructure. \n\nWhile other solutions exist, Secrets Manager provides the optimal balance of security, ease of use, cost-effectiveness, and alignment with MoJ strategies for the Modernisation Platform. \n","GenTime":"2024-08-28 13:08:01"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"text":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\n\n##Decision\nUse workspaces to manage multiple environments.\\n","MatchedIds":[621,2793,856,3789,187],"Prediction":"## Decision\nWe will manage different environments (staging, prod, etc.) using **Terraform workspaces**. This approach minimizes code duplication by allowing us to define infrastructure once and then use variables and conditional logic to customize deployments for each environment. \n\nThis aligns with Terraform's best practices, reduces the risk of configuration drift, and simplifies our infrastructure management. We will establish a clear naming convention for workspaces, likely mirroring our environment names, to ensure clarity and maintainability. \n","GenTime":"2024-08-28 13:08:05"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"text":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n\n\n##Decision\nAll limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","MatchedIds":[1050,779,4538,1924,4694],"Prediction":"## Decision\n\nIntroduce **Route Policies** as a mechanism to control the behavior of routes without modifying core logic or burdening integrators.\n\n* **Definition:** A Route Policy is a set of rules associated with a route, defining how data should be handled within that route. \n* **Configuration:**  Policies are defined at the route level, allowing for granular control. They can be configured via a dedicated DSL (Domain Specific Language) or a user-friendly interface, depending on the system's complexity. \n* **Flexibility:** Offer predefined policies for common use-cases like \"transfer-once\" or \"find-equivalent,\"  and allow for custom policies through a plugin system for extensibility. \n* **Examples:**\n    * **Transfer Once Policy:**  Ensures data flowing through the route is processed only once, preventing duplicates.\n    * **Find Equivalent Policy:** Routes data based on matching criteria to an equivalent destination, facilitating data synchronization and mapping scenarios.\n\n**Benefits:**\n\n* **Centralized Control:** Provides a unified location to manage route behavior without modifying individual integrators.\n* **Extensibility:**  Customizable policies allow adapting to specific use-cases and future requirements.\n* **Simplified Integration:** Integrators are shielded from complexity and can rely on core policies for common scenarios.\n\nThis approach promotes a clean separation of concerns, enhances maintainability, and provides the flexibility needed to handle diverse data flow requirements. \n","GenTime":"2024-08-28 13:08:12"}
{"File Name":"tove\/adr-03.md","Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"text":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n\n\n##Decision\nWe don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","MatchedIds":[5163,3831,75,3416,3987],"Prediction":"## Decision\n\nWe will utilize **Azure Blob Storage** for storing and serving the transcription files. \n\n### Justification:\n\n* **Ease of Sharing:** Azure Blob Storage provides Shared Access Signatures (SAS) which function similarly to S3 Presigned URLs, offering a secure and straightforward way to grant time-limited access to specific blobs (or zip files) for user downloads.\n* **Performance:** Blob Storage excels at handling large files and offers various tiers to optimize cost and performance. For frequent downloads, configuring a hot storage tier would ensure fast retrieval.\n* **Scalability and Cost-effectiveness:** Blob Storage is inherently scalable and designed for storing large amounts of unstructured data, making it ideal for managing potentially numerous transcription files. \n* **Integration:**  Azure offers robust SDKs and APIs for interacting with Blob Storage, simplifying the process of uploading, organizing, and managing files programmatically.\n\nWhile Azure File Services might seem appealing for its familiarity with file system structures, it doesn't offer the same level of granular access control and performance optimization for our specific use case. Additionally, generating zip files directly within File Services might introduce complexities compared to the flexibility offered by Blob Storage and its associated tools. \n\n### Implementation Details:\n\n1. **Organization:** Transcription files will be organized within containers based on relevant criteria like subject, group, or project. \n2. **Zipping On-Demand:**  Upon user request, the system will retrieve the relevant files from their respective blobs, generate a zip file dynamically, and store it as a new blob within a designated container for downloads.\n3. **SAS for Download:** A Shared Access Signature with appropriate permissions and expiration will be generated for the zip file blob, providing the user with a secure and time-limited download link. \n4. **Direct Download or Link:**  Depending on the estimated time for zipping, the system will either:\n    * Initiate a direct download if the zip generation is near-instantaneous.\n    * Provide a link to the SAS-protected zip file, allowing the user to download it at their convenience.\n\nThis approach optimizes the user experience by leveraging the strengths of Azure Blob Storage for secure, performant, and cost-effective management and delivery of transcription files. \n","GenTime":"2024-08-28 13:08:24"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"text":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n\n\n##Decision\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","MatchedIds":[4359,4179,1258,3853,1180],"Prediction":"## Decision\n\nTo address the limitations of both controller specs and overly mocked unit tests, we will adopt the following approach:\n\n1. **Prioritize Request Specs:** For testing controller logic, we will focus on writing request specs using Rack Test. This approach provides a more integrated test that more closely resembles real user interactions.\n2. **Limit Controller Spec Usage:**  We will no longer write new controller specs. Existing controller specs can be gradually migrated to request specs. \n3. **Refactor Controller Spec Helpers:** Existing controller spec helpers will be refactored into request spec helpers. These helpers should interact with the application through the Rack interface instead of mocking the CC app directly.\n\n## Consequences\n\n- **Reduced Duplication:** Eliminating controller specs will reduce test duplication between different test types.\n- **Improved Test Clarity:** Request specs provide a clearer picture of how the controller interacts with the application and its dependencies.\n- **Refactoring Effort:** Refactoring controller spec helpers into request spec helpers will require some effort and should be planned accordingly.\n- **Gradual Migration:** Migrating from existing controller specs to request specs will require time and should be done incrementally. \n","GenTime":"2024-08-28 13:08:31"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"text":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","MatchedIds":[4899,4600,4889,4896,4539],"Prediction":"## Decision\n**We will use Conventional Commits for formatting our commit messages.** \n\nThis decision is based on the following:\n\n* **Structure and Clarity**: Conventional Commits provide a lightweight, yet effective structure for commit messages, making them easily understandable for both humans and machines. This addresses the issue of unclear, unhelpful commit messages hindering long-term project maintenance.\n* **Tooling and Automation**: The structured format enables using automated tools such as changelog generators, commit analyzers, and semantic release tools. By extracting semantic meaning from commit messages, these tools streamline development processes and enhance project documentation.\n* **Industry Standard**: Conventional Commits is a widely adopted convention, making it easier for new team members to adapt and contribute effectively. This shared understanding reduces onboarding friction and improves overall consistency.\n\n**Implementation Details**:\n\n* We will utilize a linting tool (like commitlint) and possibly integrate it with our version control system (e.g., Git hooks) to enforce adherence to the Conventional Commits specification. This ensures that all commits follow the agreed-upon format. \n* The Conventional Commits format will be documented in our contribution guidelines to provide clear instructions and examples for developers.\n\nBy adopting Conventional Commits, we aim to:\n\n* Improve the readability and maintainability of our commit history.\n* Enable the use of automation tools to enhance productivity and documentation. \n* Foster a shared understanding and consistency across the team. \n","GenTime":"2024-08-28 13:08:38"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"text":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n\n##Decision\n* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","MatchedIds":[3147,211,3852,3915,1464],"Prediction":"## Decision\n\nWe will use a combination of IAM Roles and Policies to manage access to S3 buckets provisioned by the service-operator. This approach balances the need for user flexibility with the need to protect resources managed by the operator.\n\n1. **Service Operator Role:** The service operator will utilize an IAM role with permissions to manage all aspects of S3 buckets, including bucket policies, ACLs, versioning, and lifecycle rules. This allows the operator to fully manage and reconcile the state of provisioned buckets based on the Kubernetes resource definitions.\n\n2. **User-Specific IAM Roles:** For each S3 bucket provisioned, the service operator will generate a corresponding IAM role. This role will grant users the following permissions:\n\n    - **Read-Only Access to Bucket Configuration:** This allows users to understand the bucket's configuration, including lifecycle rules, versioning status, and ACLs, using AWS SDKs or CLI tools.\n    - **Full Object-Level Permissions:** Users will have full control (read, write, delete, and manage versions) over objects within the bucket. This enables them to utilize the bucket for their applications and data storage needs.\n\n3. **Restricted Permissions on Bucket-Level Operations:**  The user-specific IAM roles will *not* have permissions to modify bucket-level settings, such as:\n\n    - Modifying or deleting bucket policies\n    - Changing versioning configuration\n    - Altering or deleting lifecycle rules\n    - Modifying bucket ACLs\n\n4. **Policy Enforcement Point:**  We will leverage the AWS resource-based policy mechanism to enforce these restrictions at the bucket level. The service operator will attach a policy to each bucket it provisions, explicitly denying any actions related to modifying bucket configurations from the user-specific role.\n\n**Advantages of this approach:**\n\n- **Clear Separation of Concerns:** The service operator retains exclusive control over bucket-level configurations, ensuring the declared state in Kubernetes is consistently enforced.\n- **User Flexibility:** Users have the freedom to manage their objects within the bucket, supporting diverse application needs.\n- **Reduced Risk of Conflicts:** By restricting user permissions on sensitive bucket-level configurations, we minimize the risk of accidental or intentional modifications that could conflict with the operator's management.\n- **Auditability:** All actions performed by both the service operator and users are logged in AWS CloudTrail, providing visibility and accountability.\n\nThis approach ensures the service operator can reliably manage S3 bucket configurations while still providing users with the necessary permissions to utilize these buckets effectively and securely.\n","GenTime":"2024-08-28 13:08:50"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"text":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n\n\n##Decision\nWhere necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","MatchedIds":[3566,2793,2191,2616,2629],"Prediction":"## Decision\n\nKeep the current system of extracting help text from specially formatted comments in the command scripts.  This has the advantage of keeping documentation close to the code, which is convenient for developers.\n\nTo address the need for calculated values in the help text, we will introduce a simple templating mechanism. The help text can include placeholders that will be replaced at runtime. For example, a placeholder for the tool's configuration directory might be `{{CONFIG_DIR}}`.\n\nImplement a function that takes the raw help text and substitutes the placeholders with their calculated values. This function will be called by the `help` subcommand before displaying the text to the user. \n\nThis approach balances the need for dynamic content in the help text with the ease of development provided by the current system. \n","GenTime":"2024-08-28 13:08:54"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"text":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n\n\n##Decision\nChosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","MatchedIds":[4644,2181,1172,693,2183],"Prediction":"## Decision\nChosen option: **Separate data models**, because maintaining a single data model for all use cases introduces complexity and compromises data integrity.\n\n### Positive Consequences\n- **Improved Validation:** By using separate models for UI interaction and XML serialization\/deserialization, we can enforce stricter validation at the XML layer, ensuring that only valid data is processed by the backend and algorithms.\n- **Clearer Responsibility:** Different data models allow for a cleaner separation of concerns. For example, the UI model can be more lenient to accommodate user input in progress, while the XML model remains strict to adhere to the XSD schema.\n- **Simplified Maintenance:**  Separate models are easier to maintain and evolve independently, as changes in one model won't necessarily require modifications in others. \n\n### Negative Consequences\n- **Increased Complexity:** Introducing multiple models adds complexity to the system, requiring mapping logic between them.\n- **Potential Data Duplication:**  Depending on the implementation, there might be some data duplication across models.\n\n### Implementation Details\n1. **Define Separate Models:** Create distinct data models for:\n    - **UI:** This model should be flexible to support the UI's workflow, allowing for partially complete data.\n    - **XML:**  This model should strictly adhere to the XSD schema, ensuring that all generated XML is valid.\n    - **Internal Backend\/Algorithm:** This model can be tailored to the specific needs of the backend processing and algorithms.\n2. **Implement Mapping Logic:** Create functions or classes to handle the transformation of data between the different models.\n3. **Validate at Appropriate Stages:** Perform validation:\n    - In the UI, to provide immediate feedback to users.\n    - Before serialization to XML, to guarantee the generation of valid XML.\n4. **Minimize Data Duplication:**  Design the models and mapping logic to minimize data duplication and ensure consistency. \n\nThis approach allows for a more robust and maintainable system by separating concerns and enforcing stricter validation where necessary.\n","GenTime":"2024-08-28 13:09:04"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"text":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n\n\n##Decision\nReduce worker concurrency to 1 by using a single container (for now).\\n","MatchedIds":[3504,3915,3595,2968,4821],"Prediction":"## Decision\nImplement a distributed locking mechanism to prevent concurrent git operations on the same repository. This mechanism will ensure that only one worker can acquire a lock on a specific repository at a time, preventing conflicts and ensuring data integrity.\n\n### Positive Consequences:\n* Eliminates concurrent git operation failures.\n* Ensures data consistency within the repository.\n* Improves the reliability and predictability of git-related tasks.\n\n### Negative Consequences:\n* Introduces additional complexity for managing distributed locks.\n* May introduce minor performance overhead due to lock acquisition and release.\n\n### Implementation Details:\n* Evaluate and select a suitable distributed locking system (e.g., Redis, etcd, Consul).\n* Implement a lock acquisition and release mechanism within the workers' workflow, ensuring that all git operations are performed within the scope of the acquired lock.\n* Monitor the performance and stability of the locking mechanism to identify and address any potential bottlenecks or issues. \n","GenTime":"2024-08-28 13:09:09"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"text":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n\n##Decision\nWe have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","MatchedIds":[641,1000,3062,3853,3053],"Prediction":"## Decision\n\nWhile completely rewriting the application in a different language like Python might seem tempting, a wholesale rewrite of a large and complex legacy system is inherently risky and likely to be very costly. Instead, we should adopt a more incremental approach:\n\n**Short-term (next 6 months):**\n\n1. **Invest in improving the existing codebase:** \n    * Dedicate time for refactoring the most problematic parts of the codebase, improving its structure and maintainability.\n    * Update to the latest stable PHP version to benefit from performance improvements and security updates.\n    * Replace outdated and unsupported libraries with actively maintained alternatives.\n    * Introduce static analysis tools and linters to enforce coding standards and identify potential issues.\n2. **Strategically adopt a new language for new features or microservices:**\n    * For any new development or significant feature additions, consider using a modern language like Python. \n    * This allows us to gradually introduce new talent and expertise while keeping the existing system stable.\n    * Design these new components with clear APIs for communication with the legacy system.\n3. **Design System Migration:**\n    * Create a plan to gradually migrate the application to the latest Design System. \n    * This can be done incrementally, component by component, or by introducing the new Design System for specific sections of the application.\n\n**Mid-term (next 12-18 months):**\n\n1. **Evaluate the feasibility of a gradual strangulation of the legacy system:**\n    * Based on the experience gained in the short-term, assess if it's feasible to break down the monolith into smaller, more manageable services.\n    * Prioritize services or components that are most critical, complex, or would benefit the most from being rewritten.\n\n**Long-term:**\n\n1. **Continuously evaluate and adjust the strategy:**\n    * Regularly review the progress made and adapt the strategy based on the evolving needs of the application and the technology landscape. \n    * This approach allows for flexibility and minimizes the risks associated with large-scale changes.\n\n\nThis strategy allows us to manage the risks associated with a large legacy codebase, while gradually improving its maintainability and introducing new technologies. It also allows us to leverage the existing knowledge and experience within the team, while attracting new talent with experience in more modern technologies. \n","GenTime":"2024-08-28 13:09:21"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"text":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n\n\n##Decision\nThe complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","MatchedIds":[1631,36,3827,1151,3035],"Prediction":"## Decision\n\nWe will use a **hybrid approach** that leverages the advantages of both single and multi-package approaches while mitigating the drawbacks.\n\n**Here's how:**\n\n1. **Monorepo with Lerna:**  We'll organize our codebase as a monorepo using Lerna. This structure allows us to manage our components as individual packages within a single repository, simplifying dependency management and publishing.\n2. **Two NPM Packages:**\n    * **Component Library:** This package will contain all individual component packages, exporting them as a single unit. This caters to the \"single package\" convenience for most consumers.\n    * **Style Guide:**  This package will be published separately and will depend on the component library.  It will showcase all available components and their documentation.\n\n**Benefits:**\n\n* **Granular Control:**  Developers can choose to install the entire component library or select individual components for their applications, maximizing flexibility and minimizing bundle size.\n* **Simplified Development:** The monorepo structure with Lerna simplifies development and dependency management for component creators. \n* **Independent Release Cycles:** The component library and style guide can have independent release cycles. This means updates to the style guide (documentation, new features) won't necessarily require a new release of the entire component library.\n* **Version Management:**  Lerna and semantic versioning will ensure proper versioning of both individual components and the overall library.  \n* **Style Guide Flexibility:** The style guide can leverage Lerna's capabilities to potentially showcase different versions of components.\n\n**Addressing Concerns:**\n\n* **Component Interdependencies:**  Lerna will handle interdependencies between component packages within the monorepo.\n* **Bundle Size (Single Package):**  Developers concerned about bundle size can import individual components directly from the component library package. \n* **Style Guide Complexity:** While there's potential complexity in showcasing multiple component versions, this can be addressed through clear documentation and tooling within the style guide.\n\n**Additional Considerations:**\n\n* **Documentation:** Clear documentation is crucial for guiding developers on consuming individual components or the entire library.\n* **Tooling:**  Invest in tooling and automation (e.g., Lerna, semantic-release) to streamline development, testing, and publishing processes.\n\nThis hybrid approach strikes a balance between ease of use, flexibility, and maintainability, making it suitable for our component library and style guide. \n","GenTime":"2024-08-28 13:09:32"}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"text":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n\n\n##Decision\nThe following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","MatchedIds":[1839,5009,4021,3470,2582],"Prediction":"## Decision\nThe Schoenberg Institute decided to build a new SDBM application using Python\/Django framework and a PostgreSQL relational database. The choice was made based on the following factors:\n\n* **Flexibility and Extensibility:** The Django framework allows for a flexible data model, easily accommodating the complex relationships inherent in manuscript data.\n* **User Contributions:** Django's built-in user authentication and authorization system simplified the implementation of user-based editing and contributions. \n* **Complex Search:** PostgreSQL's full-text search capabilities, coupled with Django's ORM, provide a robust platform for complex querying of the manuscript data.\n* **Modern Technology Stack:** Python and Django are widely used, well-documented, and actively maintained, offering a stable foundation for the long-term development and maintenance of the SDBM.\n* **Open Source:** Choosing open-source technologies for both the framework and database aligns with the principles of open access and data sharing, which are central to the mission of the Schoenberg Institute.\n","GenTime":"2024-08-28 13:09:38"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"text":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n\n\n##Decision\nRely on nested objects to represent mail headers within a mapping\\n","MatchedIds":[2342,485,2896,2145,475],"Prediction":"## Decision\n\nDisable Elasticsearch dynamic mappings and introduce a static mapping for mail headers. \n\n### Justification\n\nWhile dynamic mapping offers flexibility, the functional issues outweigh the benefits in this scenario. A static mapping offers the following advantages:\n\n* **Predictability:** Eliminates issues with field type guessing and ensures all headers are indexed correctly.\n* **Performance:**  Avoids index pauses due to mapping updates, resulting in better indexing performance.\n* **Control:**  Provides fine-grained control over field types and indexing options.\n\n### Implementation Details:\n\n1. **Analyze existing data:** Determine the common mail headers and their data types across the dataset.\n2. **Define static mapping:** Create a comprehensive Elasticsearch mapping that includes all identified headers and appropriate data types.\n3. **Handle unknown headers:** Implement a strategy for handling new or unseen mail headers, such as:\n    *  **Ignoring:**  Discard unknown headers if they are not critical.\n    * **Adding to a generic field:**  Index unknown headers in a dedicated field with a flexible data type like 'text' or 'keyword'.\n4. **Reindex data:**  Reindex existing data to conform to the new static mapping.\n5. **Monitor and adapt:** Continuously monitor the mapping and adjust it over time as new headers emerge or requirements change.\n\nThis approach provides a balance between flexibility and performance, ensuring reliable indexing and optimal search capabilities. \n","GenTime":"2024-08-28 13:09:45"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"text":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n\n##Decision\nI'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","MatchedIds":[4843,1855,160,1883,875],"Prediction":"## Decision\n\nCreate separate routes and links outside the main spiral navigation for the \"Privacy Policy\" and \"About Me\" pages. \n\n**Rationale:**\n\n* **Improves User Experience:**  Forces users to read the policy and about information when navigating the spiral. Providing these pages as separate, easily accessible links respects user choice and browsing habits. \n* **Maintains Spiral Focus:** Keeps the spiral navigation focused on its core purpose (which needs further clarification based on the context).\n* **Implementation Simplicity:** Routing and linking outside the main navigation structure should be straightforward in most web frameworks.\n\n**Potential Drawbacks:**\n\n* **Discoverability:** Users might not discover the \"Privacy Policy\" and \"About Me\" pages if they are not prominently linked elsewhere.\n\n**Mitigation:**\n\n* Ensure clear and prominent links to the \"Privacy Policy\" and \"About Me\" pages from relevant locations such as:\n    * The website footer.\n    * A dedicated \"Info\" or \"Legal\" section.\n    * Within user account settings (if applicable). \n","GenTime":"2024-08-28 13:09:51"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"text":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n\n##Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","MatchedIds":[73,4497,1048,2968,4539],"Prediction":"## Decision\n\n**Effective immediately, IITD is adopting OpenJDK as its supported Java Development Kit (JDK) for all server and client-side applications.**\n\n**Rationale:**\n\n* **Cost Savings:** Oracle's shift to a subscription model for Oracle JDK necessitates a move to a cost-effective alternative. OpenJDK, being open-source and free to use, offers significant cost savings. \n* **Security:** Continued use of older JDK versions poses security risks due to the lack of public updates. Upgrading to OpenJDK 11 or later ensures applications benefit from the latest security patches and enhancements.\n* **Compatibility:** OpenJDK is highly compatible with Oracle JDK, minimizing potential migration issues. The transition from Oracle JDK to OpenJDK should be relatively smooth for most applications.\n* **Community Support:** OpenJDK benefits from a large and active open-source community, ensuring ongoing development, support, and access to a wide range of resources.\n\n**Action Items:**\n\n1. **Immediate Action:** All new Java development within IITD must utilize OpenJDK 11 or later.\n2. **Migration Plan:**  Teams responsible for applications currently running on Oracle JDK must develop and execute migration plans to OpenJDK 11 or later. Prioritize applications based on security criticality and end-of-life deadlines for the Oracle JDK versions in use.\n3. **Support and Guidance:** The IITD Infrastructure Team will provide support and guidance to application teams throughout the migration process. This includes documentation, tools, and best practices for transitioning to OpenJDK. \n\n**Sunset Plan for Oracle JDK:** \n\n* IITD will no longer support Oracle JDK for new development.\n* Existing applications running on Oracle JDK should be migrated to OpenJDK within [Define Timeframe - Consider Security Update Schedules and Application Criticality].\n\nThis decision aligns with IITD's commitment to leverage open-source technologies, enhance security posture, and optimize IT spending. \n","GenTime":"2024-08-28 13:10:01"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"text":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n\n\n##Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","MatchedIds":[2775,4741,4738,1255,472],"Prediction":"## Decision\nImplement two search methods for `Observation` objects: \n\n1. **`getById(long observationId)`:** This method will perform an efficient search for a single `Observation` object using its unique `observationId`. This approach is suitable for retrieving specific, known observations. \n\n2. **`findByCriteria(ObservationCriteria criteria)`:** This method will provide a more flexible search using an `ObservationCriteria` object to encapsulate various search parameters (e.g., date range, location, species, etc.). This approach allows researchers to define complex search criteria for retrieving sets of observations matching specific conditions. \n\nThis dual approach caters to both targeted retrieval of individual observations and the discovery of observation subsets based on diverse research needs. \n","GenTime":"2024-08-28 13:10:06"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"text":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n\n\n##Decision\nRe-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","MatchedIds":[827,810,842,21,1193],"Prediction":"## Decision\n\nTo create a scalable and maintainable documentation structure for the Cosmos SDK, we will adopt the following approach:\n\n**1. Migrate Framework\/Tool-Specific Documentation:**\n\n* **Move documentation related to development frameworks and tools into their respective GitHub repositories.** This includes:\n    * Cosmos SDK documentation moves to the `cosmos\/cosmos-sdk` repository.\n    * Cosmos Hub documentation moves to the `cosmos\/gaia` repository (or a dedicated Hub repository if one is created).\n    * LotionJS documentation moves to the `cosmos\/lotion` repository.\n    * And so forth for all other dev tools and frameworks within the Cosmos ecosystem.\n\n**2. Restructure Website Documentation:**\n\n* **The Cosmos website (cosmos.network) will host all other documentation**, including but not limited to:\n    * High-level overviews and explanations of the Cosmos ecosystem.\n    * The Cosmos Whitepaper.\n    * Frequently Asked Questions (FAQs).\n    * Tutorials and guides that apply broadly across the ecosystem, rather than being specific to a single tool or framework.\n\n**3. Establish Clear Documentation Guidelines:**\n\n*  **Develop and maintain a consistent set of documentation guidelines.** These guidelines should cover:\n    * Style and formatting.\n    * Expected content and structure for different types of documentation.\n    * Processes for contributing and updating documentation.\n    * Versioning and archiving of outdated material.\n\n**4. Implement Consistent Tooling and Infrastructure:**\n\n* **Standardize documentation tooling across repositories.** This may involve:\n    * Choosing a common documentation generator (e.g., Sphinx, MkDocs).\n    * Utilizing shared themes and templates for a unified look and feel.\n    * Implementing automated processes for building and deploying documentation.\n\n**Benefits:**\n\n* **Improved Maintainability:**  Decoupling documentation from the main website makes it easier for individual teams to maintain and update their specific documentation.\n* **Scalability:**  As the Cosmos ecosystem grows, this structure allows for documentation to scale seamlessly with the addition of new frameworks, tools, and resources.\n* **Enhanced User Experience:** By separating content based on relevance, users can easily find the information they need, whether they're looking for high-level overviews or detailed technical specifications.\n\n**Implementation:**\n\n* This change will require a coordinated effort across different teams within the Cosmos ecosystem. \n* A phased approach to migration is recommended, starting with a few key repositories and gradually expanding to cover all relevant documentation. \n","GenTime":"2024-08-28 13:10:18"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"text":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n\n\n##Decision\nTransport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","MatchedIds":[1982,1979,1495,1949,4974],"Prediction":"## Decision\n\nThe `Switch` component's responsibilities will be refactored to achieve a clearer separation of concerns. Specifically, the physical connection handling logic will be extracted from the `Switch` and moved to a dedicated component.\n\nThis new component will be responsible for:\n\n* Managing physical connections.\n* Handling connection establishment and teardown.\n* Exposing a well-defined interface to the `Switch` for sending and receiving data.\n\nThe `Switch` component will then interact with this new component for all physical connection-related operations, thus removing its direct dependency on `config.P2PConfig`. \n\nThis refactoring will improve the modularity and maintainability of the p2p package by:\n\n* **Encapsulating** physical connection handling logic.\n* **Decoupling** the `Switch` from low-level connection details.\n* **Improving testability** of individual components. \n\nThis change will be implemented as part of the iterative P2P refactor outlined in ADR-XXX (replace XXX with the appropriate ADR number addressing issue #2067). \n","GenTime":"2024-08-28 13:10:24"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"text":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n\n##Decision\nWe have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","MatchedIds":[3070,1894,1745,187,4271],"Prediction":"## Decision\n\nWe will implement a two-pronged approach for controlling access to non-production environments:\n\n**1. Authentication Barrier:**\n\n* **Basic Authentication:** Implement HTTP Basic Authentication for all non-production environments.  This presents a simple username\/password challenge to anyone accessing the site.\n* **Environment Specific Credentials:** Each environment (development, review, staging) will have a unique set of credentials. \n* **Documentation:**  Clearly document the credentials for each environment for authorized personnel.\n\n**2. Search Engine Exclusion:**\n\n* **Robots.txt:**  Implement a `robots.txt` file at the root of each non-production environment. This file will include directives to disallow all major search engine crawlers from indexing the site. \n    * Example:\n    ```\n    User-agent: *\n    Disallow: \/\n    ```\n\n**Justification:**\n\n* **Basic Authentication** provides a simple yet effective way to prevent casual access and signal that these are not the intended public versions of the site.\n* **Unique Credentials** per environment ensure that the compromise of one set of credentials doesn't grant access to all environments.\n* **Robots.txt** is a widely recognized standard for controlling search engine indexing, minimizing the risk of these environments appearing in search results.\n\n**Additional Considerations:**\n\n* **VPN:** For an extra layer of security, consider restricting access to non-production environments via a VPN.\n* **Staging to Production Similarity:** While we want to limit access to non-production environments, it's important to keep the staging environment as close to production as possible to ensure a smooth final deployment. \n\nThis approach strikes a balance between security, usability, and maintaining a clear distinction between live and non-production environments. \n","GenTime":"2024-08-28 13:10:32"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"text":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n\n\n##Decision\n* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","MatchedIds":[4843,4011,2256,2221,2397],"Prediction":"## Decision\n\nTo use two encryption keys for encrypting session cookies at any one time: a \"current\" key and a \"previous\" key. \n\n**Key Rotation Process:**\n\n1. **Initial State:** The application starts with a \"current\" key and a null \"previous\" key.\n2. **New Session:**  New sessions are always encrypted using the \"current\" key.\n3. **Key Rotation:** When it's time to rotate keys:\n    * Generate a new \"current\" key.\n    * The old \"current\" key becomes the \"previous\" key.\n4. **Existing Sessions:**\n    * On any request, the application first attempts to decrypt the session cookie using the \"current\" key. \n    * If decryption fails, the application attempts to decrypt using the \"previous\" key. \n    * If successful, the session data is re-encrypted with the \"current\" key and the updated cookie is sent back to the client.\n5. **Retirement:** After a set period (e.g., twice the session duration), the \"previous\" key can be safely retired as all active sessions would have been migrated to the \"current\" key.\n\n**Benefits:**\n\n* **Seamless Key Rotation:** Active users experience no disruption during key rotation.\n* **Enhanced Security:** Limits the impact of a key compromise to a specific timeframe.\n* **Flexibility:** Allows for frequent key rotation without service interruption.\n\n**Implementation Notes:**\n\n* Securely store and manage the encryption keys.  A key management system or a secure vault service is recommended.\n* Ensure the application can handle the potential performance overhead of decrypting with two keys.\n* Monitor the key rotation process and log any decryption failures for analysis. \n","GenTime":"2024-08-28 13:10:41"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"text":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n\n\n##Decision\nReinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","MatchedIds":[84,1442,3129,4119,1924],"Prediction":"## Decision\nGiven that routing methods MUST identify a destination instance to function properly, we revert the relaxation of requirements for `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()`.\n\nTherefore:\n\n* **Requirement Reinstated**: `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` MUST panic with an `UnrecognizedMessageType` error when passed an unexpected message type.\n* **Rationale**: This decision ensures consistent and predictable behavior for routing methods. It enforces that unexpected messages are treated as errors, prompting developers to address potential issues in message handling logic or message version compatibility.\n* **Alternatives Considered**:\n    *  Allowing routing methods to return an error value instead of panicking. However, this could lead to silent failures and inconsistencies in error handling across different implementations. \n* **Consequences**:\n    *  Developers must handle `UnrecognizedMessageType` panics gracefully in their code.\n    *  Stricter message type checking during routing may introduce breaking changes for applications with loosely defined message types in their routing logic. \n\nThis decision re-establishes the importance of explicit and correct message type handling within the routing methods to maintain the integrity and reliability of the messaging system.\n","GenTime":"2024-08-28 13:10:48"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"text":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n\n\n##Decision\n- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","MatchedIds":[2958,1924,3738,1927,118],"Prediction":"## Decision\n\nTo reduce log clutter and improve user experience, command echoing will be disabled by default. Users can explicitly enable command echoing using the `::echo::on` command and disable it using the `::echo::off` command. \n\nThis behavior will be governed by the following rules:\n\n* **Default Echoing**: Command echoing will be enabled by default if the `ACTIONS_STEP_DEBUG` secret is set to `true`. Otherwise, echoing will be disabled.\n* **Explicit Control**: Users can override the default echoing behavior at any time using the `::echo::on` and `::echo::off` commands. \n* **Error Handling**: Regardless of the echoing settings, the output of a command will always be printed to the user log if the command execution results in an error. This provides valuable context for troubleshooting failed commands.\n\nThis approach balances the need for concise logs with the ability to enable verbose output for debugging purposes, ultimately offering a more user-friendly experience. \n","GenTime":"2024-08-28 13:10:54"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"text":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n\n##Decision\nWe will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","MatchedIds":[3598,4096,2968,74,3602],"Prediction":"## Decision\n\nWe will choose to **apply extensions at build time**. This decision prioritizes the advantages of immutability and a secure, reliable deployment process over the convenience of runtime modification. \n\n**Reasons:**\n\n* **Immutability:**  Maintaining image immutability ensures consistent behavior across environments and simplifies debugging. \n* **Security:** Building extensions into the image during the build process significantly reduces the attack surface.  Runtime modifications open potential vulnerabilities, as external code could be injected. \n* **Reliability:** By integrating extensions at build time, we guarantee they are present and functioning correctly when the container starts, avoiding potential runtime errors. \n* **Testing:**  A build-time approach allows for thorough testing of the image with all extensions applied, ensuring a stable and reliable release. \n\n**Addressing the Drawbacks:**\n\nWhile we acknowledge that requiring customers to build their own images with extensions adds a step to their deployment process, we believe the benefits outweigh the drawbacks.  \n\n* **Documentation and tooling:** We will mitigate the inconvenience by providing clear documentation and tools to streamline the image building process for customers. This might include:\n    * Base images with common configurations.\n    * A CLI tool or scripts to automate the process of adding extensions to the base image.\n    * Comprehensive examples and tutorials.\n\n* **Community contributions:** We will encourage community contributions for commonly used extensions, reducing the need for individual customization.\n\nThis approach strikes a balance between flexibility for customization and the core principles of containerization, ensuring a secure, reliable, and predictable deployment experience for all users. \n","GenTime":"2024-08-28 13:11:02"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"text":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n\n\n##Decision\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","MatchedIds":[4644,4642,3488,1094,1480],"Prediction":"## Decision\n\nWe will structure the application into three main components:\n\n**1. API Gateway:** \n* This component will be responsible for handling all incoming requests from clients. \n* It will act as a reverse proxy, routing requests to the appropriate backend services (REST servers or document processors). \n* The API Gateway can also handle cross-cutting concerns such as authentication, authorization, rate limiting, and caching.\n\n**2. REST Servers:**\n* These components will expose the application's functionality through RESTful APIs. \n* Each REST server can be dedicated to a specific business domain or functionality, promoting loose coupling and scalability.\n*  They will interact with the document processors through asynchronous messaging or a shared database, depending on the specific requirements.\n\n**3. Document Processors:**\n*  These components will handle the actual processing of documents. \n* They can be implemented as separate services or workers, allowing for independent scaling and fault tolerance. \n*  Document processors will subscribe to specific events or messages indicating that a document requires processing.\n\n### Positive Consequences\n\n* **Scalability and Flexibility:**  Each component can be scaled independently based on its workload. New REST servers or document processors can be added easily as the application grows.\n* **Fault Tolerance:** The failure of one component will not bring down the entire application.\n* **Separation of Concerns:** Each component has a well-defined responsibility, making the code easier to maintain and evolve.\n* **Technology Diversity:**  Different technologies can be used for different components based on their specific requirements.\n\n### Negative Consequences\n\n* **Increased Complexity:** A distributed architecture introduces increased complexity compared to a monolithic application.\n* **Communication Overhead:**  Communication between components adds some overhead.\n* **Data Consistency:**  Maintaining data consistency across multiple components can be challenging.\n\n### Follow-up decisions required:\n\n* Choose specific technologies for each component (e.g., Flask for REST servers, Celery for document processing).\n* Define the communication mechanism between components (e.g., message queue, REST APIs).\n* Implement a robust error handling and monitoring strategy.\n","GenTime":"2024-08-28 13:11:12"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"text":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n\n##Decision\nUse Static Code Analysis to find violations of standards.\\n","MatchedIds":[1416,3853,1477,3203,1792],"Prediction":"## Decision\n\nWe will establish a strictly enforced coding standard for all aspects of the project. This standard will cover code style, naming conventions, directory structure, testing procedures, and documentation practices. Adherence to this standard will be ensured through a combination of automated linters, code reviews, and clear documentation. \n\nThis approach provides several benefits:\n\n* **Reduced Cognitive Load:** Developers can quickly understand code written by others, leading to faster onboarding and reduced bug potential.\n* **Improved Maintainability:**  A consistent codebase makes it easier to identify, diagnose, and fix issues.\n* **Reduced Risk of Errors:** Predefined patterns and best practices help avoid common pitfalls and anti-patterns.\n\nWe will use a combination of industry-recognized standards and project-specific guidelines to create a comprehensive coding standard. This will be a living document that evolves with the project's needs.\n","GenTime":"2024-08-28 13:11:37"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"text":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n\n##Decision\nThe decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","MatchedIds":[915,2093,914,4454,581],"Prediction":"## Decision\n\n**We will utilise a serverless architecture for the Alpha phase of the Childrens Social Care Placement service.**\n\n**Reasoning:**\n\n* **Increased developer productivity:** Serverless platforms abstract away infrastructure concerns, allowing developers to focus on business logic and feature development. This is paramount in an exploratory Alpha phase where rapid prototyping and iteration are crucial. \n* **Flexibility and agility:**  Serverless architectures are inherently scalable and adaptable.  This allows the service to easily evolve and pivot in response to user feedback and evolving domain understanding during the Alpha phase.\n* **Cost-effectiveness:** The pay-as-you-go model of serverless computing aligns well with the exploratory nature of the Alpha, potentially reducing costs compared to maintaining idle infrastructure.\n\n**This decision will be revisited at the end of the Alpha phase to assess its suitability for the Beta phase and beyond, taking into account factors such as performance, security requirements, and operational complexity.**\n","GenTime":"2024-08-28 13:11:42"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"text":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n\n\n##Decision\nWe will stop proposing a Quick Fix for this refactoring.\\n","MatchedIds":[5140,2689,5090,1854,1193],"Prediction":"## Decision\n\n- **Disable the \"Convert to Pure Component\" Quick Fix by default.** This will prevent the performance hit for users who don't need it. \n- **Provide a way to enable the Quick Fix in the extension settings.**  This will allow users who are willing to accept the performance cost to still use the feature. \n- **Explore alternative solutions for providing this functionality.**  This could involve finding a more performant way to implement the refactoring, or providing guidance to users on how to manually perform the refactoring. \n","GenTime":"2024-08-28 13:11:45"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"text":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n\n##Decision\n* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","MatchedIds":[2526,674,4701,4062,4307],"Prediction":"## Decision\n* consistency\n* expressiveness\nChosen option: \"Define operations as elements with attributes\", because this approach offers better consistency and expressiveness. \n### Positive Consequences\n* **Consistency:**  Using a consistent structure (element with attribute) for all operations, regardless of their complexity, makes the FDML more predictable and easier to parse.\n* **Expressiveness:** Attributes provide a clear way to define parameters for operations. For instance, an operation might have multiple parameters like \"quantity\", \"unit\", etc., which can be neatly expressed as attributes.\n* **Extensibility:** Using attributes allows for easier future extension of operations with additional parameters without modifying the element structure. \n\n### Negative Consequences \n* **Slightly more verbose:** Defining operations with attributes might require a bit more XML markup compared to simple elements for operations with single values. \n","GenTime":"2024-08-28 13:11:50"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"text":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n\n\n##Decision\nAt the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","MatchedIds":[1193,1890,3187,2667,334],"Prediction":"## Decision\n\nWe will **not** provide a `getCursor` functionality directly within `FragmentFrame`. \n\n**Reasoning:**\n\n* **Referential Transparency:** Maintaining referential transparency for `FragmentFrame` is crucial for performance and predictability. Adding `getCursor`, especially if it needs to regenerate cursors based on the potentially changing state of `FragmentFrame`, could violate this principle and lead to unnecessary re-renders.\n* **Cursor Lifespan:** Cursors are conceptually tied to the lifecycle of the root `DataFrame`. Tying them to `FragmentFrame`, which has a more transient and dynamic nature, could introduce complexities in managing their lifecycle and consistency.\n* **Centralized Control:** Keeping the `getCursor` logic within the `DataFrame` provides a single source of truth and control over cursor creation and caching. This centralization promotes maintainability and reduces potential for inconsistencies.\n\n**Alternative Solution:**\n\n1. **Pass `DataFrame` Reference:** Continue passing a reference to the root `DataFrame` down the component tree. While this might seem like prop drilling, it ensures components always access cursors from the correct source.\n2. **Context API (React):** If the application is using React, consider leveraging the Context API to provide a centralized way to access the root `DataFrame` and its `getCursor` method. This approach avoids explicit prop drilling while maintaining a clear data flow.\n\n**Additional Considerations:**\n\n* **Performance Optimization:** If passing the `DataFrame` reference becomes a performance bottleneck, explore memoization techniques or a dedicated cursor management layer.\n* **Future Flexibility:**  This decision keeps the architecture flexible for potential future changes, like switching to a column-oriented data storage within the `DataFrame`. \n\nThis approach prioritizes referential transparency, a clear separation of concerns, and long-term maintainability. \n","GenTime":"2024-08-28 13:12:00"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"text":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n\n\n##Decision\nWe will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","MatchedIds":[4292,1326,4021,2488,5009],"Prediction":"## Decision\n\nWe will manage the data in the Armadillo suite using a combination of **metadata stored in a relational database** and **raw data files stored in an object storage**.\n\n### Rationale\n\nThis approach offers several advantages:\n\n* **Flexibility:** This approach can accommodate various data formats and sizes common in research settings.\n* **Scalability:** Object storage allows for virtually unlimited storage capacity and scales efficiently with increasing data volume.\n* **Data Integrity:**  Storing raw data files independently promotes data integrity by minimizing unnecessary transformations and copies.\n* **Metadata Management:**  The relational database allows us to store structured metadata about the data files, such as:\n    * Data origin and ownership\n    * Data description and annotations\n    * Access control lists\n    * Data usage history\n* **Integration with DataSHIELD:** This strategy aligns well with the existing DataSHIELD implementation, facilitating efficient data loading and analysis.\n\n### Implementation Details:\n\n* **Database Selection:** We will choose a relational database technology (e.g., PostgreSQL, MySQL) based on factors such as performance requirements, existing infrastructure, and developer familiarity.\n* **Object Storage:** We will leverage a cloud-based object storage service (e.g., AWS S3, Azure Blob Storage, Google Cloud Storage) for its scalability, reliability, and cost-effectiveness. \n* **Data Access Layer:** We will develop a data access layer that abstracts the underlying data storage and provides a consistent API for interacting with the data. This layer will handle tasks like:\n    * Uploading and downloading data files\n    * Updating metadata\n    * Enforcing access control policies\n\nThis approach ensures secure, efficient, and scalable data management within the Armadillo suite. \n","GenTime":"2024-08-28 13:12:08"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","MatchedIds":[5061,2467,3873,1631,1193],"Prediction":"## Decision\n\nThe decision to not directly support or provide binary packages for google-cloud-cpp is reasonable, given the context. \n\nHowever, to make the decision more actionable, here's a refined breakdown:\n\n**Decision Summary:**\n\n* **No Direct Binary Distribution:** We will not directly build, publish, or support binary packages for google-cloud-cpp.\n* **Focus on Packageability:**  We prioritize making our source code easily packageable for common C++ package managers.\n* **Community Support Model:** We encourage community contributions for binary packages and will assist with reasonable requests to improve packageability. \n\n**Action Items:**\n\n* **Documentation:**  Provide clear documentation on how to package google-cloud-cpp for various popular package managers (dpkg, rpm, vcpkg, Conan, etc.). This documentation could include:\n    * Build instructions for different platforms.\n    * Recommended directory layouts for packaged artifacts.\n    * Guidance on managing dependencies. \n* **Review Process:**  Establish a clear process for reviewing and potentially merging pull requests that improve the packageability of google-cloud-cpp, even if these changes are specific to certain package managers.\n\n**Justification:**\n\n* **Avoids Fragmentation:** Directly supporting multiple package managers creates significant overhead and potential for inconsistencies.\n* **Leverages Community Expertise:** The C++ community has expertise in different package managers. Encouraging community contributions leverages this and fosters a more distributed maintenance model.\n* **Flexibility for Users:**  Users tied to specific package managers gain the flexibility to integrate google-cloud-cpp into their workflows.\n\n**Potential Concerns:**\n\n* **Support Overhead:** Even without direct support, there might be an increase in support requests related to packaging issues. Providing excellent documentation can mitigate this.\n* **Inconsistency Across Distributions:**  Different maintainers might package the library with varying levels of quality or support. \n\n**Overall:**\n\nThis approach strikes a balance between providing a consistent official source and empowering the community to tailor distributions to their needs. Clear documentation and a welcoming contribution process are crucial to this decision's success. \n","GenTime":"2024-08-28 13:12:19"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"text":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n\n\n##Decision\nPending\\n","MatchedIds":[3829,783,1243,400,1240],"Prediction":"## Decision\n\nUse the `bootstrap` gem for styling and layout. This will allow us to quickly create consistent and visually appealing user interfaces with minimal setup and customization. \n","GenTime":"2024-08-28 13:12:21"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"text":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n\n##Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","MatchedIds":[2397,577,4443,2470,729],"Prediction":"## Decision\n\nWe will use JWT (JSON Web Tokens) to secure the communication between the client and the Dropwizard server. This approach offers flexibility to accommodate different deployment scenarios and user preferences:\n\n**1. Token-based Authentication:**\n\n* The Dropwizard application will expose an authentication endpoint to verify user credentials and issue JWTs upon successful authentication.\n* Clients (e.g., Node.js) will include the JWT in the Authorization header of subsequent requests to protected resources.\n* The Dropwizard server will validate the token for each request, ensuring the client has the necessary permissions to access the requested resource.\n\n**2. Deployment Flexibility:**\n\n* **API Keys:** For machine-to-machine communication or situations where user authentication is not relevant, API keys can be used as a simpler alternative. These keys can be managed and issued separately, allowing for fine-grained access control.\n* **OAuth 2.0:**  If integration with third-party identity providers is desired, the Dropwizard application can implement OAuth 2.0 flows. This allows users to authenticate using their existing accounts (e.g., Google, Facebook) without sharing credentials with the application directly.\n\n**3. Security Considerations:**\n\n* **HTTPS:** All communication between the client and the server should be encrypted using HTTPS to protect the JWT and any sensitive data transmitted.\n* **Token Expiration and Refresh:**  Implement token expiration mechanisms to mitigate the impact of compromised tokens. Provide a secure way for clients to refresh their tokens without re-authentication.\n* **Input Validation and Sanitization:**  Ensure robust input validation and sanitization on both the client and server to prevent common vulnerabilities such as cross-site scripting (XSS) and SQL injection.\n\n**4. Library Support:**\n\n* Leverage existing libraries like `jjwt` for Java (server-side) and similar libraries available for JavaScript\/Node.js to simplify JWT handling and validation. \n\nBy adopting this approach, we provide a secure and flexible solution for client-server communication that can adapt to different user needs and deployment environments.\n","GenTime":"2024-08-28 13:12:31"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"text":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n\n\n##Decision\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","MatchedIds":[4833,1661,3657,2520,2488],"Prediction":"## Decision \nChosen option: \"MySQL\", because \n- Prior experience with MySQL in previous courses and personal projects suggests it is capable of fulfilling the data storage requirements. \n- Utilizing MySQL ensures consistency with course material and instructor examples, facilitating learning and troubleshooting. \n\n### Consequences: \nWhile acknowledged that newer database technologies might offer more advanced features, the familiarity and course alignment benefits of MySQL outweigh the risk of limited functionality for this specific assignment. \n","GenTime":"2024-08-28 13:12:34"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"text":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n\n\n##Decision\nIntegrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","MatchedIds":[2204,2802,4276,1122,1255],"Prediction":"## Decision\n\nThe decision is to integrate the Data BC Geocoder API ([https:\/\/geocoder.api.gov.bc.ca](https:\/\/geocoder.api.gov.bc.ca)) into the Inventory application to enhance the process of setting and verifying property addresses.\n\nHere's how the integration will work:\n\n1. **Address Autocomplete:** As users enter property addresses, the application will utilize the Geocoder API to suggest possible addresses, reducing input errors and improving user experience.\n2. **Automatic Latitude\/Longitude Retrieval:** Upon selecting a suggested address or manually entering a complete address, the application will automatically query the Geocoder API to retrieve the corresponding latitude and longitude coordinates. These coordinates will be populated into the relevant fields, eliminating manual entry.\n3. **Address Verification:**  The application will leverage the Geocoder API to verify the accuracy and validity of manually entered addresses. This will ensure data quality and minimize the risk of incorrect location information.\n\n**Implementation Details:**\n\n* **API Authentication:**  The application will be registered with the BC Government API Gateway to obtain the necessary API keys for secure access to the Data BC Geocoder service ([https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)).\n* **API Integration:** The API integration will be implemented using a suitable library for communication with RESTful APIs, adhering to best practices for error handling, security, and rate limiting.\n\nThis approach will leverage a reliable and authoritative source for GIS data, enhance the user experience by automating address input and verification, and improve the overall quality of property data within the Inventory application. \n","GenTime":"2024-08-28 13:12:44"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"text":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n\n##Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","MatchedIds":[798,138,4699,4428,4408],"Prediction":"## Decision\n\nWe will implement a new \"Global Components\" feature in the CMS:\n\n* **Global Components:** A new section within the CMS allowing content editors to create and manage components intended to appear across the entire site. \n    * Each Global Component will have standard fields like title, content, and visibility toggles.\n    * These components will be rendered on every page via a dedicated view composer or middleware, ensuring consistent placement. \n\n* **Warning Message Implementation:** The specific requirement of a warning message bar will be fulfilled by creating a dedicated \"Warning Message\" Global Component. Editors can control its content and visibility independently. \n\n* **Technical Approach:** \n    * The rendering of Global Components will be handled outside the main page content to guarantee consistent display regardless of page structure.\n    * Caching mechanisms can be implemented to optimize the loading of these components on each page.\n\nThis approach offers flexibility for future site-wide elements while addressing the immediate need for a warning message bar. \n","GenTime":"2024-08-28 13:12:50"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"text":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n\n##Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","MatchedIds":[2892,2150,2896,2145,2880],"Prediction":"## Decision\nFileMailQueue should be deprecated and removed from future James releases. \n\nHere is why:\n\n* **Lack of Test Coverage:** FileMailQueue does not adhere to the junit5 test contract, resulting in disabled tests and overall test suite instability. This indicates potential bugs and makes future development and maintenance risky.\n* **Redundancy and Lack of Focus:**  Reimplementing message queue functionality within James, while other suitable implementations exist, is inefficient and diverts resources from core James development. \n* **Alternative Solutions Available:**  James already offers more robust and feature-rich MailQueue implementations like the default ActiveMQ and RabbitMQMailQueue, which are better tested and maintained.\n\n**Migration Path:**\n\n* Users currently relying on FileMailQueue should be encouraged to migrate to either the default ActiveMQ or RabbitMQMailQueue depending on their needs. \n* Documentation and migration guides should be provided to assist users in the transition.\n* A clear deprecation timeline should be communicated, allowing sufficient time for users to migrate. \n\nThis decision will allow the James project to focus on improving and supporting the more robust and widely used MailQueue implementations.\n","GenTime":"2024-08-28 13:12:56"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"text":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\n\n##Decision\nwkhtmltopdf.\\nIt shows the best results.\\n","MatchedIds":[284,282,273,5059,4308],"Prediction":"## Decision \n\n**wkhtmltopdf**\n\nWhile a Java-only solution would be ideal, `wkhtmltopdf` provides the most accurate representation of the original HTML document. Here's why:\n\n* **Visual Fidelity:** `wkhtmltopdf` leverages a web rendering engine (WebKit), which is the same technology used by browsers like Chrome and Safari. This ensures that the generated PDF closely resembles how the HTML is rendered in a browser environment.\n* **Handling Complexity:** HTML can involve intricate layouts, CSS styling, and JavaScript interactions. `wkhtmltopdf` excels in handling these complexities, preserving the original look and feel effectively. \n* **Community Support:** `wkhtmltopdf` is widely used and has a mature codebase and active community. This translates to readily available documentation, troubleshooting resources, and potential future improvements.\n\n**Tradeoffs:**\n\n* **External Dependency:**  Using `wkhtmltopdf` introduces an external dependency, which needs to be installed and managed. However, given its superior performance in this context, the tradeoff is justifiable.\n\n**Alternatives Considered:**\n\n* **Pandoc:** Primarily a document converter, Pandoc might not capture all the intricacies of HTML and CSS for a pixel-perfect conversion.\n* **Unoconv:**  While suitable for document formats, Unoconv might not be as adept at handling web-based rendering nuances as `wkhtmltopdf`.\n\n**Conclusion:**\n\n For accurate and high-fidelity HTML-to-PDF conversion, `wkhtmltopdf` emerges as the preferred choice despite being an external dependency. \n","GenTime":"2024-08-28 13:13:04"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"text":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n\n\n##Decision\nWe will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","MatchedIds":[3290,3416,1924,3295,4909],"Prediction":"## Decision\n\nWe will choose **Option 3: Ship Pod\/Application logs to CloudWatch and expose CloudWatch insights**. \n\n### Rationale\n\n* **Centralized Logging:** This approach achieves the goal of a single storage location for all logs by leveraging CloudWatch, which already houses most of our log sources.\n* **Reduced Duplication:** We avoid duplicating logs already stored in CloudWatch, minimizing storage costs and complexity.\n* **Minimized Maintenance:**  We leverage a managed service (CloudWatch) instead of managing our own ELK stack, reducing operational overhead and potential maintenance burdens. \n* **Cost-Effective:** CloudWatch Insights offers a cost-effective way to analyze logs compared to shipping to and managing a separate logging platform like Splunk or Logit.io.\n\n### Considerations\n\n* **CloudWatch Insights Capabilities:** While powerful, CloudWatch Insights might have limitations compared to dedicated log analysis platforms. We need to ensure it meets our auditing and debugging needs.\n* **Fluentd Management:** We will need to manage and maintain Fluentd for shipping Pod\/Application logs to CloudWatch. This includes monitoring, updates, and potential troubleshooting.\n\n### Alternatives Rejected\n\n* **Option 1 & 4 (Logit.io & Splunk):** These options introduce unnecessary log duplication, management overhead of additional logging platforms, and assume platform users have access to these external services.\n* **Option 2 (Self-Hosted ELK):**  Managing an ELK stack adds complexity and potential maintenance burdens, which we want to avoid. \n\nThis approach prioritizes simplicity, cost-effectiveness, and leveraging existing AWS infrastructure. We can revisit this decision if our logging needs become more complex or CloudWatch Insights proves insufficient. \n","GenTime":"2024-08-28 13:13:13"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"text":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n\n##Decision\nThe application will provide an info page.\\n","MatchedIds":[1173,1745,3127,1239,2755],"Prediction":"## Decision\nWe will create a `\/status` endpoint within the application that will return:\n\n* **Application Version:** The currently deployed version of the application. This could be a semantic version number, a Git commit hash, or a build identifier.\n* **Current UTC Time:** The current time in Coordinated Universal Time (UTC) as perceived by the application server.\n\nThis endpoint will provide valuable information for debugging by allowing developers to:\n\n1. **Verify Deployed Version:**  Quickly confirm the exact version of the application running in an environment, aiding in issue diagnosis and reproduction. \n2. **Diagnose Time-Related Issues:** Analyze time discrepancies by comparing the application's reported UTC time against other relevant timestamps, helping debug issues related to time zones, scheduling, or data consistency. \n","GenTime":"2024-08-28 13:13:18"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"text":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n\n\n##Decision\nEvery route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","MatchedIds":[3210,1050,1866,2111,1823],"Prediction":"## Decision\n\n**Route Stability and Versioning:**\n\n*   **Maintain Backwards Compatibility:**  Going forward, we will strive to maintain backwards compatibility for all existing Automate UI routes. This means avoiding route renames or moves whenever possible. \n*   **Versioned Routes for Breaking Changes:** When breaking changes are unavoidable, we will introduce new, versioned routes (e.g., `\/v2\/settings\/\u2026`) instead of modifying existing ones. Old routes will be deprecated but remain functional for a defined period.\n*   **Clear Deprecation Policy:**  A clear deprecation policy with timelines will be established and communicated to users for any deprecated routes.\n\n**Decoupling Navigation from Routes:**\n\n*   **Introduce Route Metadata:** We will decouple navigation highlighting logic from the direct route structure. This will be achieved by introducing metadata or configuration associated with each route, specifying its position within the navigation hierarchy (e.g., which tab it belongs to). \n*   **Flexible Navigation Rendering:** The UI will be updated to use this metadata for determining navigation highlighting, rather than relying solely on the route structure.\n\nThis approach allows for more flexibility in organizing and evolving the Automate UI routes over time while:\n\n1.  Minimizing disruptions for users relying on existing integrations.\n2.  Providing a clear path for introducing necessary breaking changes.\n3.  Enabling more flexibility in UI design and navigation structure without being constrained by route organization. \n","GenTime":"2024-08-28 13:13:26"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"text":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n\n\n##Decision\n* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","MatchedIds":[3237,4388,4642,4358,4937],"Prediction":"## Decision\nWe will implement the API using a single Flask application deployed as an AWS Lambda function. \n\n### Justification\nWhile deploying an entire Flask app within a Lambda function might seem unconventional, the benefits outweigh the drawbacks for this specific use case.  \n\n**Key Advantages:**\n\n* **Rapid Development:**  Leveraging our existing infrastructure and the familiar Flask framework allows for a quicker start and streamlined development process. \n* **Simplified Maintenance:** Handling one lambda function, rather than a fragmented collection, reduces operational complexity. \n* **Project Isolation:** The project's self-contained nature and well-defined boundaries minimize potential issues associated with the \"monolithic\" Lambda approach.\n* **AWS Chalice Precedent:**  AWS's own Chalice library suggests that using a web framework in Lambda is a valid pattern, particularly for smaller applications.\n\n**Mitigating the Drawbacks:**\n\n* **Cold Starts:** We acknowledge the risk of increased cold start times. We will monitor this closely and implement optimization strategies (provisioned concurrency, optimizing application initialization) if necessary.\n* **\"Anti-Pattern\" Perception:** While this approach might deviate from typical microservice patterns, the project's size and isolation make it a suitable candidate.  \n\n**Alternative Considered:**\n* **Microservices with Multiple Lambdas:** While a more conventional approach, the overhead of managing multiple functions, especially with shared code, outweighs the benefits in this particular scenario.\n\n**Future Considerations:**\n* Should the project's scope expand significantly, or if integration with other services becomes necessary, we will revisit this decision and potentially refactor into a more granular microservice architecture. \n","GenTime":"2024-08-28 13:13:34"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"text":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n\n\n##Decision\nTo support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","MatchedIds":[357,2828,631,714,745],"Prediction":"## Decision\n\nTo address the requirements for multi-tenancy in Marain, we will adopt a hierarchical tenancy model with support for both direct and indirect relationships between tenants and services. This model will be implemented using the `Marain.Tenancy` service.\n\n**1. Hierarchical Tenant Structure:**\n\n* **Root Tenant:**  Represents the highest level in the hierarchy. This tenant is used for system-level configurations and for managing access to resources shared across all other tenants.\n* **Platform Tenant:**  Represents a deployed Marain instance. It sits below the Root Tenant and manages resources specific to the instance, such as default storage accounts or databases. \n* **Client Tenant:**  Represents an organization or individual using Marain, either directly subscribing to a managed service or deploying a private instance. Client Tenants are children of a Platform Tenant.\n* **Service Tenant:** Represents an instance of a Marain service within a Platform Tenant.  Each deployed service gets its own Service Tenant.\n\n**2. Tenant Relationships and Resource Access:**\n\n* **Direct Relationship:** A Client Tenant has a direct relationship with a service when it is explicitly provisioned access to that service.\n* **Indirect Relationship:** A Client Tenant has an indirect relationship with a service when it uses a service that depends on another service. For example, a Client Tenant using Workflow would have an indirect relationship with the Operations service.\n* **Resource Isolation:** \n    * Client Tenants can be configured to use either shared resources provided by the Platform Tenant or their own dedicated resources (storage accounts, databases, etc.). \n    * Service Tenants have their own dedicated resources within the Platform Tenant, ensuring isolation between services.\n    * `Marain.Tenancy` will manage access to resources based on the tenant hierarchy and relationship types.\n\n**3. Tenant Management:**\n\n* Tenant creation, modification, and deletion will be managed through the `Marain.Tenancy` service API.\n*  Access control and permissions will be enforced through a combination of `Marain.Tenancy` and underlying cloud provider IAM roles and policies.\n\n**4. Implementation Details:**\n\n* Each tenant will be represented by a unique identifier within `Marain.Tenancy`.\n* Tenant relationships will be stored and managed within `Marain.Tenancy`.\n* `Marain.Tenancy` will use a combination of configuration and runtime checks to enforce resource access based on tenant relationships.\n\nThis hierarchical tenancy model allows for the flexibility to deploy Marain in various configurations while ensuring data isolation and granular control over resource access. It also enables us to differentiate between direct and indirect service usage, allowing for accurate licensing and billing. \n","GenTime":"2024-08-28 13:13:47"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"text":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n\n\n##Decision\nI decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","MatchedIds":[718,4868,3681,583,2093],"Prediction":"## Decision\nWe will host the project on Heroku, taking advantage of their free tier. Heroku offers a straightforward deployment process and an easy-to-use environment, making it ideal for our testing needs while incurring no cost. \n","GenTime":"2024-08-28 13:13:49"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"text":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n\n##Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n","MatchedIds":[4852,4636,1464,3488,3081],"Prediction":"## Decision\n\nImplement an **application metrics system** using a combination of the following:\n\n* **Metrics library:** Utilize a library like **Prometheus client libraries** within the application code to instrument key performance indicators (KPIs). This includes tracking metrics such as request rates, response times, error rates, resource utilization, and custom business metrics.\n* **Metrics exposition:** Expose collected metrics in a standardized format, such as **Prometheus exposition format**, making them easily scrapable by monitoring tools.\n* **Time-series database:**  Use a time-series database like **Prometheus** or similar to store and persist the collected metrics. This allows for historical analysis and trend identification.\n* **Alerting and visualization:** Integrate with a monitoring and alerting system like **Grafana** or similar.  Define alerts based on metric thresholds to be notified of potential issues. Visualize key metrics through customizable dashboards for easy monitoring and analysis.\n\n**Reasoning:**\n\n* **Lightweight:** Metrics are designed to be lightweight and have minimal performance impact compared to extensive logging. They aggregate data points over time, providing a summarized view of application health.\n* **KPI Focused:** Directly addresses the need for understanding key performance indicators crucial to application health and user experience. \n* **Actionable Insights:**  Monitoring metrics enables proactive identification of trends and anomalies, allowing for timely intervention before they impact users. \n* **Scalability:**  Chosen tools are designed to scale with growing application needs and data volumes.\n\n**Additional Considerations:**\n\n* **Metrics selection:** Carefully determine relevant KPIs based on application functionality and business goals.\n* **Alerting thresholds:**  Set meaningful alert thresholds to avoid alert fatigue while ensuring timely notification of critical issues. \n* **Dashboard design:**  Create clear and informative dashboards visualizing key metrics to facilitate understanding application health at a glance. \n","GenTime":"2024-08-28 13:13:58"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"text":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n\n##Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","MatchedIds":[2498,4896,4741,3863,3203],"Prediction":"## Decision\n**We will provide comprehensive documentation and examples for all publicly exposed classes and methods.** This documentation will:\n\n* **Clearly explain the purpose of each class and method.**\n* **Use simple language and avoid jargon.**\n* **Provide practical examples demonstrating how to use the classes and methods in various scenarios.**\n* **Include code snippets that can be easily copied and pasted.**\n* **Be regularly reviewed and updated to ensure accuracy and completeness.**\n\n**Additionally, we will implement the following measures to improve the overall user experience:**\n\n* **Provide meaningful names for classes and methods that reflect their functionality.**\n* **Organize the code into logical modules and packages.**\n* **Implement a consistent and easy-to-understand API design across the library.**\n* **Develop and maintain a comprehensive suite of unit tests to ensure the quality and reliability of the code.** \n* **Consider providing helper functions or utilities for common tasks to simplify usage.** \n\nBy focusing on documentation, clear naming conventions, logical organization, and a consistent API, we can make this project more approachable and user-friendly, even for those without deep CDK expertise. \n","GenTime":"2024-08-28 13:14:04"}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"text":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n\n\n##Decision\nWe chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","MatchedIds":[1823,4011,1866,1522,425],"Prediction":"## Decision\n\nWe will transition from using integer-based trainee IDs in the URL to using UUIDs (Universally Unique Identifiers). \n\n### Reasoning:\n\n* **Security:** Integer IDs expose a predictable sequence that can be easily guessed or iterated upon, potentially leading to unauthorized access. UUIDs, being practically unique, eliminate this predictability and enhance security. \n* **Interoperability:** Decoupling internal database IDs from external-facing identifiers improves interoperability with other systems. It prevents issues when integrating with systems that might use different ID schemes or require stable identifiers regardless of internal database changes.\n* **Future-Proofing:** Adopting UUIDs provides a robust and scalable solution for future growth and potential integrations, avoiding limitations associated with integer IDs.\n\n### Implementation Notes:\n\n* **Gradual Transition:**  A phased approach should be adopted to avoid breaking existing functionality. This might involve supporting both ID types during the transition period with appropriate redirects or API versioning.\n* **Database Migration:**  Existing database records will need to be updated with new UUIDs. Careful planning and execution are required to ensure data integrity. \n* **URL Redirection:** Permanent redirects (301) from old integer-based URLs to new UUID-based URLs should be implemented to maintain existing links and SEO.\n* **Documentation and Communication:**  Clear documentation of the change and communication to relevant stakeholders (developers, users) are essential.\n\nThis decision prioritizes security, interoperability, and long-term scalability while acknowledging the need for a well-planned implementation to minimize disruption. \n","GenTime":"2024-08-28 13:14:13"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"text":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n\n##Decision\n- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","MatchedIds":[1647,4974,2470,2215,3831],"Prediction":"## Decision\n\nTo avoid `Permission Denied` errors on Android 9 and above when using `android.net.Uri` with Workers, we will adopt the following approach:\n\n1. **Persist Permission in Worker:** Instead of passing the `Uri` directly to the Worker, we will use `context.getContentResolver().takePersistableUriPermission(uri, Intent.FLAG_GRANT_READ_URI_PERMISSION)` within the Activity or other lifecycle-aware component. This obtains a persistent URI permission that survives app restarts.\n\n2. **Store and Pass Content URI:** Store the persistent content `Uri` returned by `takePersistableUriPermission`  and pass it to the Worker. This ensures the Worker has the necessary permission to access the data. \n\n3. **Access Data with ContentResolver:** Inside the Worker, use `context.getContentResolver().openInputStream(contentUri)` to open an `InputStream` for the data, leveraging the persistent permission.\n\nThis approach ensures the Worker can access the data even after the original Activity is destroyed, preventing `Permission Denied` errors. \n","GenTime":"2024-08-28 13:14:19"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"text":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n\n\n##Decision\n- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","MatchedIds":[75,779,2802,4637,3692],"Prediction":"## Decision\n\n### Approach\n\nExtend the existing request builder API to support OData features through dedicated methods and parameters, rather than introducing a separate OData-specific builder. This approach aims to minimize disruption to the existing API surface while accommodating OData-specific needs.\n\n### Specific Changes\n\n1. **Navigation Properties:** Introduce a new method, tentatively named `navigate(navigationProperty)`, to the request builder. This method allows chaining to represent navigation properties within the URL.\n\n   ```\n   \/\/ Example: GET \/People('scottketchum')\/Friends\n   requestBuilder.get(\"People\")\n                 .key(\"scottketchum\")\n                 .navigate(\"Friends\")\n                 .build();\n   ```\n\n2. **Raw Value Retrieval:** Implement a method like `rawValue()` to append the `$value` segment to the URL for retrieving the raw value of a property.\n\n   ```\n   \/\/ Example: \/People('scottketchum')\/$value\n   requestBuilder.get(\"People\")\n                 .key(\"scottketchum\")\n                 .rawValue()\n                 .build();\n   ```\n\n3. **OData System Query Options:** Consider introducing support for common OData system query options such as `$filter`, `$orderby`, `$top`, and `$skip` through dedicated methods or parameters. For instance, a `filter(condition)` method could be added to apply filters.\n\n   ```\n   \/\/ Example with filter: \/People?$filter=FirstName eq 'Scott'\n   requestBuilder.get(\"People\")\n                 .filter(\"FirstName eq 'Scott'\")\n                 .build(); \n   ```\n\n### Benefits\n\n- Maintains consistency with the existing API for non-OData requests.\n- Provides a structured and type-safe way to build OData URLs.\n- Avoids the overhead of a separate OData-specific builder.\n\n### Considerations\n\n- Backward compatibility should be carefully considered, ensuring existing code relying on the request builder continues to function as expected.\n- Thorough documentation and examples are crucial for demonstrating the usage of the new methods and parameters.\n- Future OData features might require further extensions to the request builder API. \n","GenTime":"2024-08-28 13:14:30"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"text":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n\n\n##Decision\nSpring framework will be the backbone for `menu-generation` application.\\n","MatchedIds":[1035,2500,3374,4896,1758],"Prediction":"## Decision\nWe will use Spring Framework to implement the inversion of control and dependency injection patterns required by the hexagonal architecture.\n\nWe will leverage Spring MVC to implement REST API adapters, taking advantage of the author's experience with this framework. This will provide a consistent approach with dependency injection and simplify the implementation of REST endpoints. \n","GenTime":"2024-08-28 13:14:33"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"text":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n\n\n##Decision\nScholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","MatchedIds":[2963,3831,2859,4308,2515],"Prediction":"## Decision\n\n**Do not implement a derivative generation system for the time being.** \n\nInstead, we will rely on the browser's native capabilities for displaying a wide range of file types. This approach simplifies the architecture and reduces implementation complexity.\n\n**Reasons:**\n\n* **Modern browsers are increasingly capable:** They can now natively handle playback and display of various video formats, large files, and non-standard file types that previously required derivatives. \n* **Reduced complexity and maintenance:**  Developing and maintaining a derivative generation system requires significant resources and introduces complexity. This approach avoids that overhead.\n* **Generic icons provide clarity:** Instead of thumbnails, which are format-specific,  generic icons will be used to represent different file types. This offers a consistent and clear visual representation for all files, regardless of their format. \n\n**Future Considerations:**\n\nThis decision will be revisited if:\n\n* User experience suffers due to limitations in browser capabilities for specific file types.\n* A clear need for specific derivative formats arises, outweighing the complexity of implementation.\n\nThis approach allows us to prioritize a simpler architecture and leverage the evolving capabilities of modern web browsers while remaining flexible to adapt based on future needs and user feedback. \n","GenTime":"2024-08-28 13:14:39"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"text":"## Context\\nWe need a simple way to manage our package version.\\n\n\n##Decision\nWe use versioneer to do this for us.\\n","MatchedIds":[3691,2951,3328,4225,5177],"Prediction":"## Decision\n\nUtilize `bump2version` for managing package versions. \n\nThis approach offers the following benefits:\n\n- **Simplicity:** `bump2version` is a straightforward command-line tool, easy to learn and integrate into existing workflows.\n- **Automation:** Automates the process of incrementing version numbers in various files (e.g., `setup.py`, `package.json`) reducing manual errors. \n- **Customization:** Allows configuration for different versioning schemes (e.g., semantic versioning) and file formats.\n\nThis approach avoids over-engineering the solution while providing a practical way to handle package versions efficiently. \n","GenTime":"2024-08-28 13:14:45"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"text":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n\n\n##Decision\nTesting will be made against the production bundle to catch compilation errors before they reach our users.\\n","MatchedIds":[2687,3853,3171,1416,4179],"Prediction":"## Decision\n\nTo mitigate the risk of discrepancies between development and production builds, we will implement a two-pronged approach:\n\n1. **End-to-End (E2E) Testing on Production-Like Bundles:** We will integrate E2E tests into our CI\/CD pipeline that run against the production-ready bundle generated by the Google Closure Compiler in advanced compilation mode. This will ensure that the code executed by our users behaves as expected after undergoing all optimizations and transformations.\n\n2. **Selective Source Map Generation:** While the Closure Compiler's advanced optimizations can hinder debugging, we will configure it to generate source maps for critical parts of our application. This will allow us to debug production issues more effectively by mapping them back to the original source code. We will carefully analyze and document the trade-offs of generating source maps (increased bundle size vs. improved debuggability) to find the right balance.\n\nBy incorporating these practices, we aim to catch potential issues early in the development lifecycle while ensuring a robust and reliable experience for our users. \n","GenTime":"2024-08-28 13:14:51"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"text":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n\n##Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","MatchedIds":[2181,4644,3069,1180,160],"Prediction":"## Decision\n\nWe will use the OpenAPI Specification (formerly known as Swagger) to document our API. \n\n### Positive Consequences:\n\n* **Human and Machine Readable:** OpenAPI provides both human-readable documentation via tools like Swagger UI and a machine-readable format (JSON or YAML).\n* **Automatic Client Generation:**  OpenAPI enables the automatic generation of client libraries for various languages, significantly reducing the effort of manually creating client services. \n* **Improved Developer Experience:**  Provides interactive API documentation, making it easier for developers to understand and consume the API. \n* **Wide Industry Adoption:**  OpenAPI is a widely adopted standard, ensuring good tool support and resources.\n\n### Negative Consequences:\n\n* **Learning Curve:**  Adopting OpenAPI requires a small learning curve for developers unfamiliar with the specification. \n* **Maintenance Overhead:**  Keeping the OpenAPI documentation in sync with the API code requires some effort. However, this overhead is far outweighed by the benefits of automated client generation and better documentation.\n\n### Rationale:\n\nChoosing OpenAPI addresses both decision drivers: it ensures readable API documentation through its human-readable formats and tooling, and it minimizes the effort of creating client services via automatic code generation. This approach will lead to a more maintainable codebase, improved developer experience, and faster integration with clients. \n","GenTime":"2024-08-28 13:14:58"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"text":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n\n\n##Decision\nRestored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","MatchedIds":[2536,3144,2793,621,1173],"Prediction":"## Decision\n\nA Terraform remote backend configuration will be added to the module. This will allow teams to use the module without needing to add their own backend configuration. The specific backend configuration (e.g., using AWS S3) will be determined by the requirements of the module and the Modernisation Platform.\n","GenTime":"2024-08-28 13:15:00"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"text":"## Context and Problem Statement\\nI need a way to document design decisions.\\n\n\n##Decision\nChosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","MatchedIds":[2261,4699,4428,4408,3119],"Prediction":"## Decision\nWe will use Architecture Decision Records (ADRs), as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 13:15:03"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"text":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n\n\n##Decision\n- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","MatchedIds":[3031,1193,157,1119,1631],"Prediction":"## Decision\n\nWe will create a dedicated private Angular library to house the shared components. This library will be published to a private repository (e.g., a private npm registry or an organization-wide artifact repository). \n\nHere's a breakdown of the approach and its advantages:\n\n**Approach:**\n\n1. **Create a new Angular library project:** This project will contain all shared components, directives, services, and styles.\n2. **Structure for scalability:** Organize the library into modules based on feature areas or component types. \n3. **Enforce design language:** Use a CSS framework like Angular Material or a custom-built design system to maintain visual consistency. Ensure all components within the library strictly adhere to this framework.\n4. **Craft a consistent API:** Design component inputs\/outputs and events to be intuitive, well-documented, and aligned across the library.\n5. **Rigorous testing:** Implement comprehensive unit tests for all components to guarantee stability and catch regressions early on.\n6. **Versioning and publishing:** Use semantic versioning (SemVer) for releases and publish the library to a private repository.  \n7. **Consumption in applications:** Applications within the suite will import the shared library and utilize its components.\n\n**Advantages:**\n\n* **Reusability:**  Avoid code duplication and ensure consistent implementation across applications.\n* **Maintainability:** Updates to shared components only need to happen in one place, simplifying maintenance and ensuring consistency.\n* **Faster development:**  Pre-built components reduce development time for new features and applications.\n* **Consistent design language:** A unified look and feel across all applications enhance the user experience and brand identity.\n* **Scalability:** The library can grow to accommodate new components and functionalities as needed. \n\n**Additional Considerations:**\n\n* **Documentation:** Create comprehensive documentation for the library, including usage examples, API references, and potentially a Storybook showcasing component variations.\n* **Communication:** Establish clear communication channels between the library maintainers and developers using the library to address questions, gather feedback, and announce updates. \n* **Version management:** Carefully manage library versions and updates to minimize breaking changes for consuming applications.\n\nThis approach provides a structured and maintainable way to share Angular components across multiple applications, ensuring design consistency and development efficiency. \n","GenTime":"2024-08-28 13:15:14"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"text":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n\n\n##Decision\nCredhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","MatchedIds":[210,3915,2240,4397,216],"Prediction":"## Decision\n\nWe will automate certificate rotation by leveraging Credhub's built-in capabilities. This will involve:\n\n* **Creating a Concourse pipeline dedicated to certificate rotation.** This pipeline will be triggered on a regular schedule (e.g., weekly).\n* **Integrating Credhub's certificate rotation API into the pipeline.** The pipeline will use Credhub's API to generate new certificates, sign them with the appropriate CA, and update any relevant services with the new certificates.\n* **Implementing monitoring and alerting for certificate expiration.** This will ensure that we are notified well in advance of any certificate expirations and can take appropriate action.\n* **Removing the manual certificate rotation process and the `check-certificates` job.**  These will be replaced by the automated pipeline.\n\n### Positive Consequences\n\n* **Reduced operational overhead:** Automating certificate rotation eliminates the need for manual intervention, freeing up operator time for other tasks.\n* **Improved security:**  Regular certificate rotation reduces the risk of compromise by limiting the amount of time an attacker could exploit a compromised certificate. \n* **Increased reliability:** Automating the process reduces the risk of human error and ensures that certificates are rotated on a consistent schedule.\n\n### Negative Consequences\n\n* **Increased complexity:**  Implementing and maintaining the automated pipeline will require additional effort.\n* **Potential for disruption:** If not implemented carefully, automated certificate rotation could potentially disrupt services during the rotation process. \n\nTo mitigate the negative consequences, we will:\n\n* **Thoroughly test the automated pipeline in a staging environment before deploying to production.**\n* **Implement monitoring and alerting to detect and respond to any issues during certificate rotation.**\n* **Document the new process and train operators on how to use it.** \n","GenTime":"2024-08-28 13:15:22"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"text":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n\n\n##Decision\nAdd the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","MatchedIds":[398,1193,2626,3691,3483],"Prediction":"## Decision\n\nWe will use `git submodules` to incorporate the citerus library into our project. \n\nThis approach addresses the following concerns:\n\n* **Effortless updates:** Using `git submodules` allows for easy synchronization with the upstream repository, incorporating any progress made in the citerus library without manual installation.\n* **Local Modifications:** It provides the flexibility to make and test local modifications to the library code as needed.\n* **Independent Development:**  The submodule approach allows for significant changes in the library usage without directly impacting the upstream project or requiring immediate consensus with its maintainers.\n* **Familiarity:** `git submodules`, while requiring some learning, still operate within the familiar context of a git repository, addressing the discomfort with non-standard Maven project structures.\n\nWhile `git submodules` can have their own complexities, this approach strikes a balance between ease of use, control over the codebase, and independent development. \n","GenTime":"2024-08-28 13:15:28"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"text":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n\n\n##Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","MatchedIds":[2725,1614,1067,2091,1563],"Prediction":"## Decision\n\nThis ADR proposes adopting a GitOps approach to managing GitHub organizations using tools like [go-github-org](https:\/\/github.com\/google\/go-github-org) or [probot\/settings](https:\/\/github.com\/probot\/settings). \n\nThis approach involves:\n\n1. **Configuration as Code:** Define organization settings, repository configurations, team memberships, and labels declaratively in YAML or JSON files stored within a dedicated Git repository.\n2. **Version Control:** Manage changes to organization configuration through pull requests on the dedicated repository. This enables review workflows, history tracking, and rollbacks.\n3. **Automated Enforcement:** Utilize CI\/CD pipelines to automatically apply changes from the configuration repository to the GitHub organization, ensuring consistency and reducing manual intervention.\n\n**Benefits:**\n\n* **Transparency:** All configuration details are publicly accessible within the repository.\n* **Auditing:** Changes are tracked, enabling audit trails for compliance and troubleshooting.\n* **Collaboration:** Pull requests facilitate discussions and reviews from multiple contributors.\n* **Self-Service:** Empower individuals to propose changes through pull requests, fostering a contributor-driven culture.\n* **Consistency:** Automated enforcement ensures that the organization configuration remains consistent with the defined code.\n\n**Implementation Details:**\n\n* Select a suitable tool like  `go-github-org` or `probot\/settings`.\n* Create a dedicated repository for storing organization configuration.\n* Define organization settings, teams, repositories, and label configurations in code.\n* Implement a CI\/CD pipeline to automatically apply approved changes to the organization.\n\n**Consequences:**\n\n* **Learning Curve:** Team members require familiarization with the chosen tool and GitOps principles.\n* **Maintenance Overhead:** Maintaining the configuration code and CI\/CD pipeline requires effort.\n\n**Overall, adopting GitOps for GitHub organization management aligns with the Operate First principles by promoting transparency, collaboration, and automation. While some initial investment is required, the long-term benefits in terms of governance, maintainability, and community engagement make it a worthwhile endeavor.** \n","GenTime":"2024-08-28 13:15:38"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"text":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n\n\n##Decision\n1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","MatchedIds":[2523,5213,306,2488,1180],"Prediction":"## Decision\n\n\u4e3a\u4e86\u89e3\u51b3\u4ee3\u7801\u91cd\u590d\u548c\u5e93\u7248\u672c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u642d\u5efa\u79c1\u6709 PyPI \u4ed3\u5e93\u6765\u7ba1\u7406\u6211\u4eec\u81ea\u5df1\u7684 Python \u5305\u3002\n\n### \u597d\u5904\n\n* **\u96c6\u4e2d\u7ba1\u7406**:  \u6240\u6709\u9879\u76ee\u4f7f\u7528\u540c\u4e00\u4e2a\u6e90\u83b7\u53d6\u5e93\uff0c\u907f\u514d\u62f7\u8d1d\u548c\u7248\u672c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\n* **\u6613\u4e8e\u7ef4\u62a4**:  \u5e93\u53ea\u9700\u8981\u5728\u4e00\u4e2a\u5730\u65b9\u66f4\u65b0\uff0c\u6240\u6709\u9879\u76ee\u90fd\u80fd\u4f7f\u7528\u6700\u65b0\u7248\u672c\u3002\n* **\u63d0\u9ad8\u4ee3\u7801\u590d\u7528**:  \u65b9\u4fbf\u5176\u4ed6\u9879\u76ee\u53d1\u73b0\u548c\u4f7f\u7528\u5df2\u6709\u7684\u5e93\uff0c\u51cf\u5c11\u91cd\u590d\u5f00\u53d1\u3002\n* **\u7248\u672c\u63a7\u5236**: \u53ef\u4ee5\u6e05\u6670\u5730\u8ffd\u8e2a\u5e93\u7684\u7248\u672c\u5386\u53f2\u548c\u53d8\u66f4\u8bb0\u5f55\u3002\n\n###  \u53ef\u9009\u65b9\u6848\n\n* \u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u642d\u5efa\u79c1\u6709 PyPI \u4ed3\u5e93\uff0c\u4f8b\u5982\uff1a\n    * **Nexus Repository**:  \u529f\u80fd\u5f3a\u5927\u7684\u4ed3\u5e93\u7ba1\u7406\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u683c\u5f0f\uff0c\u5305\u62ec PyPI\u3002\n    * **Devpi**:  \u8f7b\u91cf\u7ea7\u7684 PyPI \u4ed3\u5e93\uff0c\u6613\u4e8e\u90e8\u7f72\u548c\u4f7f\u7528\u3002\n* \u4f7f\u7528\u4e91\u670d\u52a1\u642d\u5efa\u79c1\u6709 PyPI \u4ed3\u5e93\uff0c\u4f8b\u5982\uff1a\n    * **AWS CodeArtifact**: AWS \u63d0\u4f9b\u7684\u5236\u54c1\u5e93\u670d\u52a1\uff0c\u652f\u6301 PyPI \u683c\u5f0f\u3002\n    * **JFrog Artifactory**: \u4f01\u4e1a\u7ea7\u5236\u54c1\u5e93\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301 PyPI \u548c\u5176\u4ed6\u591a\u79cd\u683c\u5f0f\u3002\n\n###  \u540e\u7eed\u6b65\u9aa4\n\n1.  \u9009\u62e9\u5408\u9002\u7684\u65b9\u6848\u5e76\u642d\u5efa\u79c1\u6709 PyPI \u4ed3\u5e93\u3002\n2.  \u5c06\u73b0\u6709\u7684\u901a\u7528\u5e93\u6253\u5305\u5e76\u4e0a\u4f20\u5230\u79c1\u6709 PyPI \u4ed3\u5e93\u3002\n3.  \u4fee\u6539\u9879\u76ee\u914d\u7f6e\uff0c\u4f7f\u7528\u79c1\u6709 PyPI \u4ed3\u5e93\u4f5c\u4e3a\u4f9d\u8d56\u6e90\u3002\n4.  \u5236\u5b9a\u4ee3\u7801\u5e93\u5f00\u53d1\u89c4\u8303\uff0c\u9f13\u52b1\u4ee3\u7801\u590d\u7528\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u3002\n\n### \u8d1f\u9762\u5f71\u54cd\n\n*  \u9700\u8981\u6295\u5165\u65f6\u95f4\u548c\u7cbe\u529b\u642d\u5efa\u548c\u7ef4\u62a4\u79c1\u6709 PyPI \u4ed3\u5e93\u3002\n*  \u5f00\u53d1\u8005\u9700\u8981\u5b66\u4e60\u65b0\u7684\u5de5\u5177\u548c\u6d41\u7a0b\u3002 \n","GenTime":"2024-08-28 13:15:46"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"text":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n\n\n##Decision\nWe'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","MatchedIds":[306,307,2488,2111,2446],"Prediction":"## Decision\n\nWe will implement a **feedback system specifically tailored to individual orders**, rather than relying solely on general surveys.\n\nHere's why:\n\n* **Targeted insights:** Order-specific feedback provides direct insights into customer satisfaction with a particular meal and the service surrounding it. This granular approach allows businesses to pinpoint areas for improvement more effectively than general surveys. \n* **Actionable data:**  Feedback tied to a specific order (e.g., \"The pasta was cold\") enables businesses to address immediate issues and potentially rectify a negative experience. This is more actionable than feedback from general surveys. \n* **Improved customer engagement:**  Allowing users to share their dining experience through reviews creates a direct communication channel with the business.  This can lead to increased user engagement and foster a sense of community within the app.\n\n**Implementation Considerations:**\n\n* The feedback system should be integrated into the order flow, allowing users to easily leave reviews after an order is completed (either delivery or pickup).\n*  Businesses should be able to view, manage, and respond to feedback within their respective dashboards.\n*  Consider incorporating a rating system (e.g., star ratings) alongside written reviews to provide a quick overview of customer sentiment. \n\nWhile general surveys still hold value for gathering broad feedback, focusing on order-specific feedback mechanisms will offer more targeted, actionable data, leading to more effective service improvements. \n","GenTime":"2024-08-28 13:15:53"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"text":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n\n\n##Decision\nIn order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","MatchedIds":[2934,2933,2945,2944,1193],"Prediction":"## Decision\n\nTo manage the complexity of exposing libnd4j's build flexibility, Nd4j will adopt a tiered approach:\n\n**1. Pre-built Binaries:**\n\n- Nd4j will provide pre-built binaries for common platforms and configurations (e.g., CPU-only, CUDA, etc.) via Maven Central.\n- These binaries will target a balance of performance and minimal dependencies, suitable for most users.\n\n**2. Environment Variables:**\n\n- For users requiring specific libnd4j configurations, Nd4j will expose key build options as environment variables.\n- These variables will influence the selection of pre-built binaries or trigger a custom build (see below).\n- Examples include:\n    - `ND4J_BACKEND`: Specifies the backend (CPU, CUDA, etc.).\n    - `ND4J_CUDA_VERSION`: Specifies the CUDA version.\n    - `ND4J_BLAS_IMPLEMENTATION`: Specifies the BLAS implementation (e.g., OpenBLAS, MKL).\n\n**3. Custom Builds (Advanced):**\n\n- Advanced users can trigger custom libnd4j builds directly from their build system (e.g., Maven, Gradle).\n- Nd4j will provide documentation and build scripts to facilitate this process.\n- This approach offers maximum flexibility but requires familiarity with native library building.\n\n**User Experience:**\n\n- Most users will be unaffected, automatically receiving suitable pre-built binaries.\n- Users requiring specific configurations can leverage environment variables for a streamlined experience.\n- Advanced users can opt for custom builds for full control.\n\n**Maintenance and Support:**\n\n- Tiered approach simplifies maintenance by focusing on a limited set of pre-built configurations.\n- Environment variables provide a clear mechanism for users to communicate their needs.\n- Custom builds, while flexible, are acknowledged as advanced use cases with potential support implications.\n","GenTime":"2024-08-28 13:16:04"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"text":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n\n\n##Decision\nIn order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","MatchedIds":[231,3469,2440,3915,3147],"Prediction":"## Decision\nWe will decouple the idle CPU monitor from the free memory monitor. We will establish a separate threshold for the idle CPU monitor based on empirical observation and performance testing to ensure sufficient headroom for cell function without triggering premature scaling events. \n\n## Status\nAccepted\n\n## Consequences\n* We will need to conduct analysis and potentially performance testing to determine an appropriate threshold for the idle CPU monitor.\n* This may lead to earlier scaling events based on CPU utilization than we would have seen previously when tied to free memory. \n* This should result in a more accurate and efficient utilization of resources based on actual application needs, rather than an assumed linear correlation with memory. \n","GenTime":"2024-08-28 13:16:08"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"text":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n\n\n##Decision\n- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","MatchedIds":[284,276,287,273,445],"Prediction":"You are missing the \"Considered Options\" section to provide a proper decision. Please provide a list of options (libraries, tools, approaches) that were considered for text extraction from different file types. \n\nFor example:\n\n# Considered Options\n*  **Apache Tika:** A Java library for detecting and extracting metadata and text content from various file formats. \n*  **Apache POI:** A Java library for reading and writing Microsoft Office documents, including text extraction.\n*  **(Add more options here for other file types you want to support, e.g., HTML parsing libraries, OCR for image files, etc.)** \n\n## Decision\n(Provide your decision here based on the listed options, focusing on using Java\/Scala libraries and justifying your choice). \n","GenTime":"2024-08-28 13:16:13"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"text":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n\n\n##Decision\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","MatchedIds":[1117,4766,1996,527,3572],"Prediction":"## Decision\n**Chosen option:** Implement a mapping layer within Monocle's configuration. \n\n**Reasoning:**\n\n* **Simplicity:** By handling identity merging within Monocle's configuration, we avoid adding complexity to the EL query system. This keeps the query language focused and easier to maintain.\n* **Flexibility:** Using the Monocle configuration file allows for flexible and customizable mapping rules.  Administrators can define how identities from different code review systems (e.g., GitHub, GitLab, Bitbucket) are mapped to a unified identity within Monocle. \n* **Maintainability:** This approach centralizes identity mapping logic within the Monocle configuration, making it easier to understand, modify, and troubleshoot.\n\n**Implementation Details:**\n\n1. **Configuration:** Introduce a new section in the Monocle configuration file dedicated to identity mapping. This section will allow administrators to specify rules for matching and merging identities from different code review systems. \n2. **Mapping Rules:** The mapping rules can be based on various criteria, such as:\n    * Email addresses\n    * Usernames\n    * Unique identifiers provided by the code review systems\n3. **Identity Resolution:** When Monocle processes data from code review systems, it will use the configured mapping rules to resolve and merge contributor identities.\n4. **Documentation:** Provide clear and concise documentation on configuring identity mapping within Monocle.\n\n**Positive Consequences:**\n\n* Simplified EL query system.\n* Flexible and configurable identity mapping.\n* Improved data accuracy and consistency by merging contributor information.\n* Centralized and maintainable identity resolution logic. \n","GenTime":"2024-08-28 13:16:21"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"text":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n\n\n##Decision\nCreate an instance with an easy test (Just request the keys and the metrics).\\n","MatchedIds":[2273,4868,4359,4179,1258],"Prediction":"## Decision\nWe will use Blazemeter's Taurus for benchmarking tests because it is simple to set up, offers flexibility in test creation with support for JMeter and Selenium, and provides comprehensive reporting that can be analyzed in various tools including JMeter and Jupyter Notebook. \n","GenTime":"2024-08-28 13:16:24"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"text":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n\n##Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","MatchedIds":[693,2181,1410,1172,1180],"Prediction":"## Decision\nWe will use an API documentation generator that supports both human-readable documentation and machine-readable specifications. This approach offers several advantages:\n\n* **Automatic Generation**: By integrating with our codebase, the documentation can be automatically generated, ensuring it stays in sync with the latest changes.\n* **Reduced Maintenance**: Automatic generation eliminates the tedious task of manually updating separate documentation, freeing up developers to focus on other tasks.\n* **Improved Consistency**: Using a standardized format ensures consistency across all API documentation.\n* **Testability**: Machine-readable specifications, such as OpenAPI, allow for automated testing of the API, ensuring its functionality and facilitating early detection of breaking changes.\n\n**Chosen option**: Integrate an API documentation generator like [Swagger](https:\/\/swagger.io\/) or [Postman](https:\/\/www.postman.com\/) directly into our development workflow. These tools allow us to:\n\n* Embed documentation directly in our code using annotations or comments.\n* Automatically generate interactive API documentation that developers can use to test API calls in real-time.\n* Benefit from a robust ecosystem of tools and integrations for testing, monitoring, and client code generation.\n\n### Positive Consequences:\n\n* Always up-to-date documentation.\n* Reduced maintenance effort.\n* Improved API consistency.\n* Enhanced testability and reduced risk of breaking changes.\n* Streamlined development workflow.\n\n### Negative Consequences:\n\n* Initial setup and learning curve for the chosen tool.\n* Potential need for developers to adhere to specific documentation conventions.\n","GenTime":"2024-08-28 13:16:31"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"text":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n\n\n##Decision\nWe decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","MatchedIds":[1442,4656,1985,842,1495],"Prediction":"## Decision\n\nThis ADR proposes the implementation of a Wasm light client module in ibc-go to address the challenges of adding and updating light clients. \n\n**The Wasm light client module will be implemented with the following features:**\n\n1. **Wasm Bytecode Execution:** Light clients will be implemented in Rust and compiled to Wasm bytecode. This bytecode will be executed within a secure Wasm VM integrated into the ibc-go codebase.\n\n2. **Proxy Light Client Interface:** The Wasm light client module will expose a proxy interface that conforms to the existing IBC light client interface. This will allow seamless integration with the rest of the ibc-go codebase without requiring major modifications.\n\n3. **Message Routing:** The proxy interface will handle routing incoming IBC messages related to light client functionality to the appropriate handler function within the loaded Wasm module.\n\n4. **Governance-Gated Deployment:** Uploading new Wasm light client code or updating existing ones will be a governance-controlled operation. This ensures that only vetted and approved light client implementations can be used within the IBC ecosystem. \n\n5. **Client Instantiation Flexibility:** The Wasm module will allow for the instantiation of clients using any of the available Wasm light client types, promoting flexibility and experimentation with different light client designs.\n\n**This approach offers several benefits:**\n\n* **Simplified Light Client Upgrades:**  Chains can upgrade their light client implementations on other chains via governance proposals, eliminating the need for coordinated hard forks and reducing downtime. \n* **Faster Light Client Development:** Developers can implement new light clients in Rust without modifying the ibc-go codebase, leading to quicker development and deployment cycles.\n* **Increased Innovation:**  The Wasm approach lowers the barrier to entry for light client developers, potentially leading to more diverse and innovative solutions.\n\n**Security Considerations:**\n\n* Secure Wasm VM: Choosing and implementing a secure Wasm VM with robust sandboxing is crucial to protect the core IBC application from malicious or faulty light client code.\n* Governance Risk: A malicious Wasm light client, if approved through governance, could potentially compromise the security of the IBC network.  Thorough auditing and community review of proposed light client implementations are essential.\n\n**Future Work:**\n\n* Define a standardized interface for Wasm light clients to ensure interoperability and security.\n* Develop tooling and infrastructure to support the development, testing, and deployment of Wasm light clients.\n* Implement robust governance processes for vetting and approving new Wasm light client implementations.\n\nThis decision aims to create a more flexible and future-proof approach to managing light clients in IBC while fostering innovation and community involvement. \n","GenTime":"2024-08-28 13:16:45"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"text":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n\n\n##Decision\n### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","MatchedIds":[4656,1442,1495,78,1982],"Prediction":"## Decision\n### IBC Relayer `v0.1` Assumptions\n* **Concurrency:**\\\n   * `v0.1` will be implemented using **synchronous Rust**, and will **not leverage concurrency** features of the language.\\\n   * Operations such as querying for events on a chain will block until they complete, and long-running operations will not be interruptible.\n* **Single Process, Single Thread:**\\\n   * The relayer will run within a **single operating system process**, and will execute on a **single thread**.\n* **Single Chain Pair:**\\\n   * The initial version is scoped to **relaying packets on a single pair of chains**.\n* **Minimal Error Handling and Recovery:**\\\n   * While basic error handling is expected, `v0.1` **will not implement sophisticated error recovery mechanisms**.\n   * In case of unexpected errors, the relayer may panic or halt execution. \n* **No Persistent Storage:**\\\n   * `v0.1` will **not persist its internal state to disk**.\n   * Upon restart, the relayer will begin from a clean slate. \n* **Limited Feature Set:**\\\n    * The initial version will focus on the core relaying functionality: listening for IBC events, constructing and submitting transactions for packet relaying.\\\n    * Advanced features like auto-discovery of new channels, automatic connection establishment, or support for complex IBC applications (like interchain accounts) are out of scope for this version. \n\n### Justification for Assumptions\nThese assumptions are essential for a manageable initial implementation of the relayer. They allow us to concentrate on developing a solid foundation with a clean architecture and a robust concurrency model that can be extended in future versions to encompass more complex features and scenarios.\n\n### Future Iterations\nSubsequent versions of the relayer will aim to relax these assumptions incrementally. For example, `v0.2` could introduce asynchronous operations and thread pools to manage concurrent tasks, while `v0.3` could explore persistent storage and advanced error recovery.\n\nBy clearly outlining these assumptions, we define a realistic scope for `v0.1` and establish a roadmap for future development, enabling us to deliver a valuable tool to the IBC ecosystem iteratively.\n","GenTime":"2024-08-28 13:16:56"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"text":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n\n##Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","MatchedIds":[4738,4637,1255,4378,779],"Prediction":"## Decision\n\nChosen option: **\"Implement a query DSL with a user-friendly search interface\"**, because it offers the best balance between flexibility, performance, and user experience.\n\n### Decision Drivers\n\n* **Flexibility:** A query DSL allows defining complex searches across multiple fields with various operators (AND, OR, startsWith, contains).\n* **Performance:**  Implementing complex search logic solely on the frontend can become slow and resource-intensive with large datasets. A backend solution using a query DSL can leverage database indexing and optimizations.\n* **User Experience:**  While powerful, a raw query DSL might be daunting for some users.  We will provide a user-friendly search interface that translates user input into the DSL, similar to advanced search builders found on popular websites.\n\n### Implementation Details\n\n1. **Query DSL Definition:** \n    * Define a simple query language (e.g., based on JSON) that supports the required operators (AND, OR, startsWith, contains) and field targeting.\n    * Example: \n       ```json\n       {\n         \"and\": [\n           {\"title\": {\"contains\": \"keyword\"}},\n           {\"tags\": {\"or\": [\"tag1\", \"tag2\"]}},\n           {\"author\": \"John Doe\"}\n         ]\n       }\n       ```\n2. **Backend Integration:**\n    * Implement a parser to translate the DSL into database queries.\n    * Consider using existing libraries or frameworks for parsing and executing DSL queries.\n3. **User Interface:**\n    * Design a user-friendly search interface that allows users to construct complex queries without needing to know the underlying DSL syntax.\n    * Provide features like:\n        * Dropdown menus to select fields.\n        * Autocomplete suggestions for tags and authors.\n        * Visual representation of the query being built (e.g., using logical operators).\n\n### Positive Consequences\n\n* **Improved Search Functionality:** Users can perform more specific and efficient searches.\n* **Scalability:** The backend-driven approach can handle increasing data volume and query complexity.\n* **Better User Experience:** The user-friendly interface makes complex search accessible to all users.\n\n### Negative Consequences\n\n* **Increased Development Effort:**  Implementing a query DSL and user interface requires dedicated development time.\n* **Potential Learning Curve:**  Users may need some time to familiarize themselves with the new search interface, even with a user-friendly design.\n\n### Mitigation Strategies\n\n* **Iterative Development:**  Release the new search functionality in stages, starting with basic features and gradually adding more complex options.\n* **User Feedback:**  Gather user feedback during development and after release to identify any usability issues and improve the search experience.\n* **Documentation and Examples:**  Provide clear documentation and examples to help users understand how to use the new search interface effectively. \n","GenTime":"2024-08-28 13:17:09"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"text":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n\n##Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","MatchedIds":[1659,1661,1243,3776,3470],"Prediction":"## Decision\n\nChosen option: **\"ASP.NET Web API (C#)\"**, because\n\n- **Alignment with Existing Infrastructure and Skills:** This aligns with TradeMe's existing technology stack and leverages the team's expertise in .NET, C#, and potentially Dapper, reducing the learning curve and ensuring maintainability. \n- **Modern and Performant:** Targeting .NET Core 2 provides a modern, performant, and cross-platform foundation for the API.\n- **Testability and Maintainability:** Utilizing Dapper ORM and applying a repository pattern promotes testability through clear separation of concerns and facilitates unit testing, leading to more robust and maintainable code. \n","GenTime":"2024-08-28 13:17:14"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"text":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n\n\n##Decision\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","MatchedIds":[4307,3653,1193,2526,3203],"Prediction":"## Decision\nChosen option: **\"Reuse the pattern refinement process implementation for the pattern detection process\"**, because this approach directly addresses the identified decision drivers.\n\n### Positive Consequences\n* **Reduced code duplication:** Reusing the existing implementation for subgraph replacement significantly reduces code duplication.\n* **Improved consistency:** Using the same implementation ensures consistency in how subgraph replacement is handled in both processes. \n* **Reduced development time:**  Reusing existing code speeds up the implementation of the pattern detection process.\n\n### Negative Consequences \n* **Increased coupling:** Reusing the implementation introduces a dependency between the pattern refinement and detection processes. This could potentially impact future modifications if not carefully managed. \n\n### Mitigation for Negative Consequences\n* **Abstraction:** Introduce an abstract class or interface to define the common subgraph replacement logic. The pattern refinement and detection processes can then implement this abstraction using their respective structures (Refinement Structure and Detector). This approach promotes loose coupling and allows for independent evolution of the two processes.\n","GenTime":"2024-08-28 13:17:19"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"text":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n\n##Decision\n**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","MatchedIds":[1532,2502,3069,3159,4359],"Prediction":"## Decision\n\nWe will use **Fluent Validation** for validating our API requests. \n\nWhile Data Annotations offer a simple way to enforce basic validation rules, the limitations outlined in the context present significant drawbacks for our API development:\n\n* **Maintainability:** Scattering validation logic across data model classes hinders code readability and makes it difficult to maintain a consistent validation strategy. \n* **Testability:** Isolating and testing validation logic embedded within data models can be cumbersome.\n* **Flexibility:** The lack of support for conditional validation and customizable error messages restricts the ability to implement complex business rules and provide user-friendly feedback.\n\nFluent Validation addresses these concerns by:\n\n* **Centralizing Validation Logic:**  Creating dedicated validator classes improves code organization and maintainability.\n* **Enhancing Testability:**  The decoupling of validation logic from data models simplifies unit testing.\n* **Increasing Flexibility:**  Support for conditional validation, custom error messages, and chaining validators provides greater control over validation rules.\n\nBy leveraging Fluent Validation, we can establish a robust and adaptable validation framework for our API, ensuring data integrity and improving the overall quality of our application. \n","GenTime":"2024-08-28 13:17:25"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"text":"## Context\\nWe need to record the architectural decisions made for our team.\\n\n\n##Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 13:17:27"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n\n\n##Decision\nDecorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Prediction":"## Decision\n\n**Decorator objects** will be used to add responsibilities to individual objects at runtime. This approach will be favored when:\n\n* The added behavior is specific to an individual object instance.\n* The added behavior is dynamic and may not be needed for all instances of the class.\n* We want to avoid subclassing explosion and keep the core class focused on its primary responsibilities.\n\n**Presenter objects**, on the other hand, will be used to format and prepare data for presentation in the UI layer. These objects will be responsible for:\n\n* Retrieving data from one or multiple domain objects.\n* Formatting data into a view-friendly structure, potentially using helper classes.\n* Handling UI-specific logic, like pagination or filtering of presented data.\n* Not modifying the underlying domain objects' data.\n\nThis distinction ensures a clear separation of concerns: decorators modify object behavior dynamically, while presenters focus on presentation logic and data formatting for the UI. \n","GenTime":"2024-08-28 13:17:33"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"text":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n\n##Decision\nWe will use GitHub pages for all PACE project documentation.\\n","MatchedIds":[5177,5178,5171,1741,2968],"Prediction":"## Decision\nThe user documentation for all PACE projects will be built and deployed to **GitHub Pages**. \n\nThis decision is made for the following reasons:\n\n* **Consistency:** Using a single platform for all PACE projects simplifies maintenance and provides a consistent user experience.\n* **C library support:** While both platforms support Sphinx, GitHub Pages offers more flexibility for building documentation that includes APIs from compiled C libraries, which is a requirement for Brille.\n* **Integration with GitHub:** Hosting documentation on GitHub Pages offers seamless integration with the existing code repositories for the PACE projects.\n\nThis approach allows for a streamlined documentation process that can accommodate the technical requirements of all PACE projects. \n","GenTime":"2024-08-28 13:17:37"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"text":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n\n\n##Decision\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","MatchedIds":[1660,1661,3470,1240,1243],"Prediction":"## Decision\nChosen option: **\"Angular (with TypeScript)\"**, because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects. \n","GenTime":"2024-08-28 13:17:39"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"text":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n\n\n##Decision\nConsidering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","MatchedIds":[1992,1125,5090,783,381],"Prediction":"## Decision\n\nWe will adopt a hybrid approach combining **Constructible Stylesheets** and a **shared CSS library** structured with ITCSS principles to style Web Components effectively while minimizing code duplication and maintaining consistency.\n\n**Rationale:**\n\n- **Constructible Stylesheets** allow defining styles within the component's JavaScript module, scoping styles to the Web Component's shadow DOM. This ensures style encapsulation and avoids conflicts with global styles or styles from other components.\n- **Shared CSS Library (ITCSS):**  A centralized library structured with ITCSS will house shared styles, variables, mixins, and utility classes. This library can be imported into individual component stylesheets to maintain consistency and avoid code duplication for common styles.\n\n**Implementation:**\n\n1. **Shared Library:**\n    - Create a shared CSS library organized with ITCSS layers (Settings, Tools, Generic, Elements, Objects, Components, Trumps).\n    - Define common variables, mixins, and utility classes in the appropriate layers.\n    - Utilize CSS Custom Properties (variables) extensively to allow for easy customization and theming at the component level.\n\n2. **Component Styles:**\n    - Utilize Constructible Stylesheets to create component-specific styles within the component's JavaScript module.\n    - Import the shared library into each component's stylesheet using `@import`.\n    - Leverage shared styles, mixins, and variables from the imported library.\n    - Override or extend shared styles as needed within the component's stylesheet using more specific selectors or by customizing CSS Custom Property values.\n\n**Example:**\n\n```javascript\n\/\/ shared-styles.css (ITCSS structure)\n\/* Settings *\/\n:root {\n  --primary-color: #007bff;\n}\n\n\/* ... other layers ... *\/\n\n\/\/ my-component.js\nimport { LitElement, html, css } from 'lit';\nimport '\/path\/to\/shared-styles.css';\n\nclass MyComponent extends LitElement {\n  static styles = css`\n    :host {\n      display: block;\n    }\n\n    .button {\n      background-color: var(--primary-color); \/* Using shared variable *\/\n      \/* ... other styles ... *\/\n    }\n  `;\n\n  render() {\n    return html`\n      <button class=\"button\">Click me<\/button>\n    `;\n  }\n}\n```\n\n**Positive Consequences:**\n\n- **Encapsulation and Specificity:**  Constructible Stylesheets provide style encapsulation, and ITCSS minimizes specificity conflicts.\n- **Reusability and Maintainability:** The shared library promotes reuse and simplifies maintenance by centralizing common styles.\n- **Scalability:** The ITCSS structure ensures a scalable and organized approach as the project grows.\n- **Familiarity:** Leverages existing knowledge of ITCSS and CSS within the team.\n\n**Negative Consequences:**\n\n- **Learning Curve:**  Constructible Stylesheets may require a slight learning curve for developers unfamiliar with the approach.\n- **Increased Bundle Size:**  Importing the shared library into multiple components could slightly increase the overall bundle size. However, code splitting and tree-shaking can mitigate this.\n\nThis approach balances style encapsulation with code reuse, leverages the benefits of ITCSS, and provides a maintainable and scalable solution for styling Web Components. \n","GenTime":"2024-08-28 13:17:55"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"text":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n\n\n##Decision\nWe provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","MatchedIds":[779,3337,2316,2791,311],"Prediction":"## Decision\n\nWe will implement **option 3 - cache based enriching of data** for the signal enrichment feature. \n\n**Reasoning:**\n\n* **Performance and Scalability:** While option 1 minimizes roundtrips, it introduces significant overhead for all consumers, even those not utilizing signal enrichment. Option 2 offers better performance but still involves multiple remote calls. Option 3, with its caching mechanism, presents the best balance by minimizing remote calls and reducing latency for event subscribers who opt for signal enrichment. \n* **Security:** Option 3 maintains the robust security model of Ditto by leveraging the existing concierge mechanism for policy enforcement and filtering. The configurable cache duration allows for a trade-off between performance and policy freshness.\n* **Flexibility:**  Option 3 allows for fine-tuning the cache duration to meet specific use case requirements.  \n\n**Implementation Details:**\n\n* A time-based cache will be used to store enriched data for sessions\/connections requesting `extraFields`.\n* The cache key will be a combination of the connection identifier and requested `extraFields`.\n* The cache duration will be configurable.\n* Upon cache miss, the system will fall back to enriching data on demand and populating the cache. \n* The cache will be invalidated upon policy changes relevant to the cached data. \n\n**Consequences:**\n\n* **Improved performance for subscribers utilizing signal enrichment.**\n* **Potential for slightly outdated data within the configurable cache duration.**\n* **Added complexity of implementing and managing the cache.**\n\nThis approach strikes a balance between performance, security, and maintainability. It ensures that the new feature doesn't negatively impact the overall performance and scalability of Ditto while providing the desired functionality.\n","GenTime":"2024-08-28 13:18:04"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"text":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n\n##Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n","MatchedIds":[3328,4225,1117,241,2626],"Prediction":"## Decision\n\nWe will implement a tool to automate the process of updating dependent packages within the Smarthub SDK. \n\n**Possible Options:**\n\n* **Lerna:** Lerna is a popular tool designed for managing multi-package projects with complex dependency graphs. It can automate the process of updating dependencies across packages and publishing new versions.\n* **Yarn Workspaces:** Similar to Lerna, Yarn workspaces provide a mechanism for managing dependencies across multiple packages within a single repository. \n* **Custom Scripting:** If the project has specific needs, we can develop custom scripts leveraging tools like NPM or Yarn to traverse the dependency graph and automate the update process. \n\n**Chosen Option Justification:**\n\nThe choice between Lerna, Yarn Workspaces, or custom scripting will depend on factors like the existing tooling ecosystem, the complexity of the dependency graph, and the team's familiarity with each option.  We will evaluate these factors to determine the most suitable approach for automating dependency management within the Smarthub SDK. \n","GenTime":"2024-08-28 13:18:10"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"text":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n\n##Decision\nGiven the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","MatchedIds":[1576,3827,3428,1117,1240],"Prediction":"## Decision\n\nWe will use **Webpack** in conjunction with **NPM** for our static asset pipeline.\n\n**Reasons:**\n\n* **Developer Experience:** Webpack offers hot reloading and a built-in dev server, providing the fast feedback loop our team values. \n* **Speed and Features:** While Gulp is inherently faster due to its streaming architecture, Webpack provides crucial features like bundle splitting and dependency graphs, which are essential for modern web development.  Webpack's speed is sufficient for our needs.\n* **Team Familiarity and Industry Trends:**  There's existing positive experience with Webpack within Azavea. Additionally, Webpack enjoys wider community support and adoption compared to Gulp, making it easier to find solutions and leverage existing resources.\n* **Reduced Complexity:** Moving away from Grunt directly addresses the team's concerns about its configuration complexity and compile times. \n\n**Addressing Concerns:**\n\n* **NPM Stability:** While acknowledged, this concern is better addressed through strategies like a lazy NPM cache rather than choosing a different build tool.\n* **Webpack Complexity:** Initial setup proves manageable, and we can mitigate future challenges by documenting our Webpack configuration and leveraging the wider community for support.\n\n**Moving Forward:**\n\nThis decision allows us to adopt a modern, performant, and feature-rich static asset pipeline. We will actively monitor and address any complexity concerns related to Webpack through documentation and knowledge sharing within the team. \n","GenTime":"2024-08-28 13:18:17"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"text":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n\n##Decision\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","MatchedIds":[2526,929,1924,4852,3159],"Prediction":"## Decision\n\nWe will implement a queuing system to handle incoming PayPal IPNs. This addresses the transparency and automation concerns by providing a record of all IPN requests and allowing for automatic retry upon failure. \n\n**Implementation Details:**\n\n* A persistent queue (e.g., using RabbitMQ or a database-backed queue) will be used to store incoming IPN messages.\n* Upon receiving an IPN, the application will:\n    1. Immediately queue the IPN message.\n    2. Return a success response to PayPal, acknowledging receipt.\n    3. Asynchronously process the queued IPN message.\n* Error handling and logging will be implemented around the IPN processing logic.\n* A mechanism to manually re-process failed messages from the queue will be provided.\n\n### Positive Consequences \n\n* **Improved Debugging:** Provides a clear record of all IPN requests and processing attempts.\n* **Increased Robustness:** System failures will not lead to lost IPN messages.\n* **Automated Recovery:**  IPN processing will automatically resume upon system recovery.\n\n### Negative Consequences \n\n* **Increased Complexity:**  Introduces a new component (the queue) and asynchronous processing.\n* **Potential for Delays:** IPN processing will no longer be immediate.\n* **Requires Monitoring:** The queue system will need to be monitored for errors and backlogs. \n\n### Mitigation\n\n* Implement robust error handling and monitoring for the queue system.\n* Use a reliable and scalable queuing technology.\n* Design the system to minimize the impact of potential processing delays on the user experience. \n","GenTime":"2024-08-28 13:18:25"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"text":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n\n##Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","MatchedIds":[5213,1786,3722,1784,5009],"Prediction":"## Decision\n\nWe will use GovUK Notify for sending emails to users across all workflows. \n\n### Justification\n\n* **Existing Integration:** Hackney already utilizes GovUK Notify for other services, demonstrating familiarity and simplifying integration.\n* **Cost-Effective:**  GovUK Notify offers a cost-effective solution compared to third-party providers, especially considering potential volume.\n* **Reliability:** GovUK Notify provides a reliable and robust infrastructure specifically designed for government services. \n* **Standardization:** Adopting GovUK Notify across the board promotes consistency and reduces reliance on disparate tools.\n* **Future-Proofing:** Aligns with Hackney's existing technology stack and potential future adoption within the API Playbook. \n\nThis decision will be reviewed if significant changes in requirements arise or GovUK Notify ceases to meet our needs.\n","GenTime":"2024-08-28 13:18:29"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"text":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n\n##Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","MatchedIds":[1089,3217,4414,583,1182],"Prediction":"## Decision\n**Use a SaaS identity provider.** \n\nWhile acknowledging the potential for higher cost and vendor lock-in at scale, the benefits outweigh the drawbacks in this situation:\n\n* **Reduced Management Overhead:**  Focusing on a SaaS solution frees up valuable development resources from having to manage and secure a complex identity infrastructure. \n* **Faster Time to Market:**  SaaS solutions generally offer faster integration and deployment compared to building and maintaining a self-hosted option.\n* **Robust Security:**  Established SaaS providers invest heavily in security measures that would be costly and time-consuming to replicate in a self-hosted solution.\n* **Simplified Evolution:** While switching providers might be challenging, the core identity concepts (OpenID Connect, login flows) are relatively stable and standardized. The risk of architectural upheaval is minimized.\n\nThis decision prioritizes rapid development, reduced operational complexity, and strong security posture. We accept the potential for higher costs at scale with the understanding that those costs will be offset by increased agility and faster value delivery. \n","GenTime":"2024-08-28 13:18:35"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"text":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n\n##Decision\n- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","MatchedIds":[1278,160,2513,282,2884],"Prediction":"## Decision\n\nIntroduce **user-defined metadata** in docspell. This allows users to define custom key-value pairs for each document. \n\n**Implementation Details:**\n\n* **Storage:** Metadata will be stored alongside the document itself. This ensures metadata persists and is easily retrievable.\n* **Data format:**  A simple key-value store like JSON or YAML can be used to store the metadata.  \n* **User Interface:** Provide a user-friendly way (e.g., dedicated fields in the UI, import\/export functionality) to add, edit, and delete metadata.\n* **Search and Filtering:** Extend search functionality to allow filtering and searching based on metadata.\n\n**Positive Consequences:**\n\n* **Flexibility:** Users have full control over the metadata they add.\n* **Organization:**  Metadata facilitates better organization and categorization of documents.\n* **Improved Search:** Metadata enhances search capabilities, allowing for more specific and efficient retrieval. \n\n**Negative Consequences:**\n\n* **Increased Complexity:**  Implementing metadata adds complexity to the application.\n* **Storage Requirements:** Storing metadata might slightly increase storage space requirements.\n\n**Alternatives Considered:**\n\n* **Using existing file metadata:**  This approach is limited, as it doesn't provide the flexibility of custom fields.\n* **External databases:** While powerful, external databases introduce overhead and complexity.\n\n**Rationale:** \n\nUser-defined metadata offers the best balance between flexibility, ease of implementation, and performance. It addresses the user requirement for custom metadata while maintaining docspell's core functionality and simplicity. \n","GenTime":"2024-08-28 13:18:44"}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"text":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n\n##Decision\nUse a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","MatchedIds":[4843,1799,560,3845,779],"Prediction":"## Decision\n\nTo mitigate the risk of session data overwrites caused by concurrent AJAX requests, we will implement a locking mechanism for session data access in service-front.\n\nHere's a breakdown of the chosen approach:\n\n**Option:** Implement optimistic locking for session data.\n\n**Rationale:**\n\n* **Targeted solution:** Directly addresses the problem of concurrent writes to the session by ensuring only one process can modify session data at a time.\n* **Minimizes complexity:**  Optimistic locking provides a relatively simple implementation in PHP using Redis's built-in \"check-and-set\" operations.\n* **Performance:**  Optimistic locking introduces minimal overhead for the majority of requests where session data isn't heavily contended. \n\n**Implementation Details:**\n\n1. **Session locking:** Before any controller modifies the session, it will acquire a lock on the session ID in Redis. This lock can be implemented using Redis's `SETNX` command with a short expiration time to prevent deadlocks.\n2. **Concurrency handling:**\n    * If the lock acquisition is successful, the controller proceeds with its operations, modifies the session, and releases the lock.\n    * If the lock acquisition fails (another process holds the lock), the controller can either:\n        * **Retry:**  Wait for a short period and retry acquiring the lock (a few retries with exponential backoff can be implemented).\n        * **Fail gracefully:** Return an error message to the user indicating that the request couldn't be processed at this time due to concurrent operations. This approach prioritizes data consistency over potential user inconvenience.\n3. **CSRF Token Management:**  Given the criticality of CSRF protection, special care should be taken.  The CSRF token should be regenerated and associated with the session *after* the lock is acquired to guarantee its consistency.\n\n**Additional Considerations:**\n\n* **Lock timeout:**  A reasonable timeout for the session lock should be determined based on typical request durations to avoid unnecessary delays.\n* **Error handling:** Robust error handling and logging should be implemented around the locking mechanism to diagnose and troubleshoot potential issues.\n* **User experience:** If the \"fail gracefully\" approach is chosen for concurrency handling, consider how to best communicate the situation to the user and provide guidance for retrying their action.\n\nThis approach prioritizes the integrity of session data and security by preventing data corruption due to concurrency. It balances complexity and performance considerations, offering a pragmatic solution for the described scenario. \n","GenTime":"2024-08-28 13:18:56"}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"text":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n\n\n##Decision\n1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","MatchedIds":[5177,241,3653,398,138],"Prediction":"## Decision\n\nPCMT will adopt a versioning scheme that combines semantic versioning with a prefix indicating the Akeneo version dependency. This scheme will use the format: \n\n**`AkeneoVersion-Major.Minor.Patch`**\n\n* **`AkeneoVersion`**: This prefix will clearly denote the major version of Akeneo that the PCMT release is compatible with (e.g., `5.0`, `6.0`). \n* **`Major.Minor.Patch`**: This portion follows semantic versioning:\n    * **`Major`**: Incremented when incompatible changes are introduced, signifying a significant release potentially requiring substantial updates for existing users. This will also align with global stakeholder readiness (e.g., v1, v2).\n    * **`Minor`**: Incremented when adding backward-compatible functionality. This aligns with national stakeholder readiness within a given major version.\n    * **`Patch`**:  Incremented for backward-compatible bug fixes.\n\n**Examples:**\n\n* `5.0-0.1.0`:  A pre-release version compatible with Akeneo 5.0, intended for global stakeholder testing.\n* `5.0-1.0.0`:  The first production-ready release compatible with Akeneo 5.0, suitable for global stakeholders.\n* `5.0-2.0.0`:  A major feature release compatible with Akeneo 5.0, introducing features specifically for national stakeholders.\n* `5.0-2.1.0`:  A minor release adding backward-compatible features to the `5.0-2.x` series. \n* `5.0-2.1.1`: A patch release fixing bugs in the `5.0-2.1.0` release.\n* `6.0-1.0.0`: The first production-ready release compatible with Akeneo 6.0.\n\n**Benefits:**\n\n* **Clarity for Stakeholders**: Immediately conveys project maturity and target audience.\n* **Semantic Versioning**: Enables dependency management and communicates release impact.\n* **Akeneo Compatibility**: Clearly indicates the corresponding Akeneo version.\n* **Flexibility**: Allows for future expansion and adaptation to changing project needs. \n","GenTime":"2024-08-28 13:19:07"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"text":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n\n\n##Decision\nWe will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","MatchedIds":[783,3815,2386,3784,1855],"Prediction":"## Decision\n\nWe will use a CSS preprocessor like Sass or Less to manage the website's styling. This will allow us to:\n\n* **Maintain a single source of truth for GDS design system styles:**  We can define variables and mixins based on the GDS guidelines, ensuring consistency across the site.\n* **Easily theme the website:** By overriding specific variables (e.g., colours, fonts), we can switch between the standard GDS theme and a food.gov theme without significant code changes. \n* **Improve maintainability and scalability:**  The use of nested rules, functions, and other preprocessor features will help us organise and manage the CSS as the website grows. \n\nThis approach allows us to adhere to the GDS design language while retaining the flexibility to apply custom theming in the future.\n","GenTime":"2024-08-28 13:19:12"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"text":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n\n##Decision\nWe decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","MatchedIds":[3797,3416,4533,4057,3907],"Prediction":"## Decision\n\nWe will use the **S3 Presigned URL** functionality for file uploads. This approach offers several advantages over the other options:\n\n**Benefits:**\n\n* **Reduced server load:** The client uploads files directly to S3, minimizing the workload on the API server. This improves scalability and reduces resource consumption.\n* **Improved upload speed:** Direct uploads bypass the API server, potentially leading to faster file transfers, especially for large files. \n* **Enhanced security:** Clients interact directly with S3, eliminating the need to handle sensitive credentials or file data within the API. Security is inherently managed by AWS IAM policies associated with the presigned URL.\n* **Simplified API implementation:** The API logic is simplified as it only needs to generate and provide presigned URLs. This reduces code complexity and maintenance effort. \n\n**Considerations:**\n\n* **Client-side implementation:** Clients need to handle the file upload logic, including potential retries and error handling. \n* **Limited control over uploads:**  Since uploads are handled client-side, the API has less control over the process. This might require additional mechanisms for validation or monitoring uploads on the S3 side. \n\n**Mitigation:**\n\n* Provide clear documentation and examples for client-side implementation.\n* Leverage S3 events and notifications to monitor and validate uploads asynchronously.\n\n**Conclusion:**\n\nUsing S3 Presigned URLs offers a scalable, efficient, and secure solution for file uploads, aligning with best practices for building cloud-native applications. While it requires careful consideration of client-side implementation and monitoring, the benefits outweigh the drawbacks. \n","GenTime":"2024-08-28 13:19:19"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"text":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n\n\n##Decision\nWhat we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","MatchedIds":[1193,523,3863,798,138],"Prediction":"## Decision\n\nWe will refactor the drawing tools with the following goals:\n\n**1. Improve Performance and Reduce Re-rendering:**\n\n- **Decouple Rendering from Annotation Updates:** Instead of updating the classification annotation on every touch\/pointer event, we will implement an intermediate data structure within the drawing tool component. \n- **Utilize a Canvas-Based Approach:** Leverage HTML5 Canvas for rendering the markings. This allows for direct manipulation of pixels, reducing DOM manipulation and improving performance.\n- **Implement Change Detection:** Introduce a mechanism to detect actual changes in the drawn shape before updating the classification annotation, minimizing unnecessary re-renders.\n\n**2.  Enforce Separation of Concerns and Clear Responsibility:**\n\n- **Introduce a Dedicated Drawing Tool Store:** This store will hold the state of the active drawing tool and the intermediate drawing data.  It will be solely responsible for updating the classification annotation based on changes in the drawing tool's state.\n- **Refactor Components:**\n    - **`MarkingsRenderer`:** This component will become purely responsible for rendering markings from the data provided by the Drawing Tool Store. It will no longer have the ability to update the annotation directly.\n    - **`FrameAnnotator`:** This component will be responsible for managing user interactions with the canvas and delegating those interactions to the appropriate methods in the Drawing Tool Store.\n\n**3.  Simplify the API and Enhance Usability:**\n\n- **Eliminate Static Methods:** Refactor drawing tools to be instantiated as classes, simplifying their API and improving code organization.\n- **Use Events for Communication:** Instead of directly calling methods on parent components, the Drawing Tool Store will emit events that parent components can listen to and respond accordingly.\n\n**4. Standardize Annotation Payloads:**\n\n- **Define a Consistent Schema:** Establish a clear and consistent schema for annotation payloads across all drawing tools. This will ensure uniformity in how data is represented, including rotation direction and other attributes.\n- **Implement Data Validation:** Introduce validation for annotation data to ensure it adheres to the defined schema, preventing inconsistencies and errors.\n\n**Implementation Plan:**\n\nWe will adopt an iterative approach to this refactor, prioritizing the most impactful changes first. We will work closely with stakeholders to ensure a smooth transition and minimize disruption to existing functionality.\n","GenTime":"2024-08-28 13:19:30"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"text":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n\n\n##Decision\nThe relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","MatchedIds":[3374,1180,3579,779,3372],"Prediction":"## Decision\nThis API will be considered an internal API and will not be exposed publicly. Authentication and authorization will be enforced to ensure only authorized systems and users can modify the capacity of DoS services. \n","GenTime":"2024-08-28 13:19:33"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"text":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n\n##Decision\n* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","MatchedIds":[85,70,3215,539,73],"Prediction":"## Decision\n\nDapr CLI versions **will not** be coupled to runtime versions.\n\n### Justification\n\nDecoupling CLI and runtime versions offers several benefits:\n\n* **Flexibility:** Users can upgrade or downgrade the CLI independently of the runtime, allowing them to leverage new features or bug fixes without requiring a simultaneous runtime update. This is particularly useful for managing multiple Dapr applications with potentially different runtime requirements. \n* **Independent Release Cycles:**  Decoupling enables separate release cycles for the CLI and runtime, fostering faster development and deployment of new features and bug fixes for both components.\n* **Simplified Version Management:**  Separate versioning simplifies dependency management and reduces potential conflicts, making it easier for users to understand and manage their Dapr installations.\n\n### Implications\n\n* **Compatibility Testing:**  Dapr CLI will need to undergo compatibility testing with a reasonable range of past and future Dapr runtime versions to ensure smooth interaction and prevent breaking changes.\n* **Clear Version Communication:**  Documentation and release notes must clearly communicate compatibility between CLI and runtime versions, guiding users on supported combinations and potential limitations.\n* **Potential Feature Limitations:**  In some cases, new CLI features might depend on specific runtime capabilities. The CLI should gracefully handle such situations, informing the user about limitations and providing guidance on necessary runtime upgrades.\n\n### Alternatives Considered\n\n* **Coupled Versioning:** This approach simplifies version management but sacrifices flexibility and independent development cycles. \n\n### Conclusion\n\nDecoupling Dapr CLI and runtime versions provides greater flexibility and enables independent development, ultimately leading to a more robust and user-friendly Dapr experience. \n","GenTime":"2024-08-28 13:19:40"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"text":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n\n##Decision\nUse Terraform\\n","MatchedIds":[4937,2062,3147,211,4358],"Prediction":"## Decision\n\nWe will use Infrastructure as Code (IaC) to manage and track resources within our AWS account. This will involve defining all infrastructure components (e.g., EC2 instances, S3 buckets, IAM roles) declaratively using a tool like Terraform. \n\nBy implementing IaC, we gain several benefits:\n\n* **Clear Visibility:** A centralized and version-controlled repository will provide a clear picture of all provisioned resources, reducing the risk of unaccounted-for infrastructure and associated costs.\n* **Improved Security:** Changes to infrastructure can be reviewed and audited, ensuring that security policies are enforced consistently. \n* **Reduced Errors:** Automated provisioning minimizes the risk of manual errors, leading to more reliable and predictable infrastructure deployments.\n\nWe will integrate this IaC approach into our existing CI\/CD pipelines to automate infrastructure provisioning and maintain an up-to-date view of our AWS resources.\n","GenTime":"2024-08-28 13:19:45"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"text":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n\n\n##Decision\n* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","MatchedIds":[4637,1545,4747,4644,160],"Prediction":"## Decision\nChosen option: **\"Enforce a Java Bean structure for application definitions\"**, because it meets the compatibility requirement while promoting clarity and simplicity. \n\n### Positive Consequences:\n* **Lombok compatibility:**  Java Bean structure aligns perfectly with Lombok annotations, allowing developers to use Lombok features without conflict.\n* **Clear requirements:** By enforcing a standard structure, the requirements for creating applications become well-defined and easier to understand.\n* **Tooling and Validation:** Bean validation frameworks can be readily applied to ensure the application definitions are valid. \n\n### Negative Consequences:\n* **Potentially more verbose:**  Using a full Java Bean structure might lead to slightly more verbose definitions compared to custom solutions. However, this is mitigated by using Lombok. \n\n### Alternatives Considered:\n* **Custom DSL\/Schema:** While powerful, this would require building custom parsing and validation, and potentially clash with Lombok's code generation. \n\n### Justification: \nThe Java Bean structure provides the most straightforward and robust solution that inherently meets the mandatory Lombok compatibility requirement. It promotes clear requirements for application creation within the user interface.\n","GenTime":"2024-08-28 13:19:51"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n\n##Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","MatchedIds":[2140,2127,885,786,311],"Prediction":"## Decision\nImplement a distributed task queue with the following features:\n\n* **Task submission and distribution:** Clients submit `Task`s to the queue. The queue distributes tasks to available nodes in the cluster, ensuring even load distribution.\n* **Task execution and status reporting:** Nodes pick up `Task`s from the queue, execute them, and report back their status (e.g., running, completed, failed) to the queue.\n* **Termination event signaling:** When a `Task` terminates on a node (due to completion, failure, or other reasons), the node publishes a termination event to the queue.\n* **Client notification:** Clients can subscribe to termination events for specific `Task`s. Upon receiving a termination event, the queue notifies the subscribing client. \n\nThis approach provides a centralized mechanism for task management, ensuring reliable execution, status tracking, and notification for distributed tasks within the cluster.\n","GenTime":"2024-08-28 13:19:56"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"text":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","MatchedIds":[4889,2696,4890,429,2498],"Prediction":"## Decision\n\n**We will use CQRS (Command Query Responsibility Segregation) and Event Sourcing architectural patterns as implemented in the open-source Axon Framework.**\n\n### Decision details\n\nThis decision is based on the following:\n\n- **Alignment with DDD:** Axon Framework is built around DDD principles. It provides explicit support for strategic DDD concepts like bounded context and ubiquitous language, allowing for a clear separation of concerns and promoting a model-driven approach. \n- **Tactical DDD support:** Axon offers first-class support for tactical DDD building blocks. Aggregates, entities, value objects, and domain events are natural elements within the framework, simplifying their implementation and fostering a rich domain model. \n- **Asynchronous communication:**  Asynchronous communication through events is a core feature of Axon. This aligns perfectly with the requirements for microservices and promotes loose coupling and scalability.\n- **Coherent framework and platform:** Axon provides a comprehensive ecosystem for building event-driven applications. This includes features like testing support, distributed tracing, and integration with various messaging solutions.\n- **Flexibility:** While designed for event-driven systems, Axon doesn't enforce a strict all-or-nothing approach. It allows for a gradual adoption of CQRS and Event Sourcing, permitting their use only for specific parts of the application where their benefits are most pronounced.\n\n#### Reasoning\n\n- Using a framework specifically designed for DDD and event-driven architectures provides significant advantages over building custom solutions. It offers battle-tested components, reduces boilerplate code, and provides a common language and understanding within the development team.\n- While alternative approaches and frameworks exist, Axon Framework provides a compelling combination of DDD alignment, event-driven architecture support, and a mature ecosystem, making it a solid choice for the project.\n\n#### Alternatives considered\n\n- **Building a custom solution:**  While possible, it requires significant effort and expertise. It also carries the risk of reinventing the wheel and introducing potential inconsistencies.\n- **Other DDD-focused frameworks:** Alternatives exist, but they may not offer the same level of maturity, comprehensive features, or community support as Axon Framework.\n\n#### Consequences\n\n- **Learning curve:**  Adopting a new framework requires an initial investment in learning its concepts and APIs. However, this investment pays off in the long run through increased development efficiency and a more maintainable codebase.\n- **Potential impact on existing code:** Integrating Axon with an existing codebase may require adaptations, especially if the existing architecture doesn't adhere to DDD principles.\n\n#### Future considerations\n\n- **Eventual consistency:**  Embracing an event-driven approach introduces the concept of eventual consistency. This needs to be carefully considered, especially when dealing with data consistency requirements across different parts of the system.\n- **Monitoring and observability:** Monitoring event flows and overall system health becomes crucial in an event-driven architecture. Utilizing tools and techniques for effective monitoring and observability should be factored into the development process.\n","GenTime":"2024-08-28 13:20:11"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"text":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n\n##Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","MatchedIds":[2466,779,2499,929,2500],"Prediction":"## Decision\n\nGiven the fixed deadline, the lack of retry mechanisms, and the potential administrative overhead, we will implement a **time-boxed manual recovery strategy for the initial launch**. \n\nThis means:\n\n* **No automated retries** will be built for the initial go-live.\n* A **dedicated team or individual** will be responsible for monitoring Delius integration failures. \n* **Clear escalation procedures and manual recovery scripts** will be documented and tested. These scripts will allow for the retriggering of failed Spring Application Events. \n* We will **prioritize investigating and fixing the root causes** of any integration failures encountered during this period. \n\nThis approach balances the need for a robust launch with the limited development time available. After the initial launch and stabilization period, we will revisit the need for automated retry mechanisms based on the encountered failure rates and types. \n","GenTime":"2024-08-28 13:20:16"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"text":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n\n\n##Decision\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","MatchedIds":[1614,539,3853,3159,1927],"Prediction":"## Decision\nUniffi will require all user-defined types exposed through the FFI layer to be `Send + Sync`. The use of the `[Threadsafe]` marker will be discontinued in favor of this implicit requirement.\n\nThis decision effectively removes the hidden mutex approach previously used by Uniffi for non-thread-safe types.  Instead, component authors will be fully responsible for ensuring the thread safety of their exposed types, either by making them inherently `Send + Sync` or by implementing appropriate synchronization mechanisms internally. \n\nThis approach prioritizes:\n\n* **Explicitness and clarity**: By removing the `[Threadsafe]` attribute, the requirement for thread-safety becomes implicit and directly tied to Rust's type system. This makes it clearer to developers that they are responsible for managing thread safety for any types exposed via Uniffi. \n* **Performance and flexibility**: Eliminating the hidden mutex avoids potential performance bottlenecks and enables Uniffi to evolve with more advanced features like alternative method receivers and passing interface references over the FFI. \n* **Alignment with Rust's concurrency model**: This decision aligns with Rust's philosophy of placing the onus of thread safety on the developer while providing the tools and guarantees to do so effectively.\n","GenTime":"2024-08-28 13:20:23"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\\nWe need a workflow to build our appliation and components.\\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"text":"## Context\\nWe need a workflow to build our appliation and components.\\n\n\n##Decision\nWe use Storybook only for building new self contained components.\\n","MatchedIds":[3682,4899,4358,5009,1151],"Prediction":"## Decision\n\nWe will use a multi-stage build process orchestrated by a tool like **GitHub Actions** or **GitLab CI\/CD**. This allows for efficient and automated building, testing, and deployment of both the application and individual components. \n\n**The workflow will consist of the following stages:**\n\n1. **Component Build:** Each component will have its own build process triggered on every push to its respective repository. This process will include linting, unit testing, building the component, and packaging it for distribution (e.g., as an NPM package).\n2. **Component Publication:** Successfully built and tested components will be automatically published to a private repository (e.g., private NPM registry).\n3. **Application Build:** The main application's build pipeline will trigger on every push to its repository. This pipeline will install the necessary component dependencies from the private repository.\n4. **Application Test:** The application, with all its integrated components, will undergo various tests like integration and end-to-end tests.\n5. **Application Deployment:** Upon successful testing, the application will be deployed to the desired environment (e.g., staging or production).\n\n**This approach offers several advantages:**\n\n* **Modularity & Reusability:** Individual component builds promote code reusability and isolation.\n* **Faster Build Times:** Building and testing components independently saves time during the main application build process.\n* **Improved Continuous Integration:**  The automated workflow ensures continuous integration and reduces the risk of integration issues.\n* **Versioning & Dependency Management:**  A private repository allows for better version control and management of component dependencies.\n\n**Tooling:**\n\n* **GitHub Actions\/GitLab CI\/CD:** For defining and orchestrating the multi-stage build pipeline.\n* **NPM\/Yarn:** For managing component dependencies.\n* **Private Registry:** (e.g., GitHub Packages, GitLab Packages, JFrog Artifactory) For hosting and managing component versions. \n","GenTime":"2024-08-28 13:20:32"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"text":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n\n\n##Decision\nShell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","MatchedIds":[4761,4271,2687,1416,472],"Prediction":"## Decision\n\nWe will use **Go** as our primary programming language. Here's why:\n\n* **Universality:** Go is a general-purpose language suitable for various tasks, from web services to system utilities.\n* **Simplicity:** Go's syntax is clean and straightforward, making it easy to learn and read. This promotes maintainability and reduces the chance of errors.\n* **Testability:** Go has built-in testing support, making it easy to write unit and integration tests. This focus on testing aligns with our requirement for easily testable code. \n* **Minimal Dependencies:** Go encourages small, self-contained programs with minimal external dependencies. This aligns with our desire for a streamlined development process. \n\nWhile other options have their merits, Go strikes a strong balance between simplicity, testability, and a minimalistic approach to dependencies, making it well-suited for this project. \n","GenTime":"2024-08-28 13:20:37"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"text":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n\n\n##Decision\nWe decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","MatchedIds":[1792,3784,276,2502,2181],"Prediction":"## Decision\n\nWe will adopt a modular, layered architecture based on Clean Architecture principles. This will promote separation of concerns, maintainability, and testability.\n\n### Architecture Overview:\n\nThe application will be structured in three main layers:\n\n* **Domain:** This layer will house the business logic, entities, and use cases related to evidence management. It will remain independent of any specific technology or framework and will define core rules and processes. This includes:\n    * **Entities:**  Represent evidence documents, rights, retention policies, requests, approvals, etc.\n    * **Use Cases:** Implement business rules for document upload, storage, retrieval, sharing, approval, and retention management.\n    * **Interfaces:** Define contracts for interacting with external dependencies like databases or external services.\n\n* **Application:** This layer will orchestrate the interaction between the Domain layer and the Infrastructure layer. It will handle user requests, execute use cases, and prepare data for presentation. It will implement the API endpoints as defined in the HackIT API Playbook.\n\n* **Infrastructure:** This layer will deal with technical concerns and interactions with external systems. This includes:\n    * **Data Access:** Implement repositories for persistence of evidence documents and related data using a suitable database technology.\n    * **External Services:**  Handle communication with third-party services for document scanning, OCR, or other functionalities.\n    * **Security:** Implement authentication, authorization, and access control mechanisms.\n\n### Communication:\n\n* Layers will communicate through well-defined interfaces.\n* The Domain layer will remain unaware of the other layers.\n* The Application layer will depend on interfaces defined in the Domain layer.\n* The Infrastructure layer will implement interfaces defined in the Domain and Application layers.\n\n### Benefits:\n\n* **Testability:**  Each layer can be tested independently.\n* **Maintainability:** Changes in one layer will have minimal impact on other layers.\n* **Scalability:**  The modular design allows for scaling specific components as needed.\n* **Technology Agnostic:**  The Domain layer can be reused with different technologies and frameworks.\n\n### Next Steps:\n\n* Define detailed module structure within each layer.\n* Choose specific technologies for data storage, API framework, and other infrastructure components.\n* Develop a proof-of-concept implementation to validate the chosen architecture. \n","GenTime":"2024-08-28 13:20:48"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"text":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n\n\n##Decision\nNo. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","MatchedIds":[3104,620,2240,3909,4056],"Prediction":"## Decision\n\nWe will consolidate on Istio for ingress traffic management and deprecate the use of nginx-ingress.\n\n### Rationale\n\n* **Feature Overlap:** Istio's Virtual Service provides all the functionality of nginx-ingress, and more. Maintaining a separate ingress system introduces unnecessary complexity.\n* **Service Mesh Alignment:** As we are committed to using Istio as our service mesh (ADR019), leveraging its ingress capabilities provides a unified approach to traffic management.\n* **Reduced Operational Overhead:** Consolidating on a single system simplifies configuration, monitoring, and troubleshooting.\n\n### Transition Plan\n\n1. **New Services:** All new services will be deployed using Istio VirtualService resources for ingress.\n2. **Existing Services:** Existing services using nginx-ingress will be migrated to Istio VirtualService in a phased approach, prioritizing based on criticality and complexity.\n3. **Deprecation:** Once all services are migrated, the nginx-ingress system will be decommissioned.\n\n### Considerations\n\n* **Migration Complexity:** Migrating existing services from nginx-ingress to Istio VirtualService will require careful planning and execution to minimize disruption.\n* **Training and Documentation:**  Teams will need to be trained on using Istio VirtualService for ingress, and updated documentation will need to be provided.\n\n### Positive Consequences\n\n* **Simplified Architecture:** A single, unified ingress system reduces complexity.\n* **Enhanced Functionality:** Istio's advanced traffic management features become available for all services.\n* **Improved Maintainability:**  A single system is easier to manage and troubleshoot.\n\n### Negative Consequences\n\n* **Migration Effort:**  Migrating existing services will require time and effort.\n* **Learning Curve:** Teams will need to invest time in learning Istio's VirtualService concepts. \n","GenTime":"2024-08-28 13:20:57"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"text":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n\n\n##Decision\nAs response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","MatchedIds":[779,5118,3159,1193,1924],"Prediction":"## Decision\n\nWe will adopt a consistent approach for serializing complex response types by using JSON (JavaScript Object Notation) across the board. This approach addresses the limitations of using pipes and ad-hoc JSON encoding for specific cases, providing a unified and scalable solution.\n\nHere's a breakdown of the decision:\n\n* **Standardized Format:** All response types, including cascade and option types, will be serialized as JSON objects. This ensures consistency and makes it easier for clients to parse and consume data.\n* **Flexibility and Extensibility:** JSON's ability to represent complex data structures with nested objects and arrays accommodates the introduction of new properties and response types without requiring modifications to existing parsing logic. \n* **Code Property Integration:** Instead of adding a separate `code` property, we will include it within the JSON object representing the relevant data structure (e.g., cascade, option).  This maintains a clean and structured response format.\n\n**Example:**\n\nInstead of:\n\n```\n\"value1|value2|{\"type\": \"FeatureCollection\", \"features\": [...]}\" \n```\n\nWe will have:\n\n```json\n{\n  \"field1\": \"value1\",\n  \"field2\": \"value2\",\n  \"geoshapes\": {\n    \"type\": \"FeatureCollection\",\n    \"features\": [...]\n  },\n  \"cascadeOption\": {\n    \"value\": \"optionValue\",\n    \"code\": \"optionCode\"\n  }\n} \n```\n\nThis approach provides a robust and future-proof solution for handling increasingly complex response types in a consistent and developer-friendly manner. \n","GenTime":"2024-08-28 13:21:05"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"text":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n\n##Decision\nUse ports and adapters architecture style.\\n","MatchedIds":[4021,4741,1031,1792,276],"Prediction":"## Decision\n\nWe will adopt the **Model-View-ViewModel (MVVM)** architectural pattern for structuring the POC. This pattern provides a clean separation of concerns, making the code easier to test, maintain, and scale. \n\nHere's how the components will be organized:\n\n* **Model:** This layer will be responsible for representing the hierarchical table of contents data. It will be designed as a tree-like data structure, potentially using classes or custom types, capable of holding multi-level list information. To simplify rendering logic, the model will encapsulate helper functions to traverse and manipulate the data, for example, providing methods to flatten the structure or extract specific levels.\n* **View:** This layer will be responsible for rendering the table of contents on the screen. It will be kept as passive as possible, relying on data binding to display information provided by the ViewModel.  \n* **ViewModel:** This layer will act as an intermediary between the Model and the View. It will retrieve the raw table of contents data from a data source (which could be an API, local storage, or hardcoded for the POC), transform it into the hierarchical Model structure, and expose properties and methods that the View can easily bind to.\n\nThis approach offers several advantages:\n\n* **Clear Separation:** The MVVM pattern enforces a clear separation of concerns, improving code readability and maintainability.\n* **Testability:** Each component can be tested independently, making it easier to achieve high test coverage.\n* **Flexibility:** The ViewModel provides an abstraction layer over the Model, allowing for changes in the data source or data structure without impacting the View.\n* **Reusability:**  The Model and ViewModel can be reused across different views or even different parts of the application.\n\nBy adopting the MVVM pattern, the POC will benefit from a robust and scalable architecture, allowing for future enhancements and modifications with ease. \n","GenTime":"2024-08-28 13:21:15"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"text":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n\n\n##Decision\n### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","MatchedIds":[5149,1614,1067,1545,5185],"Prediction":"## Decision\n\n### Utilize a combination of manifest declaration and event-based hooks:\n\n#### Extension Manifest:\n\n- Extension developers declare \"extension points\" within their extension's manifest file (e.g., `extension.json`).\n- These extension points specify:\n    - **Location:**  Where the custom component\/view should be injected (e.g., \"Product Details Page\", \"Order Summary Sidebar\").  Use a predefined list of location IDs for consistency.\n    - **Component:** Reference to the extension's component responsible for rendering the view (can be a path to a JS module).\n    - **Priority:** (Optional) Numerical value to determine rendering order if multiple extensions target the same location.\n\n#### Event-Based Hooks:\n\n- The administration application will emit lifecycle events at relevant points (e.g., \"Product Details Loaded\", \"Order Summary Rendering\").\n- Extension components can register listeners for specific events.\n- Upon event triggering, the application provides context data (e.g., product details, order information) to the extension component.\n\n####  Benefits:\n\n- **Decoupled Architecture:** Extensions don't need direct knowledge of the application's internals, fostering maintainability.\n- **Flexibility:**  Supports a wide range of extensions, each focusing on specific areas or data points.\n- **Centralized Management:**  The manifest file acts as a single source of truth for extension capabilities.\n\n#### Example Manifest Entry:\n\n```json\n{\n  \"extensionPoints\": [\n    {\n      \"location\": \"product-details-sidebar\",\n      \"component\": \".\/components\/ProductRecommendations.js\",\n      \"priority\": 10\n    }\n  ]\n}\n```\n\n#### Example Event Handling (pseudo-code):\n\n```javascript\n\/\/ In extension's ProductRecommendations.js component\naddEventListener('ProductDetailsLoaded', (event) => {\n  const productData = event.detail;\n  \/\/ ... use productData to render custom recommendations\n}); \n```\n\n#### Additional Considerations:\n\n- **Security:** Implement sandboxing or strict data validation to mitigate risks associated with third-party code execution.\n- **Documentation:** Provide clear guidelines and examples to guide extension developers.\n- **UI\/UX Consistency:** Offer best practices or even a basic component library to maintain a consistent look and feel across extensions. \n","GenTime":"2024-08-28 13:21:27"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"text":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n\n\n##Decision\nYes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","MatchedIds":[306,1091,2499,1610,2672],"Prediction":"## Decision\n\n**Yes**, the messages from Order Management, Rating Manager, and Recommendation Manager can be asynchronous. \n\nHere's why:\n\n* **Loose Coupling:** Asynchronous messaging promotes loose coupling between services. Order Management, Rating Manager, and Recommendation Manager can operate independently without being tightly synchronized.\n* **Improved Performance:** Asynchronous communication allows the sending service to continue processing without waiting for a response. This is particularly beneficial if any of these services perform time-consuming operations.\n* **Increased Fault Tolerance:** If one service experiences downtime, the others can continue to function. Messages will be queued and processed when the service is back online.\n\nHowever, it's important to consider:\n\n* **Handling Failures:**  Robust error handling and retry mechanisms are crucial for asynchronous communication. \n* **Eventual Consistency:** Asynchronous operations introduce eventual consistency. Data across different services might be temporarily out of sync.\n\nTherefore, while adopting asynchronous messaging for these services offers significant advantages, it's crucial to implement appropriate strategies for error handling and managing eventual consistency.\n","GenTime":"2024-08-28 13:21:33"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"text":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n\n\n##Decision\nWe will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","MatchedIds":[102,2884,5123,485,282],"Prediction":"## Decision\n\nGiven the increasing number of document types not utilizing Elasticsearch's full-text capabilities and the introduction of one type per index in Elasticsearch 6, we will adopt a **multi-index approach** for these specific document types. \n\nThis decision is based on the following considerations:\n\n* **Simplified Management:**  Managing a smaller number of larger indices is generally easier than managing numerous smaller indices.  By grouping document types with similar indexing and search requirements, we can reduce the overall number of indices.\n* **Performance Optimization:**  Elasticsearch performance can be enhanced by separating documents with distinct characteristics into different indices. This allows for tailored index settings and mappings for optimal search and indexing speed.\n* **Resource Utilization:**  With one type per index, indices with low cardinality in their data might not be using resources effectively.  Grouping similar document types can lead to better resource utilization.\n\n**Implementation Details:**\n\n1. **Grouping Criteria:** We will group document types based on factors like:\n    * Data structure and fields\n    * Frequency of updates\n    * Search requirements\n    * Data retention policies \n2. **Index Naming Convention:** We will establish a clear and consistent naming convention for the new indices to ensure easy identification and management.\n3. **Monitoring & Evaluation:**  We will closely monitor the performance and resource utilization of the new multi-index structure and make adjustments as needed.\n\nThis approach strikes a balance between Elasticsearch's evolving capabilities and our need for efficient data management and search optimization. \n","GenTime":"2024-08-28 13:21:41"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"text":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n\n\n##Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","MatchedIds":[2896,2145,2888,2892,2150],"Prediction":"## Decision\n\nTo optimize the retrieval of the JMAP message preview and avoid the overhead of processing the full message body on each request, we will introduce a dedicated preview field:\n\n* **Store a pre-computed preview:** Upon message ingestion and processing, extract the first 256 characters of meaningful text (after MIME parsing and HTML stripping) and store it as a separate metadata field alongside the message body.\n* **Serve preview directly:** When handling JMAP requests for message lists, directly retrieve and serve the pre-computed preview field, avoiding the need to fetch and process the entire message body.\n* **Handle preview updates:** If a message's body is modified after the initial preview generation, ensure the preview field is updated accordingly to reflect the changes.\n\nThis approach offers several advantages:\n\n* **Significantly improves performance** by eliminating redundant processing of the message body for each preview request.\n* **Reduces latency** for JMAP clients, delivering a smoother user experience.\n* **Decreases load on the system** by minimizing resource-intensive MIME parsing and HTML stripping operations.\n\n**Implementation Details:**\n\n* The preview field can be stored within the existing message metadata storage (e.g., Cassandra).\n* The preview generation logic can be integrated into the existing message processing pipeline.\n* Existing messages can have their preview fields populated asynchronously during system downtime or using a background process to avoid impacting performance. \n","GenTime":"2024-08-28 13:21:48"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"text":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n\n\n##Decision\nWe will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","MatchedIds":[631,856,729,3147,4707],"Prediction":"## Decision\n\nWe will adopt a strategy of **running multiple clusters** to achieve the required isolation levels. Each Service Team will have **one or more dedicated Kubernetes clusters** deployed to their **existing AWS accounts**. This approach addresses the key concerns outlined in the context:\n\n1. **Strong Isolation:** Dedicated clusters provide the strongest form of network and compute isolation, essential for Service Teams with unique requirements or sensitive workloads. This eliminates the risk of resource contention or unintended interactions between teams.\n\n2. **Simplified Migration:** Deploying clusters to Service Teams' existing VPCs eases migration by minimizing changes to their networking setup. They can gradually onboard services to Kubernetes while retaining existing infrastructure.\n\n3. **Clear Billing Separation:** As each cluster resides within a Service Team's AWS account, billing remains transparent and directly attributed to the respective team.\n\n4. **Flexibility for Environments:** The model allows for separate clusters for different environments (Staging, Production, etc.) within a Service Team's AWS account, further enhancing isolation and control.\n\n**While this approach offers significant advantages, it introduces complexities in centralized management and potential duplication of resources. To mitigate these, we will:**\n\n* **Implement Infrastructure-as-Code (IaC):** Standardize cluster creation and management using IaC tools like Terraform. This ensures consistency, repeatability, and simplifies lifecycle management.\n* **Explore Shared Services:** Identify opportunities for centralizing shared services (e.g., monitoring, logging) that can serve multiple Service Team clusters. This balances isolation benefits with resource efficiency.\n* **Develop Clear Governance:** Establish clear guidelines and policies for cluster provisioning, security configurations, and resource usage. This promotes consistency and best practices across Service Teams.\n\nThis multi-cluster strategy provides a robust foundation for secure and isolated deployments, aligned with Service Teams' existing infrastructure and operational models.  The focus on automation and shared services will be crucial to ensure scalability and efficiency in the long term.\n","GenTime":"2024-08-28 13:21:57"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"text":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n\n\n##Decision\nAccepted (02-16-2021)\\n","MatchedIds":[847,1119,4281,3715,3657],"Prediction":"## Decision\n\nWe will adopt the **Mapbox GL Style Specification** for client-side map rendering.\n\n### Reasons:\n\n* **Reduces maintenance:** This decision enables the replacement of Mapnik, simplifying our stack and reducing maintenance overhead.\n* **Flexibility and Framework Support:** The Mapbox GL Style Specification is compatible with major client-side mapping frameworks like MapLibre GL, Leaflet, and OpenLayers. This allows us to experiment with different rendering engines and choose the best fit for our needs.\n* **Future-Proofing:**  Adopting a widely-used standard like the Mapbox GL Style Specification ensures our maps infrastructure is well-positioned for future development and integration with other tools.\n\n### Positive Consequences:\n\n* Decreased maintenance burden on the maps infrastructure team.\n* Increased agility in experimenting with different rendering engines.\n* Improved compatibility and potential for integration with other mapping tools and services. \n\n### Negative Consequences:\n\n* Potential learning curve for developers unfamiliar with the Mapbox GL Style Specification. \n* Possible limitations in directly migrating existing Kartotherian functionalities that might not be fully covered by the Mapbox GL Style Specification.\n\n### Mitigation:\n\n* We will invest in training and resources to help developers learn and implement the Mapbox GL Style Specification effectively. \n* Thoroughly evaluate potential migration challenges from Kartotherian and develop workarounds or alternative solutions for any functionalities not directly supported by the chosen specification. \n","GenTime":"2024-08-28 13:22:04"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"text":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n\n\n##Decision\nCreate style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","MatchedIds":[621,3144,2539,1372,328],"Prediction":"## Decision\n\nWe will adopt a modular structure for our Terraform codebase using Terraform modules. \n\nEach module will adhere to the following guidelines:\n\n* **Single Responsibility:** Each module should focus on managing a single, well-defined component or concept (e.g., a VPC, an ECS cluster, a security group rule).\n* **Descriptive Naming:**  Module names should clearly indicate their purpose and the infrastructure they manage.\n* **Comprehensive README:** Every module must include a README file that clearly outlines:\n    *  The module's purpose and functionality.\n    *  Required input variables and their descriptions.\n    *  Output values provided by the module.\n    *  Example usage demonstrating how to call the module.\n* **Minimal File Structure:**  Strive to keep the number of files within a module minimal, ideally contained within a single `main.tf` file for simple modules. For more complex scenarios, use additional files to improve readability and organization (e.g., `variables.tf`, `outputs.tf`). \n\nBy following this consistent structure, we aim to enhance code readability, maintainability, and reusability. This will enable developers to quickly understand the structure and purpose of our Terraform codebase. \n","GenTime":"2024-08-28 13:22:11"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"text":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n\n\n##Decision\nWe decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","MatchedIds":[2410,4841,216,745,1182],"Prediction":"## Decision\n\nWe will host all application components and data within data centers physically located within the European Union. To ensure compliance, we will leverage a cloud provider that:\n\n* **Offers data residency guarantees:**  Select a provider with a clear and explicit commitment to data residency within the EU, demonstrably compliant with GDPR regulations.\n* **Provides data sovereignty features:** Utilize provider features that restrict data processing and backups to EU-based data centers.\n* **Maintains transparent data handling practices:** Employ a provider that openly discloses data handling practices and offers auditable logs to verify data location and movement.\n\nFurthermore, we will:\n\n* **Contractually obligate the provider:** Include strict data residency clauses in our service agreements to legally enforce data localization within the EU.\n* **Implement data encryption:** Encrypt data at rest and in transit to maintain confidentiality and control, even in the unlikely event of unauthorized access or transfer.\n* **Regularly audit and monitor:** Establish a process for regular audits and monitoring of data storage and transfer activities to ensure ongoing compliance with EU data residency requirements. \n\nBy adopting this multi-layered approach, we ensure that the data entrusted to us remains protected and within the confines of the European Union, meeting both regulatory requirements and user trust. \n","GenTime":"2024-08-28 13:22:18"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"text":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n\n\n##Decision\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","MatchedIds":[3657,4747,2520,4909,1167],"Prediction":"## Decision\n\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\n* Technical complexity and brittleness of Algolia integration\n* Lack of customisability and lack of configuration-as-code on Algolia\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\n* Lack of ability to integration test search results when using SaaS search engine\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\n* Potential for richer geographical querying using PostGIS\n\nChosen option: **Migrate away from Algolia to leverage PostgreSQL for search functionality.**\n\n### Positive Consequences\n* **Cost Reduction:** Eliminates the financial burden of the Algolia subscription.\n* **Reduced Complexity:** Simplifies our architecture by removing the need for a separate search service and its integration.\n* **Improved Testability:** Allows for seamless integration testing of search functionality within our existing testing framework.\n* **Increased Control and Customization:** Provides greater control over search customization and allows for configuration-as-code practices.\n* **Leveraging Existing Resources:** Capitalizes on the existing strengths and capabilities of PostgreSQL, including full-text search and PostGIS for geographical queries.\n\n### Negative Consequences \n* **Potential Performance Limitations:** PostgreSQL's search performance, while constantly improving, might not match the specialized capabilities of a dedicated search engine like Algolia, especially for extremely high-volume, complex searches. This risk can be mitigated with proper indexing, query optimization, and potential future scaling options.\n* **Development Effort for Migration:** Transitioning away from Algolia will require development time and effort to implement and test the PostgreSQL-based search solution.\n\n### Mitigation Strategies\n\n* **Performance Monitoring and Optimization:**  Implement robust monitoring to track search performance closely and leverage PostgreSQL's indexing and query optimization features to ensure responsiveness. Consider database scaling options if necessary.\n* **Phased Migration Approach:**  Gradually transition from Algolia to PostgreSQL search, starting with specific features or sections of the application to minimize disruption and allow for iterative testing and refinement.\n","GenTime":"2024-08-28 13:22:28"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"text":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n\n\n##Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","MatchedIds":[4847,2997,4358,187,1464],"Prediction":"## Decision\n\n### Separate AWS Accounts\n\nWe will use two separate AWS accounts: one for development\/staging and one for production. \n\n**Reasons:**\n\n* **Security:**  Isolating production resources significantly reduces the risk of accidental or malicious changes impacting live data and applications. \n* **Compliance:** Many compliance frameworks require strict separation between production and non-production environments.\n* **Resource Limits:** Separate accounts provide independent resource limits, preventing development\/staging activities from impacting production workloads.\n* **Billing Clarity:**  Separate accounts simplify cost allocation and analysis for production vs. non-production environments.\n\n### ECS Cluster Separation\n\nWithin each account (development\/staging and production), we will use separate ECS clusters for development\/staging and production environments.\n\n**Reasons:**\n\n* **Resource Isolation:** This prevents resource contention between environments and ensures predictable performance for production workloads.\n* **Deployment Flexibility:**  Allows for independent deployment schedules and rollback procedures for each environment.\n* **Configuration Management:**  Simplifies configuration management and reduces the risk of configuration drift between environments.\n\n### Infrastructure as Code (IaC)\n\nWe will leverage Terraform to manage the infrastructure for both accounts and all environments. \n\n**Reasons:**\n\n* **Consistency:** IaC ensures consistent infrastructure provisioning across all environments.\n* **Repeatability:** Enables rapid creation and teardown of environments for testing and experimentation.\n* **Version Control:**  Provides a history of infrastructure changes and facilitates rollbacks if necessary.\n* **Automation:**  Automates the infrastructure deployment process, reducing manual errors and increasing efficiency.\n\n\n### Diagram\n\n```\n                    +-----------------+       +-----------------+\n                    |  Dev\/Staging    |       |   Production    |\n                    |  AWS Account    |       |   AWS Account    |\n                    +-----------------+       +-----------------+\n                          |                        |\n                          |                        |\n                    +-------+--------+       +-------+--------+\n                    | Dev\/Staging  |       | Production |\n                    | ECS Cluster  |       | ECS Cluster|\n                    +-------+--------+       +-------+--------+\n                          |                        |\n                          |                        |\n                    +-------+--------+       +-------+--------+\n                    |  ECS Service 1 |       |  ECS Service 1 | \n                    +-------+--------+       +-------+--------+\n                    |  ECS Service 2 |       |  ECS Service 2 |\n                    +-------+--------+       +-------+--------+\n\n```\n\nThis setup provides a secure, scalable, and manageable solution for deploying applications to ECS across development, staging, and production environments. \n","GenTime":"2024-08-28 13:22:40"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"text":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n\n\n##Decision\nTrack metrics at the base_path level\\n","MatchedIds":[1823,3469,3802,3784,4636],"Prediction":"## Decision\n\nWe will migrate performance and quality metrics tracking from a `content_id` level to a `base_path` level for Guides and Travel Advice.  This change prioritizes user needs for aggregated data analysis while minimizing potential disruption to existing reporting functionalities.\n\nThis migration will involve:\n\n1. **Data Model Update:**  Modify the existing data model or create a new one to store metrics against `base_path` instead of `content_id`.\n2. **Data Migration:** Develop and execute a migration script to aggregate existing metrics from `content_id` to `base_path`. Historical data associated with specific `content_id`s might be archived for potential future analysis.\n3. **Code Refactoring:** Update the codebase to record and retrieve metrics using `base_path` instead of `content_id`.\n4. **Monitoring & Validation:**  Implement robust monitoring to ensure data integrity during and after the migration. Validate that dashboards and reports reflect the changes accurately.\n\nThis approach balances the need to fulfill user requirements with the importance of maintaining data integrity and system stability.\n","GenTime":"2024-08-28 13:22:46"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"text":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n\n\n##Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","MatchedIds":[1924,4852,2975,3290,2958],"Prediction":"## Decision\n\nUtilize a logging library that supports asynchronous logging with a dedicated logging thread and internal buffering. \n\nHere's how it addresses the requirements:\n\n* **Avoid Interleaving:** A dedicated logging thread will handle all log writes.  Application threads will push log messages into an in-memory queue, preventing them from directly writing to the output (file or STDOUT). The logging thread will then process the queue and write the entries sequentially, guaranteeing no interleaving.\n\n* **Performance:** Asynchronous logging ensures the main application threads are not blocked by I\/O operations associated with writing logs. The internal buffering within the logging library allows it to accumulate multiple log messages before flushing them to the output, further reducing overhead.\n\n* **Library Choice:**  The specific library chosen will depend on the programming language and ecosystem.  Popular choices with asynchronous capabilities include:\n    * **Python:** `logging` module with `QueueHandler` and a separate thread.\n    * **Java:** Log4j 2 with `AsyncAppender`. \n    * **Node.js:** Winston with a transport that supports asynchronous operations (e.g., using a queue).\n\n**Implementation Notes:**\n\n* **Queue Size:** Configure the internal queue size to handle bursts of log messages without causing excessive memory usage. \n* **Error Handling:**  Implement robust error handling within the logging thread to prevent the loss of log entries in case of issues. Consider strategies like writing to a secondary log or sending alerts.\n* **Configuration Flexibility:** Allow for configuration options to enable\/disable asynchronous logging and adjust buffer sizes based on the application's specific performance and logging needs. \n","GenTime":"2024-08-28 13:22:54"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"text":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n\n\n##Decision\nFor the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","MatchedIds":[2508,3831,1216,3253,138],"Prediction":"## Decision\n\nImplement a queue-based asynchronous processing system for archiving. This system will manage the different stages of the archiving process as individual jobs. Each job represents a specific stage and will process a batch of items.\n\nThis approach provides several benefits:\n\n* **Improved performance and responsiveness:** By offloading the archiving process from the main application flow, user-facing operations will not be impacted by the long-running archiving tasks.\n* **Increased scalability and fault tolerance:** The queue-based system allows for scaling the number of workers processing the archive jobs independently. Additionally, if a worker fails during processing, the job can be retried by another worker, ensuring fault tolerance.\n* **Better monitoring and management:**  With a queue-based system, it's easier to monitor the progress of each archiving stage, identify bottlenecks, and track the overall health of the process.\n\nThe specific queuing technology (e.g., RabbitMQ, Kafka, SQS) will be chosen based on project requirements and constraints.\n","GenTime":"2024-08-28 13:23:00"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"text":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n\n\n##Decision\nWe will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","MatchedIds":[2181,3069,2261,160,1180],"Prediction":"## Decision\n\nWe will use [C4 Model](https:\/\/c4model.com\/) for our architectural diagrams. \n\n### Justification\n\nC4 Model provides a structured approach to visualizing software architecture that is easy to learn and apply. It scales well from context diagrams down to detailed component diagrams.\n\n### Tooling\n\nWe will use [Structurizr](https:\/\/structurizr.com\/) as our diagramming tool for C4 modelling. \n\n* **Integration:** It integrates well with our existing development workflow, allowing us to generate diagrams programmatically or through a user-friendly web interface.\n* **Maintainability:** This will make it easier to keep our diagrams consistent and up-to-date as our system evolves.\n* **Accessibility:** It offers collaborative features for team members to contribute and provide feedback.\n\n### Diagram Repository\n\nAll C4 model diagrams will be stored in a dedicated folder within our main code repository. This ensures version control, easy access for developers, and keeps the diagrams alongside the code they represent. \n\n### Diagram Conventions\n\nWe will create and maintain a simple document outlining the conventions used in our C4 diagrams, ensuring consistency across the team. This document will live alongside the diagrams in the repository. \n","GenTime":"2024-08-28 13:23:06"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"text":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n\n\n##Decision\nBut I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","MatchedIds":[4338,1404,4224,458,3853],"Prediction":"## Decision\nWe will not use a linter to enforce the formatting described in ADR1.\n\nWhile using a linter is a common approach for code style enforcement, the unique formatting of \"katafied\" code clashes with standard linting practices.  Enforcing this specific format via a linter would require a highly customized configuration, potentially leading to complexity and maintainability issues. \n\nFurthermore, the \"katafied\" code's primary purpose is to exist in a specifically commented-out state for users to manually uncomment and work on.  Therefore, subjecting it to standard linting rules might not significantly improve the overall code quality or developer experience. \n\nInstead of relying solely on automated linting, we will rely on other measures such as:\n\n* **Clear documentation:** The specific formatting requirements outlined in ADR1 will be clearly documented and readily accessible to developers. \n* **Code reviews:**  Reviewers will pay particular attention to ensure new kata submissions adhere to the established formatting guidelines.\n* **Potentially, a dedicated CLI tool:**  If the need arises, we can develop a simple command-line tool specifically designed to validate and\/or automatically \"katafy\" code blocks according to the defined format. \n\nThis approach prioritizes clarity, maintainability, and a focused solution tailored to the specific needs of managing kata formatting.\n","GenTime":"2024-08-28 13:23:13"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","MatchedIds":[4701,794,4804,3820,1145],"Prediction":"## Context and Problem Statement\nOur application needs to persist data. We need to decide between using a relational database (SQL) or a NoSQL database. Which database technology best suits the needs of our application and its expected data model? \n\n## Decision Drivers\n* **Data Structure:** The shape and relationships within the data we need to store. \n* **Scalability Requirements:** Expected read\/write load and future growth expectations. \n* **Development Speed:** The time sensitivity of delivering the application. \n* **Team Expertise:** Existing knowledge and experience with different database technologies within the team. \n* **Cost:**  Budgetary constraints regarding database licensing, infrastructure, and maintenance. \n## Decision\n* **Data Structure:** TBD\n* **Scalability Requirements:** TBD\n* **Development Speed:** TBD\n* **Team Expertise:** TBD\n* **Cost:** TBD\nChosen option: TBD, because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\n### Positive Consequences \n*  \n### Negative Consequences \n* \n","GenTime":"2024-08-28 13:23:19"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"text":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n\n##Decision\nConcepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","MatchedIds":[1442,4650,4119,1614,1441],"Prediction":"## Decision\n\n### Separation of Concerns\n\nThe IBC handlers within the `ibc` crate will be structured to maintain a clear separation of concerns:\n\n1. **Protocol Logic**: The core logic of each IBC protocol, as defined in the ICS specifications, will be implemented in a dedicated module. This module will be responsible for handling messages, processing state transitions, and generating events according to the protocol rules.  It should remain agnostic to specific application logic or chain-specific details.\n\n2. **Chain Interface**: A separate module will handle all interactions with the underlying chain. This includes accessing the chain's state, sending transactions, and querying data. This module will provide a generic interface that abstracts away the specifics of the underlying chain implementation.\n\n3. **Application Callbacks**:  Application-specific logic will be implemented through a set of well-defined callback functions. These callbacks will be invoked by the protocol logic module at specific points in the protocol execution, allowing the application to perform custom actions or enforce specific constraints.\n\n### Handler Structure\n\nEach IBC message type will have a corresponding handler function. The handler function will be responsible for:\n\n1. **Message Validation**: Ensuring the incoming message is well-formed and adheres to the protocol specifications.\n2. **State Manipulation**:  Updating the IBC module's state based on the message content.\n3. **Application Interaction**: Invoking the appropriate application callbacks, if necessary.\n4. **Event Emission**: Generating events that signal significant state changes or actions performed.\n\n### Example\n\nLet's consider the `MsgRecvPacket` message as an example:\n\n```rust\n\/\/ Protocol Logic Module\n\nfn handle_recv_packet(\n    ctx: &mut Context<'_>,\n    msg: MsgRecvPacket,\n) -> Result<(), IbcError> {\n    \/\/ 1. Message Validation\n    \/\/ ...\n\n    \/\/ 2. State Manipulation\n    \/\/ ...\n\n    \/\/ 3. Application Interaction\n    let result = ctx.application().on_recv_packet(\n        msg.packet,\n        msg.proofs,\n        msg.proof_height,\n    );\n\n    \/\/ Handle application result\n    \/\/ ...\n\n    \/\/ 4. Event Emission\n    \/\/ ...\n}\n```\n\n```rust\n\/\/ Application Module\n\ntrait Application {\n    fn on_recv_packet(\n        &mut self,\n        packet: Packet,\n        proofs: Proofs,\n        proof_height: Height,\n    ) -> Result<(), ApplicationError>;\n}\n```\n\n### Benefits\n\nThis approach provides several benefits:\n\n* **Modularity and Reusability**:  The separation of concerns allows for a more modular and reusable codebase.\n* **Testability**:  The individual components can be easily tested in isolation.\n* **Flexibility**:  The callback mechanism allows for easy customization and extension of the IBC protocol logic by applications.\n* **Maintainability**: The clear separation of responsibilities improves the overall maintainability of the codebase.\n \n### Future Considerations\n\nAs the IBC protocol and its implementations evolve, this ADR will serve as a guideline for future development.  Future improvements could include:\n\n* **Asynchronous Handlers**:  Exploring asynchronous handling of messages to improve performance.\n* **Error Handling**:  Defining a comprehensive error handling strategy for IBC handlers. \n* **Instrumentation**:  Adding metrics and logging to monitor the performance and health of the IBC handlers. \n","GenTime":"2024-08-28 13:23:36"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"text":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n\n\n##Decision\nWe will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","MatchedIds":[2261,138,4699,4428,4408],"Prediction":"## Decision\n\nWe will adopt the use of Architecture Decision Records (ADRs) to document all significant architectural decisions made on this project. This practice will provide a clear and easily accessible history of why certain architectural choices were made, mitigating the risks associated with unclear decision-making as the project evolves. We will follow established best practices for ADRs, such as those described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions. \n","GenTime":"2024-08-28 13:23:40"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"text":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n\n\n##Decision\nWe will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","MatchedIds":[885,2499,1575,84,3127],"Prediction":"## Decision\n\nWe will use WebSockets to implement the PUSH channel for JMAP Push notifications. \n\n### Rationale\n\n* **Bidirectional Communication:** WebSockets provide full-duplex communication, allowing the server to push updates to the client and the client to acknowledge receipt.\n* **Low Latency:** WebSockets offer low latency, which is crucial for near real-time updates.\n* **Wide Browser Support:** WebSockets are well-supported by all major modern browsers.\n* **Scalability:** Existing infrastructure and tools can be leveraged for scaling WebSocket connections.\n\n### Implementation Details:\n\n1. **Connection Establishment:** Clients will initiate a WebSocket connection to a dedicated endpoint on the server. \n2. **Subscription Management:** Upon connection, clients will subscribe to specific data types or resources they want to receive updates for.\n3. **Change Detection and Notification:** The server will track changes to subscribed resources and push notifications containing the changeset to the relevant clients.\n4. **Client-Side Updates:** Clients will receive these notifications and update their local state accordingly, providing a seamless update experience for the user.\n\n### Alternatives Considered:\n\n* **Long Polling:** While simpler to implement, long polling suffers from higher latency and resource consumption compared to WebSockets. \n* **Server-Sent Events (SSE):** SSE is unidirectional, limiting the client's ability to acknowledge receipt or communicate back to the server.\n\n### Potential Challenges and Mitigations:\n\n* **Scalability:** Load balancing and horizontal scaling mechanisms will be crucial to handle a large number of concurrent WebSocket connections.\n* **Error Handling and Reconnection:** Implement robust error handling and automatic reconnection logic to ensure continuous service in case of network interruptions.\n\n### Future Considerations:\n\n* **Message Queues:** For further scalability and fault tolerance, integrating a message queue system can be explored.\n* **Push Notification Services:** Utilizing platform-specific push notification services like Apple Push Notification Service (APNS) and Firebase Cloud Messaging (FCM) can optimize battery life on mobile devices. \n","GenTime":"2024-08-28 13:23:50"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"text":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n\n\n##Decision\nBy default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","MatchedIds":[2706,1614,3109,3853,2725],"Prediction":"## Decision\n\nWe will follow the \"Scripts to Rule Them All\" pattern as described in [dxw's RFC-023](https:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md). This means:\n\n*  We will strive to use executable scripts for as much of our infrastructure and application deployment and management as possible. \n*  We will prioritize scripting languages that are widely used and understood within the team, while remaining open to using the most appropriate tool for the task.\n*  Documentation will live alongside the scripts themselves, ensuring clarity and maintainability.\n\n### Rationale\n\nThis approach aligns with our goals of:\n\n* **Increased automation and repeatability:**  Reduces manual errors and ensures consistency.\n* **Improved transparency and collaboration:** Scripts are inherently more auditable and easier to collaborate on than manual processes.\n* **Simplified knowledge sharing and onboarding:** New team members can quickly understand how systems work by examining the scripts.\n\n### Implementation Details\n\n* A dedicated directory within this repository will house all relevant scripts.\n* Each script will include a header with a clear description of its purpose, usage instructions, and any dependencies.\n* We will actively explore opportunities to replace manual processes with automated scripts.\n*  We will establish a process for reviewing and approving proposed changes to critical scripts.\n\n### Potential Drawbacks\n\n* **Maintenance overhead:** While scripts can reduce operational burden, they require ongoing maintenance.\n* **Complexity:**  Complex systems might require intricate scripting, potentially impacting readability.\n\nWe will mitigate these drawbacks through:\n\n*  Writing clear, well-documented, and modular scripts.\n*  Encouraging code reviews to ensure quality and maintainability. \n*  Continuously evaluating the complexity and benefits of automation for each specific case. \n","GenTime":"2024-08-28 13:24:00"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"text":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n\n\n##Decision\nTo minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","MatchedIds":[1924,1927,3741,4843,577],"Prediction":"## Decision\n\nIntroduce a new data type, `Secret`, to represent sensitive data that should not be logged.\n\n* The `Secret` type should be a wrapper around a string, and should not provide any methods that would allow its content to be accidentally logged.\n*  The `logger` object (from previous decisions) should be modified to recognize the `Secret` type.\n* When a `Secret` is passed to the logger, it should be replaced with a placeholder string (e.g. \"<redacted>\") in the log output. \n* Any string formatting functions used by the logger should also be modified to recognize and redact `Secret` objects.\n\n```typescript\n\/\/ Example implementation in TypeScript\nclass Secret {\n  constructor(private readonly value: string) {}\n\n  toString(): string {\n    return \"<redacted>\";\n  }\n}\n\n\/\/ Usage example\nconst password = new Secret(\"mysecretpassword\");\nlogger.info(\"User attempted login with password: \", password);\n\n\/\/ Log output:\n\/\/ INFO: User attempted login with password: <redacted>\n```\n\nThis approach ensures that sensitive data is handled explicitly and reduces the risk of accidental disclosure through logs. \n","GenTime":"2024-08-28 13:24:07"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"text":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n\n##Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","MatchedIds":[2520,1031,4909,4747,1574],"Prediction":"## Decision\n\nTo address the limitations of using PostgreSQL for our CI\/CD state management, we will adopt an **event sourcing** pattern combined with a dedicated **read-optimized data store**. \n\n### Rationale\n\n- **Event Sourcing:** Instead of storing the current state of entities, we will store a series of events that represent each change made to the system. This inherently provides us with:\n    - **Auditability:** Every change is recorded, making it easy to track history and perform rollbacks.\n    - **Scalability:** The append-only nature of event logs eliminates locking issues and facilitates scaling out.\n    - **Schema Evolution:** Changes to event structure are easier to manage than altering database schemas.\n- **Separate Read Model:** We will introduce a separate data store optimized for reads and analytical queries. This could be a NoSQL database, a columnar store, or a search index, depending on specific needs. This decoupling allows us to:\n    - **Improve Performance:** Run complex analytical queries without impacting the performance of the core CI\/CD operations.\n    - **Choose Specialized Storage:** Leverage different storage technologies best suited for specific query patterns.\n\n### Implementation Details\n\n- A message queue system like Kafka can be used to capture the events.\n- Consumers can then process these events to update the read-optimized data store and generate projections relevant for different purposes (e.g., dashboards, historical reports).\n- For querying the current state, we can either rebuild the state from the event log or maintain a materialized view in a dedicated read model.\n\n### Positive Consequences\n\n- Improved performance and scalability for both operational and analytical workloads.\n- Enhanced auditability and traceability for debugging and compliance.\n- Increased flexibility in schema management and evolution.\n\n### Negative Consequences\n\n- Increased complexity in the system architecture.\n- Requires a mind-shift to an event-driven paradigm.\n- Potential for eventual consistency challenges, requiring careful handling.\n\n### Mitigation Strategies\n\n- Introduce robust monitoring and alerting around the event processing pipeline.\n- Implement appropriate mechanisms for handling eventual consistency and data consistency across projections.\n- Invest in developer training and documentation to ensure a smooth transition to event sourcing.\n","GenTime":"2024-08-28 13:24:17"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\\nAgentFramework is also use for Browser\\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"text":"## Context\\nAgentFramework is also use for Browser\\n\n\n##Decision\nWe need minify build because package.json don't have a property called `minMain`\\n","MatchedIds":[4271,1091,1240,1243,4592],"Prediction":"## Decision\n\nThis context lacks important details to make an informed architectural decision.  To provide a proper decision, please provide more context on the following:\n\n* **What is AgentFramework?** Is it a library, a framework, a service? What is its purpose? \n* **What is the specific use case in the browser?** What functionality will AgentFramework provide in the browser environment?\n* **What are the constraints and requirements?** Are there any performance considerations, security concerns, or browser compatibility requirements?\n\nFor example, knowing more about the context could lead to decisions like:\n\n* **\"We will create a custom build of AgentFramework for the browser environment, minimizing its footprint and optimizing for client-side performance.\"** (If size and performance are critical)\n* **\"We will expose AgentFramework functionality in the browser through a dedicated API, ensuring secure communication and data handling.\"** (If security is a major concern)\n* **\"We will leverage a JavaScript module bundler to package AgentFramework for browser consumption, ensuring compatibility across target browsers.\"** (If browser compatibility is crucial) \n\nProvide more context, and I will be happy to give a more specific and helpful decision. \n","GenTime":"2024-08-28 13:24:23"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"text":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\n\n##Decision\nWe recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","MatchedIds":[947,3142,2256,578,1561],"Prediction":"## Decision\n\nWe will implement a cross-platform key management system with the following features:\n\n**Abstraction Layer:**\n\n*  A common interface will be defined to abstract away the specific details of the underlying secret storage backend. This interface will provide methods for storing, retrieving, and managing keys and their metadata.\n\n**Backend Implementations:**\n\n* **Platform-Specific Secret Stores:** Leverage native OS-level secure storage solutions where available. This will include:\n    * **Linux:** Utilize Keychain on macOS and GNOME Keyring or KWallet on Linux systems.\n    * **Windows:** Integrate with the Windows Credential Manager. \n* **File-Based Fallback:** For platforms without a built-in secure storage solution (or for users who opt-out), provide a file-based fallback. This fallback mechanism will:\n    *  Encrypt all sensitive data (keys and metadata) using strong encryption algorithms (e.g., AES-256) before writing to disk.\n    *  Utilize the user's passphrase to derive a unique encryption key for their data, ensuring that even if the storage file is compromised, the data remains inaccessible without the passphrase.\n\n**Security Considerations:**\n\n* **Minimize Attack Surface:** The fallback storage mechanism will be designed to minimize the attack surface by limiting file system permissions and using secure file handling practices.\n* **Passphrase Strength:** The application will encourage users to set strong, unique passphrases and enforce minimum complexity requirements.\n* **Compromise Recovery:** Implement mechanisms for users to recover their keys in case of a forgotten passphrase or device loss. This may involve using a secondary recovery key or a secure key recovery process.\n\n**Implementation Details:**\n\n* The implementation will prioritize the use of platform-specific secret stores to maximize security. \n* The file-based fallback will be thoroughly tested to ensure its security and reliability.\n* The user interface will clearly communicate which storage mechanism is being used and provide options for users to configure their preferred storage method. \n\n**Consequences:**\n\n* **Enhanced Security:** Significantly improve the security posture of the application by protecting key material and metadata from unauthorized access.\n* **Cross-Platform Compatibility:** Provide a consistent experience across different operating systems.\n* **Increased Complexity:**  Introducing multiple backend implementations and an abstraction layer will add complexity to the codebase. \n* **Maintenance Overhead:** Ongoing maintenance will be required to keep up with changes in platform-specific APIs and ensure the security of the fallback mechanism. \n\nThis decision prioritizes the security of user data by leveraging the most secure storage solutions available on each platform while providing a robust fallback option for environments where native solutions are not feasible.\n","GenTime":"2024-08-28 13:24:36"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"text":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n\n\n##Decision\nInstall `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","MatchedIds":[5033,1067,1614,2091,3635],"Prediction":"## Decision\nWe will use `adr-tools` to simplify the creation and management of ADRs for this project. This decision leverages our prior positive experience with the tool and its ability to streamline the documentation process. \n","GenTime":"2024-08-28 13:24:39"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"text":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\n\n##Decision\nTo use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","MatchedIds":[3827,2687,1202,3328,4225],"Prediction":"## Decision\n\neLife will follow the Node.js Long Term Support (LTS) schedule for all projects. \n\n* **New projects** should use the latest LTS version of Node.js. \n* **Existing projects** should upgrade to the latest LTS version of Node.js at the earliest opportunity.\n* We will evaluate the adoption of new Node.js LTS releases on a quarterly basis, with the aim of upgrading to the latest LTS version within 3 months of its release. \n\nBy following the Node.js LTS schedule, we ensure all projects benefit from long-term support, including critical bug fixes and security updates. This approach offers a balance between stability and access to new features while simplifying dependency management across eLife projects.\n","GenTime":"2024-08-28 13:24:43"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"text":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n\n\n##Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","MatchedIds":[979,4096,3488,885,4694],"Prediction":"## Decision\n\nChosen option: **RabbitMQ** because:\n\n* **Easy to use and learn:** RabbitMQ has a relatively low barrier to entry, with ample documentation and examples available.\n* **Docker support:**  RabbitMQ offers official Docker images, simplifying deployment and integration within containerized environments. \n* **Robust feature set:** While being easy to use for simple scenarios, RabbitMQ offers powerful features (topics, exchanges, queues) that can support complex routing and message patterns required by evolving EDA architectures.\n\nWhile other message brokers like Kafka or NATS exist, their strengths lie in high-throughput, distributed streaming scenarios. Given the decision driver's emphasis on ease of use and learning, RabbitMQ provides a more suitable starting point for this architecture. \n\n**Future Considerations:**\n\n* **Scalability:** As the system grows and message volume increases, evaluate if RabbitMQ's scaling capabilities remain sufficient. \n* **Alternative brokers:**  If specific requirements arise (e.g., high throughput, global scale), revisit the choice of the messaging server, potentially considering Kafka or NATS as alternatives. \n","GenTime":"2024-08-28 13:24:49"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"text":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n\n##Decision\nAshley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","MatchedIds":[745,4166,1171,1180,699],"Prediction":"## Decision\n\nWe will establish a secure connection to Newslabs' PSTT service using the following approach:\n\n**Method:** Implement a dedicated API client within our application to interact with Newslabs PSTT. This client will handle authentication, data transmission, and response processing.\n\n**Authentication:** Utilize API keys provided by Newslabs for secure access to PSTT. These keys will be stored securely as environment variables within our application's deployment environment. \n\n**Data Transmission:**  Transmit audio data to PSTT via HTTPS POST requests to ensure data confidentiality and integrity during transit. The API client will handle audio encoding and formatting as required by PSTT.\n\n**Error Handling:** Implement robust error handling within the API client to gracefully manage potential issues such as network interruptions, service unavailability, or authentication failures. This includes logging errors for debugging and potentially implementing retry mechanisms. \n\n**Monitoring:** Integrate monitoring of the PSTT connection to track request latency, success rates, and potential errors. This will help ensure the stability and performance of the integration.\n\n**Benefits:** \n\n* **Security:**  API keys and HTTPS encryption provide a secure connection to PSTT.\n* **Maintainability:** A dedicated API client provides a centralized and maintainable way to interact with the service.\n* **Scalability:**  This approach can easily scale with increased usage of the STT service. \n\n**Considerations:**\n\n* **Latency:**  Network latency between our application and PSTT should be considered, especially for real-time transcription scenarios.\n* **Rate Limits:**  Familiarize ourselves with Newslabs PSTT rate limits and implement appropriate handling within the API client to prevent request throttling.\n\nThis approach provides a secure, reliable, and scalable solution for connecting to Newslabs' shared STT service.\n","GenTime":"2024-08-28 13:24:58"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"text":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n\n\n##Decision\nDeprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","MatchedIds":[4307,4644,2181,276,786],"Prediction":"## Decision\n\nChosen option: **\"Develop a Simplified Configuration Model\"**\n\n### Justification:\nThe current DynamicServiceLocator, while designed for extensibility, suffers from over-complexity and tight coupling due to its reliance on XML and an inflexible structure. This makes it difficult to understand and use, particularly within a UI context. \n\nMigrating to a centralised processing model for the standard document trading network presents an opportunity to re-evaluate and simplify the configuration. This will improve usability and maintainability.\n\n### Description:\n\n* **Create a new, simplified configuration model:** This model will focus on the essential information required for mapping specifications and key data, eliminating any extraneous elements.\n* **Prioritize clarity and ease of use:** The new model should be easily understandable and configurable, especially for users interacting with it through a UI.  \n* **Leverage a more human-readable format:**  Consider using JSON or YAML instead of XML for defining configurations, as these formats are generally easier to read and write, especially for UI interactions.\n* **Decouple components:** Design the new model to minimize dependencies between components. This will improve maintainability and flexibility for future modifications.\n* **Provide migration tools:**  To ensure a smooth transition, develop tools to assist in migrating existing configurations from the old DynamicServiceLocator format to the new simplified model.\n\n### Positive Consequences:\n\n* **Improved usability:** A simplified configuration model will be easier for users to understand and interact with, especially through a UI.\n* **Reduced complexity:** Removing unnecessary elements and dependencies will make the system easier to maintain and evolve.\n* **Increased agility:**  A more flexible configuration model will make it easier to adapt to future changes in the document trading network.\n\n### Negative Consequences:\n\n* **Development effort:**  Designing, implementing, and testing the new configuration model and migration tools will require development time.\n* **Potential compatibility issues:**  Thorough testing and migration procedures will be essential to minimize disruptions for existing users during the transition. \n\n\nThis decision prioritizes long-term benefits of maintainability, usability, and flexibility over the short-term costs of refactoring. \n","GenTime":"2024-08-28 13:25:08"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"text":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n\n\n##Decision\nTo take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","MatchedIds":[4508,2262,4500,5185,4507],"Prediction":"## Decision\n\nWe will change the theme inheritance mechanism to use **composition over inheritance** for all theme resources:\n\n* **Config:** Themes will no longer inherit config values directly from the default theme. Instead, each theme will define its own set of config values, with the option to explicitly reference and override specific values from the default theme's config. This will make config inheritance explicit and prevent unintended side effects from changes in the default theme.\n\n* **Templates:** Themes will continue to inherit templates from the default theme, but we will introduce a mechanism to explicitly override individual templates. This will allow themes to customize specific parts of the Storefront without needing to copy and maintain large portions of the default theme's templates.\n\n* **Scripts:** Themes will manage their own scripts independently. This means they will no longer inherit scripts from the default theme. To avoid code duplication, we will encourage the use of shared script libraries that can be included in multiple themes.\n\n* **Theme Activation:** Upon activation, a theme will be responsible for resolving all its dependencies, including referenced default theme config values and required script libraries. Changes made to the default theme will not automatically propagate to activated themes. Themes will need to be re-activated to inherit any updates.\n\nThis approach addresses the following:\n\n* **Explicit Inheritance:** Makes inheritance relationships clear and avoids unintended consequences from changes in the default theme. \n* **Granular Control:** Offers fine-grained control over which aspects of the default theme are inherited or overridden.\n* **Update Resilience:**  Reduces the likelihood of conflicts and issues arising from updates to the default theme or individual theme components. \n\nThis will require changes to the theme management system, including the theme administration interface and the underlying mechanisms for resolving theme dependencies and loading resources. We will provide clear documentation and migration guides to support developers in adapting to the new system. \n","GenTime":"2024-08-28 13:25:17"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"text":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n\n\n##Decision\nWe would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","MatchedIds":[28,1558,839,1957,4101],"Prediction":"## Decision\n\n### Design\n\nThis ADR proposes a new double-sign risk reduction mechanism implemented as an additional safety check within the validator node daemon. This mechanism, named \"recent blocks double-vote check\", will operate during the pre-voting phase for each new block.\n\n#### Recent Blocks Double-Vote Check\n\n1. **Configuration:** The mechanism will be configurable with a parameter `N`, representing the number of recent blocks to check. This parameter can be adjusted based on the chain's block time and desired security level.\n\n2. **Pre-Voting Check:** Before a validator signs a vote for a new block at height `H`, it will perform the following:\n   - Retrieve the signatures for the previous `N` blocks (`H-1`, `H-2`, ... , `H-N`).\n   - For each of these blocks, verify if the validator's key has already signed a vote. If a duplicate signature is found, the validator will immediately halt the voting process and log an error, preventing a potential double-signing incident.\n\n3. **Error Handling:** Upon detecting a potential double-sign scenario, the validator node should:\n   - Log detailed error information including the block height and the detected duplicate signature.\n   - Alert the validator operator through configured alerting channels (e.g., email, SMS).\n   - Pause the validator to prevent further voting until the operator investigates and resolves the issue.\n\n#### Coexistence with Other Mechanisms\n\nThis new mechanism is designed to complement, rather than replace, existing double-sign prevention techniques like slashing. It serves as an additional layer of defense by catching potential double-signing attempts before they occur on-chain, minimizing the impact on the validator's stake.\n\n### Implementation\n\nThe \"recent blocks double-vote check\" will be implemented within the validator node daemon. It requires access to:\n\n- Local signature storage or a reliable mechanism to query historical signatures.\n- Blockchain data to retrieve signatures from past blocks.\n\nThe implementation should be performant to minimize any overhead added to the voting process. Efficient data structures and optimized signature retrieval methods should be utilized.\n\n### Additional Considerations\n\n- **False Positives:** The check needs to accurately distinguish between genuine double-signing attempts and legitimate scenarios like validator key rotation. The implementation should account for such cases to avoid false positives.\n- **Parameterization:** The optimal value for `N` should be determined based on factors like block time, network latency, and the desired balance between security and performance. \n- **Monitoring and Alerting:** Robust monitoring and alerting mechanisms are crucial for operators to be promptly informed of any detected double-sign risks.\n\nThis risk reduction mechanism provides a proactive approach to mitigate accidental double-signing by validators. By integrating this check into the validator node's workflow, we can enhance the network's security and reduce the likelihood of costly double-signing incidents.\n","GenTime":"2024-08-28 13:25:31"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"text":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n\n\n##Decision\nThe simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","MatchedIds":[3841,3504,2689,1310,4179],"Prediction":"## Decision\n\nUpdate the \"caia\" project to use mbtest library version 2.5.2 or later, and refactor any code accessing the results of `get_actual_requests` to handle the new `Iterable[Request]` return type instead of the deprecated `Mapping` type. \n","GenTime":"2024-08-28 13:25:33"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"text":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n\n\n##Decision\nI will use Serverless to describe and provision my AWS components.\\n","MatchedIds":[2062,2752,2409,1464,2723],"Prediction":"## Decision\nWe will manage our infrastructure as code using the Serverless framework. This will allow us to easily provision and manage our services in a consistent and reproducible way. \n","GenTime":"2024-08-28 13:25:35"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"text":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n\n\n##Decision\nThe `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","MatchedIds":[3210,1924,779,674,4538],"Prediction":"## Decision\nIntroduce a new dedicated endpoint for route synchronization that caters specifically to this requirement.\n\n**Rationale:**\n\n* **Specificity:** Avoids forcing the functionality into existing REST API endpoints where it doesn't conceptually belong.\n* **Simplicity:**  A dedicated endpoint allows for a clear and concise API surface for this particular operation, avoiding complexity in existing endpoints.\n* **Limited Information Context:** Given the trigger is a log event with limited information, the new endpoint can be designed to effectively operate within this constraint, potentially accepting a minimal set of parameters.\n\n**Implementation Details:**\n\n* **Endpoint:** `\/api\/routes\/sync` (example)\n* **Method:** `POST` \n* **Payload:** \n    * `action`:  \"add\" or \"replace\"\n    * `route_identifier`: Unique identifier for the route to be synchronized. The specific identifier type (e.g., route ID, name, etc.) will be determined based on the available information from the log event.\n\n**Additional Considerations:**\n\n* **Authentication\/Authorization:** Implement appropriate security measures to control access to this endpoint.\n* **Error Handling:** Define specific error responses for various scenarios, such as route not found, synchronization failure, etc., to facilitate debugging and monitoring.\n\nThis approach provides a clean, maintainable, and scalable solution that addresses the specific context of triggering route synchronization from a log event with limited information, while adhering to API design best practices. \n","GenTime":"2024-08-28 13:25:43"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"text":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n\n\n##Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","MatchedIds":[2968,641,4833,4096,74],"Prediction":"## Decision\nWe will configure PHP-FPM without memory limits (`memory_limit = -1`) within the Docker images. \n\nThis decision is based on the following:\n\n* **Leveraging Orchestrator Resource Management:** By removing the PHP memory limit, we allow the Docker orchestrator (e.g., Kubernetes) to manage resource allocation effectively. \n* **Improved Orchestrator Awareness:** If the application exceeds the allocated memory resources defined at the orchestrator level, the orchestrator can gracefully handle the situation (e.g., restarting the container, scaling resources).  This approach avoids fatal PHP errors that might go undetected by the orchestrator. \n* **Simplified Configuration:**  Relying on the orchestrator for resource management simplifies the PHP-FPM configuration within the Docker image. \n\n**Important Considerations:**\n\n* **Resource Limits at the Orchestrator:** It is crucial to set appropriate resource requests and limits for containers within the orchestrator to prevent resource starvation.\n* **Monitoring and Optimization:**  Closely monitor application resource usage within the orchestrator to identify potential memory leaks or areas for optimization. \n","GenTime":"2024-08-28 13:25:49"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"text":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n\n\n##Decision\nWe will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","MatchedIds":[856,2997,3920,4096,4056],"Prediction":"## Decision\n\n**Adopt a container-based orchestration system with Infrastructure as Code (IaC) principles, utilizing Docker for packaging and a tool like Terraform for infrastructure management.**\n\nThis decision addresses the need for consistency and maintainability across teams and infrastructure:\n\n* **Containerization with Docker**:\n    * Enforces a standardized method for packaging applications and dependencies.\n    * Enables consistent deployment across environments (development, testing, production).\n    * Promotes code reuse and reduces \"reinvention of the wheel\" across teams.\n\n* **Orchestration for Consistency**:\n    * Evaluate and choose a platform like Kubernetes or Amazon ECS to manage containerized applications.\n    * Utilize the chosen platform's features for service discovery, scaling, and self-healing, ensuring consistency across deployments.\n\n* **Infrastructure as Code with Terraform**:\n    * Define infrastructure (servers, networks, load balancers) in code using a tool like Terraform.\n    * Apply version control to infrastructure code, enabling tracking, rollback, and collaboration.\n    * Implement a consistent infrastructure provisioning process across different teams and environments. \n\n**Advantages**:\n\n* **Consistency**:  Provides a unified approach to deploying and managing applications, reducing discrepancies between teams.\n* **Maintainability**: Code-based infrastructure and standardized deployments simplify maintenance and updates.\n* **Scalability**:  Container orchestration platforms allow for easy scaling of applications based on demand.\n* **Collaboration**: Shared Docker images and infrastructure modules encourage collaboration and code reuse.\n\n**Next Steps**:\n\n* **Evaluation:** Research and compare Kubernetes and Amazon ECS to determine the best fit for GDS's needs.\n* **Piloting:** Implement a pilot project with the chosen platform and Terraform to demonstrate the benefits and address potential challenges.\n* **Training:** Provide training to teams on containerization, orchestration, and IaC best practices.\n* **Gradual Adoption:** Migrate existing applications and infrastructure incrementally to the new system, ensuring a smooth transition.\n","GenTime":"2024-08-28 13:25:59"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"text":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n\n\n##Decision\n1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","MatchedIds":[3653,2393,4021,2098,4539],"Prediction":"## Decision\n\nWe will implement a **Strategy Pattern** to handle the different training schemes. \n\nThis will involve:\n\n* **Defining a common interface** for calculating payments, regardless of the specific scheme.\n* **Creating separate concrete strategy classes** for each training scheme (e.g., `SchemeOnePaymentCalculator`, `SchemeTwoPaymentCalculator`). Each class will implement the common interface and encapsulate the specific rules, calculations, and banding systems for that scheme.\n* **Introducing a context object** that will hold a reference to the currently selected strategy (i.e., the specific payment calculation strategy). The context will delegate payment calculations to the active strategy object.\n\nThis approach provides several benefits:\n\n* **Improved maintainability and extensibility:** Adding new training schemes in the future will involve creating a new concrete strategy class, minimizing changes to existing code.\n* **Clear separation of concerns:** Each strategy class focuses on a specific training scheme, improving code readability and organization.\n* **Flexibility:** The strategy pattern allows for runtime selection of payment calculation logic, accommodating potential future requirements. \n\nWe acknowledge that this approach introduces some initial complexity compared to a simpler, hardcoded solution. However, we believe the long-term benefits of maintainability, extensibility, and flexibility outweigh the initial overhead, especially considering the potential for future additions and changes to training schemes. \n","GenTime":"2024-08-28 13:26:06"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"text":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n\n\n##Decision\nWe will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","MatchedIds":[3827,3328,4225,3031,241],"Prediction":"## Decision\n\nWhile a package is already present in the project, we shouldn't assume it's the best choice without further analysis. Blindly using it might lead to technical debt later. \n\nHere's a proposed course of action:\n\n1. **Evaluation:** Research and compare the available Electron auto-update packages, including the one already installed. Consider factors like:\n    - **Popularity and community support:** A larger community means more resources, better documentation, and potentially quicker issue resolution.\n    - **Ease of use and integration:**  How easily can the package be integrated into the existing codebase?\n    - **Features and customization:** Does the package offer the required functionalities like cross-platform support, different release channels, or update scheduling?\n    - **Security:** How does the package handle update verification and integrity checks?\n    - **Active maintenance and development:** Is the package actively maintained with recent updates and bug fixes?\n\n2. **Decision:**  Based on the evaluation, choose the most suitable package for the project's needs. \n    - If the already installed package aligns well with the project's requirements and scores high in the evaluation, using it is the most efficient approach. \n    - Otherwise, select the best alternative and replace the existing package.\n\n3. **Documentation:** Record the decision, including the chosen package and the rationale behind it, in an Architectural Decision Record (ADR). This will provide valuable context for future developers and justify the choice. \n\nBy following this process, we can ensure we're using the most suitable auto-update solution for our Electron app while minimizing potential issues in the future. \n","GenTime":"2024-08-28 13:26:15"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"text":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n\n##Decision\nWe will deploy the Broker as an AWS Lambda\\n","MatchedIds":[4358,3290,2471,4388,3291],"Prediction":"## Decision\n\nWe will deploy the Broker to AWS Lambda as well. \n\nHere's why:\n\n* **Consistency:** Since the Adapter is already deployed as a Lambda function, deploying the Broker to the same environment keeps things consistent and simplifies our infrastructure. \n* **Go Support:** AWS Lambda provides excellent support for Go, making it straightforward to deploy and manage both the Adapter and the Broker. \n* **Scalability and Cost:**  Lambda offers automatic scaling and a pay-as-you-go pricing model, making it cost-effective for handling variable workloads.\n* **Integration:** Deploying the Broker to AWS allows for easy integration with existing monitoring and logging tools already in use for the Adapter and CSLS infrastructure.\n\n**Pipeline:** We will create a dedicated CI\/CD pipeline for the Broker, similar to the Adapter's pipeline, to automate the build, test, and deployment processes to AWS Lambda. This ensures that updates to the Broker are deployed consistently and reliably.  \n","GenTime":"2024-08-28 13:26:20"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"text":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n\n##Decision\nCommand-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","MatchedIds":[899,866,1758,4741,674],"Prediction":"## Decision\n\nWe will fully adopt the command-query separation principle for newly written code in the `menu-generation` application. \n\nThis decision is driven by the following factors:\n\n* **Increased confidence in side-effect-free code:**  Queries will be guaranteed to not modify the application state, making them easier to test and reason about.\n* **Improved maintainability:** Clear separation of concerns will make the codebase more understandable and less error-prone during future development.\n* **Alignment with existing terminology:**  Since command and query terminology is already in use, formally adopting the principle will bring consistency and clarity.\n\nFor existing code, we will refactor towards command-query separation gradually as opportunities arise during regular development. \n","GenTime":"2024-08-28 13:26:25"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"text":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n\n\n##Decision\nBalances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","MatchedIds":[812,1442,21,839,842],"Prediction":"## Decision\n\nTo mitigate the DoS vulnerability of unbounded denomination growth within accounts while preventing griefing attacks, we introduce the concept of **\"recognized denominations\"** and a dedicated **Denomination Metadata module**.\n\n**Recognized Denominations:**\n\n- Each chain will maintain a registry of \"recognized denominations\" within the new Denomination Metadata module. \n- Recognized denominations represent a curated list of coins deemed desirable for use within the chain. \n- Recognition can be granted through governance or other on-chain mechanisms.\n\n**Denomination Metadata Module:**\n\n- This module will store metadata associated with both recognized and unrecognized denominations, including:\n    - Denomination information (display name, symbol, etc.)\n    - Recognition status\n    - Potentially, other relevant data like IBC counterparty information\n\n**Account Balances:**\n\n- Accounts will continue to store balances for all denominations, but with a key difference:\n    - Balances for **recognized denominations** are stored directly within the account object.\n    - Balances for **unrecognized denominations** are moved to a separate store, keyed by the account address and denomination.\n\n**IBC Transfers:**\n\n- Incoming IBC transfers with unrecognized denominations will be allowed, but:\n    - The received coins will be placed in the separate store for unrecognized denominations.\n    - An event will be emitted, notifying the recipient of the unrecognized denomination deposit.\n\n**User Interaction:**\n\n- Users can interact with unrecognized denominations through a dedicated module interface, potentially allowing them to:\n    - View balances of unrecognized denominations.\n    - \"Recognize\" a denomination through governance or other mechanisms.\n    - Send unrecognized denominations to other chains via IBC (subject to receiving chain's policy).\n\n**Benefits:**\n\n- **DoS Protection:** By storing unrecognized denominations separately, we prevent bloating the account object with potentially numerous, undesirable denominations.\n- **Griefing Prevention:**  Users can still receive all denominations, preventing attackers from blocking legitimate transfers.\n- **Flexibility:** Chains can define their own recognition criteria, accommodating various use cases.\n\n**Further Considerations:**\n\n- The exact mechanisms for recognizing denominations (governance, token curated registries, etc.) require further discussion and should be tailored to specific chain needs.\n- Gas costs for interacting with unrecognized denominations should be carefully considered to prevent economic DoS vectors. \n- UX considerations for managing unrecognized denominations are important to ensure a user-friendly experience.\n\nThis approach balances security, flexibility, and usability, paving the way for permissionless IBC while mitigating potential vulnerabilities. \n","GenTime":"2024-08-28 13:26:38"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"text":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n\n\n##Decision\nWe will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","MatchedIds":[279,2791,4821,351,1739],"Prediction":"## Decision\n\nWe will move away from relying solely on Heroku's scheduler for the static site builds. Instead, we will implement a separate build queue system:\n\n1. **Decouple the Build Trigger:** Instead of directly triggering a build every 10 minutes, we will use a simple cron job or a scheduled task that adds a \"build\" job to a dedicated queue. This queue can be implemented using a service like AWS SQS, RabbitMQ, or a similar technology.\n\n2. **Asynchronous Build Processing:** A dedicated worker process (or multiple worker processes for redundancy and scalability) will be responsible for consuming jobs from the queue and executing the static site build. Since the workers are independent of the scheduling mechanism, they can run for as long as necessary to complete the build process without being terminated prematurely. \n\n3. **Build Status Monitoring (Optional):** For improved visibility, we can implement a mechanism to track and report the status of builds. This could involve updating a database record, sending notifications upon completion\/failure, or using a dedicated monitoring service.\n\nThis approach offers several benefits:\n\n* **Reliability:** Builds are no longer constrained by the Heroku scheduler's time limits, ensuring they have sufficient time to complete.\n* **Scalability:** The worker-queue model allows for scaling the build process by adding more workers to handle increased build frequency or reduce build times. \n* **Flexibility:** This decoupled system provides greater flexibility for future modifications or additions to the build process without affecting other scheduled tasks. \n","GenTime":"2024-08-28 13:26:46"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"text":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n\n\n##Decision\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","MatchedIds":[21,810,816,1442,885],"Prediction":"## Decision\n\n### Typed Events\n\n1. **Event Type Definition**: Each module SHOULD define its events as custom types within its `types` package. These types SHOULD encapsulate the relevant event data in a structured manner, moving away from raw strings.\n\n   ```go\n   \/\/ Example from x\/bank\/types\/events.go\n   package types\n\n   type EventType string\n\n   const (\n       EventTypeTransfer = \"transfer\"\n   )\n\n   type EventTransfer struct {\n       Sender    sdk.AccAddress\n       Recipient sdk.AccAddress\n       Amount    sdk.Coins\n   }\n   ```\n\n2. **Event Emitter Interface**: The SDK SHOULD provide an `EventEmitter` interface that modules will use to emit events. This interface SHOULD handle the serialization of the typed events into the appropriate format for the underlying event system.\n\n   ```go\n   \/\/ Example EventEmitter interface\n   type EventEmitter interface {\n       EmitEvent(ctx sdk.Context, event Event) error\n   }\n   ```\n\n3. **Integration with Existing Event System**: The SDK's existing event system (typically using ABCI `Tags`) SHOULD be updated to work seamlessly with the `EventEmitter` and typed events. The `EventEmitter` SHOULD handle the conversion of the typed event data into the appropriate format for the `Tags`.\n\n### Event Consumption\n\n1. **Event Subscriber**: The SDK SHOULD offer utility functions or a dedicated package to subscribe to and filter events. This could involve registering callbacks for specific event types or querying historical events based on various criteria (e.g., block height, module, event type).\n\n2. **Deserialization and Handling**: Consumers subscribing to events SHOULD have access to functions that automatically deserialize the raw event data into the corresponding typed event structs. This eliminates the need for manual string parsing and makes event handling type-safe.\n\n### Example Usage:\n\n```go\n\/\/ Bank module emits an EventTransfer\ntransferEvent := types.EventTransfer{\n    Sender:    senderAddr,\n    Recipient: recipientAddr,\n    Amount:    coins,\n}\n\n\/\/ Assuming bankKeeper implements EventEmitter\nbankKeeper.EmitEvent(ctx, transferEvent)\n\n\/\/ Client subscribes to EventTransfer\neventSubscriber.Subscribe(types.EventTypeTransfer, func(event types.EventTransfer) {\n    \/\/ Handle transfer event\n    fmt.Printf(\"Transfer occurred: %s sent %s to %s\\n\", event.Sender, event.Amount, event.Recipient)\n})\n```\n\n### Advantages:\n\n* **Type Safety**: Typed events provide compile-time checks, reducing errors and making code more robust.\n* **Improved Developer Experience**:  Easier to understand, emit, and consume events.\n* **Standardization**:  Promotes consistency across modules.\n* **Facilitates Event-Driven Applications**: Makes it simpler to build applications that rely on real-time on-chain data.\n\nThis approach provides a clear migration path for the Cosmos SDK to adopt typed events, improving the developer experience and paving the way for more sophisticated event-driven applications within the Cosmos ecosystem. \n","GenTime":"2024-08-28 13:27:01"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"text":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n\n\n##Decision\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","MatchedIds":[1924,2975,3863,1035,4852],"Prediction":"## Decision\n\n**Migrate from apache-commons-logging to SLF4J for logging.**\n\n**Rationale:**\n\n* **Java 9 Compatibility:** SLF4J offers out-of-the-box compatibility with Java 9, addressing the immediate compatibility issue with apache-commons-logging 1.2.\n* **Flexibility:** By using SLF4J, we avoid direct dependency on log4j. This allows us to switch to a different logging framework in the future with minimal code changes if required.\n* **Reduced Dependencies:** As SLF4J is already a transitive dependency, migrating to it directly simplifies our dependency management and potentially reduces the final application size.\n* **Existing Usage:**  Given that log4j is already a dependency and SLF4J is present through a third-party library, the migration effort is significantly reduced. \n\n**Implementation:**\n\n1. **Replace direct usages of apache-commons-logging with SLF4J** using the appropriate facade classes.\n2. **Remove the apache-commons-logging dependency.**\n3. **Configure SLF4J to use log4j as the backend logging implementation.** This can usually be achieved through a simple configuration file.\n\n**Benefits:**\n\n* Ensures compatibility with Java 9 and future versions.\n* Improves maintainability by using a widely adopted logging facade.\n* Provides flexibility for potential future changes in logging frameworks. \n","GenTime":"2024-08-28 13:27:08"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"text":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n\n\n##Decision\nWe'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","MatchedIds":[4498,2270,425,5185,2703],"Prediction":"## Decision\n\nTo simplify theme management and minimize the impact of renaming, we will **not use namespaces for themes**. \n\nTo address potential naming collisions with child themes, we will implement the following:\n\n* **Enforce unique theme names:**  When a new theme is created, the system will enforce a unique name across both parent and child themes. This prevents accidental overwrites or conflicts.\n* **Child themes as directory structure:** Instead of relying on namespaces, child themes will be managed through a directory structure within the active parent theme. For example:\n\n   ```\n   themes\/\n     - ParentTheme\/\n       - childTheme1\/\n       - childTheme2\/\n   ```\n* **Clear naming conventions:** Encourage clear and descriptive names for both parent and child themes to avoid confusion. Documentation will provide guidance on recommended naming practices.\n\nThis approach prioritizes ease of use and reduces friction when working with themes, while still mitigating potential naming issues. \n","GenTime":"2024-08-28 13:27:14"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\\nWe need to choose how we will deploy our app.\\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"text":"## Context\\nWe need to choose how we will deploy our app.\\n\n\n##Decision\nWe use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","MatchedIds":[4358,1089,3657,1243,718],"Prediction":"## Decision\n\nWe will use a containerized deployment strategy using Docker. This offers several advantages:\n\n* **Environment Consistency:** Ensures our app runs the same way in development, testing, and production, minimizing \"works on my machine\" issues.\n* **Portability:**  We gain flexibility to deploy on various cloud providers or on-premise servers without being tied to a specific platform.\n* **Scalability:** Container orchestration tools like Kubernetes can easily scale our app horizontally to handle traffic fluctuations.\n\n\ud83d\udc49 **Note:** We will explore managed Kubernetes offerings like Google Kubernetes Engine (GKE) or Amazon Elastic Kubernetes Service (EKS) for simplified cluster management and auto-scaling. We can also consider Heroku or other PaaS solutions if they better suit our needs for ease of deployment and cost-effectiveness. \n","GenTime":"2024-08-28 13:27:19"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"text":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n\n\n##Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","MatchedIds":[2261,138,160,4699,4428],"Prediction":"## Decision\n\nWe will use Markdown as the plain text format for storing decision records. \n\n### Rationale\n\n* **Version Control Friendliness:** Markdown is plain text, ensuring compatibility with version control systems like Git. Changes are easily trackable and viewable.\n* **Tooling and Automation:**  The simple structure of Markdown allows for tools to easily parse the content, modify the status of records, and automatically manage hyperlinks when decisions are superseded.\n* **Readability and Formatting:** Markdown offers a good balance between readability in raw format and the ability to apply basic formatting like lists, code blocks, and headers. This satisfies the need for both technical clarity and human readability.\n* **Conversion to Richer Formats:** Markdown can be easily converted to HTML or PDF for printing or viewing in a more visually appealing manner. This addresses the requirement for a printable and readable output.\n\n### Considered Alternatives\n\n* **Plain Text (ReStructuredText, AsciiDoc):** While viable, they often have a steeper learning curve and might not be as widely supported by various tooling as Markdown.\n* **WYSIWYG Editors:**  While user-friendly, these often store data in a format not conducive to version control or automated manipulation.\n\nThis choice balances the technical requirements of versioning and tooling with the need for human-readable and presentable documentation. \n","GenTime":"2024-08-28 13:27:26"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"text":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n\n##Decision\nUse one S3 bucket per data source.\\n","MatchedIds":[4388,3797,3987,4025,3831],"Prediction":"## Decision\n\nTo accommodate varying levels of control over data source environments and support source-specific processing, we will adopt a two-pronged approach:\n\n**1. Bucket Prefixing:**\n\n* Each data source will be assigned a unique prefix to be used when uploading files to the S3 bucket.\n* This prefix will serve as the primary identifier for the data source. \n* For example, data from source \"A\" could be uploaded to `s3:\/\/<bucket>\/source_a\/<filename>`, while data from source \"B\" would be uploaded to `s3:\/\/<bucket>\/source_b\/<filename>`.\n* Even with limited control over the sending environment, we can usually dictate a specific path within a bucket for uploads.\n\n**2. Lambda Event Filtering and Routing:**\n\n* S3 event notifications will be configured at the bucket level to trigger Lambda functions.\n* Lambda functions will be designed to handle events from specific prefixes. This can be achieved by:\n    * **Filtering events based on the `s3.object.key` prefix within the Lambda function's event handler.** This provides flexibility if a single Lambda function handles multiple sources.\n    * **Configuring S3 event notifications to trigger different Lambda functions based on prefix routing.** This provides better isolation and scalability. \n* By filtering events based on prefixes, we ensure that each Lambda function receives only the data it's intended to process.\n\n**Benefits of this approach:**\n\n* **Simplicity:**  Easy to implement and manage, even with limited control over data source environments.\n* **Flexibility:** Accommodates different data sources and processing needs.\n* **Scalability:** Allows for independent scaling of processing for each data source.\n* **Cost-effective:**  Avoids the need for dedicated buckets per data source. \n\n**Additional Considerations:**\n\n* For scenarios where prefixing is impossible, we can explore alternative identification methods like reading metadata from the file uploaded or using a secondary notification system alongside S3 events.\n* Security policies will be implemented to ensure only authorized entities can upload to specific prefixes. \n","GenTime":"2024-08-28 13:27:37"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"text":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n\n##Decision\nWe will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","MatchedIds":[2488,2221,847,779,5009],"Prediction":"## Decision\n\nWe will implement autocomplete functionality using a debounced search approach. As the user types, suggestions will be fetched from the server and displayed in a dropdown list. \n\nHere is a more detailed breakdown:\n\n**Technical Implementation:**\n\n- **Debouncing:**  We'll use a debounce function to limit the rate of requests sent to the server. This prevents excessive requests as the user types.\n- **Server-Side Search:** A dedicated endpoint on the backend will handle search queries and return relevant suggestions. The specific search logic will depend on the data and desired search behavior.\n- **Frontend Library:**  A JavaScript library like `react-autosuggest` or `downshift` will be used for handling user input, displaying suggestions, and managing the interaction with the dropdown list.\n- **Styling:** The dropdown list will be styled to integrate seamlessly with the overall application design.\n\n**Alternatives Considered:**\n\n- **Client-Side Search:** For smaller datasets, performing the search entirely on the client-side is an option. However, this may not be suitable for large datasets or complex search logic.\n- **Third-Party Libraries:** While we can utilize existing autocomplete libraries, we will prioritize building a custom solution for better control over the search behavior and styling.\n\n**Additional Considerations:**\n\n- **Accessibility:** We'll ensure the autocomplete component is accessible to users with disabilities, including keyboard navigation and screen reader compatibility. \n- **Performance Optimization:** Techniques like caching and efficient indexing on the server-side will be considered to optimize search speed and responsiveness. \n","GenTime":"2024-08-28 13:27:46"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"text":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n\n##Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","MatchedIds":[5082,1400,472,3715,3718],"Prediction":"## Decision\nWe will use **Windows Presentation Foundation (WPF)**  with **C#** for the touch table application. \n\n## Rationalization\n\n* **Multi-touch Support & UI Demands**: WPF has robust support for multi-touch gestures and a flexible UI system capable of handling complex designs and interactions.\n* **Windows Compatibility**: As a Microsoft technology, WPF integrates seamlessly with the Windows ecosystem, ensuring compatibility with our touch table device.\n* **Team Familiarity & Resources**: While not our primary language, the transition to C# is manageable, and we have existing knowledge and resources available for this language. WPF's use in projects like DeepTree provides a reference point. \n* **Community & Documentation**:  C# and WPF have extensive documentation and an active .NET community, providing ample resources for troubleshooting and support.\n\n## Considerations\n\n* **WPF's Future**: While WPF is a mature technology, it's essential to acknowledge concerns about its long-term future given its perceived decline in popularity and the deprecation of related technologies. \n* **Exploration of Alternatives**: We should allocate time to explore newer technologies and frameworks in the future, which might offer advantages in terms of performance, maintainability, or community support.\n\nThis decision balances our current needs and expertise with a pragmatic approach to future considerations. While acknowledging potential long-term concerns, we believe WPF provides the best path forward for this project. \n","GenTime":"2024-08-28 13:27:53"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"text":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n\n##Decision\nWe will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","MatchedIds":[4637,4909,3469,4747,311],"Prediction":"## Decision\nChosen option: **Data Pipeline with Summary Tables**\n\n### Reason\nWhile materialized views have served well to a point, a dedicated data pipeline with summary tables offers better scalability, maintainability, and flexibility for evolving reporting needs. \n\n### Positive Consequences\n* **Improved Performance & Scalability:** Dedicated data pipelines can handle large data volumes more efficiently, leading to faster report generation even with growing datasets.\n* **Flexibility:** Summary tables can be easily tailored to store various aggregations (daily, monthly, quarterly) and can be queried in flexible ways to accommodate new report requirements without impacting transactional systems.\n* **Simplified Maintenance:**  Managing updates and changes within a structured data pipeline is often easier than complex materialized view dependencies.\n\n### Negative Consequences\n* **Increased Complexity:**  Implementing a data pipeline introduces additional components and infrastructure compared to the materialized view approach.\n* **Development Time:** Initial setup and development of the data pipeline will require dedicated effort.\n\n### Implementation Considerations\n* **Technology Choices:** Evaluate suitable technologies for building the data pipeline (e.g., Apache Spark, Apache Flink) and storing the summary tables (columnar databases like Apache Cassandra or ClickHouse might be beneficial for analytical workloads).\n* **Data Refresh Frequency:** Determine the appropriate refresh rate for summary tables based on reporting needs and data update frequency. Consider incremental updates for efficiency.\n* **Monitoring & Alerting:**  Establish monitoring and alerting for the data pipeline to ensure data accuracy and pipeline health.\n\nThis approach provides a robust and scalable solution for the growing reporting and analytical needs, offering long-term benefits in performance and adaptability. \n","GenTime":"2024-08-28 13:28:01"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\\nVellum node fails\\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"text":"## Context\\nVellum node fails\\n\n\n##Decision\nVellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","MatchedIds":[3841,2968,3504,2121,405],"Prediction":"## Decision\n\nWithout more information on the specific failure of the Vellum node, it's impossible to provide a single, decisive solution. Instead, I'll outline a general approach and some potential decisions based on common failure scenarios:\n\n**General Approach:**\n\n1. **Gather Information:** \n    * What specific error messages are associated with the failure? \n    * When did the failure start occurring? Were any changes made to the system around that time? \n    * Is the failure consistent, or intermittent?\n    * Which functionalities are impacted by the failing Vellum node?\n2. **Isolate the Issue:** \n    * Can you reproduce the failure in a controlled environment (e.g., development or staging)?\n    *  Identify if the issue is related to:\n        * Vellum node configuration (e.g., insufficient resources, misconfigured settings)\n        * Dependencies (e.g., network connectivity, database access)\n        * Code defects within the Vellum node itself\n3. **Implement & Verify Solution:**  \n    * Choose the most appropriate solution based on the root cause. \n    * Thoroughly test the implemented solution to ensure it resolves the failure and doesn't introduce new issues.\n\n**Potential Decisions based on Common Scenarios:**\n\n* **Resource Exhaustion (CPU, Memory, Disk):**\n    * **Decision:** Increase the allocated resources to the Vellum node. This might involve vertical scaling (upgrading the node's hardware) or horizontal scaling (adding more nodes if the workload can be distributed).\n* **Network Connectivity Problems:**\n    * **Decision:** Investigate and resolve network issues between the Vellum node and other relevant services. This may involve checking firewall rules, DNS configurations, or network latency.\n* **Data Corruption or Loss:**\n    * **Decision:** Restore data from a known good backup if available.  Implement data integrity checks and robust backup\/recovery procedures to prevent future occurrences. \n* **Software Bug:**\n    * **Decision:**  \n        * If the bug is identified in a specific Vellum version, upgrading or downgrading to a stable version might resolve the issue.\n        * For open-source projects, consider contributing a fix upstream. Otherwise, contact the vendor or development team for support.\n\n**Remember:** This is a general guide.  Providing a more precise and helpful decision requires a clearer understanding of the specific Vellum node failure. \n","GenTime":"2024-08-28 13:28:13"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"text":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n\n\n##Decision\nN\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","MatchedIds":[3187,697,1631,1202,3784],"Prediction":"## Decision\n\nComponents will be published from the DPE Client repository.  The Storybook repository will consume these components from NPM. The Storybook will not live inside the Client repo; it will remain a separate entity.\n\n### Rationale\n\n- **Publishing from DPE Client:** This leverages the existing work and avoids code duplication. It ensures the Storybook always showcases components in their current, published state.\n- **Storybook as a Separate Entity:** This keeps concerns separated. The Storybook acts as a documentation and testing tool, while the DPE Client repo focuses on component development. It also allows for independent versioning and development cycles if needed.\n\n### Potential Issues\/Considerations\n\n- **Versioning Alignment:**  Ensure that the Storybook always utilizes the correct version of components published from the DPE Client repo. Implementing a clear versioning strategy (e.g., semantic versioning) and using tools for dependency management will be crucial. \n- **Build Process:**  The Storybook build process should fetch the components from NPM, potentially requiring additional configuration.\n\n### Next Steps\n\n1. **Formalize versioning strategy:** Define how both repositories will handle versioning to maintain compatibility.\n2. **Update Storybook configuration:** Configure the Storybook project to consume components from the DPE Client's NPM package.\n3. **Automate Storybook deployment:** Consider automating the deployment of the Storybook whenever new component versions are released to ensure it stays up-to-date. \n","GenTime":"2024-08-28 13:28:22"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","MatchedIds":[1477,3062,1243,3470,4761],"Prediction":"## Decision\n\nWe will use **Ruby on Rails** as our web framework. \n\nHere's why:\n\n* **Developer Productivity:** Ruby on Rails is renowned for its focus on convention over configuration and its extensive use of pre-built components (gems). This allows developers to rapidly build web applications without getting bogged down in boilerplate code, aligning with our goal of minimizing effort on non-domain specific problems.\n* **Abstraction and Elegance:** Rails provides a high level of abstraction over lower-level web development complexities like routing, database interactions, and security. This allows developers to focus on implementing business logic rather than reinventing the wheel.\n* **Large and Active Community:** The Rails community is known for being large, active, and supportive. This translates to a wealth of documentation, tutorials, and readily available solutions to common problems.\n\nBy choosing Ruby on Rails, we can leverage a mature and well-supported framework to accelerate development and focus on delivering business value. \n","GenTime":"2024-08-28 13:28:28"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"text":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n\n##Decision\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","MatchedIds":[1857,3469,2526,3187,2517],"Prediction":"## Decision\n\nWe will introduce a middleware layer between the version control system (e.g. Git) and the analytics platform. This middleware will be responsible for:\n\n1. **Receiving webhooks** on code changes from the version control system.\n2. **Extracting** the task ID (e.g., ticket number) from the commit message using a configurable regular expression.\n3. **Querying** the task tracker API for the task details using the extracted task ID.\n4. **Enriching** the change data with relevant fields from the task (e.g., type - Feature Request, Bug, etc.).\n5. **Sending** the enriched change data to the analytics platform.\n\n### Positive Consequences \n\n* **Simple implementation:** The middleware can be implemented as a lightweight service with clear responsibilities.\n* **No assumption about the task tracker:**  The middleware interacts with the task tracker through its API, making it agnostic to the specific tool used.\n* **Support of a set of generic fields related to a task:** The middleware can be configured to extract and use any field available through the task tracker API.\n* **Flexibility:** This approach allows for future extension to support additional data sources and metrics.\n\n### Negative consequences \n\n* **Maintenance overhead:** The middleware component needs to be maintained and kept up to date with any changes in the version control system or the task tracker API.\n* **Potential performance bottleneck:** The middleware adds an additional hop in the data flow, which could potentially impact performance. \n\n### Additional Considerations\n\n* **Error handling and retries:** Implement robust error handling and retry mechanisms for API calls and data processing.\n* **Data consistency:** Ensure data consistency between the version control system, the task tracker, and the analytics platform.\n* **Security:** Securely store and manage any API credentials required for accessing the task tracker.\n","GenTime":"2024-08-28 13:28:38"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"text":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n\n\n##Decision\nWe feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","MatchedIds":[2523,3987,2122,4533,3831],"Prediction":"## Decision\n\nChosen option: **4. Email a link to the form owner allowing the ability to download all attachments in one go.**\n\n### Rationale\n\nThis option addresses the most significant concerns:\n\n* **Reduces email clutter:** Instead of multiple emails, form owners receive a single notification with a download link.\n* **Bypasses the 10MB email limit:**  Downloading from a link eliminates the size constraint imposed by AWS SES.\n* **Improves user experience:** Provides a more streamlined and efficient way for form owners to access submissions.\n\n### Positive Consequences\n\n* Improved efficiency for form owners.\n* Less potential for errors caused by email overload.\n* Scalable solution that accommodates forms with numerous or large attachments.\n\n### Negative Consequences\n\n* Requires development effort to implement the download link functionality.\n* Potential security considerations depending on the implementation of the download link and file storage.\n\n### Rejected Options and Considerations:\n\n1. **Attach as many files as possible to a single email up to the 10MB limit:** This still risks exceeding the limit for submissions with multiple or large files.\n\n2. **Hard cap limit on 10MB in total across all the files uploaded per form:** This negatively impacts users with legitimate needs to submit larger files and might not be a feasible long-term solution.\n\n3. **Admin interface for form owners to log in and retrieve file uploads per submission:** While a valid option, this might be a heavier implementation compared to a simple download link and defers the notification aspect to a separate mechanism. \n\n### Further Considerations:\n\n* Security measures around the download link (e.g., time-limited access, authentication) should be explored.\n* User experience considerations for the download process itself (e.g., progress indication, error handling) should be addressed.\n","GenTime":"2024-08-28 13:28:48"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"text":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n\n\n##Decision\nFor now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","MatchedIds":[2543,2791,4385,3416,2538],"Prediction":"## Decision\n\nFor simplicity, we will use a **single partition key** for the DynamoDB table at this time. \n\nWhile a composite key with StreamName and Event Number offers performance and organizational advantages in a production setting, the added complexity of managing event numbers is not justified at this stage. \n\nThe focus currently is on proving functionality and establishing a baseline. Once the core features are validated and performance needs are better understood, we can revisit the primary key structure and introduce a composite key if necessary.\n\nThis approach prioritizes simplicity and speed of development without precluding future optimization. \n","GenTime":"2024-08-28 13:28:52"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"text":"## Context\\nWe need a way to deploy our application to AWS.\\n\n\n##Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","MatchedIds":[1239,2471,4096,4937,4025],"Prediction":"## Decision\n\nWe will use **AWS CloudFormation** to deploy our application to AWS. \n\nHere's why:\n\n* **Infrastructure as Code:** CloudFormation allows us to define our infrastructure (servers, databases, networking, etc.) in code (JSON or YAML templates). This ensures consistent deployments, easy version control, and reproducible environments.\n* **Automation:** CloudFormation automates the provisioning and configuration of resources. This eliminates manual errors and speeds up the deployment process.\n* **Integration with AWS Ecosystem:** CloudFormation integrates seamlessly with other AWS services, allowing us to leverage the full potential of the AWS ecosystem.\n* **Repeatable Deployments:** CloudFormation enables us to create reusable templates for different environments (development, staging, production), ensuring consistency across deployments.\n\n**Next Steps:**\n\n1. **Define Infrastructure:** Define the desired infrastructure components (e.g., EC2 instances, S3 buckets, load balancers) using a CloudFormation template.\n2. **Package Application:** Package the application in a deployment-ready format (e.g., Docker image, ZIP archive).\n3. **Configure Deployment Pipeline:** Set up a continuous integration and deployment (CI\/CD) pipeline to automate the build, testing, and deployment of the application using CloudFormation.\n\nThis approach provides a robust, automated, and scalable solution for deploying applications to AWS. \n","GenTime":"2024-08-28 13:28:59"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"text":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n\n\n##Decision\nUse helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","MatchedIds":[1376,3109,620,4096,187],"Prediction":"## Decision\n\nWe will adopt Helm to manage our Kubernetes deployments. This allows us to:\n\n* **Reduce code duplication:** Define our deployments once as Helm Charts, reusing them across different environments (gcp iowa-a, aws frankfurt) with environment-specific values. \n* **Improve consistency:**  Using the same deployment templates across environments minimizes configuration drift and ensures predictable behavior.\n* **Simplify deployment process:** Manage deployments through a single tool (`helm`) rather than numerous `kubectl` commands.\n\n**Implementation Details:**\n\n1. **Create Helm Charts:** We will migrate existing `yaml` definitions into reusable Helm Charts for each of our primary services (bedrock\/www, snippets, basket). \n2. **Parameterize Environment Variables:**  Within the charts, we'll use Helm's templating engine to parameterize environment-specific variables (e.g., resource limits, ingress settings)\n3. **Define Environment-Specific Values:** Create separate value files (e.g., values-gcp-iowa-a.yaml, values-aws-frankfurt.yaml) containing the appropriate configuration for each environment.\n4. **Utilize Helm for Deployments:** Deployments will be handled via `helm install` or `helm upgrade` commands, specifying the relevant chart and values file for the target environment.\n\n**Benefits:**\n\n* **Improved code maintainability and readability.**\n* **Reduced risk of errors due to configuration inconsistencies.** \n* **Faster and simpler deployments across environments.**\n\n**Potential Challenges:**\n\n* **Initial investment in learning Helm and migrating existing deployments.**\n* **Potential complexity for managing a large number of charts and values files.**\n\n**Mitigations:**\n\n* **Provide training and documentation for team members on Helm usage.**\n* **Establish clear naming conventions and folder structures for charts and value files.**\n* **Explore Helm tooling for simplifying management of complex deployments.**\n\nThis decision will be reviewed periodically to assess its effectiveness in reducing configuration drift and improving deployment efficiency. \n","GenTime":"2024-08-28 13:29:10"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"text":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n\n##Decision\nBloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","MatchedIds":[1196,1545,1480,4747,3784],"Prediction":"## Decision\n\nAfter careful consideration of the factors listed, we decided to utilize Bloomreach's native 'Channel' concept for managing the various sites on the platform. \n\n**Rationale:**\n\n* **Ease of Use and Component Sharing:**  The 'Channel' concept within Bloomreach provides a structured and intuitive way to manage multiple sites within a single platform. It offers built-in mechanisms for component sharing, simplifying the creation of new sites and ensuring consistency across the platform. \n* **Content Segregation and Sharing:**  Bloomreach's 'Channel' allows for clear segregation of content at the site level, preventing unintentional cross-site content mixing. Simultaneously, it offers mechanisms to share content selectively across channels where appropriate, promoting reusability and efficiency.\n* **Permissions Model:** Bloomreach provides a robust and granular permissions model within the 'Channel' structure. This allows us to define specific roles and access levels for different user groups, ensuring content security and appropriate editorial control for each site.\n\n**Document Management:** \n\nRegarding document management, we decided **against** integrating an external platform at this time.  Bloomreach's built-in asset management capabilities are sufficient for the initial MVP needs.  We will revisit the need for a dedicated document management platform as the platform scales and requirements evolve.\n\nThis approach offers a balance between feature richness, ease of use, and control, aligning well with the project's current needs and future scalability. \n","GenTime":"2024-08-28 13:29:18"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"text":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[3784,2261,302,4699,4428],"Prediction":"## Decision\nWe will use Architecture Decision Records (ADRs) to document the architectural decisions made on this project. The ADRs will follow a lightweight template similar to [Michael Nygard's proposal](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) and will be stored alongside the codebase in a version-controlled manner. This approach will help maintain a clear and accessible history of the decision-making process for the support platform, addressing the need for transparency and knowledge sharing.\n","GenTime":"2024-08-28 13:29:22"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"text":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n\n##Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","MatchedIds":[1416,1418,2262,1854,994],"Prediction":"## Decision\n\nWe will establish and maintain a unified glossary of terms and their definitions to be used across the entire codebase. This glossary will be easily accessible to all developers and will be reviewed and updated regularly. \n\nWe will encourage the use of code linters and\/or static analysis tools that can help identify potential naming inconsistencies and enforce the use of the agreed-upon terminology.\n","GenTime":"2024-08-28 13:29:26"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"text":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n\n##Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","MatchedIds":[4533,2520,5157,4747,3831],"Prediction":"## Decision\n\nA new custom entity will be created within Dynamics CRM to store the file reference attribute, linked to the relevant case\/incident entities using the native Dynamics CRM relationship model. \n\n### Justification:\n\nThis approach fulfills both decision drivers:\n\n* **Accessibility & Filterability**: Creating a custom entity allows for seamless integration with the Dynamics CRM WebAPI. This ensures that the frontend application can easily access, filter, and display the file references. \n* **Dynamics UI Integration**: The custom entity can be easily surfaced within the Dynamics UI, enabling administrators to view linked files directly. Dynamics CRM provides built-in functionality to display web resources and custom buttons within the entity forms, allowing us to present links to Azure files without significant customization. \n\n### Implementation Details:\n\n1. **Custom Entity:** Create a new custom entity named \"Document Upload\" with the following attributes:\n    *  **File Name:** (Text) Stores the original file name.\n    *  **Azure Blob URL:** (Text) Stores the full URL of the uploaded file in Azure Blob storage.\n    *  **Case\/Incident Lookup:** (Lookup) Creates a relationship with the relevant case\/incident entity.\n2. **Relationship:** Establish a 1:N relationship between the Case\/Incident entity and the Document Upload entity. This allows multiple documents to be associated with a single case\/incident.\n3. **Dynamics UI Integration:** Add a sub-grid to the Case\/Incident form to display related \"Document Uploads\". Optionally, include a custom button that dynamically constructs and opens the Azure Blob URL in a new tab.\n4. **Frontend Integration:**  Utilize the Dynamics CRM WebAPI to retrieve and display document information and links within the frontend application.\n\n### Advantages:\n\n* Leverages native Dynamics CRM functionality.\n* Maintains data integrity and consistency.\n* Enables efficient data management and retrieval. \n\n### Considerations:\n\n* Data storage within Dynamics CRM may incur additional storage costs. Monitor storage usage and consider data archiving strategies if necessary. \n","GenTime":"2024-08-28 13:29:37"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"text":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n\n##Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","MatchedIds":[447,445,446,443,1016],"Prediction":"## Decision\n\n**Implement a two-step approach to address the data extraction and validation challenges:**\n\n1. **Data Extraction with Metadata:**\n    - Develop a scraping mechanism to extract data from the Koshvani platform along with available metadata. This metadata should include:\n        - Treasury\/District information\n        - Data updation status timestamps provided by the platform \n        - Timestamp of data extraction\n    - Store the extracted data and metadata together to maintain data lineage.\n\n2. **Data Validation and Reconciliation:**\n    - Implement a validation process that cross-references the extracted data timestamps with the platform's daily update status for each treasury.\n    - Develop a reconciliation strategy to handle discrepancies between data timestamps and update statuses. This may involve:\n        - Prioritizing data from treasuries with the latest update timestamps.\n        - Implementing logic to identify and flag potential data inconsistencies for further investigation.\n        - Potentially triggering re-scraping of data for specific treasuries based on the validation results.\n\n**This approach allows for a robust data extraction process while acknowledging the limitations of the platform's temporal data availability. By incorporating metadata and implementing a thorough validation process, we can strive for the highest possible data accuracy.** \n","GenTime":"2024-08-28 13:29:44"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"text":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n\n##Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","MatchedIds":[856,733,3915,3104,631],"Prediction":"## Decision\n\nWe will move to a model where each namespace in the cluster gets its own ingress controller, and a corresponding load balancer. \n\n### Reasoning\n\nThis approach addresses the problems outlined above for the following reasons:\n\n* **Scalability:** Each ingress controller will be responsible for a smaller number of ingresses, resulting in smaller config files and reduced resource consumption. This allows the system to scale horizontally as more services are added.\n* **Isolation:** Issues with one namespace's ingress controller will not affect other namespaces, improving the overall stability and resilience of the platform. \n* **Resource Management:** Namespaces will have dedicated load balancers, eliminating resource contention and allowing for better performance isolation.\n\n### Implementation Details\n\n1. **Namespace-Specific Ingress Controllers:** We will modify our deployment process to automatically provision a dedicated ingress controller and load balancer for each new namespace. Existing namespaces will need to be migrated.\n2. **Automation:**  The provisioning and configuration of ingress controllers and load balancers will be automated, likely through our existing infrastructure-as-code setup.\n3. **Cost Management:**  While this approach increases the number of load balancers used, the cost is outweighed by the improved stability and scalability. We will investigate potential cost optimizations, such as using smaller load balancer tiers for low-traffic namespaces.\n\n### Positive Consequences\n\n* Improved scalability and performance of the ingress layer.\n* Enhanced isolation between namespaces, reducing the impact of failures.\n* Easier management and troubleshooting of ingress configurations.\n\n### Negative Consequences\n\n* Increased cost due to the use of multiple load balancers.\n* Added complexity to the deployment and management of ingress controllers.\n\n### Mitigation\n\n* We will carefully monitor the cost impact of this change and explore options for optimization.\n* Documentation and automation will be crucial to manage the increased complexity. We will update our documentation to reflect the new architecture and provide clear guidelines for service teams. \n","GenTime":"2024-08-28 13:29:55"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"text":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n\n\n##Decision\nTendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","MatchedIds":[1979,1956,1985,1949,4097],"Prediction":"## Decision\n\nTendermint will adopt [LibP2P](https:\/\/libp2p.io\/) as the basis for its networking layer, replacing the existing custom implementation. This will involve a breaking protocol change and will not maintain compatibility with previous versions of the Tendermint P2P protocol. \n\nThis decision is motivated by the following factors:\n\n* **Reduced Maintenance Burden:** LibP2P provides a robust, well-tested, and actively maintained P2P networking stack, allowing the Tendermint team to focus on core consensus and application logic.\n* **Improved Functionality and Performance:**  LibP2P offers a wider array of features, better performance, and increased flexibility compared to the existing implementation. \n* **Ecosystem Alignment:**  Adopting LibP2P aligns Tendermint with a broader ecosystem of tools, libraries, and best practices in the P2P space.\n\nThe transition to LibP2P will be treated as a major version change for Tendermint, providing a clear break from the legacy system and enabling a cleaner, more maintainable architecture. \n","GenTime":"2024-08-28 13:30:01"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"text":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n\n##Decision\n`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","MatchedIds":[1271,1031,1558,3692,1180],"Prediction":"## Decision\n\nWe will use a language-agnostic serialization protocol like Protocol Buffers or Apache Avro for serializing basic data types into a binary format. This approach offers several benefits:\n\n* **Efficiency:** Both Protocol Buffers and Avro are designed for efficient serialization and deserialization, resulting in compact binary representations and faster processing.\n* **Schema evolution:** These protocols support schema evolution, allowing for backward and forward compatibility as data types are modified over time. \n* **Language neutrality:** Both options are language-agnostic, enabling seamless interoperability between different components of the system that might be written in various programming languages.\n\nWe will choose the specific library (Protocol Buffers or Avro) based on a more detailed analysis of their performance characteristics and suitability for our specific use case. \n","GenTime":"2024-08-28 13:30:06"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"text":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n\n\n##Decision\nAxon Framework will be used for integration events implementation.\\n","MatchedIds":[2499,885,1035,649,2498],"Prediction":"## Decision\n\nWe will refactor the existing integration event implementation to utilize Axon Framework's messaging capabilities. This aligns with our architectural principle of leveraging Axon for DDD constructs and provides a clear path for potential future enhancements like event sourcing. \n\nThis decision provides several benefits:\n\n* **Consistency:** By using Axon for both integration events and DDD components, we ensure a consistent approach to event-driven communication within the application.\n* **Feature Richness:**  Axon offers advanced features for event handling, such as distributed message buses and sagas, which we can leverage in the future if needed. \n* **Event Sourcing Pathway:**  Migrating to Axon's event mechanism prepares us for a smoother transition to event sourcing if that requirement arises later.\n\n**Implementation Notes:**\n\n* Existing Spring-based event listeners will be replaced with Axon's event handlers. \n* Event publishing will be done through Axon's command gateway or event bus, depending on the specific use case.\n* Thorough testing will be conducted to ensure a seamless transition and maintain the existing functionality. \n","GenTime":"2024-08-28 13:30:13"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"text":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n\n##Decision\nWe have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","MatchedIds":[2539,2793,2538,4937,621],"Prediction":"## Decision\n\nMigrate away from using `cloud.conf` for provisioning these resources. Instead, we will manage these resources directly within Terraform. \n\nThis approach provides several benefits:\n\n* **Removes the 16kb configuration limit** imposed by `cloud.conf`.\n* **Eliminates configuration duplication** across different instance Terraform code, promoting maintainability and consistency.\n* **Enables faster feedback during testing** as changes are applied directly through the Terraform infrastructure as code workflow. \n\nWe will investigate existing Terraform modules or develop custom solutions to efficiently provision and manage these resources within our existing Terraform codebase.\n","GenTime":"2024-08-28 13:30:17"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"text":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n\n##Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","MatchedIds":[560,2207,1654,3526,2405],"Prediction":"## Decision\n\n**We will remove the requirement for users to log in to access the standalone contact-frontend pages.**  All traffic will be directed to the \"-unauthenticated\" routes, and the routes requiring login will be removed. \n\n### Justification\n\nThis decision is driven by the following factors:\n\n* **Improved User Experience:**  Requiring login for functionality that is equally available without login creates an unnecessary barrier for users, potentially leading to frustration and drop-off. This is particularly problematic in cases where users are unexpectedly signed out while attempting to use the service.\n* **Increased Accessibility and Feedback:**  Removing the login requirement makes it easier for all users to provide feedback, including those who may face additional accessibility challenges.  This broader reach will likely result in more comprehensive and valuable user feedback.\n* **Simplified Development and Maintenance:**  Maintaining two versions of the same page (authenticated and unauthenticated) adds unnecessary complexity to the codebase, slowing down development and testing processes.  Consolidating to a single code path simplifies maintenance and reduces the potential for inconsistencies. \n\n### Addressing Concerns\n\nWhile the decision to remove the login requirement offers clear advantages, it's important to acknowledge and address the concern regarding potential loss of user information for Deskpro agents:\n\n* **Loss of Enrolment Information:**  We acknowledge that removing the login requirement will result in a loss of enrolment information being passed to Deskpro agents when logged-in users access the unauthenticated routes. To mitigate this, we will investigate alternative methods for capturing and providing relevant user context to agents, such as:\n    * **Optional User Information Fields:** Providing optional fields for users to voluntarily share relevant information (e.g., user ID, service being used) when submitting feedback.\n    * **Enhancing Contextual Information:** Exploring ways to capture and pass along more contextual information from the user's session or referral source.\n\nBy addressing this concern, we can move forward with removing the login requirement to improve user experience, accessibility, and development efficiency, while still striving to provide Deskpro agents with sufficient context for effective support. \n","GenTime":"2024-08-28 13:30:31"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"text":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n\n\n##Decision\nIt's Lens time!\\n","MatchedIds":[2473,779,1495,1031,4021],"Prediction":"## Decision\nWe will use the `lens` library to manage our nested game state. This will allow us to:\n\n* **Simplify updates to nested data structures**: Lenses provide a composable way to access and modify deeply nested values without tedious pattern matching.\n* **Improve code readability**: Lens syntax can be more concise and expressive than manual record updates, making the code easier to understand and maintain. \n* **Enable modularity with \"classy lenses\"**: By defining lenses for specific subsystems, we can isolate their dependencies and allow them to work with only the relevant parts of the game state.\n\nThis approach should lead to cleaner, more maintainable code for managing our complex game state. \n","GenTime":"2024-08-28 13:30:35"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"text":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n\n\n##Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","MatchedIds":[3069,1337,2652,2181,2914],"Prediction":"## Decision\n\nWe will use TypeDoc to generate API documentation from our TypeScript code. \n\nThis approach offers several advantages:\n\n* **Automatic generation:** TypeDoc leverages TypeScript's type information and JSDoc-style comments, eliminating the need to maintain separate API documentation. This ensures that the documentation stays in sync with the codebase. \n* **Reduced duplication:**  We avoid redundant type definitions in comments since TypeDoc utilizes TypeScript's existing type system.\n* **Improved maintainability:**  Automatic generation reduces the risk of outdated documentation and minimizes manual effort to keep documentation aligned with code changes.\n\nWe will integrate TypeDoc into our build process to automatically generate documentation on each build. The documentation will be published alongside our code to ensure developers have easy access to the latest API documentation. \n","GenTime":"2024-08-28 13:30:40"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n\n\n##Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","MatchedIds":[3598,3917,1119,3657,4397],"Prediction":"## Decision\n\nWe will use **GitHub Pages** to host the Helm Chart Repository. \n\nWhile not a dedicated Helm Chart repository solution, GitHub Pages fulfills our core requirements:\n\n* **Publicly accessible:** GitHub Pages are inherently public-facing.\n* **Custom domain:**  We can use a custom domain with GitHub Pages, redirecting from our chosen domain. \n* **No IT support:**  Leveraging GitHub Pages requires minimal setup and is entirely managed by GitHub.\n* **Pipeline friendly:**  Publishing charts to GitHub Pages can be easily integrated into our existing CI\/CD pipelines.\n\nWhile options like ChartMuseum, JFrog Artifactory, and self-hosting on S3 offer more specialized features, the simplicity and ease of use of GitHub Pages outweigh these benefits for our immediate needs.\n\nThis approach allows us to quickly establish a public Helm Chart repository without significant overhead, relying on our existing familiarity with GitHub and minimizing maintenance efforts. We can re-evaluate the need for more specialized features in the future. \n","GenTime":"2024-08-28 13:30:45"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"text":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n\n\n##Decision\nWe have chosen to store our packages on NPM.\\n","MatchedIds":[783,3470,36,1196,1240],"Prediction":"## Decision\n\nWe will use [NPM](https:\/\/www.npmjs.com\/) as the package repository for our front-end framework. \n\nThis decision is driven by the following factors:\n\n* **Existing usage within NHS:** Leveraging NPM aligns with the established practices of other NHS UK and NHS Digital projects. This promotes consistency, reduces cognitive load for developers familiar with the ecosystem, and potentially allows for easier sharing and reuse of components across projects in the future.\n* **Mature Ecosystem:** NPM boasts a mature and robust ecosystem with a vast collection of publicly available packages. This provides us with a wealth of pre-built functionalities and reduces the need to reinvent the wheel for common tasks.\n* **Industry Standard:**  NPM is the de facto standard for JavaScript package management. Choosing a widely adopted solution ensures access to a large community, ample documentation, and readily available support. \n\nThis approach will streamline development, encourage code reuse within the NHS ecosystem, and minimize the learning curve for developers already familiar with NPM. \n","GenTime":"2024-08-28 13:30:51"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"text":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n\n\n##Decision\nHospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","MatchedIds":[2298,1854,4613,2667,1193],"Prediction":"## Decision\n\nHospitalRun Frontend will migrate away from Redux for managing application state and async data fetching. The following changes will be implemented:\n\n- **Async Data Fetching:** Replace `redux-thunk` with `react-query` for handling all API requests and data management. This provides built-in caching, request deduplication, and simplifies data fetching logic within components. \n- **Global Application State:** Utilize the React Context API to manage the few instances of global state needed, such as session information. This approach avoids unnecessary overhead from Redux while still providing a structured way to share state across components.\n- **Business Logic:** Migrate business logic currently residing in Redux actions and reducers into separate helper functions or custom React hooks. This promotes reusability and keeps logic closely coupled to the components that use it.\n- **Testing:** With the removal of Redux, component testing will be simplified.  Components can be tested in isolation by directly mocking `react-query` hooks or providing mock data to the Context providers.\n\nThis migration will result in a leaner, more maintainable codebase, simplified testing, and improved developer experience.\n","GenTime":"2024-08-28 13:30:57"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"text":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n\n\n##Decision\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","MatchedIds":[215,3147,620,2968,1372],"Prediction":"## Decision\n\nTo ensure high availability in case of an AZ failure, we will adopt a capacity planning strategy based on **reserved organization quotas**. This decision considers that actual memory usage is significantly lower than the reserved amount due to the nature of org quotas and app memory limits.\n\nWe will maintain **at least 50% of the total reserved org memory capacity available** in the remaining zones after a single AZ failure. This means each of the remaining zones must be able to accommodate an additional 50% of the total reserved capacity to handle the load from the failed zone. \n","GenTime":"2024-08-28 13:31:01"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"text":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n\n\n##Decision\nWe will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","MatchedIds":[4847,5102,351,1739,328],"Prediction":"## Decision\n\n### Streamline Deployment Pipeline\n\nWe will consolidate the CI and Staging environments into a single pre-production environment. This will simplify our deployment pipeline and accelerate the path to production.\n\n**Reasoning:**\n\n- **Reduced Overhead:** Maintaining two separate non-production environments (CI and Staging) introduces unnecessary complexity and overhead. Merging them reduces the resources required for infrastructure, maintenance, and deployment processes.\n\n- **Faster Feedback Loop:** Collapsing the environments allows for quicker feedback on code changes. Developers can identify and address issues earlier in the development cycle without waiting for deployments through multiple stages.\n\n- **Optimized Deployment Time:** Removing a stage in the pipeline directly reduces the overall time it takes for a change to reach production. This streamlined process allows for faster delivery of value to users.\n\n**Implementation:**\n\n1. **Environment Consolidation:** Merge the CI and Staging environments into a single pre-production environment that mirrors the production environment as closely as possible.\n\n2. **Pipeline Adjustment:** Update the deployment pipeline to reflect the new structure, eliminating the stage previously dedicated to either CI or Staging.\n\n3. **Automated Testing:** Implement robust automated testing within the pre-production environment to ensure the quality and stability of releases before they are promoted to production.\n\n**Monitoring and Evaluation:**\n\nWe will closely monitor the impact of this change on deployment speed and production stability. Key metrics include lead time for changes, deployment frequency, and production incident rate. Based on the data collected, we will make any necessary adjustments to ensure the efficiency and reliability of our deployment process. \n","GenTime":"2024-08-28 13:31:09"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"text":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n\n\n##Decision\nAll tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","MatchedIds":[4336,4179,1258,1588,3572],"Prediction":"## Decision\nTo ensure the katas remain relevant and functional, we will introduce a CI step that runs on each push. This step will execute each kata, verifying it still fails as expected. If a kata passes unexpectedly, it indicates either a fix within the kata itself (which should be reverted) or a change in the underlying engine behavior that needs further investigation. This approach allows us to detect broken katas or engine incompatibilities early and keep the learning experience consistent for users. \n","GenTime":"2024-08-28 13:31:13"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"text":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n\n\n##Decision\nWe put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","MatchedIds":[2261,4699,4428,4408,3119],"Prediction":"## Decision \n\nWe will introduce a time-boxed Request For Comments (RFC) process for all new ADRs.  The journey team proposing the architectural decision will own the RFC process: \n\n* **Drafting:** The journey team will draft the ADR and submit it for team-wide review.\n* **RFC Period:** A fixed time period (e.g., one week) will be allocated for all team members to provide feedback on the ADR.  \n* **Feedback Incorporation:**  The journey team will be responsible for considering all feedback received during the RFC period. They will incorporate the feedback into the ADR where appropriate or document alternative viewpoints and the rationale for not implementing them.\n* **Final Decision:** After the RFC period, the journey team makes the final decision on the ADR, taking into account the feedback received. This decision, along with the documented feedback, will be recorded in the final ADR document.\n\nThis process balances the need for broad team input on architectural decisions with the agile nature of the journey model by providing a structured, time-limited approach to gathering feedback and making decisions.\n","GenTime":"2024-08-28 13:31:18"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"text":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n\n\n##Decision\nWe will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","MatchedIds":[2410,4056,4041,920,1182],"Prediction":"## Decision\n\nTo achieve a balance between DfE's strategic direction and the project's immediate need for agility, we will adopt a two-pronged approach:\n\n**Phase 1: Rapid Prototyping and Development (Short-term)**\n\nWe will leverage the [Gov.UK PaaS](https:\/\/www.cloud.service.gov.uk\/) for the initial development and user research phase. This decision is based on the following factors:\n\n* **Speed and Agility:** Gov.UK PaaS allows for rapid deployment and iteration, crucial for prototyping and user research activities.\n* **Reduced Bureaucracy:** The platform offers a less restrictive environment compared to the DfE Azure setup, enabling faster feedback loops.\n* **Cost-Effectiveness:** The PaaS model aligns with the exploratory nature of this phase, allowing us to scale resources as needed. \n\n**Phase 2: Transition to DfE Azure (Long-term)**\n\nOnce the service matures beyond the initial research and development phase, we will initiate a transition to the DfE Azure platform. This transition will be meticulously planned and executed, taking into account:\n\n* **Security and Compliance:** We will collaborate closely with the DfE Cloud Infrastructure Program to ensure seamless integration with existing security protocols and compliance standards.\n* **Knowledge Transfer:**  We will prioritize knowledge transfer from the development team to the DfE Azure team to ensure operational sustainability.\n* **Data Migration Strategy:** A robust data migration strategy will be defined and executed to ensure data integrity and minimal service disruption.\n\nThis approach allows us to move quickly in the early stages while aligning with the DfE's long-term cloud strategy. The transition to DfE Azure will be treated as a separate project phase, with dedicated resources and timelines.\n","GenTime":"2024-08-28 13:31:27"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"text":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n\n\n##Decision\nWe have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","MatchedIds":[3657,4021,875,1797,4539],"Prediction":"## Decision\n\nWe will add a new vector tile endpoint to the existing Django web application which will make the `ST_AsMVT` query. \n\n### Reasoning\n\nWhile setting up a dedicated `ST_AsMVT`-based vector tile server like Martin offers potential performance advantages in the long term, it introduces added complexity in terms of deployment, maintenance, and potential scaling challenges. For the time being, leveraging the existing Django application infrastructure provides a more straightforward and lower-maintenance solution for several reasons:\n\n- **Simplified Deployment:**  Integrating the vector tile endpoint within the existing Django application streamlines deployment. It avoids introducing an additional service and the associated infrastructure overhead. \n- **Centralized Logic:** Keeping the vector tile generation logic within the Django application centralizes data access and filtering mechanisms. This simplifies maintenance and ensures consistency with other parts of the application.\n- **Existing Expertise:** The team already possesses strong Django expertise, enabling faster implementation and easier troubleshooting within a familiar environment.\n\n### Tradeoffs\n\n- **Potential Performance Bottleneck:** Utilizing the existing Django application could create a performance bottleneck, especially as the number of facilities increases. However, we believe that with proper optimization techniques (e.g., database indexing, caching) and by leveraging Django's ORM effectively, we can mitigate this risk.\n- **Increased Complexity of Django Application:** Adding the vector tile endpoint will increase the complexity of the Django application to some extent. However, with careful design and modularization, we can manage this complexity and ensure the application remains maintainable.\n\n### Future Considerations\n\nIf performance becomes a significant concern as the number of facilities grows, we can revisit the option of migrating to a dedicated vector tile server like Martin. This migration path can be facilitated by designing the vector tile generation logic in a modular way, allowing for relatively straightforward decoupling from the Django application in the future.\n","GenTime":"2024-08-28 13:31:35"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"text":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n\n\n##Decision\nHeavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","MatchedIds":[475,485,5123,2884,1866],"Prediction":"## Decision\n\nWe will implement separate indices for each supported locale. \n\n**Reasoning:**\n\n* **Simplified Analysis:** Using an index per locale allows us to leverage Elasticsearch's built-in language analyzers directly, without complex mappings or custom analyzers. This keeps the configuration straightforward and maintainable.\n* **Search Optimization:** Searching within a locale-specific index will be inherently faster and more accurate than filtering by locale within a larger, mixed-locale index. \n* **Scalability:**  Separate indices provide better horizontal scaling opportunities. As content in certain locales grows, we can scale those specific indices independently without impacting others.\n\n**Tradeoffs:**\n\n* **Increased Management Overhead:** Managing multiple indices adds some operational overhead, but this is mitigated by Elasticsearch's robust index management tools and automation capabilities.\n\n**Implementation Notes:**\n\n* Index names will incorporate the locale (e.g., `kitsune-content-en`, `kitsune-content-de`).\n* Search requests will target the appropriate locale index based on user context. \n* A management process will handle the creation and maintenance of locale-specific indices.\n\nThis approach offers the best balance of simplicity, performance, and scalability for handling multilingual content in Elasticsearch. \n","GenTime":"2024-08-28 13:31:42"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"text":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n\n\n##Decision\nAny processes and workflows need to be very fast and fully automated.\\n","MatchedIds":[187,929,5102,3497,4868],"Prediction":"## Decision\n\nWe will strive to eliminate all unnecessary wait times for developers. This will involve:\n\n1. **Identifying bottlenecks:** We will profile our development processes to pinpoint the biggest time sinks for developers. This could include build times, test execution, deployments, infrastructure provisioning, or any other process that forces developers to wait. \n2. **Automating wherever possible:**  Manual processes are often slow and error-prone. We will prioritize automating tasks like building, testing, deploying, and provisioning infrastructure to reduce wait times and human error. \n3. **Parallelizing where feasible:**  Tasks that can be run concurrently should be.  We will explore parallelizing builds, tests, and deployments to minimize wait times.\n4. **Optimizing resource allocation:** If developers are waiting due to resource constraints, we will investigate more efficient resource allocation strategies. This may involve upgrading hardware, optimizing cloud infrastructure, or implementing resource sharing mechanisms. \n5. **Investing in tooling and infrastructure:**  We will prioritize tools and infrastructure that improve developer experience and reduce wait times. This could include investing in faster build systems, more powerful testing infrastructure, or streamlined deployment pipelines. \n\nBy minimizing developer downtime, we aim to improve productivity, accelerate development cycles, and enable faster iteration and problem-solving. \n","GenTime":"2024-08-28 13:31:49"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"text":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n\n\n##Decision\nThe module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","MatchedIds":[1990,2538,2791,1310,2884],"Prediction":"## Decision\n\nTo simplify the handling of additional attributes in DynamoDB and ensure consistency, the decision is to **always** include the range\/hash keys in the additional attributes, regardless of whether they are explicitly declared. \n\nThis approach offers several benefits:\n\n* **Reduced Complexity:** Eliminates the need for conditional logic to handle cases where range\/hash keys are declared or undeclared.\n* **Improved Consistency:**  Guarantees that all additional attributes have a consistent structure, improving code readability and maintainability.\n* **Future Proofing:**  Accommodates potential future scenarios where range\/hash keys might be required for other functionalities.\n\nWhile this decision might result in slightly larger data structures, the benefits in terms of simplicity, consistency, and future-proofing outweigh the minor increase in storage requirements. \n","GenTime":"2024-08-28 13:31:54"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"text":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n\n\n##Decision\nSetup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","MatchedIds":[2968,771,1180,469,2997],"Prediction":"## Decision\nWe will use kubeadm to easily provision a Kubernetes cluster as it provides a streamlined approach for setting up the core Kubernetes components.\n","GenTime":"2024-08-28 13:31:55"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"text":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n\n\n##Decision\nThe placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","MatchedIds":[920,914,4454,581,2093],"Prediction":"## Decision\n\nThe decision is to use a combination of lightweight technologies and services that enable rapid prototyping and experimentation. \n\n**Justification:**\n\n* **Speed and Flexibility:**  Alpha phases prioritize exploring various solutions and iterating quickly based on user feedback. A lightweight tech stack allows for rapid development, deployment, and modification of prototypes without the overhead of complex infrastructure or data management.\n* **Cost-Effectiveness:**  Limiting reliance on expensive tools and services during the exploratory phase can be more cost-effective. Open-source tools and platforms with generous free tiers are suitable for early-stage prototypes. \n* **Ease of Use:**  Choosing technologies familiar to the team or with a low learning curve allows developers to focus on building and testing prototypes rather than grappling with new technologies.\n\n**Specific Recommendations:**\n\n1. **Frontend:** A JavaScript framework like React, Vue, or Svelte, combined with a UI library like Material UI or Bootstrap, enables quick creation of interactive user interfaces.\n2. **Backend:**  Consider a lightweight backend framework like Express.js (Node.js) or Flask (Python) for prototyping APIs and handling data interactions. \n3. **Database:** A lightweight, potentially in-memory, database like SQLite or a managed cloud database service with a generous free tier can be used for initial development.\n4. **Deployment:** Platform-as-a-Service (PaaS) solutions like Heroku or Netlify are ideal for their ease of deployment and scaling for prototype applications.\n5. **Data Mocking:** Since real user data is not being used, leverage data mocking libraries and techniques to generate realistic but fake data for prototyping and testing purposes.\n\n**Important Considerations:** \n\n* While prioritizing agility in the Alpha phase is crucial, it's essential to select technologies that can be easily adapted or migrated as the service transitions to Beta and beyond, should the prototypes prove successful. \n* Maintain clear documentation of the chosen technologies and the reasoning behind their selection. This documentation will be helpful for future development phases and team onboarding. \n","GenTime":"2024-08-28 13:32:06"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"text":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n\n##Decision\nCompletely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","MatchedIds":[4618,1193,2573,476,2968],"Prediction":"## Decision\n\nWe will no longer support FreeTFT and TFTA tokens within the SDK. This decision is driven by the following factors:\n\n* **Reduced Maintenance Overhead:** Removing deprecated or unused features simplifies the SDK and reduces potential points of failure, resulting in easier maintenance.\n* **Focus on Core Functionality:**  This allows developers to focus on the core features and functionalities currently supported by the SDK.\n* **Security and Stability:**  Eliminating potential vulnerabilities associated with outdated or less-used components enhances security and stability.\n\n**Further Actions:**\n\n* **Deprecation Period:**  Announce a reasonable deprecation period, providing developers time to transition away from using FreeTFT and TFTA tokens within the SDK.\n* **Documentation and Migration Guide:** Provide comprehensive documentation outlining the rationale behind this decision, alternative solutions, and a step-by-step migration guide for developers currently utilizing the affected features. \n* **Community Support:** Offer dedicated support channels to assist developers during the transition period. \n\nBy removing outdated features, we can ensure a more streamlined, secure, and maintainable SDK moving forward. \n","GenTime":"2024-08-28 13:32:11"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"text":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n\n\n##Decision\nHandle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","MatchedIds":[3372,3831,3374,2859,3852],"Prediction":"## Decision\n\nDOS will utilize the Handle System to generate persistent identifiers for ingested files. These Handles will be used in URLs provided to and stored by ArchivesSpace. \n\nThis decision provides the following benefits:\n\n* **Persistent Identifiers:** Ensures that URLs remain valid even if the underlying storage location or DOS service endpoint changes.\n* **Externalized Functionality:** Leverages an existing system, avoiding the need to develop and maintain custom persistent identifier generation within DOS.\n* **Reusability and Portability:** Enables other systems to adopt the same Handle System for their persistent identifier needs, fostering interoperability.\n* **Organizational Familiarity:**  Capitalizes on existing knowledge and experience with the Handle System within the organization. \n\nBy adopting the Handle System, DOS can ensure long-term accessibility and management of digital objects while promoting consistency and collaboration across different platforms and services.\n","GenTime":"2024-08-28 13:32:16"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"text":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n\n\n##Decision\nAfter discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","MatchedIds":[4097,1985,1979,1982,1956],"Prediction":"## Decision\nMigrate the communication protocol used by Tendermint's private validator client (`privval`) from the existing custom protocols (`SecretConnection`, raw TCP\/Unix sockets) to gRPC. \n\n### Motivation\n\nThis decision addresses the following issues:\n\n- **Security Concerns:**  The previous `SecretConnection` protocol had vulnerabilities to man-in-the-middle attacks, and while these have been addressed, switching to a well-vetted protocol like gRPC provides a more robust and secure foundation.\n- **Integration Complexity:** Custom protocols create a barrier to entry for developers who are not familiar with Golang. gRPC, being language-agnostic, will streamline integration with a wider variety of signing solutions.\n- **Maintenance Burden:** Maintaining custom protocols requires significant effort and expertise. Migrating to gRPC leverages a mature and widely-used technology, reducing the maintenance burden on the Tendermint team.\n- **Improved User Experience:**  Migrating to gRPC can potentially resolve the minor issues encountered by users with the raw TCP\/Unix socket communication.\n\n### Details\n\nThe migration to gRPC will involve the following steps:\n\n1. **Define gRPC Services and Messages:** Define a clear gRPC interface for the private validator client, specifying the services (e.g., `Signer`) and messages (e.g., `SignVoteRequest`, `SignVoteResponse`) used for communication.\n2. **Implement gRPC Server and Client:** Implement a gRPC server within the `privval` component and a corresponding client within Tendermint core to handle the new communication protocol.\n3. **Deprecate Existing Protocols:**  Gradually deprecate the existing `SecretConnection` and raw TCP\/Unix socket protocols. Provide a migration path for users relying on the old protocols.\n\n### Potential Drawbacks\n\n- **Performance Overhead:** gRPC, being a more feature-rich protocol, might introduce some performance overhead compared to the lightweight custom protocols. Careful benchmarking and optimization will be necessary to mitigate any potential impact.\n\n### Conclusion\n\nMigrating to gRPC offers significant benefits in terms of security, maintainability, and ease of integration. The potential performance implications can be addressed through careful implementation and optimization. This migration will enhance the robustness and user-friendliness of the remote signing system for Tendermint validators. \n","GenTime":"2024-08-28 13:32:27"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"text":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n\n\n##Decision\nMeals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","MatchedIds":[2672,1712,2681,3337,4843],"Prediction":"## Decision\n\nTo enhance the resilience of the fridge system in offline scenarios, we will implement a two-step verification process leveraging both unique meal IDs and generated access codes:\n\n1. **Meal Pre-authorization:**\n    - When a user orders a meal (especially customized ones), the system will generate a unique ID for that specific meal. \n    - At the time of purchase or meal preparation, a unique access code will be generated, linked to the meal ID. This code should be time-sensitive to mitigate unauthorized access risks.\n    - The user's associated device (e.g., their phone app) will be updated with both the meal ID and its corresponding access code. This ensures the information is readily available even if network connectivity is lost later. \n\n2. **Offline Fridge Access:**\n    - When a user wants to retrieve their meal from the fridge, they will first input the meal ID via the fridge's keypad. The fridge will internally validate the meal ID against its stored data (updated during delivery). \n    - Upon successful meal ID verification, the fridge will prompt the user to enter the access code. \n    - If the provided access code matches the one associated with the meal ID, the fridge will unlock and grant access to the designated meal compartment.\n\nThis approach provides several benefits:\n\n* **Offline Functionality:**  Enables meal retrieval even when network connectivity is unavailable.\n* **Security:**  The combination of unique meal IDs and time-sensitive access codes adds an extra layer of security, preventing unauthorized access. \n* **Customization Handling:**  Accommodates scenarios where specific meals are linked to individual users, ensuring accurate delivery and pickup.\n\nFurthermore, the system can be enhanced with additional features:\n\n* **Code Expiration:** Implement access code expiration to limit potential misuse if a code is compromised.\n* **Offline Logging:**  Store access attempts (successful and unsuccessful) locally on the fridge for later analysis and security audits once connectivity is restored. \n","GenTime":"2024-08-28 13:32:37"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"text":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n\n\n##Decision\nThe naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","MatchedIds":[1464,631,74,3147,2062],"Prediction":"## Decision\nImplement a comprehensive tagging strategy for all cloud assets encompassing the following:\n\n1. **Standardization:** Define a clear and concise set of tags with specific key-value pairs. This set should cover aspects like environment (e.g., dev, test, prod), application, cost center, owner team, and data sensitivity. \n2. **Automation:** Integrate tagging into the provisioning process. Utilize Infrastructure-as-Code tools like Terraform or CloudFormation to automatically apply tags based on resource properties and deployment context. \n3. **Enforcement:** Enforce tagging compliance through automated scripts and policies.  Reject resources that don't adhere to the defined tagging standards.\n4. **Documentation:** Maintain a central repository documenting the tagging strategy. This repository should include descriptions of each tag, their purpose, and allowed values. \n5. **Cost Allocation:** Leverage tags to categorize resources for accurate cost allocation and reporting. This allows for effective chargeback or showback to specific business units based on their resource consumption.\n\nThis strategy will streamline resource management, simplify cost attribution, and enhance security by enabling tag-based access control policies. \n","GenTime":"2024-08-28 13:32:44"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"text":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n\n\n##Decision\n* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","MatchedIds":[4609,4896,4644,4618,697],"Prediction":"## Decision\n\n**We will implement a clear separation of concerns between the kernel (TypeScript) and Unity layers using a well-defined API.** This approach involves:\n\n1. **Defining a strict API Contract:** A formal interface will be established to govern communication between the kernel and Unity. This contract will define data structures, commands, and events exchanged between the two layers.\n\n2. **Treating Unity as a Client:** The kernel will act as the authoritative source for business logic and data management. Unity will primarily function as a client, sending requests to and receiving updates from the kernel through the defined API.\n\n3. **Minimizing Kernel Dependencies on Unity:** The kernel's implementation will be as independent of Unity-specific concepts as possible. Any Unity-related logic will be abstracted away behind the API.\n\n4. **Leveraging Asynchronous Communication:**  Asynchronous communication mechanisms (e.g., message queues, event buses) will be employed to decouple the kernel and Unity, enabling parallel development and reducing blocking dependencies. \n\n### Positive Consequences\n\n- **Improved Maintainability:** A clear separation of concerns simplifies code maintenance and evolution.\n- **Increased Development Velocity:** Parallel development efforts by kernel and Unity teams become feasible, reducing bottlenecks.\n- **Enhanced Testability:**  Isolated testing of kernel logic and Unity components becomes more straightforward.\n- **Technology Agnosticism:**  Decoupling the kernel from Unity enhances flexibility for potential future technology changes on either side.\n\n### Negative Consequences\n\n- **Initial Development Overhead:** Designing and implementing a well-defined API and communication layer will require upfront effort. \n- **Increased Communication Complexity:** Asynchronous communication can introduce additional complexity compared to direct method calls.\n\n### Justification\n\nWhile there is some initial overhead, the long-term benefits of decoupling the kernel and Unity layers, including improved maintainability, development speed, and flexibility, outweigh the drawbacks. \n","GenTime":"2024-08-28 13:32:54"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"text":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n\n\n##Decision\nOpenchs server will have capabilities to be deployed on the cloud\\n","MatchedIds":[1712,2828,3657,1708,4859],"Prediction":"## Decision\n\n### Architecture\n\n- **Move away from per-NGO installation.**  Instead, adopt a centralized, cloud-based solution. This eliminates the need for individual Postgres and Tomcat instances, reducing maintenance overhead significantly.\n- **Utilize a serverless backend.** This allows for scaling resources based on usage, optimizing cost for NGOs with varying sizes and usage patterns.\n- **Implement a robust offline-first mobile application.** This enables health workers to operate seamlessly in the field, even without internet connectivity.  \n- **Synchronize data during visits to the head office.** The application should automatically sync data with the central server whenever internet connectivity is available. \n\n### Technology Considerations\n\n- **Backend:** Consider using a serverless platform like AWS Lambda or Google Cloud Functions coupled with a managed database service like AWS Aurora or Google Cloud SQL.\n- **Mobile:** Continue using a cross-platform framework like React Native or Flutter to minimize development effort.  Implement a local database like SQLite for offline data storage and synchronization logic.\n\n### Advantages\n\n- **Cost-effective:**  Shared infrastructure and serverless technologies significantly reduce costs compared to individual installations.\n- **Simplified maintenance:** Centralized management eases maintenance and software updates.\n- **Improved data accessibility and analysis:**  A central database facilitates data aggregation and analysis across the organization.\n- **Scalability:** The solution can easily scale to accommodate future growth and increased data volume.\n- **Future-proof:** Aligns with the increasing availability of internet connectivity and the trend towards cloud-based solutions.\n\n### Considerations\n\n- **Data security:** Implement robust security measures to protect sensitive patient data stored and transmitted.\n- **Offline functionality:** Design a comprehensive offline mode that allows for continued data entry and access even with intermittent connectivity.\n- **Data synchronization:** Develop a reliable and efficient data synchronization mechanism to ensure data consistency between the mobile application and the central server. \n","GenTime":"2024-08-28 13:33:02"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"text":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n\n\n##Decision\nWe can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","MatchedIds":[21,4623,1962,1944,72],"Prediction":"## Decision\n\nTo address the malleability issue inherent in the second form of valid ECDSA signatures (r, -s), we will implement a standardized normalization process for all secp256k1 signatures within our system. \n\nHere's the breakdown:\n\n1. **Canonical Signature Selection:** We will enforce the use of the signature with the lower `s` value.  \n    * During signature verification, both `(r, s)` and `(r, -s)` will be checked. \n    * The signature with the lower `s` value will be considered the canonical form and will be the only one accepted for transaction processing and storage.\n\n2. **Signature Verification Update:**  Our signature verification logic will be updated to incorporate this canonicalization step:\n    *  After a signature `(r, s)` is recovered, it will be compared with `(r, -s)`.\n    *  Only the signature with the lower `s` value will be considered valid.\n\nThis approach effectively eliminates the malleability concern stemming from the dual valid signature forms while maintaining compatibility with existing secp256k1 implementations. By standardizing on a single canonical signature representation, we ensure that transactions have a deterministic and unique signature, preventing any discrepancies in transaction hashes and ensuring reliable transaction processing. \n","GenTime":"2024-08-28 13:33:09"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"text":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n\n\n##Decision\nFor the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","MatchedIds":[3203,276,4359,3853,4338],"Prediction":"## Decision\n\nDogma's testing features will reside in the `dogma` module itself. \n\nWhile separate versioning offers some advantages for maintainers, we believe that the simplicity and ease-of-use for the user outweigh those benefits. \n\nIncluding the testing features within the main `dogma` module ensures:\n\n* **Seamless user experience:** Users can access testing utilities directly without additional dependencies or version compatibility concerns. \n* **Encourages best practices:**  Easy access to testing features promotes a test-driven approach for users building with Dogma.\n\nWe acknowledge the potential for more frequent releases due to changes in testing features. However, we believe that a well-structured module can accommodate these changes without significant burden. We will prioritize clear documentation and communication around any breaking changes within the `dogma` module. \n","GenTime":"2024-08-28 13:33:14"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"text":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n\n\n##Decision\nWe decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","MatchedIds":[3094,3096,3090,3657,1307],"Prediction":"## Decision\n\nWe will proceed with the option of having Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it. \n\nHere's why:\n\n* **Simplicity and Robustness:** While requiring a manual step, this approach is straightforward and minimizes the potential for errors. The process of checking for the table and creating it if absent is well-defined and easy to reason about.\n* **Automation Potential:** The manual intervention can be readily automated through scripts or configuration management tools, eliminating the need for manual intervention in most deployment scenarios.\n* **Clear Failure Mode:** If the _locks table is not present, the client will refuse to start, providing an immediate and unambiguous indication of the issue. This clear failure mode simplifies debugging and issue resolution. \n\nWhile using the lock service directly seems appealing for its automation, the risk of silent failures due to locking issues outweighs the benefits. The Paxos-based approach introduces unnecessary complexity, making the system harder to understand and maintain. Similarly, relying on a designated \"lockLeader\" host increases configuration overhead and introduces a single point of failure. \n","GenTime":"2024-08-28 13:33:33"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"text":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n\n##Decision\n* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","MatchedIds":[4359,452,4868,4179,1258],"Prediction":"## Decision\n\nWe will use the [unittest] framework built into [Python] because it provides a robust environment to write tests first and then develop features. To support mocking out functions during testing we will use the [unittest.mock] library. This approach will allow us to test functions in isolation and simulate different scenarios and edge cases.\n","GenTime":"2024-08-28 13:33:35"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"text":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n\n\n##Decision\n[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","MatchedIds":[3166,4359,2502,4179,1258],"Prediction":"## Decision\n\nWe will use **Cucumber** framework with **Gherkin** syntax to write our acceptance tests.\n\n* **Good**, because Gherkin's natural language syntax makes tests easy to understand for both technical and non-technical users, thereby improving collaboration and documentation.\n* **Good**, because Cucumber promotes the use of reusable step definitions, simplifying the creation of new acceptance tests and reducing code duplication.\n* **Good**, because Cucumber tests are stored alongside the application source code, enabling a seamless Behavior-driven development (BDD) workflow and allowing developers to run acceptance tests locally throughout the development lifecycle. \n","GenTime":"2024-08-28 13:33:39"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"text":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n\n##Decision\nDo not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","MatchedIds":[1279,1318,1307,2215,1302],"Prediction":"## Decision\nIntroduce `spec-mount` similar to `system\/elektra\/mountpoints`. For each plugin\nconfiguration in `spec-mount`, the storage plugin **must** store all\n`spec:` metadata.  It **must not** store this metadata in the global key\ndatabase. The `spec-mount` metadata is only used for validation purposes\nand to generate the specification. It is **not** used at runtime.\n- If the storage plugin is able to store the `spec:` metadata natively\n  (e.g., YAML), it should do so.\n- Otherwise, it **must** store the `spec:` metadata in a separate file.\n- Add a new plugin setting `spec\/store` to specify how the `spec:` metadata\n  should be stored. Possible values are:\n    - `native`: Store metadata natively in the same file.\n    - `separate`: Store metadata in a separate file.\n- The separate file **must** be a sibling of the original configuration file.\n  Its name **must** be the same as the original configuration file with the\n  suffix `.spec`. For example, if the original configuration file is\n  `\/etc\/myapp\/config.yaml`, the separate file for the `spec:` metadata\n  **must** be `\/etc\/myapp\/config.yaml.spec`.\n- The format of the separate file **must** be YAML.\n- During `kdb mount`, if `spec:` metadata is found in the global key\ndatabase for keys below the mountpoint, a warning is printed, and the\nmetadata is **not** considered for mounting.\n- The validation with `spec:` metadata **must** happen after the metadata\nis merged from different namespaces. This ensures that the validation\nis always performed against the final set of metadata.\n\nThis approach allows us to use storage plugins with `spec` even if they\ndo not natively support all the required metadata. It also avoids the\nproblem of metadata from different namespaces being mixed up.\n","GenTime":"2024-08-28 13:33:49"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"text":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n\n\n##Decision\nWe decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","MatchedIds":[2703,138,2261,3784,2502],"Prediction":"## Decision\n\n### 1. Dataflow Diagram Requirement\n* Every `config` folder within a repository MUST contain at least one Dataflow Diagram (DFD).\n\n### 2. DFD Format \n* The DFD can be created using any tool or method that effectively communicates the data flow (e.g., Draw.io, Lucidchart, Visio, hand-drawn and scanned image). \n* Regardless of the tool used, the DFD should be committed to the repository in a readable format (e.g., PNG, JPEG, PDF).\n\n### 3. DFD Purpose\n* The DFD should illustrate the flow of data within the specific component or module represented by the `config` folder. \n* It should clearly depict data sources, data transformations, data destinations, and any relevant data processing steps.\n\n### 4. Naming Convention\n* The DFD file should be named `dataflow-diagram` or `data-flow-diagram` followed by an optional descriptive suffix and the appropriate file extension (e.g., `dataflow-diagram.png`, `data-flow-diagram_authentication.pdf`).\n\n### 5. Placement \n* The DFD should be placed directly within the `config` folder for easy access and association with the configuration files.\n\n### 6. Tooling (Optional)\n*  Consider providing a recommended tool or set of templates for creating DFDs to ensure consistency across projects.\n","GenTime":"2024-08-28 13:33:57"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"text":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n\n\n##Decision\nWe have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","MatchedIds":[1563,2914,1067,3554,4610],"Prediction":"## Decision\n\nWe will manage ADRs as code with the following process:\n\n1. **Proposing an ADR:**\n    * Create a new markdown file for the ADR in the `\/adr` directory, following the established template (refer to ADR-0000 for the template).\n    * Use a descriptive file name for the ADR, reflecting its main subject (e.g., `ADR-0005-use-redux-for-state-management.md`).\n    * Submit the ADR as a draft pull request to the main branch.\n\n2. **Discussing an ADR:**\n    * Initiate a discussion around the proposed ADR in the pull request, inviting relevant stakeholders to participate.\n    * Clearly state the problem, considered options, chosen solution, and rationale in the ADR document.\n    * Encourage constructive feedback, alternative solutions, and identification of potential drawbacks.\n    * Update the ADR iteratively based on the received feedback, reflecting the ongoing discussion in the document.\n\n3. **Accepting or Rejecting an ADR:**\n    * Once a consensus is reached, mark the pull request as ready for review.\n    * Assign the pull request to designated reviewers with expertise in the related domain.\n    * Reviewers should focus on the ADR's clarity, completeness, and soundness of the chosen solution.\n    * Upon successful review, merge the pull request to accept the ADR.\n    * If the ADR is rejected, clearly state the reasons in the pull request and close it.\n    * Document the rejection rationale within the ADR itself before closing the pull request.\n\n4. **Updating an ADR:**\n    * When changes to an existing ADR are required, create a new pull request, referencing the original ADR.\n    * Follow the same process of discussion and review as with new ADRs.\n    * Update the status and changelog sections within the ADR to reflect the changes.\n\nThis process ensures transparency, promotes collaboration, and maintains a well-documented architectural evolution for the project.\n","GenTime":"2024-08-28 13:34:07"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"text":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n\n##Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","MatchedIds":[1545,311,1558,1180,2181],"Prediction":"## Decision\n\nWe will introduce an abstraction layer (e.g., a client library or repository pattern) to interact with the off-chain data. This abstraction layer will provide a clear interface for accessing the data and allow us to easily mock the API calls in our unit tests. \n\nThis will improve testability by:\n\n* **Decoupling:** Isolating the system's logic from the external API calls.\n* **Mocking:**  Facilitating the creation of controlled test scenarios by mocking the responses from the abstraction layer.\n* **Maintainability:** Centralizing the API interaction logic and reducing code duplication. \n","GenTime":"2024-08-28 13:34:11"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"text":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n\n\n##Decision\nWe release the Shared Jenkins Library under Apache 2.0.\\n","MatchedIds":[2181,1180,631,745,2968],"Prediction":"## Decision \n\nWe will publish the myCloud API spec openly and make it publicly accessible. \n\n### Rationale\n\n* **Open by default:** Our commitment to transparency and collaboration aligns with the \"open by default\" principle. \n* **Non-confidential information:**  The myCloud API spec does not contain sensitive or confidential data.\n* **Benefits of openness:** Publicly available specifications promote:\n    * **Clean and secure design:** Open scrutiny encourages robust and secure API design practices.\n    * **High-quality documentation:** Public availability incentivizes clear, comprehensive, and accessible documentation.\n    * **Community engagement:**  Openness fosters collaboration and contributions from the wider community. \n* **No compelling reasons for confidentiality:**  In the absence of specific confidentiality concerns, the default position is to embrace openness.\n\n### Implementation\n\n* **Publication location:** We will host the myCloud API spec in a publicly accessible repository. \n* **Format:**  The specification will be provided in a machine-readable format (e.g., OpenAPI\/Swagger) and human-readable documentation.\n* **Versioning:**  We will use a clear versioning scheme to manage changes and ensure compatibility. \n\nThis decision may be revisited if future circumstances introduce confidentiality requirements.\n","GenTime":"2024-08-28 13:34:22"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"text":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n\n##Decision\nThe decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","MatchedIds":[5255,1408,4226,3321,400],"Prediction":"## Decision\n\nWe will enforce that all files in Kyma projects using JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON have their style enforced by Prettier.\n\nTo achieve this:\n\n* We will provide a base Prettier configuration for all Kyma projects to ensure consistency.\n* Developers are encouraged to use the Prettier plugin for their IDE to get real-time formatting feedback.\n* We will set up pre-commit Git hooks to automatically run Prettier before each commit. This prevents committing non-compliant code.\n* We will set up continuous integration to reject commits that are not correctly formatted according to Prettier rules. \n","GenTime":"2024-08-28 13:34:27"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"text":"## Context\\nWe want to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 13:34:29"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"text":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n\n\n##Decision\nWe choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","MatchedIds":[4868,1243,1180,472,536],"Prediction":"## Decision\nWe will use [Tweepy](https:\/\/www.tweepy.org\/). It is a widely adopted and well-documented Python library for accessing the Twitter API. It offers comprehensive support for various API functionalities, including tweets, users, timelines, and direct messages. Furthermore, its large community ensures active maintenance, frequent updates, and readily available support resources. \n","GenTime":"2024-08-28 13:34:32"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"text":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n\n##Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","MatchedIds":[279,1574,2520,1990,2526],"Prediction":"## Decision\n\n*   Solution must kept as simple as possible\n*   Even it could delay the problem, increase read and write capacity of\n    DynamoDB is not an architectural solution\n\nChosen option: \"Introduce a secondary persistence mechanism\", because just storing state in a faster data store will reduce the contention over DynamoDB.\n\n### Positive Consequences \n\n* Reduce dramatically the load on DynamoDB, which will be used just for history\/auditing purposes.\n* It's easy to implement and reason about.\n\n### Negative Consequences \n\n*   Another moving piece in the system.\n*   Increases complexity slightly.\n\n### Description\n\nThe idea here is to keep using DynamoDB as the main persistence mechanism but delegating the state management to a different system more suitable for concurrent access like Redis. A short TTL can be set to automatically remove old state data in Redis. \n\nThe flow would be as follows:\n\n1.  Component stores data in DynamoDB.\n2.  Component stores state in Redis.\n3.  Components that require to read the state will query Redis.\n4.  DynamoDB is only accessed for historical data or auditing purposes. \n\nThis solution leverages the strengths of each system - DynamoDB for persistent data storage and Redis for fast, ephemeral state management. This separation also improves overall performance and scalability by reducing contention and allowing each component to operate independently.\n","GenTime":"2024-08-28 13:34:39"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"text":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n\n\n##Decision\n### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","MatchedIds":[3722,1193,2703,1792,317],"Prediction":"## Decision\n\nWe will create a new branch specifically for the v1 to v2 migration scaffold. This will allow us to:\n\n* **Maintain a clean master branch:** The master branch will contain a clean, up-to-date scaffold for building new apps from scratch. This provides a better first impression and lowers the entry barrier for new developers.\n* **Continue supporting the migration tools:**  The App Migrator and ZAT can continue to use the migration-specific branch as their download source, ensuring existing users are not disrupted. \n\n### Implementation Details:\n\n1. **Create a new branch:** Branch off from the current master (or an appropriate commit) and name it descriptively, for example, `v1-to-v2-migration`. \n2. **Update Documentation:** \n   * Clearly document the purpose of each branch in the README.\n   * Provide instructions for developers on which branch to use based on their needs (new app vs. migration).\n   * Update the download links in the App Migrator and ZAT documentation to point to the new `v1-to-v2-migration` branch.\n3. **Consider long-term maintenance:**  Decide on a strategy for maintaining the migration branch:\n    * **Active Maintenance:** Regularly update the branch with bug fixes and improvements.\n    * **Deprecation Plan:** Set a timeline for phasing out the migration branch and communicate this clearly to users. \n\nThis approach ensures both new and migrating developers have the resources they need while maintaining a clean and focused master branch. \n","GenTime":"2024-08-28 13:34:48"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"text":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n\n\n##Decision\nWe have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","MatchedIds":[2249,2240,4705,50,49],"Prediction":"## Decision\n\nWe will enable Istio's `sidecar.istio.io\/rewriteAppHTTPProbers` annotation on all injected pods by default. This will allow the sidecar injection process to rewrite the liveness and readiness probes to go through the `pilot-agent` instead of the Envoy sidecar, ensuring probes continue to function correctly when Istio mTLS is enabled in `STRICT` mode.\n\n### Positive Consequences\n\n*  Liveness and readiness probes will continue to work as expected in `STRICT` mTLS mode, ensuring proper pod lifecycle management and traffic routing by Kubernetes.\n* This approach leverages Istio's built-in functionality and avoids workarounds or custom solutions.\n\n### Negative Consequences \n\n*  Relying on Istio to rewrite probes creates a dependency on this feature. If Istio's probe rewriting mechanism changes or breaks, it will impact probe functionality. \n*  Developers deploying applications need to be aware of this behavior and understand how probes are routed in `STRICT` mTLS mode.\n\n### Alternative Options Considered\n\n1. **Disable strict mTLS for kubelet:** This option would involve configuring Istio to not enforce mTLS for traffic originating from the kubelet. This could introduce security risks and might not be feasible for all deployments.\n2. **Implement custom health checks:** Develop custom health checks that use mTLS authentication to communicate with the application. This would add complexity and maintenance overhead.\n\n### Conclusion\n\nEnabling Istio's probe rewriting functionality provides the most straightforward and robust solution for maintaining liveness and readiness probe functionality in a `STRICT` mTLS environment. This approach aligns with Istio's best practices and minimizes potential disruptions while preserving the security benefits of mTLS. \n","GenTime":"2024-08-28 13:34:56"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"text":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n\n\n##Decision\nWe will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","MatchedIds":[3691,2626,1035,3863,1193],"Prediction":"## Decision\nWe will follow semantic versioning for this library. Any time Guice introduces a breaking change in a new major version, this library will also have a corresponding major version upgrade. \n\nThis decision minimizes disruption for users by making it clear when breaking changes are introduced and aligns with common dependency management practices. \n","GenTime":"2024-08-28 13:34:59"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[5255,1408,4226,3451,5087],"Prediction":"## Decision\n\nWe will enforce that all files supported by Prettier have their style enforced by it.\n\nWe will set up Git hooks to automatically run the formatter before committing.\n\nWe will set up continuous integration to reject commits that are not correctly formatted. \n","GenTime":"2024-08-28 13:35:01"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"text":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n\n##Decision\nWe will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","MatchedIds":[2687,3156,1067,3635,4618],"Prediction":"## Decision\n\nWe will use **TypeScript** as the primary language for the Alfa project. This decision offers several advantages:\n\n* **Static Typing:** TypeScript introduces a strong type system to JavaScript, addressing the core concern of undefined values and incorrect parameters. This will significantly improve code reliability and reduce runtime errors.\n* **Enhanced Tooling:** TypeScript unlocks advanced tooling features like static analysis, autocompletion, and refactoring support, which are crucial for managing a large and complex codebase.\n* **Browser Compatibility:** TypeScript compiles to clean, readable JavaScript, ensuring continued compatibility with browser environments and enabling development of tools like the Chrome extension.\n* **Gradual Adoption:**  While we commit to TypeScript for new development, existing JavaScript code can be gradually migrated, easing the transition process.\n\nThis approach allows us to leverage the benefits of a robust type system while maintaining the flexibility and reach of JavaScript for browser-based components. We acknowledge that using TypeScript might slightly increase the initial development time. However, we believe that the long-term benefits in terms of code maintainability, reduced bugs, and improved developer experience will outweigh the initial cost. \n","GenTime":"2024-08-28 13:35:07"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"text":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n\n\n##Decision\nBuild a SPA.\\n","MatchedIds":[2317,798,1907,1240,4271],"Prediction":"## Decision\n\nWe will implement a **hybrid approach** that leverages the strengths of both Single Page Applications (SPAs) and traditional multi-page websites, minimizing the drawbacks of each:\n\n**1. Core Audio Experience as an SPA:**\n\n- The audio player and its directly related functionality (e.g., playlist management, track details) will be built as an SPA. This ensures seamless audio playback without interruptions from page transitions.\n\n**2. Strategic SPA Integration:**\n\n- Instead of building the entire website as an SPA, we will integrate the audio player SPA into specific pages where continuous playback is crucial. This could be a dedicated \"Listen\" page or embedded within content where audio is a primary element.\n\n**3. SEO Optimization:**\n\n- For pages outside the core audio experience, we will maintain a traditional multi-page structure, allowing for optimized SEO through server-side rendering and targeted metadata.\n-  Content related to the audio content (e.g., episode descriptions, transcripts) can be made accessible on separate, SEO-friendly pages.\n\n**4. Enhanced User Experience:**\n\n- The SPA-based player can utilize features like background audio playback (where supported by browsers) and persistent controls, improving the user experience.\n-  We will prioritize a user-friendly, responsive design for the audio player SPA, ensuring a consistent experience across different devices.\n\nThis approach provides a balance between seamless audio playback and the benefits of traditional web development practices. It minimizes the SEO challenges of a full SPA while avoiding the clunky experience of pop-up players. \n","GenTime":"2024-08-28 13:35:15"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"text":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n\n\n##Decision\nWe will apply the principle of least privilege.\\n","MatchedIds":[2256,745,3890,866,2502],"Prediction":"## Decision\n\nAll users, applications, and processes will be granted the absolute minimum access rights required to perform their designated functions. This principle of least privilege will be enforced through a combination of:\n\n* **Role-Based Access Control (RBAC):**  Defining granular roles and permissions mapped to specific job functions will ensure users only access necessary resources.\n* **Regular Access Reviews:** Periodic reviews of access rights will ensure privileges remain aligned with evolving responsibilities and identify any access creep.\n* **Automated Provisioning\/Deprovisioning:**  Automating the granting and revoking of access upon job changes or employee departure will minimize the potential for lingering permissions.\n* **Secure Secret Management:** Sensitive credentials and API keys will be stored securely and accessed using temporary, time-limited tokens with restricted permissions.\n\nThis approach minimizes the potential attack surface and mitigates the impact of any potential compromise by limiting access to only what is absolutely necessary. \n","GenTime":"2024-08-28 13:35:22"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\\nDerived attributes are read-only.\\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"text":"## Context\\nDerived attributes are read-only.\\n\n\n##Decision\nFollowing attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","MatchedIds":[2968,5142,2823,4804,3820],"Prediction":"## Decision\n\nDerived attributes are not directly modifiable by users or client applications. Their values are calculated and updated automatically based on the defined logic and the values of the source attributes. Any attempt to modify a derived attribute will be rejected. \n","GenTime":"2024-08-28 13:35:27"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"text":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n\n\n##Decision\nURI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","MatchedIds":[2763,2740,4538,2181,2342],"Prediction":"## Decision\nWe will use the `ipns` URI scheme. `ipns` is a decentralized naming system built on top of IPFS, making it suitable for identifying resources on decentralized storage. \n\nThis approach offers several advantages:\n\n* **Decentralization:** Aligned with the decentralized nature of the storage, ensuring no single point of failure for accessing EDN files.\n* **Content Addressing:**  The URI can directly map to the EDN file's content hash, ensuring integrity and immutability.\n* **Human Readability (optional):**  While not a primary concern for machine-to-machine communication, human-readable names can be mapped to `ipns` addresses for easier management.\n\nFor example, an extension EDN file might be identified with:  `ipns:\/\/<IPNS_ADDRESS_OF_EDN_FILE>`\n\nThis decision assumes the decentralized storage utilizes IPFS or a compatible system. \n","GenTime":"2024-08-28 13:35:32"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"text":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n\n\n##Decision\nWe will create an AMI generation and management capability based on AWS Image Builder.\\n","MatchedIds":[923,3147,1376,4358,4025],"Prediction":"## Decision\n\nWe will use **AWS Image Builder** as our preferred solution for building and managing customised AMIs. \n\nHere's why:\n\n* **Reduced Operational Overhead:** As a managed service, Image Builder removes the complexity of setting up and maintaining our own AMI building infrastructure. \n* **Security Benefits:** Leveraging AWS Systems Manager removes the need for SSH access and open ports during the build process, enhancing security.\n* **Integration and Extensibility:** Image Builder offers a rich set of features including pipelines, recipes, and components, providing flexibility and easy integration with other AWS services. It also allows us to leverage existing marketplace components.\n* **Future Proofing:** AWS Image Builder is a relatively new service and is actively being developed.  Opting for a managed service over a custom solution or third-party tool will likely offer more features and better integration with other AWS services in the future. \n\nWhile Image Builder currently lacks built-in lifecycle management for AMIs, this functionality can be addressed through additional scripting or integration with other AWS services. We believe that the benefits of using a managed service outweigh this current limitation. We will re-evaluate this decision as AWS Image Builder evolves and if our lifecycle management needs change significantly. \n","GenTime":"2024-08-28 13:35:39"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"text":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\n\n##Decision\nWe will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","MatchedIds":[2397,276,428,1758,3187],"Prediction":"## Decision\n\nWe will create a single application with two distinct modules:\n\n1. **Topic Management Module:** Responsible for handling topic creation, deletion, and potentially listing existing topics. This module will encapsulate all logic related to managing the lifecycle of topics.\n\n2. **Credential Management Module:** Responsible for generating, storing, and providing credentials required for interacting with specific topics. This module will handle authentication and authorization aspects related to topic access. \n\n**Reasoning:**\n\nWhile the Single Responsibility Principle encourages separation, creating two separate applications introduces potential overhead for communication and deployment.  The tight conceptual connection between topic management and credential provisioning justifies keeping them within the same application. \n\n**Benefits:**\n\n* **Simplified Deployment:**  Deploying and managing a single application is less complex than managing two separate ones.\n* **Centralized Logic:**  Keeping related functionality together promotes better code organization and easier maintenance.\n* **Potential Performance Gains:**  Internal communication between modules within an application can be more efficient than inter-application communication.\n\n**Mitigation Strategies for SRP Concerns:**\n\n* **Clearly Defined Interfaces:** Establish well-defined interfaces between the two modules to minimize coupling and maintain separation of concerns.\n* **Modular Design:** Employ a modular design within the application to ensure that each module remains focused on its specific responsibility.\n* **Future Refactoring:**  If the need arises, the modular design will allow for easier refactoring into separate applications in the future. \n","GenTime":"2024-08-28 13:35:46"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n\n\n##Decision\nWe define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","MatchedIds":[2763,2963,2183,2181,2074],"Prediction":"## Decision\n\nWe will use the following pattern for constructing URIs for data elements on the ODH platform:\n\n```\ntag:<domain>.<tld>,<year>:<path>\n```\n\nWhere:\n\n* **`<domain>.<tld>`** is the reversed domain name of the data owner. This ensures uniqueness across different organisations. For example, `vwt.digital`\n* **`<year>`** represents the year the URI is created. This helps with versioning and tracking data lineage.\n* **`<path>`** represents the hierarchical path to the resource within the data owner's domain. This path should be constructed using meaningful and human-readable attributes that ensure uniqueness. \n\n**Example:**\n\n```\ntag:vwt.digital,2023:sensors\/factory-a\/temperature-sensor-001\n```\n\nThis URI identifies a temperature sensor with ID `temperature-sensor-001` located in `factory-a` under the `sensors` category, owned by an organization with domain `vwt.digital`.\n\n**Guidelines for constructing the `<path>`:**\n\n* Use singular nouns for resource types (e.g., `sensor` instead of `sensors`).\n* Use hyphens to separate words within a path segment (e.g., `temperature-sensor` instead of `temperaturesensor`).\n* Use lowercase letters for better readability and consistency.\n* Avoid using special characters or spaces in the path.\n* Ensure the chosen attributes for the path guarantee the uniqueness of the URI.\n* Consider adding a version or timestamp to the path if the data element is subject to updates or changes.\n\n**Advantages:**\n\n* **Globally unique:** The combination of reversed domain, year, and hierarchical path guarantees the uniqueness of the generated URIs.\n* **Human-readable:** The use of meaningful attributes within the path makes the URIs easy to understand and interpret.\n* **Decentralized:** The URI structure allows different organizations to manage their own URIs without the need for a central authority.\n\n**Considerations:**\n\n* The chosen attributes for the path should be carefully selected to ensure uniqueness and avoid collisions.\n* The length of the URI should be kept reasonable to avoid potential issues with storage and processing.\n* A mechanism for managing and resolving potential conflicts or changes in the data hierarchy should be established. \n","GenTime":"2024-08-28 13:35:57"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"text":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n\n##Decision\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","MatchedIds":[1119,1151,1240,3187,160],"Prediction":"## Decision\n\nChosen option: **[Library Name]**, because it offers a good balance between features, usability and has strong Angular support. \n\n**Justification:**\n\n*   **Angular Integration:** [Library Name] provides dedicated Angular components, making integration straightforward. \n*   **Dependencies:**  While not having the absolute fewest dependencies, [Library Name]'s dependency list is manageable and doesn't introduce significant bloat. \n*   **Features:** [Library Name] allows for creating both nodes and connections and offers sufficient customization options for our use case.\n*   **Usability:** [Library Name] has clear documentation and examples, which will speed up development.\n\n### Positive Consequences\n\n*   Fast integration with our Angular application.\n*   The library provides a solid foundation for visually modeling nodes and edges.\n\n### Negative Consequences\n\n*   We might need to explore workarounds if very specific customization is required beyond the library's standard features.\n\n**Alternatives Considered:**\n\n*   **[Alternative Library 1]:** [Briefly explain why it was not chosen, e.g., too complex, limited Angular support, etc.]\n*   **[Alternative Library 2]:**  [Briefly explain why it was not chosen] \n\n**Additional Notes:**\n\n*   We will create proof-of-concept implementations with [Library Name] to validate its suitability thoroughly. \n*   If specific advanced features become essential, we will re-evaluate our choice. \n","GenTime":"2024-08-28 13:36:06"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"text":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n\n\n##Decision\nLux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","MatchedIds":[2582,2578,2397,2470,5024],"Prediction":"## Decision\n\nBoth Lux and Curate will leverage Emory's existing Shibboleth infrastructure for authentication and authorization.  Given that:\n\n* Both applications already have access to Shibboleth with the same endpoints (prod, test, arch) and required attributes.\n* Neither application requires additional authentication sources for production.\n\nThis approach will minimize development effort and maintenance overhead. \n\nTo accommodate Lux's requirement for new user registration, implement a separate onboarding process within Lux that provisions new user accounts within Emory's identity management system, subsequently granting them access via Shibboleth. This approach ensures centralized user management while fulfilling Lux's specific needs. \n","GenTime":"2024-08-28 13:36:10"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"text":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n\n\n##Decision\nWe will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","MatchedIds":[2710,2183,2181,4021,2261],"Prediction":"## Decision\n\nWe will be using the following schema types for our database development:\n\n* **Application Schema:** Each application will have its own schema. This schema will contain all the tables, views, and other database objects that are specific to that application. This allows for clear separation of concerns and simplifies development and deployment. \n* **Integration Schema:** For integrations with external systems, a dedicated integration schema will be used. This schema will house tables and views necessary for data exchange with the external system, clearly separating integration logic from core application data.\n* **Reference Schema:** This schema will hold common lookup tables and reference data used across multiple applications. Centralizing this data promotes consistency and reduces redundancy.\n\n**Rationale for this approach:**\n\n* **Isolation:** Separating application data into distinct schemas minimizes the risk of conflicts and unintended dependencies between applications.\n* **Security:**  Schema level security can be implemented more effectively, granting access to specific schemas based on application roles and responsibilities.\n* **Maintainability:**  Clear schema separation simplifies database maintenance tasks such as backups, upgrades, and schema changes.\n\n**Further Considerations:**\n\n*  **Naming Conventions:** Consistent naming conventions for schemas, tables, and columns will be essential to ensure clarity and ease of understanding. \n*  **Database User Access:**  Database users will be granted access to specific schemas based on the principle of least privilege. This enhances security and control over data access.\n\nThis schema strategy will be documented in detail, including naming conventions and access control guidelines, and will serve as a standard for all database development going forward. \n","GenTime":"2024-08-28 13:36:19"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"text":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n\n##Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","MatchedIds":[2204,4378,2410,5213,3470],"Prediction":"## Decision\n\nMigrate from using the external service, [Postcodes.io](https:\/\/postcodes.io\/), to the new in-house postcode lookup API.\n\n### Reasons\n\n* **Strategic Alignment:** Aligns with the strategic decision to bring high-value, reusable APIs in-house. This promotes consistency, reduces reliance on external providers, and potentially saves costs across the organization in the long term.\n* **Support and Control:** Utilizing an in-house solution provides greater control over the API's roadmap, feature development, and performance. It also offers direct access to internal support teams for faster issue resolution.\n* **Potential Cost Savings:** While the free tier of Postcodes.io was sufficient, migrating to a centralized in-house solution can potentially reduce costs by eliminating the need for future paid tiers as usage grows across different products. \n\n### Consequences\n\n**Positive:**\n\n* Increased control over the postcode lookup functionality.\n* Faster response times and improved performance due to reduced latency.\n* Enhanced security by keeping sensitive data within the organizational network.\n* Fosters API reusability and reduces redundant implementations across products.\n* Easier integration with other internal systems and data sources.\n\n**Negative:**\n\n* Requires development effort for the migration itself.\n* Potential maintenance and operational overhead for the in-house API.\n* Initial setup and infrastructure costs for hosting and maintaining the new API.\n\n### Mitigation\n\n* **Phased Migration:** Implement a phased approach to migrate from Postcodes.io to the in-house API. This allows for thorough testing and reduces disruption to existing functionalities.\n* **Comprehensive Documentation and Training:** Provide clear documentation and training materials to developers on how to use the new in-house API effectively.\n* **Performance Monitoring and Optimization:** Continuously monitor the performance of the in-house API and implement optimizations as needed to ensure it meets performance requirements.\n","GenTime":"2024-08-28 13:36:30"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"text":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n\n##Decision\nWe decided that LinShare could currently download a file one by one.\\n","MatchedIds":[1091,1647,2880,1643,2140],"Prediction":"## Decision\n\nTo enable multi-file download functionality in LinShare Flutter application, we will leverage the existing `flutter_downloader` library. Instead of downloading files individually, we will implement a mechanism to:\n\n1. **Iterate through the list of selected files for download.**\n2. **For each file, enqueue the download using `flutter_downloader`'s `enqueue` method.** \n3. **Utilize the `taskId` returned by `enqueue` to track the download progress of individual files.**\n\nThis approach allows users to initiate multiple downloads simultaneously while keeping the existing single-file download functionality intact. Additionally, users can still utilize existing functions provided by `flutter_downloader` to clear or cancel the entire download queue if needed.\n","GenTime":"2024-08-28 13:36:35"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"text":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n\n\n##Decision\nWe use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","MatchedIds":[2181,1924,1180,4600,4644],"Prediction":"## Decision\nWe will use [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) (Protobuf) as the standard format for messages exchanged by the routing protocol. \n\n### Reasons for Choosing Protobuf:\n\n* **Efficiency:** Protobuf offers a compact binary format, resulting in smaller message sizes and faster serialization\/deserialization compared to text-based formats like JSON or XML. This is crucial for a routing protocol where performance is paramount.\n* **Strong typing:** Protobuf enforces data types for fields in messages, ensuring data integrity and reducing the likelihood of errors when routers with different implementations communicate.\n* **Extensibility:** The Protobuf schema allows for adding new fields to messages without breaking backwards compatibility. This provides flexibility to evolve the routing protocol over time without disrupting existing deployments.\n* **Language neutrality:** Protobuf supports code generation for a wide array of programming languages, making it suitable for a heterogeneous environment where routers might be implemented in different languages.\n* **Maturity & Tooling:** Protobuf is a mature technology with robust libraries and tooling available for various platforms, simplifying development and debugging.\n\n### Alternatives Considered:\n\n* **JSON:** While human-readable, JSON is less efficient for network transmission and lacks strict type enforcement.\n* **XML:** Similar to JSON, XML is verbose and less performant than Protobuf.\n* **Custom binary format:**  Developing a custom format would introduce complexity and potential interoperability issues.\n\n###  Consequences:\n\n* **Learning curve:** Teams will need to familiarize themselves with Protobuf syntax and tooling.\n* **Schema management:** A centralized process for maintaining and versioning the Protobuf schema will be essential.\n\n###  Next Steps:\n\n* Define the initial Protobuf schema for the core routing messages.\n* Establish a versioning strategy for the schema.\n* Develop or integrate libraries for handling Protobuf serialization\/deserialization in the chosen implementation languages. \n","GenTime":"2024-08-28 13:36:44"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"text":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n\n##Decision\nInstead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","MatchedIds":[4974,1643,3297,1180,1712],"Prediction":"## Decision\nWe will adopt the system's `DownloadManager` for handling background downloads. This decision is based on the following advantages:\n\n- **Robustness:** `DownloadManager` provides built-in handling for HTTP interactions, retries upon failures, and persistence across connectivity changes and system reboots, leading to a more reliable download experience.\n- **System Integration:** Utilizing a system service ensures better optimization with Android's internal mechanisms, potentially reducing resource consumption and improving battery life compared to custom solutions.\n- **Background Operation:**  `DownloadManager` inherently supports background downloads, allowing the app to remain responsive and providing a seamless experience for users. \n- **Progress Monitoring:** The broadcast receiver mechanism allows us to track download progress, handle failures gracefully, and inform the user about the download status.\n\nBy leveraging the `DownloadManager`, we benefit from a well-tested and efficient solution while reducing the complexity and maintenance burden of a custom implementation. \n","GenTime":"2024-08-28 13:36:51"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"text":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n\n\n##Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","MatchedIds":[1278,1307,1310,1289,1279],"Prediction":"## Decision\n\nIntroduce a new \"infos\/limitations\" metadata key for storage plugins.\n\n- This key will hold a structured description of limitations the plugin imposes on the KeySet structure.\n- Examples of limitations to be documented:\n    - Maximum depth of the configuration hierarchy.\n    - Allowed character set for key names.\n    - Support for arrays, including limitations on size and indexing.\n    - Handling of binary data. \n    - Support for metadata, including namespace restrictions.\n- The format of the \"limitations\" metadata will be standardized (e.g., using a simple key-value syntax or a more expressive format like JSON).\n- Tools like `kdb info` will be updated to display these limitations in a user-friendly way.\n\n**Benefits**:\n\n- Enhanced transparency for plugin users about potential compatibility issues.\n- Easier selection of suitable storage plugins based on specific needs.\n- Enables applications to make informed decisions and potentially work around limitations.\n\n**Additional Considerations**:\n\n- Define a clear and consistent format for describing limitations.\n- Ensure the \"limitations\" metadata is kept up-to-date as plugins evolve.\n- Consider providing tools to programmatically query and interpret the limitations metadata. \n","GenTime":"2024-08-28 13:36:57"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4539,2091,138,2261,3784],"Prediction":"## Decision\n\nWe will use Architecture Decision Records (ADRs) as described by Michael Nygard: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions to record architectural decisions going forward. \n\nThe existing documentation in the [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) will be marked as deprecated. This can be achieved by:\n\n* Adding a clear notice at the top of these documents stating they are deprecated and no longer maintained.\n* Providing a link to the new ADR system for current architectural decisions.\n* Considering moving these documents to a separate directory named \"deprecated\" or \"archive\" to clearly distinguish them from active documentation. \n\nThis approach ensures a clean break from the old documentation while preserving its historical value. \n","GenTime":"2024-08-28 13:37:02"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"text":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n\n\n##Decision\n### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","MatchedIds":[1962,1614,1442,405,4354],"Prediction":"## Decision\n\n### Renaming of ABCI messages\n\nThe following ABCI messages dealing with validators shall be renamed as follows:\n\n* `Validator` remains unchanged.\n* `ValidatorUpdate` is renamed to `ValidatorChanged`.\n* `ValidatorSet` is renamed to `ValidatorSetUpdates`.\n\nThe reasoning for these renames is:\n\n* **Clarity:** \n    * `ValidatorChanged` is a more accurate description of the message's purpose, which is to notify of a change in a validator's information.\n    * `ValidatorSetUpdates` better reflects that the message can contain multiple updates to the validator set, rather than a complete set.\n* **Consistency:** \n    * Aligns with the naming convention used for other ABCI messages like `Evidence`.\n\n### Usage of ABCI messages\n\nClarify the expected use cases for the ABCI messages related to validators:\n\n* **`Validator`:** This message is used exclusively during the handshake process (`InitChain`) to inform the application about the initial validator set. \n* **`ValidatorChanged`:** This message is used by Tendermint to inform the application about any changes to the validator set. This includes new validators joining, existing validators being removed, or changes in voting power. It is crucial to note that this message pertains to **individual validator changes** and will be sent for each change within a block.\n* **`ValidatorSetUpdates`:** This message is sent at the end of each block and contains a comprehensive list of all validator changes that occurred within that block. This provides a consolidated view of the validator set updates for applications that require it.\n\n### Removal of Redundancy\n\nWith the introduction of the `ValidatorChanged` message and its consistent usage, the need to send a full `ValidatorSetUpdates` message every block might be deemed redundant for many applications. \n\n**Decision:** Further investigation is required to determine if the `ValidatorSetUpdates` message can be deprecated in favor of relying solely on the `ValidatorChanged` message for all validator updates. This analysis should consider the tradeoffs between potential performance improvements and the impact on applications that rely on the aggregated information provided by `ValidatorSetUpdates`. \n","GenTime":"2024-08-28 13:37:14"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"text":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n\n\n##Decision\nThe raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","MatchedIds":[1016,4234,4747,545,672],"Prediction":"## Decision\n\nA two-stage approach will be taken:\n\n1. **Data Cleaning and Transformation:** We will implement an automated data pipeline, potentially using tools like Apache Spark or Python with libraries like Pandas and DataPrep, to clean, validate, and transform the raw HES data. This pipeline will handle data inconsistencies, missing values, and format discrepancies. The cleaned data will be stored in a structured format. \n\n2. **Storage and Access:** We will utilize a data lakehouse architecture. \n\n    * **Cleaned HES Data:** The cleaned and transformed HES datasets will be stored as partitioned Parquet files in cloud storage (like AWS S3 or Azure Blob Storage). Partitioning will be based on relevant attributes (e.g., year, data source) to improve query performance.\n\n    * **Derived Datasets:**  Derived datasets and variables will be generated from the cleaned HES data using the automated pipeline. These will also be stored as Parquet files in the data lakehouse. \n\n    * **Metadata Management:** A data catalog will be implemented to track data lineage, schema evolution, and provide data discovery capabilities.\n\n    * **Analysis Layer:**  A query engine optimized for large datasets, such as Apache Spark, Presto, or AWS Athena, will be used to provide fast and efficient access to the data for analysis.\n\nThis approach addresses the challenges of data volume, complexity, and the need for efficient access for analysis. By automating the cleaning and transformation process, we ensure data quality and consistency. The data lakehouse architecture provides flexibility, scalability, and cost-effectiveness for storing and querying both raw and processed data. \n","GenTime":"2024-08-28 13:37:21"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"text":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n\n##Decision\nThe frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","MatchedIds":[1240,3715,1180,2181,3470],"Prediction":"## Decision\n\nWe will develop a Single Page Application (SPA) using a Javascript framework. This approach offers several advantages:\n\n* **Responsiveness:** SPAs can deliver a smooth and responsive user experience on a variety of devices, including desktops, laptops, tablets, and smartphones, as they dynamically update content without requiring full page reloads. \n* **API-driven:** This aligns well with our existing API-first approach, allowing for efficient data exchange between the frontend and backend.\n* **Modern Tooling:**  Modern Javascript frameworks come with robust tooling for development, testing, and deployment, improving developer productivity and ensuring code quality. \n\nWe will select a specific Javascript framework (e.g., React, Vue, Angular) in a subsequent ADR, considering factors such as team expertise, project complexity, and ecosystem support.\n","GenTime":"2024-08-28 13:37:27"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"text":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n\n##Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","MatchedIds":[693,2181,1180,276,4307],"Prediction":"## Decision\n\n**Chosen option:** Define all public interfaces using OpenAPI 3.0 (OAS3) and establish a strict module interaction policy enforced through these interfaces.\n\n### Justification\n\n* **Standardized Definition:** OAS3 provides a structured and language-agnostic way to define APIs, including request\/response structures, data types, and error handling. This fosters consistency and clarity across all modules.\n* **Decoupling and Maintainability:**  Clear interface definitions decouple modules, enabling independent development and evolution.  Changes in one module's implementation will not impact others as long as the agreed-upon interface remains intact. \n* **Documentation and Tooling:** OAS3 offers automatic documentation generation, which keeps the API specification up-to-date and accessible. Furthermore, it integrates with various tools for validation, testing (e.g., contract testing), and client code generation.\n* **Flexibility and Future-Proofing:** OAS3 supports defining both RESTful APIs (common for webapps\/workers) and message schemas, suitable for defining asynchronous communication patterns (potentially relevant for worker-database interactions). \n\n### Implementation Details:\n\n1. **Interface Definition:** Each module exposing functionality will have a corresponding OAS3 document defining its public API. This includes the webapp, config module, database interactions (potentially abstracted as a separate service), and any worker-exposed APIs. \n2. **Centralized Repository:**  Store all OAS3 definitions in a version-controlled repository accessible to all developers.\n3. **Code Generation and Validation:** Leverage OAS3 tools to:\n    * Generate client and server stubs to facilitate implementation and ensure adherence to the defined interfaces.\n    * Integrate API validation into the build pipeline to catch interface inconsistencies early on.\n4. **Strict Adherence:** Enforce a strict policy mandating that all inter-module communication must occur solely through the defined OAS3 interfaces. \n5. **Versioning and Evolution:**  Adopt a clear versioning strategy for the OAS3 definitions to manage API changes over time while ensuring backward compatibility whenever possible.\n\n### Positive Consequences:\n\n* Improved maintainability and evolvability of the system due to modularization and clear interface definitions.\n* Reduced inconsistencies and errors resulting from implicit contracts.\n* Enhanced collaboration between teams through a shared understanding of the APIs. \n* Automatic generation of documentation and potential for automated testing.\n\n### Negative Consequences: \n\n* Initial effort required to define the interfaces and adapt existing code.\n* Potential learning curve for developers unfamiliar with OAS3.\n* Enforcing strict adherence to interfaces might require cultural shifts and disciplined development practices. \n\n### Mitigation:\n\n* Provide training and documentation on OAS3 and the chosen tooling.\n* Introduce gradual adoption, starting with critical components or new features.\n* Leverage automated tools to streamline the process and minimize overhead. \n","GenTime":"2024-08-28 13:37:40"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"text":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n\n\n##Decision\nHaving determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","MatchedIds":[2393,3470,1243,2093,1202],"Prediction":"## Decision\n\nWe will use **GitHub Actions** to automate the build, test, and deployment of the HEE National Website Platform. \n\nThis decision is based on the following factors:\n\n* **Integration with GitHub:** As we are already using GitHub for source code management, using GitHub Actions provides a seamless and integrated experience.\n* **Cost-effective:** GitHub Actions offers a generous free tier and competitive pricing for private repositories, making it a cost-effective solution.\n* **Flexibility and Customization:** GitHub Actions allows for flexible workflow customization using YAML syntax. This enables us to tailor the build, test, and deployment pipelines to our specific needs.\n* **Community Support and Resources:** GitHub Actions enjoys strong community support with a vast library of pre-built actions and workflows, easing development and troubleshooting.\n\nBy choosing GitHub Actions, we streamline our development process, improve code quality, and enable faster and more frequent deployments.\n","GenTime":"2024-08-28 13:37:45"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"text":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n\n##Decision\nUse TypeScript.\\n","MatchedIds":[2652,452,2273,1404,4224],"Prediction":"## Decision\n\nWe will use TypeScript (option 2) to type-check the interaction between components in the main function, rather than relying solely on unit tests with mocks (option 1) or no automated checks (option 3). \n\nWhile unit tests with mocks can provide a level of confidence, they can become brittle and time-consuming to maintain as the codebase grows. TypeScript offers a way to ensure the components are correctly connected at compile time, providing immediate feedback and reducing the likelihood of runtime errors. This approach aligns with the efficiency gains expected with static typing while still maintaining thorough integration testing to validate the overall system behavior. \n","GenTime":"2024-08-28 13:37:49"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"text":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n\n\n##Decision\n### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","MatchedIds":[5163,810,3831,84,1924],"Prediction":"## Decision\n\nIntroduce a new field called `operation` in the binding request object.\n\n```json\n{\n  \"operation\": \"create|delete|get|list\",\n  \"data\": {\n    \/* binding specific data *\/\n  },\n  \"metadata\": {\n    \/* binding specific metadata *\/\n  }\n}\n```\n\n### Positive Consequences\n\n* Provides a clear and consistent way to specify the desired operation for a binding.\n* Allows for a single API endpoint to handle both input and output bindings.\n* Improves code readability and maintainability.\n\n### Negative Consequences\n\n* Introduces a breaking change to the existing API.\n* Requires updating client applications to include the `operation` field in their requests.\n\n### Mitigation\n\n* Provide clear documentation on the new API change and how to migrate existing applications.\n* Release a new major version of the SDKs to reflect the breaking change.\n* Potentially offer a compatibility layer for a limited time to ease the transition for existing users. \n","GenTime":"2024-08-28 13:37:55"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"text":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n\n##Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","MatchedIds":[1982,1956,1949,1985,4097],"Prediction":"## Decision\n\nGiven the age, technical debt, and identified weaknesses of the existing `p2p` package, a complete refactor is approved. This refactor will prioritize:\n\n1. **Modularity and Clean Abstractions:** Decoupling components and establishing clear interfaces will improve maintainability, testability, and extensibility.\n2. **Security:** Addressing known DoS vulnerabilities and hardening the codebase against potential attack vectors is paramount. \n3. **Performance:** Exploring more performant transport protocols like QUIC and optimizing data structures and algorithms will be investigated.\n4. **Standardization:** Where possible, leveraging widely adopted protocols and libraries instead of custom solutions will reduce complexity and improve interoperability. This aligns with Informal Systems' request to adopt QUIC for `Tendermint-rs`, potentially leading to a shared transport layer across implementations.\n\nThis refactor will be broken down into manageable phases with separate ADRs outlining the design and implementation details of each phase. Each ADR will undergo thorough review and discussion before implementation.\n","GenTime":"2024-08-28 13:38:00"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","MatchedIds":[4062,794,4804,3820,1145],"Prediction":"## Context and Problem Statement\nOur application needs to handle a growing volume of data and user requests. We want to ensure scalability and high availability. How do we design the architecture to best accommodate future growth and maintain responsiveness? \n## Decision Drivers\n* **Scalability:** The system must be able to handle increasing amounts of data and user traffic.\n* **High Availability:**  The system should remain operational even in the event of hardware or software failures. \n* **Cost-Effectiveness:** The solution should be implemented in a cost-effective manner, avoiding unnecessary expenses.\n* **Maintainability:** The architecture should be easy to understand, modify, and maintain over time. \n## Decision\nTBD \n","GenTime":"2024-08-28 13:38:04"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"text":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n\n##Decision\nViewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","MatchedIds":[3722,1545,4021,2488,1837],"Prediction":"## Decision\n\nWe will develop a customized Grant Management System (GMS) using a microservices architecture. \n\nThis approach provides the following advantages:\n\n* **Scalability and Flexibility:** Microservices allow for independent scaling of individual components, accommodating future growth and feature additions.\n* **Maintainability:** Smaller, independent services are easier to understand, develop, and maintain compared to a monolithic application. \n* **Technology Diversity:**  Different services can utilize the most appropriate technologies, allowing us to leverage existing expertise and integrate with external systems effectively.\n\n**Key components of the GMS:**\n\n* **User Management Service:** Handles user authentication, authorization, and role-based access control.\n* **Application Service:** Manages the application process, including form submissions, validation, and progress tracking.\n* **Review Service:** Facilitates the review process, assigning reviewers, collecting feedback, and managing scoring.\n* **Decision Engine:** Automates decision-making based on predefined criteria and reviewer input.\n* **Notification Service:**  Keeps applicants and reviewers informed about the status of their applications and reviews.\n* **Reporting & Analytics Service:**  Provides insights into application trends, success rates, and other relevant metrics.\n\n**Technology Choices:**\n\n* **Programming Language:**  Node.js for its asynchronous nature and large ecosystem of libraries.\n* **Database:** PostgreSQL for its robust feature set and ability to handle relational data effectively.\n* **Message Queue:**  RabbitMQ to facilitate asynchronous communication between microservices. \n* **API Gateway:** Kong for routing requests, managing API versions, and enforcing security policies.\n* **Containerization & Orchestration:** Docker and Kubernetes for packaging and managing the deployment of microservices.\n\nThis approach will enable us to build a robust, scalable, and maintainable Grant Management System capable of adapting to future requirements. \n","GenTime":"2024-08-28 13:38:13"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"text":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n\n\n##Decision\nWe start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","MatchedIds":[3488,2471,4358,302,3657],"Prediction":"## Decision\n\nTo accelerate learning and minimize time-to-market, we will utilize a Platform-as-a-Service (PaaS) solution for our initial deployment. This approach offers several benefits:\n\n* **Faster Learning Curve:** PaaS abstracts away much of the underlying infrastructure complexity, allowing the team to focus on core application logic and Clojure ecosystem specifics. \n* **Rapid Prototyping and Deployment:** PaaS simplifies deployment pipelines and provides readily available scaling and resilience features.\n* **Reduced Operational Overhead:** Platform-managed infrastructure reduces the operational burden on the team, freeing up resources for development and optimization.\n\nWe will specifically target a PaaS that supports:\n\n* **Clojure Deployments:**  Ensuring seamless integration with our chosen language.\n* **Autoscaling:**  Dynamically adjusting resources based on real-time demand.\n* **Resilience Features:** Built-in redundancy, failover mechanisms, and health checks.\n* **Multi-Cloud Potential:**  The ability to port or extend the application across multiple cloud providers in the future.\n\nExamples of suitable PaaS options include:\n\n* **Heroku:** Supports Clojure deployments and offers a robust set of features for scalability, resilience, and global deployment.\n* **AWS Elastic Beanstalk:**  Provides a more infrastructure-as-code approach with flexibility and integration within the AWS ecosystem.\n* **Google App Engine:** Offers a fully managed environment well-suited for scalable web applications and microservices.\n\nOnce the application matures and the team gains deeper expertise, we can revisit the decision to migrate to a more customized infrastructure setup for greater control and cost optimization. \n","GenTime":"2024-08-28 13:38:22"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"text":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n\n##Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","MatchedIds":[4832,5257,3657,1758,2221],"Prediction":"## Decision\nWe will use PostgreSQL as our database. It provides strong data consistency, a robust feature set including JSON support for flexible data structures, and it integrates well with most backend frameworks.\n\n### Consequences: \nWhile PostgreSQL is a powerful and versatile choice, it may be more complex to set up and manage compared to simpler database solutions like SQLite. If the data structure is very simple and the application doesn't require complex querying,  using SQLite could be sufficient and easier to manage for a smaller project. \n","GenTime":"2024-08-28 13:38:25"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"text":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n\n\n##Decision\nWe decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","MatchedIds":[4601,798,856,4618,4609],"Prediction":"## Decision\n\nWe will implement a debouncing mechanism for profile\/scene deployments to mitigate the risk of DDoS attacks and reduce wasted bandwidth. This will involve the following:\n\n**1. Introduce a Delay Timer:**\n- After a user triggers a profile\/scene deployment (e.g., changing avatar, muting\/unmuting), initiate a delay timer. \n- The timer duration should be configurable and set to a value that strikes a balance between responsiveness and preventing abuse (e.g., 5-10 seconds).\n\n**2. Queue Deployments:**\n- During the delay timer's countdown, queue up any subsequent deployment requests from the same user. \n- Discard duplicate requests within the queue.\n\n**3. Execute the Latest Deployment:**\n- Once the delay timer expires, execute only the latest deployment request in the queue. This ensures that only the most recent user action triggers a deployment.\n\n**4. Rate Limiting (Optional):**\n- To further enhance protection against DDoS attacks, consider implementing rate limiting at the server-side. This would restrict the number of deployment requests from a single user or IP address within a specific timeframe.\n\n**5. Monitoring and Adjustment:**\n- Continuously monitor the system for performance and potential abuse after implementation. Adjust the delay timer duration and rate limiting parameters as needed to optimize performance and security.\n\n**Rationale:**\n\nThis approach addresses the problem by introducing a controlled delay before executing deployments. This prevents rapid-fire requests from overwhelming the system and reduces unnecessary bandwidth consumption. By queueing and executing only the latest request, we maintain responsiveness while minimizing redundant deployments.  \n","GenTime":"2024-08-28 13:38:33"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"text":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n\n\n##Decision\nUse a Value Object wherever possible.\\n","MatchedIds":[2498,3853,1418,1193,1031],"Prediction":"## Decision\nWe will prioritize the use of Value Objects (VOs) when modeling data structures within the `Wegwijs` project. This means:\n\n1. **Identifying Value Object Candidates:** Actively look for opportunities to represent data as VOs, particularly when:\n    - The data represents a conceptual whole.\n    - The data is immutable.\n    - The data's equality is determined by its value, not its identity.\n2. **Implementing Value Objects:** Define specific classes or types for each VO, ensuring they encapsulate:\n    - **Data:** The relevant attributes of the value object.\n    - **Behavior:** Any operations or validations specific to that value, ensuring immutability.\n    - **Equality Logic:** Override equality operators to compare VOs based on their values.\n3. **Enforcing Immutability:**  Design VOs to be immutable after creation. This can involve using read-only properties or other language-specific features.\n4. **Leveraging Libraries (Optional):** Explore and consider using libraries or language features that provide specific support for defining and working with value objects.\n\nThis increased adoption of VOs will enhance type safety, improve code clarity, and reduce the risk of bugs related to accidental data mutations.\n","GenTime":"2024-08-28 13:38:42"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"text":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n\n##Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","MatchedIds":[3203,3853,5070,1418,4271],"Prediction":"## Decision\nWe will enforce a comprehensive documentation strategy at all levels of the software development lifecycle. This includes:\n\n**1. Code-Level Documentation:**\n*   **Self-Documenting Code:**  We will prioritize clean code principles, using clear and concise naming conventions for variables, functions, and classes. Code should be readable and its purpose understandable without extensive comments.\n*   **Inline Comments:** Comments will be used sparingly to explain the \"why\" behind decisions, not just the \"what.\" They should focus on complex logic or non-obvious implementation details. \n*   **Docstrings\/Docblocks:**  For all public APIs, functions, classes, and modules, we will use standardized docstrings\/docblocks (e.g., JSDoc for JavaScript, reStructuredText for Python). These will be used to automatically generate API documentation.\n\n**2. Architecture and Design Documentation:**\n*   **Architectural Decision Records (ADRs):**  We will use ADRs to capture significant architectural decisions, their context, consequences, and tradeoffs. This provides a clear history of architectural evolution.\n*   **Diagrams:**  Visualizations such as UML diagrams, flowcharts, and data models will be used to illustrate relationships and structure within the codebase.\n*   **README Files:** Each repository and significant subdirectory will have a README file providing an overview of its purpose, how to build\/run it, and links to further documentation.\n\n**3. Project and Feature Documentation:**\n*   **Project Wiki\/Internal Documentation Platform:** A centralized location will be used to store high-level documentation about the project's goals, user stories, technical design, and deployment processes.\n*   **User Guides\/Tutorials:** If applicable, user-facing documentation will be created to help users understand and interact with the software.\n\n**Tooling:**\n\n*   To streamline documentation, we will adopt tools to automate the generation and management of documentation, such as:\n    *   **Documentation Generators:** (e.g., Sphinx, Doxygen, JSDoc)\n    *   **API Documentation Browsers:** (e.g., Swagger UI)\n*   We will integrate these tools into our CI\/CD pipeline to ensure documentation is kept up-to-date and easily accessible.\n\n**Culture of Documentation:** \n\n*   We will promote a culture where documentation is seen as a core part of the development process, not an afterthought. \n*   Regular code reviews will include a check for documentation quality and completeness.\n\nBy implementing these strategies, we aim to make our codebase accessible and understandable to all stakeholders, regardless of their familiarity with the code itself, leading to improved collaboration, maintainability, and knowledge sharing within the team. \n","GenTime":"2024-08-28 13:38:55"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"text":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n\n\n##Decision\nWe implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","MatchedIds":[545,2183,1514,2410,4292],"Prediction":"## Decision\n\nRefactor the MOLGENIS \"Armadillo\" service to use a plugin architecture for data provider integration. \n\n### Rationale\n\nThis approach addresses the desired outcomes by:\n\n* **Usability for other parties:**  A plugin system allows other organizations to develop and integrate their own data providers without modifying the core Armadillo service. \n* **Integration with MOLGENIS:** A dedicated MOLGENIS plugin can be maintained to ensure continued compatibility.\n* **Independent release cycle:** The Armadillo service can have its own release cadence, independent of specific data provider updates.\n* **Development by other parties:**  The open plugin system enables a broader community to contribute and extend the service's data provider capabilities.\n\n### Implementation Considerations\n\n* **Well-defined plugin interface:** Establish a clear and comprehensive interface that data provider plugins must adhere to, ensuring seamless communication and functionality.\n* **Plugin discovery and management:** Implement a mechanism for the Armadillo service to discover, load, and manage available data provider plugins. \n* **Documentation and examples:** Provide thorough documentation and examples for developing and integrating custom data provider plugins.\n* **Security model:** Design a robust security model that governs access and permissions for different data providers within the plugin ecosystem. \n","GenTime":"2024-08-28 13:39:02"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"text":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n\n\n##Decision\n### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","MatchedIds":[885,842,1612,3129,2500],"Prediction":"## Decision\n\n**Option 1: Introduce new manifest types**\n\nCreate distinct manifest types for State Stores, Message Buses, and Bindings. This offers the clearest separation and allows for specific configuration options for each type. This approach enhances type safety and validation but introduces a breaking change requiring updates to existing manifests.\n\n**Option 2: Extend existing manifest type**\n\nAdd a `type` field to the existing manifest to differentiate between State Stores, Message Buses, and Bindings. This minimizes disruption by preserving compatibility with existing manifests. However, it might lead to a less clean manifest structure and could require conditional logic based on the `type` field. \n\n**Decision:**\n\nWe choose **Option 1** - Introduce new manifest types. \n\n**Rationale:**\n\nWhile introducing a breaking change, we believe that the long-term benefits of clarity, type safety, and future extensibility outweigh the short-term migration cost.  Clear separation of concerns and dedicated configuration options for each manifest type will improve maintainability and reduce potential errors. \n","GenTime":"2024-08-28 13:39:07"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"text":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n\n\n##Decision\nSolution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","MatchedIds":[2499,2500,428,84,4896],"Prediction":"## Decision\n\nWe will adopt a **message broker** as the central nervous system for our event-driven architecture. This message broker will decouple our modules, providing a robust and scalable platform for asynchronous communication. \n\n**Implementation Details:**\n\n* **Technology Selection:**  We will evaluate popular message brokers like RabbitMQ, Kafka, and Amazon SQS based on our specific needs (throughput, message persistence, fault tolerance, etc.). \n* **Event Schema Definition:** A clear and well-defined event schema (using formats like JSON or Avro) will ensure interoperability between modules.\n* **Error Handling and Monitoring:** Strategies for handling message failures, retries, and dead-letter queues will be implemented. Robust monitoring of the message broker will be essential to ensure system health and identify potential bottlenecks.\n \n**Justification:**\n\nA message broker provides numerous benefits for inter-modular communication in an event-driven architecture:\n\n* **Decoupling:** Modules remain independent and can evolve without impacting other parts of the system.\n* **Scalability:** The message broker can handle increasing message volumes as the system grows.\n* **Flexibility:**  New modules can be easily integrated into the system by subscribing to relevant events.\n\n**Future Considerations:**\n\n*  Explore event streaming platforms like Kafka for more advanced use cases requiring real-time data processing. \n*  Investigate serverless options like AWS Lambda or Azure Functions triggered by events for increased scalability and cost-effectiveness. \n","GenTime":"2024-08-28 13:39:15"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"text":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n\n##Decision\nA RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","MatchedIds":[2520,1574,4266,4747,3071],"Prediction":"## Decision\n**Chosen option:** PostgreSQL \n\n**Reasoning:** PostgreSQL strikes a good balance between simplicity, power, reliability, flexibility, and widespread understanding. \n\n* **Simple:** While powerful, PostgreSQL is relatively straightforward to set up and use, especially for common use cases. \n* **Powerful:** PostgreSQL is a robust relational database capable of handling large datasets and complex queries, offering ample room for growth as CHIP's data needs evolve.\n* **Reliable:** PostgreSQL is known for its data integrity and stability, making it a trustworthy choice for critical application data.\n* **Flexible:** PostgreSQL supports both structured and unstructured data (using JSON fields) offering flexibility in data modeling.\n* **Widely Understood:**  PostgreSQL benefits from a large and active community, abundant learning resources, and readily available expertise.\n\nThis approach ensures CHIP's data storage can accommodate future growth and evolving requirements without needing a complete overhaul.\n","GenTime":"2024-08-28 13:39:20"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","MatchedIds":[3421,2256,5057,3872,578],"Prediction":"## Decision\n\nWe will use an **asymmetric signing algorithm, either RSA or ECDSA**, for generating and verifying JWT signatures. \n\n## Reasoning\n\n* **Enhanced Security with Multi-tenancy:**  Asymmetric algorithms allow each tenant to have their own unique public\/private key pair.  This eliminates the security risk of a single shared secret (as in HMAC) compromising all tenants.\n* **Scalability and Key Rotation:**  Public key infrastructure simplifies key management in a multi-tenant environment. Each tenant can rotate their keys independently without impacting others.\n* **Industry Standards and Interoperability:**  RSA and ECDSA are widely supported algorithms for JWTs, ensuring compatibility with various libraries and tools.\n\n## Consequences\n\n* **Increased Complexity:** Asymmetric cryptography is generally more complex to implement than symmetric cryptography (HMAC).\n* **Performance Considerations:** Signature generation and verification using asymmetric algorithms are computationally more expensive than HMAC.  However, this difference is often negligible in real-world scenarios, and caching mechanisms can mitigate potential performance impacts. \n\n## Status\n\nAccepted\n\nThis decision aligns with the requirement for secure multi-tenant JWT implementation and provides a robust and scalable solution. \n","GenTime":"2024-08-28 13:39:26"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"text":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n\n\n##Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","MatchedIds":[2802,528,399,598,527],"Prediction":"## Decision\n\nThe decision is to **implement a custom serialization and deserialization method for the `steps` attribute** instead of using existing libraries. \n\nThis approach will address the concerns raised:\n\n- **Preserves Order:** By implementing our own logic, we can explicitly define the serialization to represent the ordered key-value pairs of the map. This could involve converting the map into an array of key-value pairs before serialization, ensuring the order is maintained.\n- **Transparency and Consistency:** A custom implementation will make the serialization and deserialization process clear and understandable for all developers working on the Zooniverse codebase, regardless of the client library they use (JavaScript, Ruby, or Python). This transparency allows for easier maintenance and consistency across different parts of the Zooniverse platform.\n- **Control and Flexibility:**  Directly managing the conversion process gives us more fine-grained control over how the `steps` attribute is represented in JSON format. It also provides flexibility for future modifications or additions to the serialization logic if needed. \n\nWhile this approach might require a slightly larger initial implementation effort, the benefits of clarity, maintainability, and consistency across the Zooniverse ecosystem outweigh the drawbacks of using a third-party library with inherent limitations for our use case. \n","GenTime":"2024-08-28 13:39:33"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"text":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n\n\n##Decision\nImplement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","MatchedIds":[3203,1193,1151,4644,78],"Prediction":"## Decision\n\nWe will adopt a two-pronged approach to transition to a library of composable functions while maintaining backwards compatibility for existing users:\n\n**Phase 1: Facade Pattern and Internal Refactoring**\n\n1. **Introduce a Facade Layer:** Create a new facade layer that mirrors the existing `tamr_unify_client` API. This layer will provide the same interface that existing users rely on, ensuring backward compatibility.\n\n2. **Internal Refactoring:** Behind the facade layer, start refactoring the codebase into composable functions. These functions should encapsulate specific functionalities and be independently testable and reusable.\n\n3. **Gradual Migration:** Gradually migrate the logic from the existing implementation behind the facade to the new composable functions. The facade will act as an abstraction layer, shielding users from these internal changes. \n\n**Phase 2:  Exposing Composable Functions**\n\n1. **Introduce New API:**  Introduce a new API for the library of composable functions. This API should be well-documented and highlight the benefits of using this approach.\n\n2. **Provide Migration Guidance:** Offer comprehensive documentation and examples to guide existing users on how to transition from the legacy API to the new composable functions.\n\n3. **Deprecation Plan:** Clearly communicate a deprecation timeline for the legacy `tamr_unify_client` API. This allows users sufficient time to migrate to the new API.\n\n**Additional Considerations:**\n\n* **Versioning:** Implement clear versioning for both the legacy and new APIs to manage dependencies and communicate changes effectively.\n* **Documentation and Examples:**  Provide comprehensive documentation and practical examples to illustrate the usage of both the legacy and new APIs.\n* **Testing:** Implement rigorous testing at all levels - unit tests for individual composable functions, integration tests for the facade layer, and end-to-end tests for the overall system.\n\nThis approach balances the need for backward compatibility with the long-term benefits of migrating to a more modular and reusable codebase. \n","GenTime":"2024-08-28 13:39:44"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"text":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n\n##Decision\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","MatchedIds":[442,446,445,443,5009],"Prediction":"## Decision\n\nGiven that the site crash issues have been resolved, we will revert to extracting data from the `Grant-wise expenditure` section on the Koshvani platform. This approach offers the following advantages:\n\n- **Data Granularity:**  The `Grant-wise expenditure` section provides data at a more granular level compared to the `DDO-wise expenditure` section, aligning better with the project's analytical goals. \n- **Reduced Data Processing:** Directly using the `Grant-wise expenditure` data eliminates the need for additional aggregation and processing steps that would have been required when using `DDO-wise expenditure` data.\n- **Consistency:** Utilizing a single, consistent data source (`Grant-wise expenditure`) minimizes potential discrepancies and complexity in data analysis. \n\nWe will implement robust error handling and logging mechanisms within the data extraction process to mitigate risks associated with potential future site instabilities.  \n","GenTime":"2024-08-28 13:39:52"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"text":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n\n##Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","MatchedIds":[138,5009,4804,3820,1145],"Prediction":"## Decision\n\nInstead of adding a generic `--require` parameter, we will modify lifecorp-init to make the `path_url` prompt **mandatory** when creating a web1 app.  This means users will be explicitly prompted for the URL and cannot proceed without providing a value. \n\n## Consequences\n\n* **Improved User Experience:**  Developers won't have to manually update the HTML file later, aligning with the goal of reaching a \"Hello World\" state quickly.\n* **Reduced Errors:** Prevents potential errors from missing or incorrect URL paths.\n* **No Additional Complexity:**  Avoids introducing a generic `--require` parameter, keeping the lifecorp-init command simpler for common use cases.\n\n## Status\n\nAccepted \n","GenTime":"2024-08-28 13:39:58"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"text":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n\n\n##Decision\nWe will use Dutch terms because it is an external requirement dictated by the organisation.\\n","MatchedIds":[598,3715,4878,2181,3159],"Prediction":"## Decision\n\nWe will use English for all API endpoints, UI elements, and resource field names. \n\nWhile the primary users of the application are part of the Flemish Government, using English offers several advantages:\n\n* **Wider reach and interoperability:**  Using English makes the API accessible and understandable to a broader audience, including developers outside the Flemish region who may interact with the data or integrate with the API in the future.\n* **Technical standard:**  English is the de facto language for software development and API documentation. Sticking to this standard improves the developer experience and allows for easier integration with existing tools and libraries.\n* **Future-proofing:**  Using English makes the application more adaptable to potential future expansions or collaborations beyond the Flemish Government. \n\nWhile using Dutch might seem intuitive for the primary user base, it could lead to unnecessary complications and limitations in the long run. \n\nTo ensure clarity and accessibility for Flemish users, we will provide comprehensive documentation and translations for all user-facing elements within the UI. This approach balances user needs with the technical benefits of using English as the primary language for the API and resource fields. \n","GenTime":"2024-08-28 13:40:04"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"text":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n\n\n##Decision\nI propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","MatchedIds":[4361,4699,4428,4408,3119],"Prediction":"## Decision\n\nWe will rename the project to avoid confusion with the existing `ashlar` package on PyPi. \n\nWhile finding a new name is non-trivial, the branding and potential user confusion outweighs the difficulty of choosing a new name. We will select a new name from the proposed options or explore other alternatives that fit the project's core functionality and purpose. \n","GenTime":"2024-08-28 13:40:07"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4428,4408,3119,1436,294],"Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions \n","GenTime":"2024-08-28 22:23:51"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","MatchedIds":[2145,2897,2122,2892,2134],"Prediction":"## Decision\n\nTo improve read latency for smaller blobs and optimize James's performance, we will implement a two-tiered storage system using both Object Storage and Cassandra:\n\n**1. Object Storage (e.g., S3, Swift):**\n\n- Remains the primary storage for all blobs. \n- Well-suited for larger blobs (e.g., message bodies) and infrequent access due to its cost-effectiveness and durability.\n\n**2. Cassandra Cache:**\n\n- Implement a dedicated Cassandra keyspace acting as a write-through cache for frequently accessed, small blobs (e.g., message headers).\n- **Configuration:**\n    - **Replication Factor:** Set to 1 for speed, as data is also persisted in Object Storage.\n    - **Consistency Level:**  Use ONE for fast reads, accepting the possibility of slightly stale data.\n    - **TTL (Time-to-live):** Configure a reasonable TTL to automatically evict older cached data based on access patterns.\n    - **Size Threshold:** Define a maximum blob size eligible for caching in Cassandra. \n    - **Compaction Strategy:** Utilize TimeWindow compaction strategy for efficient handling of time-based data eviction.\n\n**Data Flow:**\n\n1. **Write:** When a new blob is written:\n   - Store the blob in Object Storage.\n   - If the blob size is below the configured threshold, asynchronously write the blob to the Cassandra cache.\n\n2. **Read:** When a blob is requested:\n   - Attempt to read the blob from the Cassandra cache.\n   - **Cache Hit:** Return the blob from the cache.\n   - **Cache Miss:** \n      - Fetch the blob from Object Storage.\n      - If the blob size is below the threshold, asynchronously cache it in Cassandra for future requests.\n\n**Benefits:**\n\n- **Improved Read Latency:** Faster retrieval of small, frequently accessed blobs from Cassandra.\n- **Reduced Object Storage Costs:** Fewer requests to Object Storage for cached data, potentially lowering costs.\n- **Leverages Existing Technology:** Utilizing Cassandra avoids introducing a new technology stack. \n\n**Monitoring and Evaluation:**\n\n- Implement monitoring to track cache hit\/miss rates, read latencies, and Cassandra performance. \n- This data will help fine-tune the cache configuration (TTL, size threshold) for optimal performance.\n- Run performance tests before and after implementation to quantify the improvements and identify any potential bottlenecks. \n","GenTime":"2024-08-28 22:24:02"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4408,3119,1436,294],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:04"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,3119,1436,294],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:06"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[3323,452,2273,4359,4785],"Prediction":"## Decision\n\nWe will use Jest as our testing framework. Jest is widely regarded as the standard for React applications, and its robust TypeScript support aligns well with our project requirements. \n","GenTime":"2024-08-28 22:24:08"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[3820,1145,4616,3546,4963],"Prediction":"Please provide the context and influencing factors for the decision. For example:\n\n**Context:** \n\nOur application needs to handle a growing number of user requests.  We are currently using a monolithic architecture, which is proving difficult to scale and maintain. We are also facing pressure to deliver new features more quickly. \n\n**OR**\n\n**Context:**\n\nWe need to choose a message broker for our microservices architecture.  We need a solution that is reliable, scalable, and supports a variety of messaging patterns. Our team has experience with RabbitMQ and Kafka.\n\nOnce you provide this information, I can help you make an informed architectural decision. \n","GenTime":"2024-08-28 22:24:12"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,1436,294],"Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions \n","GenTime":"2024-08-28 22:24:14"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,294],"Prediction":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions \n","GenTime":"2024-08-28 22:24:17"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","MatchedIds":[2896,2897,2122,2892,2134],"Prediction":"## Decision\n\n**Introduce a two-tiered storage system for blobs, leveraging both Object Storage and Cassandra to optimize for different access patterns.**\n\n* **Object Storage:** Retain Object Storage for storing large blobs (e.g., message bodies) where storage cost is a primary concern and access speed is less critical. \n* **Cassandra Cache:** Implement a dedicated Cassandra keyspace to act as a cache for small, frequently accessed blobs (e.g., message headers). This will leverage Cassandra's low latency reads for optimal performance.\n\n**Implementation Details:**\n\n* **Blob Size Threshold:** Configure a threshold to determine which blobs are cached in Cassandra. Blobs smaller than the threshold will be written to both Cassandra and Object Storage, while larger blobs will only be stored in Object Storage.\n* **Cache Population:** The cache will be populated on write operations. For blobs smaller than the threshold, write the blob to both Cassandra and Object Storage. \n* **Read Path:** \n    * On read, first check if the blob exists in the Cassandra cache. \n    * If present, serve the blob directly from Cassandra. \n    * If not, fetch the blob from Object Storage and store it in the Cassandra cache for future accesses, if its size is below the threshold.\n* **Cache Eviction:** Implement a Time-To-Live (TTL) policy on the cached blobs in Cassandra to prevent the cache from growing indefinitely. The TTL should be configurable based on access patterns and storage capacity.\n* **Consistency:** Utilize a consistency level of `LOCAL_QUORUM` for both reads and writes to the Cassandra cache. This will ensure high availability and data consistency within a data center.\n\n**Benefits:**\n\n* **Improved Performance:** Significantly reduce latency for reading small, frequently accessed blobs by serving them directly from the low-latency Cassandra cache. \n* **Cost-Effective:** Optimize storage costs by leveraging Object Storage for large blobs while using Cassandra's performance for frequently accessed data.\n* **Flexibility:** Maintain the flexibility to tune the system by adjusting the blob size threshold and cache TTL based on evolving needs and access patterns.\n\n**Monitoring and Testing:**\n\n* Implement monitoring of cache hit\/miss rates to assess performance gains and identify potential bottlenecks.\n* Conduct thorough performance testing to validate the effectiveness of the caching strategy and fine-tune the configuration (e.g., blob size threshold, TTL, Cassandra resources). \n","GenTime":"2024-08-28 22:24:28"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:30"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:32"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","MatchedIds":[1258,4359,4868,3572,3853],"Prediction":"## Decision\nIntegrate a continuous integration tool like **Jenkins** or **GitHub Actions** to run automated tests on every pull request and merge. Use a coverage analysis tool like **SonarQube** or **Codecov** to track coverage metrics and identify gaps. Set up alerts to notify developers of coverage regressions. \n","GenTime":"2024-08-28 22:24:34"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:37"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:24:39"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:24:41"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"text":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n\n\n##Decision\nWe have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Prediction":"## Decision\n\n**We will prioritize the anonymization of data entities that meet at least two of the defined criteria:**\n\n1. **Directly Identifying or Close Proxy:** This includes information like names, phone numbers, email addresses, physical addresses, IP addresses, and any unique identifiers.\n2. **\"Special Categories\" of Information:**  We will prioritize anonymizing sensitive data classified as \"special categories,\" such as medical or health information, racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, and data about sexual orientation or sexual activity. \n3. **Present in Non-Trivial Quantities:**  Data entities occurring frequently within the dataset will be prioritized for anonymization. This ensures we address the most prevalent privacy risks and maximize the impact of our efforts within the given time constraints. \n\n**This approach allows us to focus on the most critical data for anonymization, balancing privacy concerns with the project's time constraints and the performance impact on the model.** \n","GenTime":"2024-08-28 22:24:47"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","MatchedIds":[4443,578,3470,2294,2397],"Prediction":"## Decision\n\nWe will initially build a Node.js client library that leverages Passport.js for authentication and authorization. This will offer the smoothest experience for users already familiar with this stack. \n\nTo accommodate users of other languages and frameworks, we will define a language-agnostic RESTful API. This API will provide comprehensive documentation and examples to simplify integration regardless of the user's technology stack.\n\nIn the future, we will prioritize building client libraries for other popular languages and frameworks based on user demand and feedback. \n","GenTime":"2024-08-28 22:24:50"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:24:52"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","MatchedIds":[581,4442,1180,915,920],"Prediction":"## Decision\nWe will create a Stub Verify Hub that will simulate the behaviour of the real Verify Hub for the purposes of user research and testing.\n\n**Functionality:**\n\n* The Stub Hub will **not** have a live integration with Verify.\n* It will provide a mock interface that mimics the expected interactions between the Service Provider and the real Verify Hub. \n* This includes:\n    * Receiving and parsing authentication requests from the Service Provider.\n    * Displaying mock Verify journeys to the user, including:\n        * Simulated user account selection\n        * Mock identity provider selection\n        * Success and failure scenarios\n    * Redirecting the user back to the Service Provider with a simulated SAML response containing predefined user attributes based on the chosen scenario.\n\n**Implementation Details:**\n\n* The Stub Hub will be a lightweight application, potentially using a framework like Node.js or Python with a templating engine for the mock user interfaces.\n* The mock Verify journeys and associated user data will be configurable to allow for testing different user flows and scenarios.\n* The simulated SAML responses will be configurable to test different user attributes and assertion data.\n\n**Benefits:**\n\n* **Controlled Testing Environment:** The Stub Hub allows for controlled and repeatable testing of the Service Provider integration with Verify without relying on a live connection to the real Hub.\n* **Realistic User Journeys:** By simulating the Verify user interface, the Stub Hub allows researchers to gather feedback on the complete user journey, including aspects related to Verify itself.\n* **Flexibility and Iteration:** The configurable nature of the Stub Hub enables quick iteration and testing of different user flows, scenarios, and response data.\n\n**Deployment:**\n\n* The Stub Hub will be deployed to an environment that is easily accessible by the user research participants. This could be a publicly accessible server or a local development environment.\n\nThis approach allows for comprehensive user research and testing of the Service Provider's integration with Verify in a controlled and flexible manner.\n","GenTime":"2024-08-28 22:25:01"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,1145,4616,3546,4963],"Prediction":"Please provide the context and influencing factors for the decision.  For example:\n\n**Context:** \n\nOur application needs to securely store user passwords. We need to choose a method that is considered best practice and meets our security requirements. \n\n**OR**\n\n**Context:**\n\nWe are currently using a monolithic architecture and need to decompose it into microservices. We need to decide on a method for inter-service communication that allows for loose coupling and scalability.\n\nOnce you provide the context, I can give you an appropriate architectural decision. \n","GenTime":"2024-08-28 22:25:05"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[1408,4226,3321,3451,5087],"Prediction":"## Decision\n\nWe will enforce that everything supported by Prettier has its style enforced by it. We will set up Git hooks to automatically run the formatter before committing.  We will set continuous integration up to reject commits that are not correctly formatted. \n","GenTime":"2024-08-28 22:25:07"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:09"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:11"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","MatchedIds":[577,578,3470,2294,2397],"Prediction":"## Decision\n\nWe will develop a lightweight, language-agnostic API for interacting with our service. To cater to the user currently utilizing Node.js and Passport, we will create a dedicated client library tailored for this framework, ensuring seamless integration with their existing setup. \n\nThis approach prioritizes a smooth onboarding experience for the known user while establishing a foundation for broader adoption. As we gain a clearer understanding of the languages and frameworks employed by other users, we will prioritize developing dedicated client libraries accordingly. \n\nProviding clear API documentation will be crucial in enabling users of unsupported languages and frameworks to interact with the service directly until dedicated client libraries are available.\n","GenTime":"2024-08-28 22:25:15"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[5255,4226,3321,3451,5087],"Prediction":"## Decision\n\nWe will enforce that everything supported by Prettier has its style enforced by it. We will achieve this with the following:\n\n1. **Configuration:** A shared Prettier configuration file will be created and referenced by all projects to ensure a unified style across our codebase.\n2. **Git Hooks:** We will set up a pre-commit Git hook to automatically run Prettier on staged files. This will format code before it's committed, preventing unformatted code from reaching the repository.\n3. **Continuous Integration:** Our CI pipeline will be configured to run Prettier on every pull request and reject any changes that don't adhere to the defined style guide. This ensures that only properly formatted code is merged.\n\nThis approach enforces consistent code style across all projects while minimizing manual intervention by developers. \n","GenTime":"2024-08-28 22:25:20"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:22"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","MatchedIds":[5250,3187,160,4636,4699],"Prediction":"## Decision\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n\nChosen option: **Implement a simple duration field for Voice Over segments**, because it offers a good balance between the decision drivers.\n\n### Justification\nWhile not the most accurate, adding a simple duration field to Voice Over segments allows for quick estimation within the rough cut. This provides users with basic functionality without significant development overhead.\n\n### Positive Consequences\n* **Relatively easy to implement:**  Adding a duration field requires minimal changes to the data model and UI.\n* **Improves rough cut time estimation:** Provides a more comprehensive picture compared to completely omitting voice-over duration. \n\n### Negative Consequences\n* **Less accurate than other options:**  A fixed duration field doesn't account for variations in pacing or potential edits.\n* **Limited extensibility:**  Might not be suitable for more complex scenarios where dynamic calculation or integration with external tools is needed.\n\n### Alternatives considered\n1. **Integrating with a dedicated Voice Over Tool:** This would provide the most accurate time estimations but comes with significant development effort and potential complexity in data exchange.\n2. **Implementing an algorithm based on word count:** This offers a middle ground in terms of accuracy and implementation complexity. However, it still relies on assumptions and might not be suitable for all scenarios.\n\n### Future Considerations\nAs the product evolves, explore integrating with dedicated Voice Over tools or develop more sophisticated algorithms for improved accuracy.  Consider adding functionality to adjust pacing or mark segments with potential edits to refine the estimations further. \n","GenTime":"2024-08-28 22:25:30"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","MatchedIds":[4223,452,2273,4359,4785],"Prediction":"## Decision\n\nWe will use Jest as our testing framework. Jest is widely recognized for its excellent support for React and TypeScript, making it the ideal choice for our project. \n","GenTime":"2024-08-28 22:25:32"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:34"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:36"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","MatchedIds":[2383,3853,4179,1258,1031],"Prediction":"## Decision\nIntegrate a database query performance monitoring tool into the development workflow. This tool should be capable of:\n\n* **Profiling queries**:  Identifying slow or resource-intensive queries during development and testing. \n* **Logging**: Recording query performance statistics for later analysis.\n* **Alerting**: Triggering notifications for queries exceeding predefined performance thresholds.\n\nThis approach provides developers with immediate feedback on query efficiency, preventing performance issues from reaching production. \n","GenTime":"2024-08-28 22:25:39"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:25:41"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","MatchedIds":[4224,458,2296,5255,1408],"Prediction":"## Decision\nWe will check code style using ESLint.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\nWe will use ESLint to automatically fix linting errors in a pre-commit hook. \n","GenTime":"2024-08-28 22:25:44"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","MatchedIds":[4910,4918,4544,4550,4549],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe will adopt **[Nx](https:\/\/nx.dev\/)** as our main build tool and workspace management system.  Nx offers several advantages for our current needs:\n\n* **Fast Build Times:** Nx's computation caching and dependency graph analysis allow for faster builds, especially in large codebases. This directly addresses our need to improve build performance.\n* **Monorepo Management:** Nx provides excellent tools for managing monorepos,  making it easier to share code and libraries between different parts of our application, which is essential in a microservices architecture. \n* **Modern Tooling Integration:**  Nx offers seamless integration with popular frameworks and tools like Angular, React, Jest, Cypress, and more.  This aligns well with our existing technology stack and simplifies our development process.\n\nWhile Nx primarily focuses on the Javascript ecosystem, its capabilities like task running and dependency management extend its benefits beyond a single language.\n\nWe believe that Nx's features and focus on developer experience will significantly enhance our development workflow and address the challenges we have outlined. \n","GenTime":"2024-08-28 22:25:50"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:25:52"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:25:54"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:25:56"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:25:58"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,3820,4616,3546,4963],"Prediction":"Please provide the context and constraints for the architectural decision. I need more information to make an informed decision. \n\nFor example, tell me:\n\n* What system or component are we discussing?\n* What specific problem are we trying to solve?\n* Are there any performance requirements, security concerns, or budget limitations we need to consider?\n* What are the possible solutions we've considered so far?\n\nOnce I have this context, I can provide a well-reasoned architectural decision and outline the change we should implement. \n","GenTime":"2024-08-28 22:26:02"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:26:04"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:26:06"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","MatchedIds":[4547,4918,4544,4550,4549],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\nYou haven't provided any context for me to make a decision on!  Please give me some background information, such as:\n\n* **What problem are you trying to solve?** \n* **What are the options you are considering?**\n* **What are the constraints and requirements?**\n* **Who are the stakeholders involved?**\n\nOnce I have this context, I can help you make an informed decision and document it appropriately. \n","GenTime":"2024-08-28 22:26:10"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:26:12"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:26:14"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","MatchedIds":[5255,1408,3321,3451,5087],"Prediction":"## Decision\nWe will enforce that everything supported by Prettier has its style enforced by it. We will set up Git hooks to automatically run the formatter before committing. We will set continuous integration up to reject commits that are not correctly formatted. \n","GenTime":"2024-08-28 22:26:17"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","MatchedIds":[1739,187,4397,5102,856],"Prediction":"## Decision\n\nThe pipeline has been refactored into separate **build** and **deploy** pipelines to decouple configuration changes from the build process. \n\nHere's how it works:\n\n* **Build Pipeline:**\n    * Builds the application and runs unit tests.\n    * Creates a Docker image and pushes it to a container registry (e.g., ECR, Docker Hub).\n    * Exports build artifacts:\n        * `build.json`: Contains metadata about the build (version, commit hash, etc.).\n        * `src.zip`: Archive of the source code necessary for running integration tests. \n* **Deploy Pipeline:**\n    * Can be triggered by either:\n        * New artifacts from the build pipeline (`build.json`, `src.zip`).\n        * Changes to configuration files (e.g., `app.yaml`, environment variables).\n    * If triggered by configuration changes, the pipeline uses the existing Docker image from the registry, avoiding a rebuild.\n    * Runs integration tests using the provided `src.zip`.\n    * Deploys the application using the new configuration.\n\nThis separation allows for quicker deployments of configuration changes, as it avoids unnecessary rebuilds. It also promotes better organization and maintainability of the CI\/CD process. \n","GenTime":"2024-08-28 22:26:23"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[3194,899,2963,1180,485],"Prediction":"## Decision\nWe will leverage GraphQL's built-in schema stitching capabilities to combine multiple schema files. This approach avoids introducing a new module system specific to our project, reducing complexity and leveraging existing tooling. \n\nTo facilitate this, each independently versioned module will:\n\n1. Define its own schema in a dedicated `.graphql` file.\n2. Expose an entry point for accessing its schema. This can be a function or a designated constant.\n\nThe application will then use a schema stitching library to combine these individual schemas into a single, unified schema. This approach allows for modularity and reusability of GraphQL schema definitions. \n","GenTime":"2024-08-28 22:26:27"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:26:29"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,3820,1145,3546,4963],"Prediction":"Please provide the context and issue motivating the decision. I need more information to provide a relevant architectural decision. \n\nFor example, you could describe:\n\n* **The system you are building**:  What kind of application is it? What are its main functionalities?\n* **The specific challenge you are facing**: Are you choosing a database? Deciding on a caching strategy? Selecting a messaging queue?\n* **Any constraints you are operating under**:  Budget limitations? Time constraints?  Technical limitations? \n* **The desired outcome**: What are you hoping to achieve with this decision? Improved performance? Increased scalability? Reduced cost?\n\nOnce I have this information, I can provide you with a well-reasoned architectural decision. \n","GenTime":"2024-08-28 22:26:33"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:26:35"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","MatchedIds":[5244,160,3657,4021,2488],"Prediction":"## Decision\n\nChosen option: **Versioned SQL migration scripts, stored alongside the code.**\n\n### Justification:\n\nWhile tools like DB Designer are great for visualizing schemas, relying solely on full schema exports for versioning can become difficult to manage and reason about as the project grows. Instead, we'll adopt an incremental approach using versioned migration scripts.\n\n### Positive Consequences:\n\n* **Clear evolution:** Each migration script focuses on a specific change, making it easy to understand the database's evolution over time.\n* **Reproducibility:**  Starting from a clean database, we can apply the migration scripts in order to reach any desired version.\n* **Branching and merging:** Migrations based on diffs are easier to manage when working with branches and merging code.\n* **Rollback capability:**  Each migration script can have an accompanying \"down\" script to revert the changes, facilitating rollbacks if needed.\n\n### Negative consequences:\n\n* **Increased complexity initially:** Requires a bit more setup compared to directly using schema exports. \n* **Potential for drift:** If the development database is modified outside of the migration process, it can lead to discrepancies.\n\n### Implementation Details:\n\n* We will use a library like **Flyway** or **Liquibase** to handle migration script execution. \n* Each migration script will:\n    * Have a unique identifier and a descriptive name tied to the milestone or feature.\n    * Contain SQL statements to apply changes (create\/alter tables, add columns, etc.).\n    * Optionally include a corresponding \"down\" script to revert the changes. \n* Migration scripts will be stored in a dedicated directory within the version control system.\n* CI\/CD pipelines will be adjusted to run migrations automatically when deploying different environments.\n\nThis approach balances ease of understanding, incremental updates, and robustness, making it suitable for the project's milestones and potential future development. \n","GenTime":"2024-08-28 22:26:45"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:26:48"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","MatchedIds":[4414,2397,3421,5024,2470],"Prediction":"## Decision\n* **Use Okta as the Identity Provider (IdP) for Bookit.** This leverages their robust feature set and frees us from managing user authentication directly.\n* **Use Okta's React client for client-side code.** This simplifies integration and provides a pre-built solution for interacting with Okta's authentication services.\n* **Use Okta's Spring Boot 2.x support for the backend.** Opt for the newer integration with Spring Security 5.x provided by Spring Boot 2.x for better security and future-proofing.\n* **Use Okta Access Tokens as Bearer tokens for authorization.** This aligns with standard practices for securing APIs and integrates seamlessly with Okta's authentication flow.\n\n**Justification:** Okta offers a compelling solution as an IdP, providing a robust free tier, multiple integration options, and valuable features like role-based access control and user management dashboards. This decision prioritizes rapid development, scalability, and a simplified authentication architecture.\n","GenTime":"2024-08-28 22:26:53"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:26:55"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:26:57"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","MatchedIds":[4225,3691,152,771,118],"Prediction":"## Decision\nWe will use Dependabot to automatically monitor and create pull requests for dependency updates. This will allow us to keep our dependencies current and secure with minimal manual effort. \n","GenTime":"2024-08-28 22:27:00"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","MatchedIds":[3328,3691,152,771,118],"Prediction":"## Decision\n\nWe will use Dependabot to automatically monitor and create pull requests for dependency updates. This will help us keep our dependencies secure and up-to-date with minimal manual effort. \n","GenTime":"2024-08-28 22:27:02"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","MatchedIds":[3389,4179,1258,5227,1387],"Prediction":"## Decision\n\nUse the `dotenv` gem to load and manage our environment variables.\n\n**Justification:**\n\n* **Actively maintained:** Unlike Figaro, `dotenv` is actively maintained and provides a simple, widely used solution for managing environment variables in Ruby applications. \n* **Early failure:** `dotenv` can be configured to raise an error if a required environment variable is missing, preventing unexpected `nil` values from propagating through the application.\n* **Testability:** `dotenv` allows loading variables from a hash, making it easy to stub environment variables in the test suite without affecting the actual environment.\n\n**Implementation Details:**\n\n* Use `.env` to store environment variables and ensure this file is added to `.gitignore`.\n* Configure `dotenv` to raise an error if mandatory environment variables are missing.\n* In tests, load a hash with specific values for the environment variables needed. This provides isolation and avoids conflicts when running tests in parallel.\n \n**Example:**\n\n```ruby\n# config\/initializers\/dotenv.rb\nrequire 'dotenv\/load'\n\nDotenv.load('.env')\n\n# Accessing environment variables\nDATABASE_URL = ENV.fetch('DATABASE_URL') \n\n# Testing with stubbed environment variables\nit 'uses the feature flag value' do\n  ClimateControl.modify FEATURE_FLAG: 'enabled' do\n    expect(feature_enabled?).to be true\n  end\nend\n```\n\nBy using `dotenv` and a library like `ClimateControl` for testing, we ensure a robust and predictable way to manage environment variables while keeping our tests isolated and maintainable. \n","GenTime":"2024-08-28 22:27:09"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:27:12"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","MatchedIds":[4171,3187,160,4636,4699],"Prediction":"## Decision\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n\nChosen option: **A hybrid approach**, because it balances ease of implementation with the need for accuracy and extensibility. \n\n### Rationale\n* **Option 1: Simple duration input (easiest):** While this is the easiest to implement, it lacks accuracy as voice-over time can vary greatly depending on factors like pacing and script complexity.\n* **Option 2: AI-powered estimation (most accurate):**  This offers the highest potential accuracy but could be complex to implement and integrate, requiring training data and potentially significant computational resources. \n* **Option 3:  Hybrid Approach (balanced):** We start with a simple duration input (Option 1) for immediate ease of use. In parallel, we develop a more sophisticated estimation model (Option 2)  that learns from actual voice-over recordings and user feedback. Over time, this model can be integrated to provide increasingly accurate estimates, enhancing the initial simple solution.\n\n### Positive Consequences\n* **Faster initial delivery:**  The simple duration input allows us to deliver the feature quickly.\n* **Progressive improvement:** The accuracy of estimates will improve over time as the AI model is trained and refined.\n* **Flexibility:** This approach is flexible and adaptable to future changes or refinements in estimation methods.\n\n### Negative consequences \n* **Initial inaccuracy:** The initial estimates based on simple duration will be less accurate. \n* **Development overhead:** Building and integrating the AI-powered estimation will require additional development effort. \n\n### Mitigation\n* **Clearly communicate limitations:**  Make it clear to users that the initial duration-based estimates are approximations and that more accurate estimations will become available as the system learns.\n* **Prioritize AI model development:**  Allocate sufficient resources to develop and integrate the AI-powered estimation model in a timely manner. \n","GenTime":"2024-08-28 22:27:21"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"text":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n\n\n##Decision\nFor a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","MatchedIds":[2968,108,2256,578,4096],"Prediction":"## Decision\n\nChosen option: **\"Hacking\" a continuous integration service**, because\n\n* **Security:** CI services are built for running untrusted code. They usually have resource limits and security measures in place to prevent abuse like Bitcoin mining.\n* **Scalability:** CI services handle parallelization and queuing of jobs automatically. \n* **Cost:** Leveraging an existing CI service might be more cost-effective than managing our own Docker infrastructure or dealing with potential overruns on AWS Lambda.\n* **Maintenance:**  Offloads the operational burden of managing and securing the execution environment.\n\nThis decision acknowledges the security concerns of running untrusted code. While no solution is foolproof, leveraging a purpose-built CI environment offers a balance between functionality, security, and ease of management.  \n\nHowever, this decision requires careful selection of a CI service with robust sandboxing and resource limitation features to mitigate the risks.\n","GenTime":"2024-08-28 22:27:26"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:27:28"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,3820,1145,4616,4963],"Prediction":"Please provide the context and the issue that needs a decision. I need more information to make an informed decision as an expert software architect. For example, tell me:\n\n* **What system or component are we discussing?** \n* **What specific challenges are we facing?**  \n* **Are there performance, scalability, or security requirements to consider?**\n* **What are the possible solutions we've already considered?**\n\nOnce I have this context, I can help you make a decision and document it in an Architectural Decision Record (ADR) format. \n","GenTime":"2024-08-28 22:27:32"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","MatchedIds":[4547,4910,4544,4550,4549],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\nWe will be using **Amazon DynamoDB** as our primary database for this project.\n\nDynamoDB is a fully managed NoSQL database service offered by Amazon Web Services (AWS). It provides fast and predictable performance with seamless scalability, making it a suitable choice for our application's requirements.\n\n**Reasons for choosing DynamoDB:**\n\n* **Scalability and Performance:** DynamoDB offers high scalability and low latency, essential for handling our application's expected growth and traffic. \n* **Serverless Architecture:** As a managed service, DynamoDB eliminates the need for server management, allowing us to focus on development and innovation.\n* **Cost-Effectiveness:** The pay-as-you-go pricing model of DynamoDB allows us to optimize costs based on actual usage.\n* **Integration with AWS Ecosystem:**  DynamoDB seamlessly integrates with other AWS services we are already utilizing, streamlining our development process. \n\n**Alternative Considered:**\n\n* **PostgreSQL:** While PostgreSQL offers robust features and ACID properties, its scaling capabilities and management overhead are not ideal for our current needs. \n\nThis decision aligns with our overall goal of building a scalable, performant, and cost-effective application. We believe that DynamoDB provides the necessary features and flexibility to support our project's growth. \n","GenTime":"2024-08-28 22:27:39"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","MatchedIds":[4179,4359,4868,3572,3853],"Prediction":"## Decision\nIntegrate a Continuous Integration (CI) pipeline into our development workflow. This pipeline will run automated tests on every code commit, using a tool like **SonarQube** or **Codecov** to track coverage metrics and identify gaps. We can configure the CI to fail builds if coverage drops below a defined threshold, ensuring that maintaining high coverage is an integral part of our development process.\n","GenTime":"2024-08-28 22:27:41"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:27:43"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","MatchedIds":[1404,458,2296,5255,1408],"Prediction":"## Decision\n\nWe will check code style using ESLint. \nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\nWe will use ESLint to automatically fix linting errors in a pre-commit hook where possible. \n","GenTime":"2024-08-28 22:27:46"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","MatchedIds":[1893,899,2963,1180,485],"Prediction":"## Decision\nGraphQL modules should define all their dependencies in a dedicated `schema.graphql` file using the `# import` syntax. This allows for clear dependency management and modularity. Tooling can then be used to stitch together a single executable schema from these modules.\n\nFor example:\n\n```graphql\n# schema.graphql\n# import \".\/types\/user.graphql\"\n# import \".\/queries\/getUser.graphql\"\n``` \n\nThis approach promotes:\n\n* **Reusability**: Modules can be easily shared and reused across different parts of the application or even different projects.\n* **Maintainability**: Changes within a module are isolated and have a lower risk of impacting other parts of the application. \n* **Testability**: Modules can be tested independently, simplifying the testing process.\n","GenTime":"2024-08-28 22:27:50"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","MatchedIds":[1246,3853,4179,1258,1031],"Prediction":"## Decision\nIntegrate a database performance monitoring tool, such as New Relic or Datadog, into the application's CI\/CD pipeline. This will allow for automatic identification of slow queries during the testing phase, preventing performance issues from reaching production. \n","GenTime":"2024-08-28 22:27:53"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:27:55"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:27:57"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","MatchedIds":[4547,4910,4918,4550,4549],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\nPlease provide the context for this decision. I need to understand the situation, problem, or requirements to provide you with a relevant and well-informed architectural decision. \n","GenTime":"2024-08-28 22:27:59"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:01"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:04"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:28:06"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:08"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:28:10"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:28:12"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","MatchedIds":[1254,4179,1258,5227,1387],"Prediction":"## Decision\n\nUse the `dotenv` gem to load and manage our environment variables. \n\n**Justification:**\n\n* **Active Maintenance:** `dotenv` is an actively maintained and popular gem that provides a simple and reliable way to load environment variables from a `.env` file. \n* **Centralized Management:** It allows us to manage environment variables centrally, improving code readability and maintainability by avoiding scattered `ENV.fetch` calls.\n* **Fail Fast:** `dotenv` can be configured to raise an error if a required environment variable is missing, supporting our requirement for early failure detection.\n* **Testability:** The gem allows for easy stubbing of environment variables in our test suite by temporarily overriding values, ensuring consistent and reliable test results.\n\n**Implementation Details:**\n\n* Add the `dotenv-rails` gem to the project if using Rails, otherwise, use the `dotenv` gem.\n* Create a `.env` file in the project root to store environment variables.\n* Utilize `Dotenv.load` to load the environment variables at application startup.\n* During testing, leverage `Dotenv.overload` or similar mechanisms provided by the gem to stub environment variables as needed.\n\nThis approach addresses all the concerns raised in the context, providing a robust, maintainable, and testable solution for managing environment variables.\n","GenTime":"2024-08-28 22:28:19"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","MatchedIds":[4167,160,3657,4021,2488],"Prediction":"## Decision\nChosen option: **Versioned SQL migration scripts.** \n\nThis approach involves creating a series of `.sql` scripts, each representing a single schema change. These scripts will be named sequentially and stored in a dedicated directory within the project.\n\nFor example:\n\n* V1__Initial_schema.sql (corresponding to Transcript Editing)\n* V2__Add_annotations_table.sql (corresponding to Transcript Annotations)\n* V3__Add_paper_editing_tables.sql (corresponding to Paper Editing)\n\n### Positive Consequences \n* **Incremental versioning:** Each script represents a single change, making it easy to track the schema evolution and rollback if needed.\n* **Clear history:** The numbered scripts provide a linear and auditable history of database changes. \n* **Platform agnostic:** SQL migration scripts are easily adaptable for both PostgreSQL and SQLite, ensuring consistency across web and desktop versions.\n* **Testable:** Each migration script can be tested individually.\n* **Tooling:** Various tools exist to manage and automate database migrations using SQL scripts (e.g., Flyway, Liquibase).\n\n### Negative consequences\n* **Manual Scripting:** Requires manual creation and maintenance of SQL scripts. \n* **Potential for Errors:** Manual scripting increases the risk of errors, especially with complex schema changes.\n\n### Mitigation of Negative Consequences\n* Utilize a SQL editor with good schema visualization and validation features to minimize scripting errors.\n* Implement thorough testing of migration scripts to catch errors early.\n* Consider adopting a database migration tool for automation and improved reliability as the project grows. \n\nThis approach prioritizes clarity, maintainability, and flexibility, making it suitable for the project's milestone-based development and cross-platform requirements. \n","GenTime":"2024-08-28 22:28:28"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:30"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Prediction":"Please provide the context and influencing factors for me to make an informed architectural decision. For example:\n\n**Context:** \n\"Our web application needs to be highly scalable to handle increasing traffic. We're also facing performance bottlenecks in our monolithic architecture. Our team is skilled in Python and JavaScript. We need a solution that allows for independent deployments and faster development cycles.\" \n\nOnce you provide the context, I can suggest a decision and explain its rationale. \n","GenTime":"2024-08-28 22:28:33"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","MatchedIds":[4804,3820,1145,4616,3546],"Prediction":"Please provide the context and constraints of the decision. I need more information about the specific issue you're facing to give you a relevant architectural decision. \n\nFor example, tell me:\n\n* **What system or component are we making a decision about?** \n* **What are the functional and non-functional requirements?** (e.g., performance, scalability, security, maintainability)\n* **What are the different options we've considered?**\n* **What are the tradeoffs of each option?**\n\nOnce I have this information, I can help you formulate a clear and well-reasoned architectural decision. \n","GenTime":"2024-08-28 22:28:38"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","MatchedIds":[2387,3370,2101,3469,1924],"Prediction":"## Decision\n\nStandardise application error capturing and management by migrating all applications to use Sentry.\n\n## Justification\n\nWhile Rollbar is currently in use for a majority of applications, the following factors contribute to the decision to standardise on Sentry:\n\n* **Consistency:** Using a single error tracking tool across all applications simplifies monitoring, debugging, and analysis.\n* **Developer Familiarity:** Transitioning projects are already leveraging Sentry, indicating existing knowledge and preference within the team.\n* **Rollbar Environment Inconsistencies:** The documented environment naming issues with Rollbar introduce complexity and potential confusion.\n* **Tooling Consolidation:** Reducing the number of tools in use minimizes maintenance overhead and potential points of failure.\n\n## Migration Strategy\n\nA phased approach should be adopted to migrate applications from Rollbar to Sentry:\n\n1. **New Applications:** All new applications will directly integrate with Sentry.\n2. **Transitioning Applications:** Applications currently in transition will be prioritized for migration to Sentry.\n3. **Existing Rollbar Applications:** A schedule will be devised to migrate existing applications from Rollbar to Sentry, taking into account dependencies and potential disruption.\n\n## Additional Considerations\n\n* **Rollbar Sunset:** A sunset date for Rollbar usage should be established to ensure a complete transition within a defined timeframe.\n* **Documentation:**  Update documentation and guidelines to reflect the standardized use of Sentry for error tracking.\n* **Training:**  Provide training or resources to developers unfamiliar with Sentry to facilitate a smooth transition. \n","GenTime":"2024-08-28 22:28:45"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:47"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:28:50"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n\n\n##Decision\n### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","MatchedIds":[810,842,4623,1442,816],"Prediction":"## Decision\n\n### `sdk.Msg` Interface and `Any`-based Encoding\n\nThis ADR proposes to use the existing `sdk.Msg` interface, which all messages already implement, as the basis for encoding.  No new interface needs to be defined, nor do types need to be regenerated using protobuf. Instead, protobuf messages can implement `sdk.Msg` through a simple wrapper.\n\nThe `sdk.Tx` struct will be updated to contain a repeated field of `google.protobuf.Any` messages. Each `Any` message will represent a single `sdk.Msg`. \n\nHere\u2019s an example of how this would look for the `cosmos-sdk\/x\/bank` module:\n\n```proto\n\/\/ cosmos-sdk\/proto\/cosmos\/bank\/v1beta1\/tx.proto\npackage cosmos.bank.v1beta1;\n\nimport \"cosmos\/base\/v1beta1\/coin.proto\";\nimport \"google\/protobuf\/any.proto\";\n\n\/\/ MsgSend represents a message to send coins from one account to another.\nmessage MsgSend {\n    string from_address = 1;\n    string to_address = 2;\n    repeated cosmos.base.v1beta1.Coin amount = 3 [(gogoproto.nullable) = false, (gogoproto.castrepeated) = \"github.com\/cosmos\/cosmos-sdk\/types.Coins\"];\n}\n\n\/\/ MsgSend defines the Any type for sending coins\n\/\/ this is compatible with the `sdk.Msg` interface.\nmessage MsgSendAny {\n  google.protobuf.Any msg = 1;\n}\n\n\/\/ Implement sdk.Msg for the wrapper type\nfunc (msg MsgSendAny) Route() string {\n  return sdk.MsgTypeURL(&MsgSend{})\n}\n\nfunc (msg MsgSendAny) Type() string {\n  return sdk.MsgTypeURL(&MsgSend{})\n}\n\nfunc (msg MsgSendAny) ValidateBasic() error {\n  var innerMsg = &MsgSend{}\n  err := msg.GetMsg(innerMsg)\n  if err != nil {\n      return err\n  }\n  return innerMsg.ValidateBasic()\n}\n\nfunc (msg MsgSendAny) GetSignBytes() []byte {\n  return sdk.MustSortJSON(ModuleCdc.MustMarshalJSON(msg.Msg))\n}\n\nfunc (msg MsgSendAny) GetSigners() []sdk.AccAddress {\n  var innerMsg = &MsgSend{}\n  err := msg.GetMsg(innerMsg)\n  if err != nil {\n      panic(err)\n  }\n  return innerMsg.GetSigners()\n}\n```\n\n#### Advantages of this approach:\n\n* **No Breaking Changes:** Modules don't need to regenerate their types, avoiding any breaking changes. \n* **Clear Migration Path:** Modules can gradually migrate to using protobuf for messages while maintaining compatibility with existing code.\n* **Retains Flexibility:**  The existing `sdk.Msg` interface and its functionality remain in place.\n\n#### Addressing Potential Concerns:\n\n* **Overhead of `Any` Type:** While the `Any` type adds some overhead, it's a reasonable tradeoff for a smooth migration path and to avoid significant code changes.\n* **JSON Signing:**  The `sdk.Msg` interface's `GetSignBytes` method will ensure that messages are JSON-encoded before signing, preserving the existing security model.\n\n### Updating Client Libraries\n\nClient libraries will need to be updated to support constructing transactions with `Any`-wrapped messages. This is a relatively straightforward change as client libraries already use the `sdk.Msg` interface.\n\n### Conclusion\n\nMigrating the client-side of the Cosmos SDK to Protocol Buffers is a multi-step process. This ADR proposes a pragmatic approach that leverages the existing `sdk.Msg` interface and `google.protobuf.Any` to achieve this goal without introducing breaking changes. This paves the way for a smoother transition to protobuf encoding for transactions in the Cosmos SDK ecosystem. \n","GenTime":"2024-08-28 22:29:10"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:12"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:14"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:16"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:18"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:20"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:22"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:24"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:29:27"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:29:29"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","MatchedIds":[351,187,4397,5102,856],"Prediction":"## Decision\n\nThe existing pipeline will be refactored into two separate pipelines: a **build pipeline** and a **deploy pipeline**. \n\n**Build Pipeline:**\n\n1.  This pipeline will be responsible for building the application and creating the Docker image. \n2.  In addition to pushing the Docker image to a repository (e.g., ECR), it will also publish build artifacts, such as:\n    *   **build.json:** Contains metadata about the build (version, timestamp, commit hash, etc.)\n    *   **src.zip:**  A zipped archive of the source code (if needed for deployment-time testing in the deploy pipeline).\n\n**Deploy Pipeline:**\n\n1.  This pipeline will be triggered by either:\n    *   **New artifacts from the build pipeline:** Indicating a new version of the application is available. \n    *   **Changes to a configuration file:** For example, `app.yaml` or similar, which contains environment-specific configurations.\n2.  If triggered by new build artifacts, the pipeline will use these artifacts to deploy the new application version.\n3.  If triggered by configuration changes, the pipeline will redeploy the existing application version but with the updated configuration. \n\n**Benefits:**\n\n*   **Faster deployments for configuration changes:** No need to rebuild the entire application, leading to significant time savings.\n*   **Clear separation of concerns:** Build and deployment are distinct processes, making the pipeline easier to manage and troubleshoot. \n\n**Considerations:**\n\n*   **Testing:** The deploy pipeline should include appropriate tests (using the published artifacts) to ensure the application functions correctly with the new configuration.\n*   **Artifact Management:** Decide on a suitable artifact repository (e.g., S3, Artifactory) and retention policies for the build artifacts. \n","GenTime":"2024-08-28 22:29:38"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:40"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:42"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:29:44"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:29:47"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","MatchedIds":[1996,2703,3784,4699,4428],"Prediction":"## Decision\n\n### File Naming\n\n* **Use PascalCase for all components, services, directives, interfaces, types, enums, and classes.**  This promotes consistency with Angular's style guide and clearly identifies these crucial building blocks. \n* **Use camelCase for all other files, including utility functions, constants, and configuration files.**\n\n### Directory Naming\n\n* **Use kebab-case exclusively for NX libraries and applications.** This aligns with the recommended naming conventions for npm packages and facilitates code sharing. For instance: `@my-org\/shared-ui`, `feature-login`.\n* **Use PascalCase for directories containing only React components.** This maintains consistency within the component structure. \n* **Use camelCase for all other directories, including feature modules, shared modules, and utility folders.**  This offers a clear distinction from components and libraries.\n\n### Examples\n\n```\nlibs\/\n  |__ shared-ui\/  \/\/ NX library\n      |__ src\/\n          |__ lib\/\n              |__ components\/\n                  |__ Button\/\n                      |__ Button.tsx\n                      |__ Button.types.ts\n              |__ services\/\n                  |__ authService.ts \n                  |__ userService.ts\n\napps\/\n  |__ my-app\/     \/\/ NX application\n\n```\n\n\n### Positive Consequences\n\n* **Enhanced Code Maintainability:** A unified naming convention makes the codebase easier to navigate and comprehend, ultimately reducing cognitive load and improving developer productivity. \n* **Reduced Onboarding Friction:**  New team members can quickly adapt to the project's structure, leading to a faster ramp-up time.\n* **Minimized Naming Conflicts:** Consistent naming significantly diminishes the likelihood of encountering naming collisions within the monorepo.\n\n### Negative Consequences \n\n* **Refactoring Effort:** Initially, implementing this decision will require a concerted refactoring effort to align existing code with the new conventions.\n\n### Conclusion\n\nWhile the initial refactoring may require some investment, the long-term benefits of a clear and consistent naming strategy far outweigh the costs. A well-structured monorepo enhances code clarity, improves team collaboration, and reduces the potential for errors. \n","GenTime":"2024-08-28 22:29:56"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","MatchedIds":[3217,2397,3421,5024,2470],"Prediction":"## Decision\n* **Use Okta as the Identity Provider (IdP) for Bookit.** This leverages Okta's robust feature set and simplifies our authentication and authorization process.\n* **Utilize Okta's React client for client-side code.** This eliminates the need for custom client-side code for Microsoft Azure AD integration.\n* **For the backend, adopt Okta's Spring Boot support.** Choose the integration that aligns with the chosen Spring Boot version for seamless integration. \n* **Utilize Okta Access Tokens as Bearer tokens for authentication and authorization within the application.**\n* **Defer the decision of storing user data locally.** Evaluate the need for a local user table based on future requirements and revisit if necessary. \n","GenTime":"2024-08-28 22:30:01"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","MatchedIds":[4547,4910,4918,4544,4549],"Prediction":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\nWe will implement a message queue system to handle asynchronous communication between our microservices. This decision is driven by the need for increased scalability, reliability, and decoupling in our architecture. \n\nAfter evaluating several options, we have chosen to use **[Message Queue System Name]**. Key factors in this decision include:\n\n* **[Reason 1]:**  e.g., Strong support for our chosen programming language\/framework.\n* **[Reason 2]:**  e.g., Proven track record of scalability and performance in similar use cases.\n* **[Reason 3]:**  e.g., Availability of features that align with our specific requirements (e.g., message persistence, delivery guarantees, etc.). \n \nThis approach will allow services to send messages without waiting for an immediate response, improving responsiveness and fault tolerance. \n","GenTime":"2024-08-28 22:30:06"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:08"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:10"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","MatchedIds":[4766,2703,3784,4699,4428],"Prediction":"## Decision\n\n**File Naming:**\n\n* **Default Export:** Name files after their default export. \n    * **PascalCase:** If the default export is a React Component or a class, use PascalCase (e.g., `UserProfileCard.tsx`, `ShoppingCart.js`).\n    * **camelCase:** For all other default exports, use camelCase (e.g., `useFetchData.ts`, `validationUtils.js`). \n* **Avoid:** Refrain from using kebab-case and snake_case for file names.\n\n**Directory Naming:**\n\n* **NX Apps\/Libraries:** Use kebab-case only for NX apps, libraries, and their containing folders (e.g., `feature-login`, `ui-components`).\n* **React Component Directories:** Use PascalCase for directories containing solely React components (e.g., `components\/UserProfileCard\/index.tsx`). \n* **All Other Cases:** Use camelCase for all other directories (e.g., `utils`, `services`, `hooks`).\n\n**Example:**\n\n```\n# NX Library:\nlibs\/\n  shared\/\n    ui-components\/\n      src\/\n        lib\/\n          Button\/\n            Button.tsx\n            Button.stories.tsx\n          index.ts\n\n# Feature:\napps\/\n  my-app\/\n    src\/\n      app\/\n        profile\/\n          components\/\n            UserProfileCard\/\n              UserProfileCard.tsx\n              UserProfileCard.test.tsx\n            index.ts \n          useProfileData.ts\n          profile.module.ts\n```\n\n### Positive Consequences\n\n* **Improved Navigation:**  A consistent naming strategy makes it significantly easier to navigate the monorepo and locate specific files.\n* **Enhanced Readability:**  File and directory names become self-documenting, improving code readability and understanding.\n* **Reduced Cognitive Load:** Developers can quickly infer the purpose and content of files and directories, minimizing cognitive load.\n* **Industry Alignment:** This approach aligns with widely adopted conventions in JavaScript and TypeScript development, fostering familiarity for developers. \n","GenTime":"2024-08-28 22:30:20"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:23"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","MatchedIds":[3388,3370,2101,3469,1924],"Prediction":"## Decision\n\nAdopt a phased approach to unify application error capturing and management under **Sentry**.\n\n**Justification:**\n\n* **Existing Investment:** While Rollbar is currently used by a majority of applications, introducing Sentry in some projects signifies a potential preference towards it. \n* **Future-Proofing:**  Adopting Sentry across all projects reduces reliance on a single vendor and allows for easier integration with future applications, especially if they are already leveraging Sentry.\n* **Simplified Environments:** Addressing the environment naming inconsistencies within Rollbar will require effort.  Migrating to a unified solution in Sentry presents an opportunity to establish a cleaner and consistent environment mapping from the outset.\n\n**Implementation:**\n\n1. **Assessment:** Conduct a comparative analysis of Rollbar and Sentry, focusing on features, ease of use, cost implications, and future scalability.\n2. **Pilot Migration:** Migrate a small number of applications (including one currently using Rollbar) to Sentry.  This pilot phase allows for hands-on evaluation and identification of any migration challenges.\n3. **Gradual Rollout:** Based on the pilot's success, gradually migrate the remaining applications from Rollbar to Sentry. Prioritize applications where Sentry is already in use or where a transition would be less disruptive.\n4. **Rollbar Sunsetting:**  Establish a clear timeline for sunsetting Rollbar, ensuring all applications are migrated within the stipulated period.\n\nThis phased approach allows for a controlled transition, minimizing disruption while consolidating application error management under a single, robust platform. \n","GenTime":"2024-08-28 22:30:30"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:30:33"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n","GenTime":"2024-08-28 22:30:35"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:37"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:39"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:41"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","MatchedIds":[4699,4428,4408,3119,1436],"Prediction":"## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). \n","GenTime":"2024-08-28 22:30:43"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","MatchedIds":[4454,4442,1180,915,920],"Prediction":"## Decision\nWe will develop a lightweight, purpose-built Verify stub to facilitate user research and testing of the prototype Service Provider. \n\n**Key features of the Verify stub:**\n\n* **Mimic essential Verify interactions:** The stub will simulate the core request and response flow of Verify, focusing on the interactions between the Service Provider and the Hub. This includes handling authorization requests, simulating user authentication journeys, and generating realistic SAML responses. \n* **Configurable response scenarios:**  Researchers will have the ability to easily configure the stub to produce different response scenarios, such as successful authentication, account creation prompts, various error states, and more. This flexibility allows for comprehensive testing of diverse user journeys and edge cases. \n* **Simplified deployment and management:** The stub will be designed for straightforward deployment and configuration, minimizing overhead for researchers. \n\n**Reasons for choosing a dedicated stub over alternatives:**\n\n* **Focused scope and control:** A dedicated stub allows for precise control over the simulated Verify environment, ensuring that user research focuses specifically on the Service Provider's integration and user experience, without dependencies on external systems.\n* **Flexibility and iteration speed:**  The stub's configurable nature allows for rapid iteration and testing of different user flows and response scenarios, supporting agile research methodologies.\n* **Data privacy:** As the stub is self-contained, it eliminates concerns about handling real user data during research and testing.\n\n**Implementation details:**\n\n* The stub can be implemented as a lightweight web application, utilizing technologies like Node.js or Python for rapid development and deployment.\n* The API for configuring scenarios and triggering responses can be designed using a user-friendly interface, potentially web-based, to facilitate easy use by researchers. \n\nThis approach enables efficient and focused user research, allowing the team to gather valuable feedback and iterate on the prototype Service Provider's design and integration with Verify. \n","GenTime":"2024-08-28 22:30:52"}
